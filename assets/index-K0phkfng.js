var tp=e=>{throw TypeError(e)};var fl=(e,t,r)=>t.has(e)||tp("Cannot "+r);var C=(e,t,r)=>(fl(e,t,"read from private field"),r?r.call(e):t.get(e)),ee=(e,t,r)=>t.has(e)?tp("Cannot add the same private member more than once"):t instanceof WeakSet?t.add(e):t.set(e,r),j=(e,t,r,o)=>(fl(e,t,"write to private field"),o?o.call(e,r):t.set(e,r),r),Re=(e,t,r)=>(fl(e,t,"access private method"),r);var Ys=(e,t,r,o)=>({set _(n){j(e,t,n,r)},get _(){return C(e,t,o)}});function pb(e,t){for(var r=0;r<t.length;r++){const o=t[r];if(typeof o!="string"&&!Array.isArray(o)){for(const n in o)if(n!=="default"&&!(n in e)){const s=Object.getOwnPropertyDescriptor(o,n);s&&Object.defineProperty(e,n,s.get?s:{enumerable:!0,get:()=>o[n]})}}}return Object.freeze(Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}))}(function(){const t=document.createElement("link").relList;if(t&&t.supports&&t.supports("modulepreload"))return;for(const n of document.querySelectorAll('link[rel="modulepreload"]'))o(n);new MutationObserver(n=>{for(const s of n)if(s.type==="childList")for(const i of s.addedNodes)i.tagName==="LINK"&&i.rel==="modulepreload"&&o(i)}).observe(document,{childList:!0,subtree:!0});function r(n){const s={};return n.integrity&&(s.integrity=n.integrity),n.referrerPolicy&&(s.referrerPolicy=n.referrerPolicy),n.crossOrigin==="use-credentials"?s.credentials="include":n.crossOrigin==="anonymous"?s.credentials="omit":s.credentials="same-origin",s}function o(n){if(n.ep)return;n.ep=!0;const s=r(n);fetch(n.href,s)}})();function zm(e){return e&&e.__esModule&&Object.prototype.hasOwnProperty.call(e,"default")?e.default:e}var Rm={exports:{}},Sa={},Hm={exports:{}},Y={};/**
 * @license React
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Ls=Symbol.for("react.element"),gb=Symbol.for("react.portal"),mb=Symbol.for("react.fragment"),hb=Symbol.for("react.strict_mode"),fb=Symbol.for("react.profiler"),vb=Symbol.for("react.provider"),yb=Symbol.for("react.context"),wb=Symbol.for("react.forward_ref"),bb=Symbol.for("react.suspense"),kb=Symbol.for("react.memo"),Ab=Symbol.for("react.lazy"),rp=Symbol.iterator;function Sb(e){return e===null||typeof e!="object"?null:(e=rp&&e[rp]||e["@@iterator"],typeof e=="function"?e:null)}var Mm={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},Em=Object.assign,Nm={};function Dn(e,t,r){this.props=e,this.context=t,this.refs=Nm,this.updater=r||Mm}Dn.prototype.isReactComponent={};Dn.prototype.setState=function(e,t){if(typeof e!="object"&&typeof e!="function"&&e!=null)throw Error("setState(...): takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,e,t,"setState")};Dn.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")};function qm(){}qm.prototype=Dn.prototype;function yd(e,t,r){this.props=e,this.context=t,this.refs=Nm,this.updater=r||Mm}var wd=yd.prototype=new qm;wd.constructor=yd;Em(wd,Dn.prototype);wd.isPureReactComponent=!0;var op=Array.isArray,Lm=Object.prototype.hasOwnProperty,bd={current:null},Om={key:!0,ref:!0,__self:!0,__source:!0};function _m(e,t,r){var o,n={},s=null,i=null;if(t!=null)for(o in t.ref!==void 0&&(i=t.ref),t.key!==void 0&&(s=""+t.key),t)Lm.call(t,o)&&!Om.hasOwnProperty(o)&&(n[o]=t[o]);var a=arguments.length-2;if(a===1)n.children=r;else if(1<a){for(var d=Array(a),c=0;c<a;c++)d[c]=arguments[c+2];n.children=d}if(e&&e.defaultProps)for(o in a=e.defaultProps,a)n[o]===void 0&&(n[o]=a[o]);return{$$typeof:Ls,type:e,key:s,ref:i,props:n,_owner:bd.current}}function Ib(e,t){return{$$typeof:Ls,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}function kd(e){return typeof e=="object"&&e!==null&&e.$$typeof===Ls}function Pb(e){var t={"=":"=0",":":"=2"};return"$"+e.replace(/[=:]/g,function(r){return t[r]})}var np=/\/+/g;function vl(e,t){return typeof e=="object"&&e!==null&&e.key!=null?Pb(""+e.key):t.toString(36)}function Si(e,t,r,o,n){var s=typeof e;(s==="undefined"||s==="boolean")&&(e=null);var i=!1;if(e===null)i=!0;else switch(s){case"string":case"number":i=!0;break;case"object":switch(e.$$typeof){case Ls:case gb:i=!0}}if(i)return i=e,n=n(i),e=o===""?"."+vl(i,0):o,op(n)?(r="",e!=null&&(r=e.replace(np,"$&/")+"/"),Si(n,t,r,"",function(c){return c})):n!=null&&(kd(n)&&(n=Ib(n,r+(!n.key||i&&i.key===n.key?"":(""+n.key).replace(np,"$&/")+"/")+e)),t.push(n)),1;if(i=0,o=o===""?".":o+":",op(e))for(var a=0;a<e.length;a++){s=e[a];var d=o+vl(s,a);i+=Si(s,t,r,d,n)}else if(d=Sb(e),typeof d=="function")for(e=d.call(e),a=0;!(s=e.next()).done;)s=s.value,d=o+vl(s,a++),i+=Si(s,t,r,d,n);else if(s==="object")throw t=String(e),Error("Objects are not valid as a React child (found: "+(t==="[object Object]"?"object with keys {"+Object.keys(e).join(", ")+"}":t)+"). If you meant to render a collection of children, use an array instead.");return i}function Js(e,t,r){if(e==null)return e;var o=[],n=0;return Si(e,o,"","",function(s){return t.call(r,s,n++)}),o}function Cb(e){if(e._status===-1){var t=e._result;t=t(),t.then(function(r){(e._status===0||e._status===-1)&&(e._status=1,e._result=r)},function(r){(e._status===0||e._status===-1)&&(e._status=2,e._result=r)}),e._status===-1&&(e._status=0,e._result=t)}if(e._status===1)return e._result.default;throw e._result}var We={current:null},Ii={transition:null},xb={ReactCurrentDispatcher:We,ReactCurrentBatchConfig:Ii,ReactCurrentOwner:bd};function Vm(){throw Error("act(...) is not supported in production builds of React.")}Y.Children={map:Js,forEach:function(e,t,r){Js(e,function(){t.apply(this,arguments)},r)},count:function(e){var t=0;return Js(e,function(){t++}),t},toArray:function(e){return Js(e,function(t){return t})||[]},only:function(e){if(!kd(e))throw Error("React.Children.only expected to receive a single React element child.");return e}};Y.Component=Dn;Y.Fragment=mb;Y.Profiler=fb;Y.PureComponent=yd;Y.StrictMode=hb;Y.Suspense=bb;Y.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=xb;Y.act=Vm;Y.cloneElement=function(e,t,r){if(e==null)throw Error("React.cloneElement(...): The argument must be a React element, but you passed "+e+".");var o=Em({},e.props),n=e.key,s=e.ref,i=e._owner;if(t!=null){if(t.ref!==void 0&&(s=t.ref,i=bd.current),t.key!==void 0&&(n=""+t.key),e.type&&e.type.defaultProps)var a=e.type.defaultProps;for(d in t)Lm.call(t,d)&&!Om.hasOwnProperty(d)&&(o[d]=t[d]===void 0&&a!==void 0?a[d]:t[d])}var d=arguments.length-2;if(d===1)o.children=r;else if(1<d){a=Array(d);for(var c=0;c<d;c++)a[c]=arguments[c+2];o.children=a}return{$$typeof:Ls,type:e.type,key:n,ref:s,props:o,_owner:i}};Y.createContext=function(e){return e={$$typeof:yb,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null},e.Provider={$$typeof:vb,_context:e},e.Consumer=e};Y.createElement=_m;Y.createFactory=function(e){var t=_m.bind(null,e);return t.type=e,t};Y.createRef=function(){return{current:null}};Y.forwardRef=function(e){return{$$typeof:wb,render:e}};Y.isValidElement=kd;Y.lazy=function(e){return{$$typeof:Ab,_payload:{_status:-1,_result:e},_init:Cb}};Y.memo=function(e,t){return{$$typeof:kb,type:e,compare:t===void 0?null:t}};Y.startTransition=function(e){var t=Ii.transition;Ii.transition={};try{e()}finally{Ii.transition=t}};Y.unstable_act=Vm;Y.useCallback=function(e,t){return We.current.useCallback(e,t)};Y.useContext=function(e){return We.current.useContext(e)};Y.useDebugValue=function(){};Y.useDeferredValue=function(e){return We.current.useDeferredValue(e)};Y.useEffect=function(e,t){return We.current.useEffect(e,t)};Y.useId=function(){return We.current.useId()};Y.useImperativeHandle=function(e,t,r){return We.current.useImperativeHandle(e,t,r)};Y.useInsertionEffect=function(e,t){return We.current.useInsertionEffect(e,t)};Y.useLayoutEffect=function(e,t){return We.current.useLayoutEffect(e,t)};Y.useMemo=function(e,t){return We.current.useMemo(e,t)};Y.useReducer=function(e,t,r){return We.current.useReducer(e,t,r)};Y.useRef=function(e){return We.current.useRef(e)};Y.useState=function(e){return We.current.useState(e)};Y.useSyncExternalStore=function(e,t,r){return We.current.useSyncExternalStore(e,t,r)};Y.useTransition=function(){return We.current.useTransition()};Y.version="18.3.1";Hm.exports=Y;var m=Hm.exports;const z=zm(m),Ad=pb({__proto__:null,default:z},[m]);/**
 * @license React
 * react-jsx-runtime.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Db=m,Tb=Symbol.for("react.element"),zb=Symbol.for("react.fragment"),Rb=Object.prototype.hasOwnProperty,Hb=Db.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,Mb={key:!0,ref:!0,__self:!0,__source:!0};function Bm(e,t,r){var o,n={},s=null,i=null;r!==void 0&&(s=""+r),t.key!==void 0&&(s=""+t.key),t.ref!==void 0&&(i=t.ref);for(o in t)Rb.call(t,o)&&!Mb.hasOwnProperty(o)&&(n[o]=t[o]);if(e&&e.defaultProps)for(o in t=e.defaultProps,t)n[o]===void 0&&(n[o]=t[o]);return{$$typeof:Tb,type:e,key:s,ref:i,props:n,_owner:Hb.current}}Sa.Fragment=zb;Sa.jsx=Bm;Sa.jsxs=Bm;Rm.exports=Sa;var l=Rm.exports,Wm={exports:{}},rt={},jm={exports:{}},Um={};/**
 * @license React
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */(function(e){function t(x,T){var N=x.length;x.push(T);e:for(;0<N;){var G=N-1>>>1,_=x[G];if(0<n(_,T))x[G]=T,x[N]=_,N=G;else break e}}function r(x){return x.length===0?null:x[0]}function o(x){if(x.length===0)return null;var T=x[0],N=x.pop();if(N!==T){x[0]=N;e:for(var G=0,_=x.length,Q=_>>>1;G<Q;){var Z=2*(G+1)-1,we=x[Z],ze=Z+1,te=x[ze];if(0>n(we,N))ze<_&&0>n(te,we)?(x[G]=te,x[ze]=N,G=ze):(x[G]=we,x[Z]=N,G=Z);else if(ze<_&&0>n(te,N))x[G]=te,x[ze]=N,G=ze;else break e}}return T}function n(x,T){var N=x.sortIndex-T.sortIndex;return N!==0?N:x.id-T.id}if(typeof performance=="object"&&typeof performance.now=="function"){var s=performance;e.unstable_now=function(){return s.now()}}else{var i=Date,a=i.now();e.unstable_now=function(){return i.now()-a}}var d=[],c=[],p=1,u=null,f=3,g=!1,k=!1,y=!1,b=typeof setTimeout=="function"?setTimeout:null,v=typeof clearTimeout=="function"?clearTimeout:null,h=typeof setImmediate<"u"?setImmediate:null;typeof navigator<"u"&&navigator.scheduling!==void 0&&navigator.scheduling.isInputPending!==void 0&&navigator.scheduling.isInputPending.bind(navigator.scheduling);function w(x){for(var T=r(c);T!==null;){if(T.callback===null)o(c);else if(T.startTime<=x)o(c),T.sortIndex=T.expirationTime,t(d,T);else break;T=r(c)}}function A(x){if(y=!1,w(x),!k)if(r(d)!==null)k=!0,B(S);else{var T=r(c);T!==null&&U(A,T.startTime-x)}}function S(x,T){k=!1,y&&(y=!1,v(D),D=-1),g=!0;var N=f;try{for(w(T),u=r(d);u!==null&&(!(u.expirationTime>T)||x&&!V());){var G=u.callback;if(typeof G=="function"){u.callback=null,f=u.priorityLevel;var _=G(u.expirationTime<=T);T=e.unstable_now(),typeof _=="function"?u.callback=_:u===r(d)&&o(d),w(T)}else o(d);u=r(d)}if(u!==null)var Q=!0;else{var Z=r(c);Z!==null&&U(A,Z.startTime-T),Q=!1}return Q}finally{u=null,f=N,g=!1}}var I=!1,P=null,D=-1,M=5,H=-1;function V(){return!(e.unstable_now()-H<M)}function L(){if(P!==null){var x=e.unstable_now();H=x;var T=!0;try{T=P(!0,x)}finally{T?$():(I=!1,P=null)}}else I=!1}var $;if(typeof h=="function")$=function(){h(L)};else if(typeof MessageChannel<"u"){var E=new MessageChannel,J=E.port2;E.port1.onmessage=L,$=function(){J.postMessage(null)}}else $=function(){b(L,0)};function B(x){P=x,I||(I=!0,$())}function U(x,T){D=b(function(){x(e.unstable_now())},T)}e.unstable_IdlePriority=5,e.unstable_ImmediatePriority=1,e.unstable_LowPriority=4,e.unstable_NormalPriority=3,e.unstable_Profiling=null,e.unstable_UserBlockingPriority=2,e.unstable_cancelCallback=function(x){x.callback=null},e.unstable_continueExecution=function(){k||g||(k=!0,B(S))},e.unstable_forceFrameRate=function(x){0>x||125<x?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):M=0<x?Math.floor(1e3/x):5},e.unstable_getCurrentPriorityLevel=function(){return f},e.unstable_getFirstCallbackNode=function(){return r(d)},e.unstable_next=function(x){switch(f){case 1:case 2:case 3:var T=3;break;default:T=f}var N=f;f=T;try{return x()}finally{f=N}},e.unstable_pauseExecution=function(){},e.unstable_requestPaint=function(){},e.unstable_runWithPriority=function(x,T){switch(x){case 1:case 2:case 3:case 4:case 5:break;default:x=3}var N=f;f=x;try{return T()}finally{f=N}},e.unstable_scheduleCallback=function(x,T,N){var G=e.unstable_now();switch(typeof N=="object"&&N!==null?(N=N.delay,N=typeof N=="number"&&0<N?G+N:G):N=G,x){case 1:var _=-1;break;case 2:_=250;break;case 5:_=1073741823;break;case 4:_=1e4;break;default:_=5e3}return _=N+_,x={id:p++,callback:T,priorityLevel:x,startTime:N,expirationTime:_,sortIndex:-1},N>G?(x.sortIndex=N,t(c,x),r(d)===null&&x===r(c)&&(y?(v(D),D=-1):y=!0,U(A,N-G))):(x.sortIndex=_,t(d,x),k||g||(k=!0,B(S))),x},e.unstable_shouldYield=V,e.unstable_wrapCallback=function(x){var T=f;return function(){var N=f;f=T;try{return x.apply(this,arguments)}finally{f=N}}}})(Um);jm.exports=Um;var Eb=jm.exports;/**
 * @license React
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Nb=m,tt=Eb;function R(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,r=1;r<arguments.length;r++)t+="&args[]="+encodeURIComponent(arguments[r]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var Gm=new Set,cs={};function Po(e,t){yn(e,t),yn(e+"Capture",t)}function yn(e,t){for(cs[e]=t,e=0;e<t.length;e++)Gm.add(t[e])}var ar=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),ac=Object.prototype.hasOwnProperty,qb=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,sp={},ip={};function Lb(e){return ac.call(ip,e)?!0:ac.call(sp,e)?!1:qb.test(e)?ip[e]=!0:(sp[e]=!0,!1)}function Ob(e,t,r,o){if(r!==null&&r.type===0)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return o?!1:r!==null?!r.acceptsBooleans:(e=e.toLowerCase().slice(0,5),e!=="data-"&&e!=="aria-");default:return!1}}function _b(e,t,r,o){if(t===null||typeof t>"u"||Ob(e,t,r,o))return!0;if(o)return!1;if(r!==null)switch(r.type){case 3:return!t;case 4:return t===!1;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}function je(e,t,r,o,n,s,i){this.acceptsBooleans=t===2||t===3||t===4,this.attributeName=o,this.attributeNamespace=n,this.mustUseProperty=r,this.propertyName=e,this.type=t,this.sanitizeURL=s,this.removeEmptyString=i}var De={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(e){De[e]=new je(e,0,!1,e,null,!1,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(e){var t=e[0];De[t]=new je(t,1,!1,e[1],null,!1,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(e){De[e]=new je(e,2,!1,e.toLowerCase(),null,!1,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(e){De[e]=new je(e,2,!1,e,null,!1,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(e){De[e]=new je(e,3,!1,e.toLowerCase(),null,!1,!1)});["checked","multiple","muted","selected"].forEach(function(e){De[e]=new je(e,3,!0,e,null,!1,!1)});["capture","download"].forEach(function(e){De[e]=new je(e,4,!1,e,null,!1,!1)});["cols","rows","size","span"].forEach(function(e){De[e]=new je(e,6,!1,e,null,!1,!1)});["rowSpan","start"].forEach(function(e){De[e]=new je(e,5,!1,e.toLowerCase(),null,!1,!1)});var Sd=/[\-:]([a-z])/g;function Id(e){return e[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(e){var t=e.replace(Sd,Id);De[t]=new je(t,1,!1,e,null,!1,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(e){var t=e.replace(Sd,Id);De[t]=new je(t,1,!1,e,"http://www.w3.org/1999/xlink",!1,!1)});["xml:base","xml:lang","xml:space"].forEach(function(e){var t=e.replace(Sd,Id);De[t]=new je(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1,!1)});["tabIndex","crossOrigin"].forEach(function(e){De[e]=new je(e,1,!1,e.toLowerCase(),null,!1,!1)});De.xlinkHref=new je("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0,!1);["src","href","action","formAction"].forEach(function(e){De[e]=new je(e,1,!1,e.toLowerCase(),null,!0,!0)});function Pd(e,t,r,o){var n=De.hasOwnProperty(t)?De[t]:null;(n!==null?n.type!==0:o||!(2<t.length)||t[0]!=="o"&&t[0]!=="O"||t[1]!=="n"&&t[1]!=="N")&&(_b(t,r,n,o)&&(r=null),o||n===null?Lb(t)&&(r===null?e.removeAttribute(t):e.setAttribute(t,""+r)):n.mustUseProperty?e[n.propertyName]=r===null?n.type===3?!1:"":r:(t=n.attributeName,o=n.attributeNamespace,r===null?e.removeAttribute(t):(n=n.type,r=n===3||n===4&&r===!0?"":""+r,o?e.setAttributeNS(o,t,r):e.setAttribute(t,r))))}var gr=Nb.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,Zs=Symbol.for("react.element"),Bo=Symbol.for("react.portal"),Wo=Symbol.for("react.fragment"),Cd=Symbol.for("react.strict_mode"),lc=Symbol.for("react.profiler"),Fm=Symbol.for("react.provider"),Km=Symbol.for("react.context"),xd=Symbol.for("react.forward_ref"),cc=Symbol.for("react.suspense"),dc=Symbol.for("react.suspense_list"),Dd=Symbol.for("react.memo"),Cr=Symbol.for("react.lazy"),$m=Symbol.for("react.offscreen"),ap=Symbol.iterator;function _n(e){return e===null||typeof e!="object"?null:(e=ap&&e[ap]||e["@@iterator"],typeof e=="function"?e:null)}var he=Object.assign,yl;function Qn(e){if(yl===void 0)try{throw Error()}catch(r){var t=r.stack.trim().match(/\n( *(at )?)/);yl=t&&t[1]||""}return`
`+yl+e}var wl=!1;function bl(e,t){if(!e||wl)return"";wl=!0;var r=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(t)if(t=function(){throw Error()},Object.defineProperty(t.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(t,[])}catch(c){var o=c}Reflect.construct(e,[],t)}else{try{t.call()}catch(c){o=c}e.call(t.prototype)}else{try{throw Error()}catch(c){o=c}e()}}catch(c){if(c&&o&&typeof c.stack=="string"){for(var n=c.stack.split(`
`),s=o.stack.split(`
`),i=n.length-1,a=s.length-1;1<=i&&0<=a&&n[i]!==s[a];)a--;for(;1<=i&&0<=a;i--,a--)if(n[i]!==s[a]){if(i!==1||a!==1)do if(i--,a--,0>a||n[i]!==s[a]){var d=`
`+n[i].replace(" at new "," at ");return e.displayName&&d.includes("<anonymous>")&&(d=d.replace("<anonymous>",e.displayName)),d}while(1<=i&&0<=a);break}}}finally{wl=!1,Error.prepareStackTrace=r}return(e=e?e.displayName||e.name:"")?Qn(e):""}function Vb(e){switch(e.tag){case 5:return Qn(e.type);case 16:return Qn("Lazy");case 13:return Qn("Suspense");case 19:return Qn("SuspenseList");case 0:case 2:case 15:return e=bl(e.type,!1),e;case 11:return e=bl(e.type.render,!1),e;case 1:return e=bl(e.type,!0),e;default:return""}}function uc(e){if(e==null)return null;if(typeof e=="function")return e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case Wo:return"Fragment";case Bo:return"Portal";case lc:return"Profiler";case Cd:return"StrictMode";case cc:return"Suspense";case dc:return"SuspenseList"}if(typeof e=="object")switch(e.$$typeof){case Km:return(e.displayName||"Context")+".Consumer";case Fm:return(e._context.displayName||"Context")+".Provider";case xd:var t=e.render;return e=e.displayName,e||(e=t.displayName||t.name||"",e=e!==""?"ForwardRef("+e+")":"ForwardRef"),e;case Dd:return t=e.displayName||null,t!==null?t:uc(e.type)||"Memo";case Cr:t=e._payload,e=e._init;try{return uc(e(t))}catch{}}return null}function Bb(e){var t=e.type;switch(e.tag){case 24:return"Cache";case 9:return(t.displayName||"Context")+".Consumer";case 10:return(t._context.displayName||"Context")+".Provider";case 18:return"DehydratedFragment";case 11:return e=t.render,e=e.displayName||e.name||"",t.displayName||(e!==""?"ForwardRef("+e+")":"ForwardRef");case 7:return"Fragment";case 5:return t;case 4:return"Portal";case 3:return"Root";case 6:return"Text";case 16:return uc(t);case 8:return t===Cd?"StrictMode":"Mode";case 22:return"Offscreen";case 12:return"Profiler";case 21:return"Scope";case 13:return"Suspense";case 19:return"SuspenseList";case 25:return"TracingMarker";case 1:case 0:case 17:case 2:case 14:case 15:if(typeof t=="function")return t.displayName||t.name||null;if(typeof t=="string")return t}return null}function Kr(e){switch(typeof e){case"boolean":case"number":case"string":case"undefined":return e;case"object":return e;default:return""}}function Qm(e){var t=e.type;return(e=e.nodeName)&&e.toLowerCase()==="input"&&(t==="checkbox"||t==="radio")}function Wb(e){var t=Qm(e)?"checked":"value",r=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),o=""+e[t];if(!e.hasOwnProperty(t)&&typeof r<"u"&&typeof r.get=="function"&&typeof r.set=="function"){var n=r.get,s=r.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return n.call(this)},set:function(i){o=""+i,s.call(this,i)}}),Object.defineProperty(e,t,{enumerable:r.enumerable}),{getValue:function(){return o},setValue:function(i){o=""+i},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}function Xs(e){e._valueTracker||(e._valueTracker=Wb(e))}function Ym(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var r=t.getValue(),o="";return e&&(o=Qm(e)?e.checked?"true":"false":e.value),e=o,e!==r?(t.setValue(e),!0):!1}function Vi(e){if(e=e||(typeof document<"u"?document:void 0),typeof e>"u")return null;try{return e.activeElement||e.body}catch{return e.body}}function pc(e,t){var r=t.checked;return he({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:r??e._wrapperState.initialChecked})}function lp(e,t){var r=t.defaultValue==null?"":t.defaultValue,o=t.checked!=null?t.checked:t.defaultChecked;r=Kr(t.value!=null?t.value:r),e._wrapperState={initialChecked:o,initialValue:r,controlled:t.type==="checkbox"||t.type==="radio"?t.checked!=null:t.value!=null}}function Jm(e,t){t=t.checked,t!=null&&Pd(e,"checked",t,!1)}function gc(e,t){Jm(e,t);var r=Kr(t.value),o=t.type;if(r!=null)o==="number"?(r===0&&e.value===""||e.value!=r)&&(e.value=""+r):e.value!==""+r&&(e.value=""+r);else if(o==="submit"||o==="reset"){e.removeAttribute("value");return}t.hasOwnProperty("value")?mc(e,t.type,r):t.hasOwnProperty("defaultValue")&&mc(e,t.type,Kr(t.defaultValue)),t.checked==null&&t.defaultChecked!=null&&(e.defaultChecked=!!t.defaultChecked)}function cp(e,t,r){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var o=t.type;if(!(o!=="submit"&&o!=="reset"||t.value!==void 0&&t.value!==null))return;t=""+e._wrapperState.initialValue,r||t===e.value||(e.value=t),e.defaultValue=t}r=e.name,r!==""&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,r!==""&&(e.name=r)}function mc(e,t,r){(t!=="number"||Vi(e.ownerDocument)!==e)&&(r==null?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+r&&(e.defaultValue=""+r))}var Yn=Array.isArray;function Xo(e,t,r,o){if(e=e.options,t){t={};for(var n=0;n<r.length;n++)t["$"+r[n]]=!0;for(r=0;r<e.length;r++)n=t.hasOwnProperty("$"+e[r].value),e[r].selected!==n&&(e[r].selected=n),n&&o&&(e[r].defaultSelected=!0)}else{for(r=""+Kr(r),t=null,n=0;n<e.length;n++){if(e[n].value===r){e[n].selected=!0,o&&(e[n].defaultSelected=!0);return}t!==null||e[n].disabled||(t=e[n])}t!==null&&(t.selected=!0)}}function hc(e,t){if(t.dangerouslySetInnerHTML!=null)throw Error(R(91));return he({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function dp(e,t){var r=t.value;if(r==null){if(r=t.children,t=t.defaultValue,r!=null){if(t!=null)throw Error(R(92));if(Yn(r)){if(1<r.length)throw Error(R(93));r=r[0]}t=r}t==null&&(t=""),r=t}e._wrapperState={initialValue:Kr(r)}}function Zm(e,t){var r=Kr(t.value),o=Kr(t.defaultValue);r!=null&&(r=""+r,r!==e.value&&(e.value=r),t.defaultValue==null&&e.defaultValue!==r&&(e.defaultValue=r)),o!=null&&(e.defaultValue=""+o)}function up(e){var t=e.textContent;t===e._wrapperState.initialValue&&t!==""&&t!==null&&(e.value=t)}function Xm(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function fc(e,t){return e==null||e==="http://www.w3.org/1999/xhtml"?Xm(t):e==="http://www.w3.org/2000/svg"&&t==="foreignObject"?"http://www.w3.org/1999/xhtml":e}var ei,eh=function(e){return typeof MSApp<"u"&&MSApp.execUnsafeLocalFunction?function(t,r,o,n){MSApp.execUnsafeLocalFunction(function(){return e(t,r,o,n)})}:e}(function(e,t){if(e.namespaceURI!=="http://www.w3.org/2000/svg"||"innerHTML"in e)e.innerHTML=t;else{for(ei=ei||document.createElement("div"),ei.innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=ei.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}});function ds(e,t){if(t){var r=e.firstChild;if(r&&r===e.lastChild&&r.nodeType===3){r.nodeValue=t;return}}e.textContent=t}var Xn={animationIterationCount:!0,aspectRatio:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},jb=["Webkit","ms","Moz","O"];Object.keys(Xn).forEach(function(e){jb.forEach(function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),Xn[t]=Xn[e]})});function th(e,t,r){return t==null||typeof t=="boolean"||t===""?"":r||typeof t!="number"||t===0||Xn.hasOwnProperty(e)&&Xn[e]?(""+t).trim():t+"px"}function rh(e,t){e=e.style;for(var r in t)if(t.hasOwnProperty(r)){var o=r.indexOf("--")===0,n=th(r,t[r],o);r==="float"&&(r="cssFloat"),o?e.setProperty(r,n):e[r]=n}}var Ub=he({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function vc(e,t){if(t){if(Ub[e]&&(t.children!=null||t.dangerouslySetInnerHTML!=null))throw Error(R(137,e));if(t.dangerouslySetInnerHTML!=null){if(t.children!=null)throw Error(R(60));if(typeof t.dangerouslySetInnerHTML!="object"||!("__html"in t.dangerouslySetInnerHTML))throw Error(R(61))}if(t.style!=null&&typeof t.style!="object")throw Error(R(62))}}function yc(e,t){if(e.indexOf("-")===-1)return typeof t.is=="string";switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var wc=null;function Td(e){return e=e.target||e.srcElement||window,e.correspondingUseElement&&(e=e.correspondingUseElement),e.nodeType===3?e.parentNode:e}var bc=null,en=null,tn=null;function pp(e){if(e=Vs(e)){if(typeof bc!="function")throw Error(R(280));var t=e.stateNode;t&&(t=Da(t),bc(e.stateNode,e.type,t))}}function oh(e){en?tn?tn.push(e):tn=[e]:en=e}function nh(){if(en){var e=en,t=tn;if(tn=en=null,pp(e),t)for(e=0;e<t.length;e++)pp(t[e])}}function sh(e,t){return e(t)}function ih(){}var kl=!1;function ah(e,t,r){if(kl)return e(t,r);kl=!0;try{return sh(e,t,r)}finally{kl=!1,(en!==null||tn!==null)&&(ih(),nh())}}function us(e,t){var r=e.stateNode;if(r===null)return null;var o=Da(r);if(o===null)return null;r=o[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(o=!o.disabled)||(e=e.type,o=!(e==="button"||e==="input"||e==="select"||e==="textarea")),e=!o;break e;default:e=!1}if(e)return null;if(r&&typeof r!="function")throw Error(R(231,t,typeof r));return r}var kc=!1;if(ar)try{var Vn={};Object.defineProperty(Vn,"passive",{get:function(){kc=!0}}),window.addEventListener("test",Vn,Vn),window.removeEventListener("test",Vn,Vn)}catch{kc=!1}function Gb(e,t,r,o,n,s,i,a,d){var c=Array.prototype.slice.call(arguments,3);try{t.apply(r,c)}catch(p){this.onError(p)}}var es=!1,Bi=null,Wi=!1,Ac=null,Fb={onError:function(e){es=!0,Bi=e}};function Kb(e,t,r,o,n,s,i,a,d){es=!1,Bi=null,Gb.apply(Fb,arguments)}function $b(e,t,r,o,n,s,i,a,d){if(Kb.apply(this,arguments),es){if(es){var c=Bi;es=!1,Bi=null}else throw Error(R(198));Wi||(Wi=!0,Ac=c)}}function Co(e){var t=e,r=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do t=e,t.flags&4098&&(r=t.return),e=t.return;while(e)}return t.tag===3?r:null}function lh(e){if(e.tag===13){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function gp(e){if(Co(e)!==e)throw Error(R(188))}function Qb(e){var t=e.alternate;if(!t){if(t=Co(e),t===null)throw Error(R(188));return t!==e?null:e}for(var r=e,o=t;;){var n=r.return;if(n===null)break;var s=n.alternate;if(s===null){if(o=n.return,o!==null){r=o;continue}break}if(n.child===s.child){for(s=n.child;s;){if(s===r)return gp(n),e;if(s===o)return gp(n),t;s=s.sibling}throw Error(R(188))}if(r.return!==o.return)r=n,o=s;else{for(var i=!1,a=n.child;a;){if(a===r){i=!0,r=n,o=s;break}if(a===o){i=!0,o=n,r=s;break}a=a.sibling}if(!i){for(a=s.child;a;){if(a===r){i=!0,r=s,o=n;break}if(a===o){i=!0,o=s,r=n;break}a=a.sibling}if(!i)throw Error(R(189))}}if(r.alternate!==o)throw Error(R(190))}if(r.tag!==3)throw Error(R(188));return r.stateNode.current===r?e:t}function ch(e){return e=Qb(e),e!==null?dh(e):null}function dh(e){if(e.tag===5||e.tag===6)return e;for(e=e.child;e!==null;){var t=dh(e);if(t!==null)return t;e=e.sibling}return null}var uh=tt.unstable_scheduleCallback,mp=tt.unstable_cancelCallback,Yb=tt.unstable_shouldYield,Jb=tt.unstable_requestPaint,ye=tt.unstable_now,Zb=tt.unstable_getCurrentPriorityLevel,zd=tt.unstable_ImmediatePriority,ph=tt.unstable_UserBlockingPriority,ji=tt.unstable_NormalPriority,Xb=tt.unstable_LowPriority,gh=tt.unstable_IdlePriority,Ia=null,Ft=null;function ek(e){if(Ft&&typeof Ft.onCommitFiberRoot=="function")try{Ft.onCommitFiberRoot(Ia,e,void 0,(e.current.flags&128)===128)}catch{}}var Dt=Math.clz32?Math.clz32:ok,tk=Math.log,rk=Math.LN2;function ok(e){return e>>>=0,e===0?32:31-(tk(e)/rk|0)|0}var ti=64,ri=4194304;function Jn(e){switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return e&4194240;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return e&130023424;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 1073741824;default:return e}}function Ui(e,t){var r=e.pendingLanes;if(r===0)return 0;var o=0,n=e.suspendedLanes,s=e.pingedLanes,i=r&268435455;if(i!==0){var a=i&~n;a!==0?o=Jn(a):(s&=i,s!==0&&(o=Jn(s)))}else i=r&~n,i!==0?o=Jn(i):s!==0&&(o=Jn(s));if(o===0)return 0;if(t!==0&&t!==o&&!(t&n)&&(n=o&-o,s=t&-t,n>=s||n===16&&(s&4194240)!==0))return t;if(o&4&&(o|=r&16),t=e.entangledLanes,t!==0)for(e=e.entanglements,t&=o;0<t;)r=31-Dt(t),n=1<<r,o|=e[r],t&=~n;return o}function nk(e,t){switch(e){case 1:case 2:case 4:return t+250;case 8:case 16:case 32:case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return-1;case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function sk(e,t){for(var r=e.suspendedLanes,o=e.pingedLanes,n=e.expirationTimes,s=e.pendingLanes;0<s;){var i=31-Dt(s),a=1<<i,d=n[i];d===-1?(!(a&r)||a&o)&&(n[i]=nk(a,t)):d<=t&&(e.expiredLanes|=a),s&=~a}}function Sc(e){return e=e.pendingLanes&-1073741825,e!==0?e:e&1073741824?1073741824:0}function mh(){var e=ti;return ti<<=1,!(ti&4194240)&&(ti=64),e}function Al(e){for(var t=[],r=0;31>r;r++)t.push(e);return t}function Os(e,t,r){e.pendingLanes|=t,t!==536870912&&(e.suspendedLanes=0,e.pingedLanes=0),e=e.eventTimes,t=31-Dt(t),e[t]=r}function ik(e,t){var r=e.pendingLanes&~t;e.pendingLanes=t,e.suspendedLanes=0,e.pingedLanes=0,e.expiredLanes&=t,e.mutableReadLanes&=t,e.entangledLanes&=t,t=e.entanglements;var o=e.eventTimes;for(e=e.expirationTimes;0<r;){var n=31-Dt(r),s=1<<n;t[n]=0,o[n]=-1,e[n]=-1,r&=~s}}function Rd(e,t){var r=e.entangledLanes|=t;for(e=e.entanglements;r;){var o=31-Dt(r),n=1<<o;n&t|e[o]&t&&(e[o]|=t),r&=~n}}var re=0;function hh(e){return e&=-e,1<e?4<e?e&268435455?16:536870912:4:1}var fh,Hd,vh,yh,wh,Ic=!1,oi=[],Or=null,_r=null,Vr=null,ps=new Map,gs=new Map,Dr=[],ak="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit".split(" ");function hp(e,t){switch(e){case"focusin":case"focusout":Or=null;break;case"dragenter":case"dragleave":_r=null;break;case"mouseover":case"mouseout":Vr=null;break;case"pointerover":case"pointerout":ps.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":gs.delete(t.pointerId)}}function Bn(e,t,r,o,n,s){return e===null||e.nativeEvent!==s?(e={blockedOn:t,domEventName:r,eventSystemFlags:o,nativeEvent:s,targetContainers:[n]},t!==null&&(t=Vs(t),t!==null&&Hd(t)),e):(e.eventSystemFlags|=o,t=e.targetContainers,n!==null&&t.indexOf(n)===-1&&t.push(n),e)}function lk(e,t,r,o,n){switch(t){case"focusin":return Or=Bn(Or,e,t,r,o,n),!0;case"dragenter":return _r=Bn(_r,e,t,r,o,n),!0;case"mouseover":return Vr=Bn(Vr,e,t,r,o,n),!0;case"pointerover":var s=n.pointerId;return ps.set(s,Bn(ps.get(s)||null,e,t,r,o,n)),!0;case"gotpointercapture":return s=n.pointerId,gs.set(s,Bn(gs.get(s)||null,e,t,r,o,n)),!0}return!1}function bh(e){var t=io(e.target);if(t!==null){var r=Co(t);if(r!==null){if(t=r.tag,t===13){if(t=lh(r),t!==null){e.blockedOn=t,wh(e.priority,function(){vh(r)});return}}else if(t===3&&r.stateNode.current.memoizedState.isDehydrated){e.blockedOn=r.tag===3?r.stateNode.containerInfo:null;return}}}e.blockedOn=null}function Pi(e){if(e.blockedOn!==null)return!1;for(var t=e.targetContainers;0<t.length;){var r=Pc(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(r===null){r=e.nativeEvent;var o=new r.constructor(r.type,r);wc=o,r.target.dispatchEvent(o),wc=null}else return t=Vs(r),t!==null&&Hd(t),e.blockedOn=r,!1;t.shift()}return!0}function fp(e,t,r){Pi(e)&&r.delete(t)}function ck(){Ic=!1,Or!==null&&Pi(Or)&&(Or=null),_r!==null&&Pi(_r)&&(_r=null),Vr!==null&&Pi(Vr)&&(Vr=null),ps.forEach(fp),gs.forEach(fp)}function Wn(e,t){e.blockedOn===t&&(e.blockedOn=null,Ic||(Ic=!0,tt.unstable_scheduleCallback(tt.unstable_NormalPriority,ck)))}function ms(e){function t(n){return Wn(n,e)}if(0<oi.length){Wn(oi[0],e);for(var r=1;r<oi.length;r++){var o=oi[r];o.blockedOn===e&&(o.blockedOn=null)}}for(Or!==null&&Wn(Or,e),_r!==null&&Wn(_r,e),Vr!==null&&Wn(Vr,e),ps.forEach(t),gs.forEach(t),r=0;r<Dr.length;r++)o=Dr[r],o.blockedOn===e&&(o.blockedOn=null);for(;0<Dr.length&&(r=Dr[0],r.blockedOn===null);)bh(r),r.blockedOn===null&&Dr.shift()}var rn=gr.ReactCurrentBatchConfig,Gi=!0;function dk(e,t,r,o){var n=re,s=rn.transition;rn.transition=null;try{re=1,Md(e,t,r,o)}finally{re=n,rn.transition=s}}function uk(e,t,r,o){var n=re,s=rn.transition;rn.transition=null;try{re=4,Md(e,t,r,o)}finally{re=n,rn.transition=s}}function Md(e,t,r,o){if(Gi){var n=Pc(e,t,r,o);if(n===null)Hl(e,t,o,Fi,r),hp(e,o);else if(lk(n,e,t,r,o))o.stopPropagation();else if(hp(e,o),t&4&&-1<ak.indexOf(e)){for(;n!==null;){var s=Vs(n);if(s!==null&&fh(s),s=Pc(e,t,r,o),s===null&&Hl(e,t,o,Fi,r),s===n)break;n=s}n!==null&&o.stopPropagation()}else Hl(e,t,o,null,r)}}var Fi=null;function Pc(e,t,r,o){if(Fi=null,e=Td(o),e=io(e),e!==null)if(t=Co(e),t===null)e=null;else if(r=t.tag,r===13){if(e=lh(t),e!==null)return e;e=null}else if(r===3){if(t.stateNode.current.memoizedState.isDehydrated)return t.tag===3?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null);return Fi=e,null}function kh(e){switch(e){case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 1;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"toggle":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 4;case"message":switch(Zb()){case zd:return 1;case ph:return 4;case ji:case Xb:return 16;case gh:return 536870912;default:return 16}default:return 16}}var Nr=null,Ed=null,Ci=null;function Ah(){if(Ci)return Ci;var e,t=Ed,r=t.length,o,n="value"in Nr?Nr.value:Nr.textContent,s=n.length;for(e=0;e<r&&t[e]===n[e];e++);var i=r-e;for(o=1;o<=i&&t[r-o]===n[s-o];o++);return Ci=n.slice(e,1<o?1-o:void 0)}function xi(e){var t=e.keyCode;return"charCode"in e?(e=e.charCode,e===0&&t===13&&(e=13)):e=t,e===10&&(e=13),32<=e||e===13?e:0}function ni(){return!0}function vp(){return!1}function ot(e){function t(r,o,n,s,i){this._reactName=r,this._targetInst=n,this.type=o,this.nativeEvent=s,this.target=i,this.currentTarget=null;for(var a in e)e.hasOwnProperty(a)&&(r=e[a],this[a]=r?r(s):s[a]);return this.isDefaultPrevented=(s.defaultPrevented!=null?s.defaultPrevented:s.returnValue===!1)?ni:vp,this.isPropagationStopped=vp,this}return he(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var r=this.nativeEvent;r&&(r.preventDefault?r.preventDefault():typeof r.returnValue!="unknown"&&(r.returnValue=!1),this.isDefaultPrevented=ni)},stopPropagation:function(){var r=this.nativeEvent;r&&(r.stopPropagation?r.stopPropagation():typeof r.cancelBubble!="unknown"&&(r.cancelBubble=!0),this.isPropagationStopped=ni)},persist:function(){},isPersistent:ni}),t}var Tn={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},Nd=ot(Tn),_s=he({},Tn,{view:0,detail:0}),pk=ot(_s),Sl,Il,jn,Pa=he({},_s,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:qd,button:0,buttons:0,relatedTarget:function(e){return e.relatedTarget===void 0?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==jn&&(jn&&e.type==="mousemove"?(Sl=e.screenX-jn.screenX,Il=e.screenY-jn.screenY):Il=Sl=0,jn=e),Sl)},movementY:function(e){return"movementY"in e?e.movementY:Il}}),yp=ot(Pa),gk=he({},Pa,{dataTransfer:0}),mk=ot(gk),hk=he({},_s,{relatedTarget:0}),Pl=ot(hk),fk=he({},Tn,{animationName:0,elapsedTime:0,pseudoElement:0}),vk=ot(fk),yk=he({},Tn,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),wk=ot(yk),bk=he({},Tn,{data:0}),wp=ot(bk),kk={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},Ak={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},Sk={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Ik(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):(e=Sk[e])?!!t[e]:!1}function qd(){return Ik}var Pk=he({},_s,{key:function(e){if(e.key){var t=kk[e.key]||e.key;if(t!=="Unidentified")return t}return e.type==="keypress"?(e=xi(e),e===13?"Enter":String.fromCharCode(e)):e.type==="keydown"||e.type==="keyup"?Ak[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:qd,charCode:function(e){return e.type==="keypress"?xi(e):0},keyCode:function(e){return e.type==="keydown"||e.type==="keyup"?e.keyCode:0},which:function(e){return e.type==="keypress"?xi(e):e.type==="keydown"||e.type==="keyup"?e.keyCode:0}}),Ck=ot(Pk),xk=he({},Pa,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),bp=ot(xk),Dk=he({},_s,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:qd}),Tk=ot(Dk),zk=he({},Tn,{propertyName:0,elapsedTime:0,pseudoElement:0}),Rk=ot(zk),Hk=he({},Pa,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),Mk=ot(Hk),Ek=[9,13,27,32],Ld=ar&&"CompositionEvent"in window,ts=null;ar&&"documentMode"in document&&(ts=document.documentMode);var Nk=ar&&"TextEvent"in window&&!ts,Sh=ar&&(!Ld||ts&&8<ts&&11>=ts),kp=" ",Ap=!1;function Ih(e,t){switch(e){case"keyup":return Ek.indexOf(t.keyCode)!==-1;case"keydown":return t.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function Ph(e){return e=e.detail,typeof e=="object"&&"data"in e?e.data:null}var jo=!1;function qk(e,t){switch(e){case"compositionend":return Ph(t);case"keypress":return t.which!==32?null:(Ap=!0,kp);case"textInput":return e=t.data,e===kp&&Ap?null:e;default:return null}}function Lk(e,t){if(jo)return e==="compositionend"||!Ld&&Ih(e,t)?(e=Ah(),Ci=Ed=Nr=null,jo=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return Sh&&t.locale!=="ko"?null:t.data;default:return null}}var Ok={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Sp(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t==="input"?!!Ok[e.type]:t==="textarea"}function Ch(e,t,r,o){oh(o),t=Ki(t,"onChange"),0<t.length&&(r=new Nd("onChange","change",null,r,o),e.push({event:r,listeners:t}))}var rs=null,hs=null;function _k(e){Lh(e,0)}function Ca(e){var t=Fo(e);if(Ym(t))return e}function Vk(e,t){if(e==="change")return t}var xh=!1;if(ar){var Cl;if(ar){var xl="oninput"in document;if(!xl){var Ip=document.createElement("div");Ip.setAttribute("oninput","return;"),xl=typeof Ip.oninput=="function"}Cl=xl}else Cl=!1;xh=Cl&&(!document.documentMode||9<document.documentMode)}function Pp(){rs&&(rs.detachEvent("onpropertychange",Dh),hs=rs=null)}function Dh(e){if(e.propertyName==="value"&&Ca(hs)){var t=[];Ch(t,hs,e,Td(e)),ah(_k,t)}}function Bk(e,t,r){e==="focusin"?(Pp(),rs=t,hs=r,rs.attachEvent("onpropertychange",Dh)):e==="focusout"&&Pp()}function Wk(e){if(e==="selectionchange"||e==="keyup"||e==="keydown")return Ca(hs)}function jk(e,t){if(e==="click")return Ca(t)}function Uk(e,t){if(e==="input"||e==="change")return Ca(t)}function Gk(e,t){return e===t&&(e!==0||1/e===1/t)||e!==e&&t!==t}var zt=typeof Object.is=="function"?Object.is:Gk;function fs(e,t){if(zt(e,t))return!0;if(typeof e!="object"||e===null||typeof t!="object"||t===null)return!1;var r=Object.keys(e),o=Object.keys(t);if(r.length!==o.length)return!1;for(o=0;o<r.length;o++){var n=r[o];if(!ac.call(t,n)||!zt(e[n],t[n]))return!1}return!0}function Cp(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function xp(e,t){var r=Cp(e);e=0;for(var o;r;){if(r.nodeType===3){if(o=e+r.textContent.length,e<=t&&o>=t)return{node:r,offset:t-e};e=o}e:{for(;r;){if(r.nextSibling){r=r.nextSibling;break e}r=r.parentNode}r=void 0}r=Cp(r)}}function Th(e,t){return e&&t?e===t?!0:e&&e.nodeType===3?!1:t&&t.nodeType===3?Th(e,t.parentNode):"contains"in e?e.contains(t):e.compareDocumentPosition?!!(e.compareDocumentPosition(t)&16):!1:!1}function zh(){for(var e=window,t=Vi();t instanceof e.HTMLIFrameElement;){try{var r=typeof t.contentWindow.location.href=="string"}catch{r=!1}if(r)e=t.contentWindow;else break;t=Vi(e.document)}return t}function Od(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(t==="input"&&(e.type==="text"||e.type==="search"||e.type==="tel"||e.type==="url"||e.type==="password")||t==="textarea"||e.contentEditable==="true")}function Fk(e){var t=zh(),r=e.focusedElem,o=e.selectionRange;if(t!==r&&r&&r.ownerDocument&&Th(r.ownerDocument.documentElement,r)){if(o!==null&&Od(r)){if(t=o.start,e=o.end,e===void 0&&(e=t),"selectionStart"in r)r.selectionStart=t,r.selectionEnd=Math.min(e,r.value.length);else if(e=(t=r.ownerDocument||document)&&t.defaultView||window,e.getSelection){e=e.getSelection();var n=r.textContent.length,s=Math.min(o.start,n);o=o.end===void 0?s:Math.min(o.end,n),!e.extend&&s>o&&(n=o,o=s,s=n),n=xp(r,s);var i=xp(r,o);n&&i&&(e.rangeCount!==1||e.anchorNode!==n.node||e.anchorOffset!==n.offset||e.focusNode!==i.node||e.focusOffset!==i.offset)&&(t=t.createRange(),t.setStart(n.node,n.offset),e.removeAllRanges(),s>o?(e.addRange(t),e.extend(i.node,i.offset)):(t.setEnd(i.node,i.offset),e.addRange(t)))}}for(t=[],e=r;e=e.parentNode;)e.nodeType===1&&t.push({element:e,left:e.scrollLeft,top:e.scrollTop});for(typeof r.focus=="function"&&r.focus(),r=0;r<t.length;r++)e=t[r],e.element.scrollLeft=e.left,e.element.scrollTop=e.top}}var Kk=ar&&"documentMode"in document&&11>=document.documentMode,Uo=null,Cc=null,os=null,xc=!1;function Dp(e,t,r){var o=r.window===r?r.document:r.nodeType===9?r:r.ownerDocument;xc||Uo==null||Uo!==Vi(o)||(o=Uo,"selectionStart"in o&&Od(o)?o={start:o.selectionStart,end:o.selectionEnd}:(o=(o.ownerDocument&&o.ownerDocument.defaultView||window).getSelection(),o={anchorNode:o.anchorNode,anchorOffset:o.anchorOffset,focusNode:o.focusNode,focusOffset:o.focusOffset}),os&&fs(os,o)||(os=o,o=Ki(Cc,"onSelect"),0<o.length&&(t=new Nd("onSelect","select",null,t,r),e.push({event:t,listeners:o}),t.target=Uo)))}function si(e,t){var r={};return r[e.toLowerCase()]=t.toLowerCase(),r["Webkit"+e]="webkit"+t,r["Moz"+e]="moz"+t,r}var Go={animationend:si("Animation","AnimationEnd"),animationiteration:si("Animation","AnimationIteration"),animationstart:si("Animation","AnimationStart"),transitionend:si("Transition","TransitionEnd")},Dl={},Rh={};ar&&(Rh=document.createElement("div").style,"AnimationEvent"in window||(delete Go.animationend.animation,delete Go.animationiteration.animation,delete Go.animationstart.animation),"TransitionEvent"in window||delete Go.transitionend.transition);function xa(e){if(Dl[e])return Dl[e];if(!Go[e])return e;var t=Go[e],r;for(r in t)if(t.hasOwnProperty(r)&&r in Rh)return Dl[e]=t[r];return e}var Hh=xa("animationend"),Mh=xa("animationiteration"),Eh=xa("animationstart"),Nh=xa("transitionend"),qh=new Map,Tp="abort auxClick cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");function Jr(e,t){qh.set(e,t),Po(t,[e])}for(var Tl=0;Tl<Tp.length;Tl++){var zl=Tp[Tl],$k=zl.toLowerCase(),Qk=zl[0].toUpperCase()+zl.slice(1);Jr($k,"on"+Qk)}Jr(Hh,"onAnimationEnd");Jr(Mh,"onAnimationIteration");Jr(Eh,"onAnimationStart");Jr("dblclick","onDoubleClick");Jr("focusin","onFocus");Jr("focusout","onBlur");Jr(Nh,"onTransitionEnd");yn("onMouseEnter",["mouseout","mouseover"]);yn("onMouseLeave",["mouseout","mouseover"]);yn("onPointerEnter",["pointerout","pointerover"]);yn("onPointerLeave",["pointerout","pointerover"]);Po("onChange","change click focusin focusout input keydown keyup selectionchange".split(" "));Po("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" "));Po("onBeforeInput",["compositionend","keypress","textInput","paste"]);Po("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" "));Po("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" "));Po("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var Zn="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Yk=new Set("cancel close invalid load scroll toggle".split(" ").concat(Zn));function zp(e,t,r){var o=e.type||"unknown-event";e.currentTarget=r,$b(o,t,void 0,e),e.currentTarget=null}function Lh(e,t){t=(t&4)!==0;for(var r=0;r<e.length;r++){var o=e[r],n=o.event;o=o.listeners;e:{var s=void 0;if(t)for(var i=o.length-1;0<=i;i--){var a=o[i],d=a.instance,c=a.currentTarget;if(a=a.listener,d!==s&&n.isPropagationStopped())break e;zp(n,a,c),s=d}else for(i=0;i<o.length;i++){if(a=o[i],d=a.instance,c=a.currentTarget,a=a.listener,d!==s&&n.isPropagationStopped())break e;zp(n,a,c),s=d}}}if(Wi)throw e=Ac,Wi=!1,Ac=null,e}function le(e,t){var r=t[Hc];r===void 0&&(r=t[Hc]=new Set);var o=e+"__bubble";r.has(o)||(Oh(t,e,2,!1),r.add(o))}function Rl(e,t,r){var o=0;t&&(o|=4),Oh(r,e,o,t)}var ii="_reactListening"+Math.random().toString(36).slice(2);function vs(e){if(!e[ii]){e[ii]=!0,Gm.forEach(function(r){r!=="selectionchange"&&(Yk.has(r)||Rl(r,!1,e),Rl(r,!0,e))});var t=e.nodeType===9?e:e.ownerDocument;t===null||t[ii]||(t[ii]=!0,Rl("selectionchange",!1,t))}}function Oh(e,t,r,o){switch(kh(t)){case 1:var n=dk;break;case 4:n=uk;break;default:n=Md}r=n.bind(null,t,r,e),n=void 0,!kc||t!=="touchstart"&&t!=="touchmove"&&t!=="wheel"||(n=!0),o?n!==void 0?e.addEventListener(t,r,{capture:!0,passive:n}):e.addEventListener(t,r,!0):n!==void 0?e.addEventListener(t,r,{passive:n}):e.addEventListener(t,r,!1)}function Hl(e,t,r,o,n){var s=o;if(!(t&1)&&!(t&2)&&o!==null)e:for(;;){if(o===null)return;var i=o.tag;if(i===3||i===4){var a=o.stateNode.containerInfo;if(a===n||a.nodeType===8&&a.parentNode===n)break;if(i===4)for(i=o.return;i!==null;){var d=i.tag;if((d===3||d===4)&&(d=i.stateNode.containerInfo,d===n||d.nodeType===8&&d.parentNode===n))return;i=i.return}for(;a!==null;){if(i=io(a),i===null)return;if(d=i.tag,d===5||d===6){o=s=i;continue e}a=a.parentNode}}o=o.return}ah(function(){var c=s,p=Td(r),u=[];e:{var f=qh.get(e);if(f!==void 0){var g=Nd,k=e;switch(e){case"keypress":if(xi(r)===0)break e;case"keydown":case"keyup":g=Ck;break;case"focusin":k="focus",g=Pl;break;case"focusout":k="blur",g=Pl;break;case"beforeblur":case"afterblur":g=Pl;break;case"click":if(r.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":g=yp;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":g=mk;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":g=Tk;break;case Hh:case Mh:case Eh:g=vk;break;case Nh:g=Rk;break;case"scroll":g=pk;break;case"wheel":g=Mk;break;case"copy":case"cut":case"paste":g=wk;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":g=bp}var y=(t&4)!==0,b=!y&&e==="scroll",v=y?f!==null?f+"Capture":null:f;y=[];for(var h=c,w;h!==null;){w=h;var A=w.stateNode;if(w.tag===5&&A!==null&&(w=A,v!==null&&(A=us(h,v),A!=null&&y.push(ys(h,A,w)))),b)break;h=h.return}0<y.length&&(f=new g(f,k,null,r,p),u.push({event:f,listeners:y}))}}if(!(t&7)){e:{if(f=e==="mouseover"||e==="pointerover",g=e==="mouseout"||e==="pointerout",f&&r!==wc&&(k=r.relatedTarget||r.fromElement)&&(io(k)||k[lr]))break e;if((g||f)&&(f=p.window===p?p:(f=p.ownerDocument)?f.defaultView||f.parentWindow:window,g?(k=r.relatedTarget||r.toElement,g=c,k=k?io(k):null,k!==null&&(b=Co(k),k!==b||k.tag!==5&&k.tag!==6)&&(k=null)):(g=null,k=c),g!==k)){if(y=yp,A="onMouseLeave",v="onMouseEnter",h="mouse",(e==="pointerout"||e==="pointerover")&&(y=bp,A="onPointerLeave",v="onPointerEnter",h="pointer"),b=g==null?f:Fo(g),w=k==null?f:Fo(k),f=new y(A,h+"leave",g,r,p),f.target=b,f.relatedTarget=w,A=null,io(p)===c&&(y=new y(v,h+"enter",k,r,p),y.target=w,y.relatedTarget=b,A=y),b=A,g&&k)t:{for(y=g,v=k,h=0,w=y;w;w=Lo(w))h++;for(w=0,A=v;A;A=Lo(A))w++;for(;0<h-w;)y=Lo(y),h--;for(;0<w-h;)v=Lo(v),w--;for(;h--;){if(y===v||v!==null&&y===v.alternate)break t;y=Lo(y),v=Lo(v)}y=null}else y=null;g!==null&&Rp(u,f,g,y,!1),k!==null&&b!==null&&Rp(u,b,k,y,!0)}}e:{if(f=c?Fo(c):window,g=f.nodeName&&f.nodeName.toLowerCase(),g==="select"||g==="input"&&f.type==="file")var S=Vk;else if(Sp(f))if(xh)S=Uk;else{S=Wk;var I=Bk}else(g=f.nodeName)&&g.toLowerCase()==="input"&&(f.type==="checkbox"||f.type==="radio")&&(S=jk);if(S&&(S=S(e,c))){Ch(u,S,r,p);break e}I&&I(e,f,c),e==="focusout"&&(I=f._wrapperState)&&I.controlled&&f.type==="number"&&mc(f,"number",f.value)}switch(I=c?Fo(c):window,e){case"focusin":(Sp(I)||I.contentEditable==="true")&&(Uo=I,Cc=c,os=null);break;case"focusout":os=Cc=Uo=null;break;case"mousedown":xc=!0;break;case"contextmenu":case"mouseup":case"dragend":xc=!1,Dp(u,r,p);break;case"selectionchange":if(Kk)break;case"keydown":case"keyup":Dp(u,r,p)}var P;if(Ld)e:{switch(e){case"compositionstart":var D="onCompositionStart";break e;case"compositionend":D="onCompositionEnd";break e;case"compositionupdate":D="onCompositionUpdate";break e}D=void 0}else jo?Ih(e,r)&&(D="onCompositionEnd"):e==="keydown"&&r.keyCode===229&&(D="onCompositionStart");D&&(Sh&&r.locale!=="ko"&&(jo||D!=="onCompositionStart"?D==="onCompositionEnd"&&jo&&(P=Ah()):(Nr=p,Ed="value"in Nr?Nr.value:Nr.textContent,jo=!0)),I=Ki(c,D),0<I.length&&(D=new wp(D,e,null,r,p),u.push({event:D,listeners:I}),P?D.data=P:(P=Ph(r),P!==null&&(D.data=P)))),(P=Nk?qk(e,r):Lk(e,r))&&(c=Ki(c,"onBeforeInput"),0<c.length&&(p=new wp("onBeforeInput","beforeinput",null,r,p),u.push({event:p,listeners:c}),p.data=P))}Lh(u,t)})}function ys(e,t,r){return{instance:e,listener:t,currentTarget:r}}function Ki(e,t){for(var r=t+"Capture",o=[];e!==null;){var n=e,s=n.stateNode;n.tag===5&&s!==null&&(n=s,s=us(e,r),s!=null&&o.unshift(ys(e,s,n)),s=us(e,t),s!=null&&o.push(ys(e,s,n))),e=e.return}return o}function Lo(e){if(e===null)return null;do e=e.return;while(e&&e.tag!==5);return e||null}function Rp(e,t,r,o,n){for(var s=t._reactName,i=[];r!==null&&r!==o;){var a=r,d=a.alternate,c=a.stateNode;if(d!==null&&d===o)break;a.tag===5&&c!==null&&(a=c,n?(d=us(r,s),d!=null&&i.unshift(ys(r,d,a))):n||(d=us(r,s),d!=null&&i.push(ys(r,d,a)))),r=r.return}i.length!==0&&e.push({event:t,listeners:i})}var Jk=/\r\n?/g,Zk=/\u0000|\uFFFD/g;function Hp(e){return(typeof e=="string"?e:""+e).replace(Jk,`
`).replace(Zk,"")}function ai(e,t,r){if(t=Hp(t),Hp(e)!==t&&r)throw Error(R(425))}function $i(){}var Dc=null,Tc=null;function zc(e,t){return e==="textarea"||e==="noscript"||typeof t.children=="string"||typeof t.children=="number"||typeof t.dangerouslySetInnerHTML=="object"&&t.dangerouslySetInnerHTML!==null&&t.dangerouslySetInnerHTML.__html!=null}var Rc=typeof setTimeout=="function"?setTimeout:void 0,Xk=typeof clearTimeout=="function"?clearTimeout:void 0,Mp=typeof Promise=="function"?Promise:void 0,eA=typeof queueMicrotask=="function"?queueMicrotask:typeof Mp<"u"?function(e){return Mp.resolve(null).then(e).catch(tA)}:Rc;function tA(e){setTimeout(function(){throw e})}function Ml(e,t){var r=t,o=0;do{var n=r.nextSibling;if(e.removeChild(r),n&&n.nodeType===8)if(r=n.data,r==="/$"){if(o===0){e.removeChild(n),ms(t);return}o--}else r!=="$"&&r!=="$?"&&r!=="$!"||o++;r=n}while(r);ms(t)}function Br(e){for(;e!=null;e=e.nextSibling){var t=e.nodeType;if(t===1||t===3)break;if(t===8){if(t=e.data,t==="$"||t==="$!"||t==="$?")break;if(t==="/$")return null}}return e}function Ep(e){e=e.previousSibling;for(var t=0;e;){if(e.nodeType===8){var r=e.data;if(r==="$"||r==="$!"||r==="$?"){if(t===0)return e;t--}else r==="/$"&&t++}e=e.previousSibling}return null}var zn=Math.random().toString(36).slice(2),jt="__reactFiber$"+zn,ws="__reactProps$"+zn,lr="__reactContainer$"+zn,Hc="__reactEvents$"+zn,rA="__reactListeners$"+zn,oA="__reactHandles$"+zn;function io(e){var t=e[jt];if(t)return t;for(var r=e.parentNode;r;){if(t=r[lr]||r[jt]){if(r=t.alternate,t.child!==null||r!==null&&r.child!==null)for(e=Ep(e);e!==null;){if(r=e[jt])return r;e=Ep(e)}return t}e=r,r=e.parentNode}return null}function Vs(e){return e=e[jt]||e[lr],!e||e.tag!==5&&e.tag!==6&&e.tag!==13&&e.tag!==3?null:e}function Fo(e){if(e.tag===5||e.tag===6)return e.stateNode;throw Error(R(33))}function Da(e){return e[ws]||null}var Mc=[],Ko=-1;function Zr(e){return{current:e}}function ce(e){0>Ko||(e.current=Mc[Ko],Mc[Ko]=null,Ko--)}function se(e,t){Ko++,Mc[Ko]=e.current,e.current=t}var $r={},Le=Zr($r),Ke=Zr(!1),yo=$r;function wn(e,t){var r=e.type.contextTypes;if(!r)return $r;var o=e.stateNode;if(o&&o.__reactInternalMemoizedUnmaskedChildContext===t)return o.__reactInternalMemoizedMaskedChildContext;var n={},s;for(s in r)n[s]=t[s];return o&&(e=e.stateNode,e.__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=n),n}function $e(e){return e=e.childContextTypes,e!=null}function Qi(){ce(Ke),ce(Le)}function Np(e,t,r){if(Le.current!==$r)throw Error(R(168));se(Le,t),se(Ke,r)}function _h(e,t,r){var o=e.stateNode;if(t=t.childContextTypes,typeof o.getChildContext!="function")return r;o=o.getChildContext();for(var n in o)if(!(n in t))throw Error(R(108,Bb(e)||"Unknown",n));return he({},r,o)}function Yi(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||$r,yo=Le.current,se(Le,e),se(Ke,Ke.current),!0}function qp(e,t,r){var o=e.stateNode;if(!o)throw Error(R(169));r?(e=_h(e,t,yo),o.__reactInternalMemoizedMergedChildContext=e,ce(Ke),ce(Le),se(Le,e)):ce(Ke),se(Ke,r)}var rr=null,Ta=!1,El=!1;function Vh(e){rr===null?rr=[e]:rr.push(e)}function nA(e){Ta=!0,Vh(e)}function Xr(){if(!El&&rr!==null){El=!0;var e=0,t=re;try{var r=rr;for(re=1;e<r.length;e++){var o=r[e];do o=o(!0);while(o!==null)}rr=null,Ta=!1}catch(n){throw rr!==null&&(rr=rr.slice(e+1)),uh(zd,Xr),n}finally{re=t,El=!1}}return null}var $o=[],Qo=0,Ji=null,Zi=0,it=[],at=0,wo=null,nr=1,sr="";function no(e,t){$o[Qo++]=Zi,$o[Qo++]=Ji,Ji=e,Zi=t}function Bh(e,t,r){it[at++]=nr,it[at++]=sr,it[at++]=wo,wo=e;var o=nr;e=sr;var n=32-Dt(o)-1;o&=~(1<<n),r+=1;var s=32-Dt(t)+n;if(30<s){var i=n-n%5;s=(o&(1<<i)-1).toString(32),o>>=i,n-=i,nr=1<<32-Dt(t)+n|r<<n|o,sr=s+e}else nr=1<<s|r<<n|o,sr=e}function _d(e){e.return!==null&&(no(e,1),Bh(e,1,0))}function Vd(e){for(;e===Ji;)Ji=$o[--Qo],$o[Qo]=null,Zi=$o[--Qo],$o[Qo]=null;for(;e===wo;)wo=it[--at],it[at]=null,sr=it[--at],it[at]=null,nr=it[--at],it[at]=null}var Xe=null,Ze=null,ue=!1,xt=null;function Wh(e,t){var r=lt(5,null,null,0);r.elementType="DELETED",r.stateNode=t,r.return=e,t=e.deletions,t===null?(e.deletions=[r],e.flags|=16):t.push(r)}function Lp(e,t){switch(e.tag){case 5:var r=e.type;return t=t.nodeType!==1||r.toLowerCase()!==t.nodeName.toLowerCase()?null:t,t!==null?(e.stateNode=t,Xe=e,Ze=Br(t.firstChild),!0):!1;case 6:return t=e.pendingProps===""||t.nodeType!==3?null:t,t!==null?(e.stateNode=t,Xe=e,Ze=null,!0):!1;case 13:return t=t.nodeType!==8?null:t,t!==null?(r=wo!==null?{id:nr,overflow:sr}:null,e.memoizedState={dehydrated:t,treeContext:r,retryLane:1073741824},r=lt(18,null,null,0),r.stateNode=t,r.return=e,e.child=r,Xe=e,Ze=null,!0):!1;default:return!1}}function Ec(e){return(e.mode&1)!==0&&(e.flags&128)===0}function Nc(e){if(ue){var t=Ze;if(t){var r=t;if(!Lp(e,t)){if(Ec(e))throw Error(R(418));t=Br(r.nextSibling);var o=Xe;t&&Lp(e,t)?Wh(o,r):(e.flags=e.flags&-4097|2,ue=!1,Xe=e)}}else{if(Ec(e))throw Error(R(418));e.flags=e.flags&-4097|2,ue=!1,Xe=e}}}function Op(e){for(e=e.return;e!==null&&e.tag!==5&&e.tag!==3&&e.tag!==13;)e=e.return;Xe=e}function li(e){if(e!==Xe)return!1;if(!ue)return Op(e),ue=!0,!1;var t;if((t=e.tag!==3)&&!(t=e.tag!==5)&&(t=e.type,t=t!=="head"&&t!=="body"&&!zc(e.type,e.memoizedProps)),t&&(t=Ze)){if(Ec(e))throw jh(),Error(R(418));for(;t;)Wh(e,t),t=Br(t.nextSibling)}if(Op(e),e.tag===13){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(R(317));e:{for(e=e.nextSibling,t=0;e;){if(e.nodeType===8){var r=e.data;if(r==="/$"){if(t===0){Ze=Br(e.nextSibling);break e}t--}else r!=="$"&&r!=="$!"&&r!=="$?"||t++}e=e.nextSibling}Ze=null}}else Ze=Xe?Br(e.stateNode.nextSibling):null;return!0}function jh(){for(var e=Ze;e;)e=Br(e.nextSibling)}function bn(){Ze=Xe=null,ue=!1}function Bd(e){xt===null?xt=[e]:xt.push(e)}var sA=gr.ReactCurrentBatchConfig;function Un(e,t,r){if(e=r.ref,e!==null&&typeof e!="function"&&typeof e!="object"){if(r._owner){if(r=r._owner,r){if(r.tag!==1)throw Error(R(309));var o=r.stateNode}if(!o)throw Error(R(147,e));var n=o,s=""+e;return t!==null&&t.ref!==null&&typeof t.ref=="function"&&t.ref._stringRef===s?t.ref:(t=function(i){var a=n.refs;i===null?delete a[s]:a[s]=i},t._stringRef=s,t)}if(typeof e!="string")throw Error(R(284));if(!r._owner)throw Error(R(290,e))}return e}function ci(e,t){throw e=Object.prototype.toString.call(t),Error(R(31,e==="[object Object]"?"object with keys {"+Object.keys(t).join(", ")+"}":e))}function _p(e){var t=e._init;return t(e._payload)}function Uh(e){function t(v,h){if(e){var w=v.deletions;w===null?(v.deletions=[h],v.flags|=16):w.push(h)}}function r(v,h){if(!e)return null;for(;h!==null;)t(v,h),h=h.sibling;return null}function o(v,h){for(v=new Map;h!==null;)h.key!==null?v.set(h.key,h):v.set(h.index,h),h=h.sibling;return v}function n(v,h){return v=Gr(v,h),v.index=0,v.sibling=null,v}function s(v,h,w){return v.index=w,e?(w=v.alternate,w!==null?(w=w.index,w<h?(v.flags|=2,h):w):(v.flags|=2,h)):(v.flags|=1048576,h)}function i(v){return e&&v.alternate===null&&(v.flags|=2),v}function a(v,h,w,A){return h===null||h.tag!==6?(h=Bl(w,v.mode,A),h.return=v,h):(h=n(h,w),h.return=v,h)}function d(v,h,w,A){var S=w.type;return S===Wo?p(v,h,w.props.children,A,w.key):h!==null&&(h.elementType===S||typeof S=="object"&&S!==null&&S.$$typeof===Cr&&_p(S)===h.type)?(A=n(h,w.props),A.ref=Un(v,h,w),A.return=v,A):(A=Ei(w.type,w.key,w.props,null,v.mode,A),A.ref=Un(v,h,w),A.return=v,A)}function c(v,h,w,A){return h===null||h.tag!==4||h.stateNode.containerInfo!==w.containerInfo||h.stateNode.implementation!==w.implementation?(h=Wl(w,v.mode,A),h.return=v,h):(h=n(h,w.children||[]),h.return=v,h)}function p(v,h,w,A,S){return h===null||h.tag!==7?(h=vo(w,v.mode,A,S),h.return=v,h):(h=n(h,w),h.return=v,h)}function u(v,h,w){if(typeof h=="string"&&h!==""||typeof h=="number")return h=Bl(""+h,v.mode,w),h.return=v,h;if(typeof h=="object"&&h!==null){switch(h.$$typeof){case Zs:return w=Ei(h.type,h.key,h.props,null,v.mode,w),w.ref=Un(v,null,h),w.return=v,w;case Bo:return h=Wl(h,v.mode,w),h.return=v,h;case Cr:var A=h._init;return u(v,A(h._payload),w)}if(Yn(h)||_n(h))return h=vo(h,v.mode,w,null),h.return=v,h;ci(v,h)}return null}function f(v,h,w,A){var S=h!==null?h.key:null;if(typeof w=="string"&&w!==""||typeof w=="number")return S!==null?null:a(v,h,""+w,A);if(typeof w=="object"&&w!==null){switch(w.$$typeof){case Zs:return w.key===S?d(v,h,w,A):null;case Bo:return w.key===S?c(v,h,w,A):null;case Cr:return S=w._init,f(v,h,S(w._payload),A)}if(Yn(w)||_n(w))return S!==null?null:p(v,h,w,A,null);ci(v,w)}return null}function g(v,h,w,A,S){if(typeof A=="string"&&A!==""||typeof A=="number")return v=v.get(w)||null,a(h,v,""+A,S);if(typeof A=="object"&&A!==null){switch(A.$$typeof){case Zs:return v=v.get(A.key===null?w:A.key)||null,d(h,v,A,S);case Bo:return v=v.get(A.key===null?w:A.key)||null,c(h,v,A,S);case Cr:var I=A._init;return g(v,h,w,I(A._payload),S)}if(Yn(A)||_n(A))return v=v.get(w)||null,p(h,v,A,S,null);ci(h,A)}return null}function k(v,h,w,A){for(var S=null,I=null,P=h,D=h=0,M=null;P!==null&&D<w.length;D++){P.index>D?(M=P,P=null):M=P.sibling;var H=f(v,P,w[D],A);if(H===null){P===null&&(P=M);break}e&&P&&H.alternate===null&&t(v,P),h=s(H,h,D),I===null?S=H:I.sibling=H,I=H,P=M}if(D===w.length)return r(v,P),ue&&no(v,D),S;if(P===null){for(;D<w.length;D++)P=u(v,w[D],A),P!==null&&(h=s(P,h,D),I===null?S=P:I.sibling=P,I=P);return ue&&no(v,D),S}for(P=o(v,P);D<w.length;D++)M=g(P,v,D,w[D],A),M!==null&&(e&&M.alternate!==null&&P.delete(M.key===null?D:M.key),h=s(M,h,D),I===null?S=M:I.sibling=M,I=M);return e&&P.forEach(function(V){return t(v,V)}),ue&&no(v,D),S}function y(v,h,w,A){var S=_n(w);if(typeof S!="function")throw Error(R(150));if(w=S.call(w),w==null)throw Error(R(151));for(var I=S=null,P=h,D=h=0,M=null,H=w.next();P!==null&&!H.done;D++,H=w.next()){P.index>D?(M=P,P=null):M=P.sibling;var V=f(v,P,H.value,A);if(V===null){P===null&&(P=M);break}e&&P&&V.alternate===null&&t(v,P),h=s(V,h,D),I===null?S=V:I.sibling=V,I=V,P=M}if(H.done)return r(v,P),ue&&no(v,D),S;if(P===null){for(;!H.done;D++,H=w.next())H=u(v,H.value,A),H!==null&&(h=s(H,h,D),I===null?S=H:I.sibling=H,I=H);return ue&&no(v,D),S}for(P=o(v,P);!H.done;D++,H=w.next())H=g(P,v,D,H.value,A),H!==null&&(e&&H.alternate!==null&&P.delete(H.key===null?D:H.key),h=s(H,h,D),I===null?S=H:I.sibling=H,I=H);return e&&P.forEach(function(L){return t(v,L)}),ue&&no(v,D),S}function b(v,h,w,A){if(typeof w=="object"&&w!==null&&w.type===Wo&&w.key===null&&(w=w.props.children),typeof w=="object"&&w!==null){switch(w.$$typeof){case Zs:e:{for(var S=w.key,I=h;I!==null;){if(I.key===S){if(S=w.type,S===Wo){if(I.tag===7){r(v,I.sibling),h=n(I,w.props.children),h.return=v,v=h;break e}}else if(I.elementType===S||typeof S=="object"&&S!==null&&S.$$typeof===Cr&&_p(S)===I.type){r(v,I.sibling),h=n(I,w.props),h.ref=Un(v,I,w),h.return=v,v=h;break e}r(v,I);break}else t(v,I);I=I.sibling}w.type===Wo?(h=vo(w.props.children,v.mode,A,w.key),h.return=v,v=h):(A=Ei(w.type,w.key,w.props,null,v.mode,A),A.ref=Un(v,h,w),A.return=v,v=A)}return i(v);case Bo:e:{for(I=w.key;h!==null;){if(h.key===I)if(h.tag===4&&h.stateNode.containerInfo===w.containerInfo&&h.stateNode.implementation===w.implementation){r(v,h.sibling),h=n(h,w.children||[]),h.return=v,v=h;break e}else{r(v,h);break}else t(v,h);h=h.sibling}h=Wl(w,v.mode,A),h.return=v,v=h}return i(v);case Cr:return I=w._init,b(v,h,I(w._payload),A)}if(Yn(w))return k(v,h,w,A);if(_n(w))return y(v,h,w,A);ci(v,w)}return typeof w=="string"&&w!==""||typeof w=="number"?(w=""+w,h!==null&&h.tag===6?(r(v,h.sibling),h=n(h,w),h.return=v,v=h):(r(v,h),h=Bl(w,v.mode,A),h.return=v,v=h),i(v)):r(v,h)}return b}var kn=Uh(!0),Gh=Uh(!1),Xi=Zr(null),ea=null,Yo=null,Wd=null;function jd(){Wd=Yo=ea=null}function Ud(e){var t=Xi.current;ce(Xi),e._currentValue=t}function qc(e,t,r){for(;e!==null;){var o=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,o!==null&&(o.childLanes|=t)):o!==null&&(o.childLanes&t)!==t&&(o.childLanes|=t),e===r)break;e=e.return}}function on(e,t){ea=e,Wd=Yo=null,e=e.dependencies,e!==null&&e.firstContext!==null&&(e.lanes&t&&(Fe=!0),e.firstContext=null)}function dt(e){var t=e._currentValue;if(Wd!==e)if(e={context:e,memoizedValue:t,next:null},Yo===null){if(ea===null)throw Error(R(308));Yo=e,ea.dependencies={lanes:0,firstContext:e}}else Yo=Yo.next=e;return t}var ao=null;function Gd(e){ao===null?ao=[e]:ao.push(e)}function Fh(e,t,r,o){var n=t.interleaved;return n===null?(r.next=r,Gd(t)):(r.next=n.next,n.next=r),t.interleaved=r,cr(e,o)}function cr(e,t){e.lanes|=t;var r=e.alternate;for(r!==null&&(r.lanes|=t),r=e,e=e.return;e!==null;)e.childLanes|=t,r=e.alternate,r!==null&&(r.childLanes|=t),r=e,e=e.return;return r.tag===3?r.stateNode:null}var xr=!1;function Fd(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,interleaved:null,lanes:0},effects:null}}function Kh(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,effects:e.effects})}function ir(e,t){return{eventTime:e,lane:t,tag:0,payload:null,callback:null,next:null}}function Wr(e,t,r){var o=e.updateQueue;if(o===null)return null;if(o=o.shared,X&2){var n=o.pending;return n===null?t.next=t:(t.next=n.next,n.next=t),o.pending=t,cr(e,r)}return n=o.interleaved,n===null?(t.next=t,Gd(o)):(t.next=n.next,n.next=t),o.interleaved=t,cr(e,r)}function Di(e,t,r){if(t=t.updateQueue,t!==null&&(t=t.shared,(r&4194240)!==0)){var o=t.lanes;o&=e.pendingLanes,r|=o,t.lanes=r,Rd(e,r)}}function Vp(e,t){var r=e.updateQueue,o=e.alternate;if(o!==null&&(o=o.updateQueue,r===o)){var n=null,s=null;if(r=r.firstBaseUpdate,r!==null){do{var i={eventTime:r.eventTime,lane:r.lane,tag:r.tag,payload:r.payload,callback:r.callback,next:null};s===null?n=s=i:s=s.next=i,r=r.next}while(r!==null);s===null?n=s=t:s=s.next=t}else n=s=t;r={baseState:o.baseState,firstBaseUpdate:n,lastBaseUpdate:s,shared:o.shared,effects:o.effects},e.updateQueue=r;return}e=r.lastBaseUpdate,e===null?r.firstBaseUpdate=t:e.next=t,r.lastBaseUpdate=t}function ta(e,t,r,o){var n=e.updateQueue;xr=!1;var s=n.firstBaseUpdate,i=n.lastBaseUpdate,a=n.shared.pending;if(a!==null){n.shared.pending=null;var d=a,c=d.next;d.next=null,i===null?s=c:i.next=c,i=d;var p=e.alternate;p!==null&&(p=p.updateQueue,a=p.lastBaseUpdate,a!==i&&(a===null?p.firstBaseUpdate=c:a.next=c,p.lastBaseUpdate=d))}if(s!==null){var u=n.baseState;i=0,p=c=d=null,a=s;do{var f=a.lane,g=a.eventTime;if((o&f)===f){p!==null&&(p=p.next={eventTime:g,lane:0,tag:a.tag,payload:a.payload,callback:a.callback,next:null});e:{var k=e,y=a;switch(f=t,g=r,y.tag){case 1:if(k=y.payload,typeof k=="function"){u=k.call(g,u,f);break e}u=k;break e;case 3:k.flags=k.flags&-65537|128;case 0:if(k=y.payload,f=typeof k=="function"?k.call(g,u,f):k,f==null)break e;u=he({},u,f);break e;case 2:xr=!0}}a.callback!==null&&a.lane!==0&&(e.flags|=64,f=n.effects,f===null?n.effects=[a]:f.push(a))}else g={eventTime:g,lane:f,tag:a.tag,payload:a.payload,callback:a.callback,next:null},p===null?(c=p=g,d=u):p=p.next=g,i|=f;if(a=a.next,a===null){if(a=n.shared.pending,a===null)break;f=a,a=f.next,f.next=null,n.lastBaseUpdate=f,n.shared.pending=null}}while(!0);if(p===null&&(d=u),n.baseState=d,n.firstBaseUpdate=c,n.lastBaseUpdate=p,t=n.shared.interleaved,t!==null){n=t;do i|=n.lane,n=n.next;while(n!==t)}else s===null&&(n.shared.lanes=0);ko|=i,e.lanes=i,e.memoizedState=u}}function Bp(e,t,r){if(e=t.effects,t.effects=null,e!==null)for(t=0;t<e.length;t++){var o=e[t],n=o.callback;if(n!==null){if(o.callback=null,o=r,typeof n!="function")throw Error(R(191,n));n.call(o)}}}var Bs={},Kt=Zr(Bs),bs=Zr(Bs),ks=Zr(Bs);function lo(e){if(e===Bs)throw Error(R(174));return e}function Kd(e,t){switch(se(ks,t),se(bs,e),se(Kt,Bs),e=t.nodeType,e){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:fc(null,"");break;default:e=e===8?t.parentNode:t,t=e.namespaceURI||null,e=e.tagName,t=fc(t,e)}ce(Kt),se(Kt,t)}function An(){ce(Kt),ce(bs),ce(ks)}function $h(e){lo(ks.current);var t=lo(Kt.current),r=fc(t,e.type);t!==r&&(se(bs,e),se(Kt,r))}function $d(e){bs.current===e&&(ce(Kt),ce(bs))}var pe=Zr(0);function ra(e){for(var t=e;t!==null;){if(t.tag===13){var r=t.memoizedState;if(r!==null&&(r=r.dehydrated,r===null||r.data==="$?"||r.data==="$!"))return t}else if(t.tag===19&&t.memoizedProps.revealOrder!==void 0){if(t.flags&128)return t}else if(t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var Nl=[];function Qd(){for(var e=0;e<Nl.length;e++)Nl[e]._workInProgressVersionPrimary=null;Nl.length=0}var Ti=gr.ReactCurrentDispatcher,ql=gr.ReactCurrentBatchConfig,bo=0,ge=null,ke=null,Se=null,oa=!1,ns=!1,As=0,iA=0;function He(){throw Error(R(321))}function Yd(e,t){if(t===null)return!1;for(var r=0;r<t.length&&r<e.length;r++)if(!zt(e[r],t[r]))return!1;return!0}function Jd(e,t,r,o,n,s){if(bo=s,ge=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,Ti.current=e===null||e.memoizedState===null?dA:uA,e=r(o,n),ns){s=0;do{if(ns=!1,As=0,25<=s)throw Error(R(301));s+=1,Se=ke=null,t.updateQueue=null,Ti.current=pA,e=r(o,n)}while(ns)}if(Ti.current=na,t=ke!==null&&ke.next!==null,bo=0,Se=ke=ge=null,oa=!1,t)throw Error(R(300));return e}function Zd(){var e=As!==0;return As=0,e}function _t(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return Se===null?ge.memoizedState=Se=e:Se=Se.next=e,Se}function ut(){if(ke===null){var e=ge.alternate;e=e!==null?e.memoizedState:null}else e=ke.next;var t=Se===null?ge.memoizedState:Se.next;if(t!==null)Se=t,ke=e;else{if(e===null)throw Error(R(310));ke=e,e={memoizedState:ke.memoizedState,baseState:ke.baseState,baseQueue:ke.baseQueue,queue:ke.queue,next:null},Se===null?ge.memoizedState=Se=e:Se=Se.next=e}return Se}function Ss(e,t){return typeof t=="function"?t(e):t}function Ll(e){var t=ut(),r=t.queue;if(r===null)throw Error(R(311));r.lastRenderedReducer=e;var o=ke,n=o.baseQueue,s=r.pending;if(s!==null){if(n!==null){var i=n.next;n.next=s.next,s.next=i}o.baseQueue=n=s,r.pending=null}if(n!==null){s=n.next,o=o.baseState;var a=i=null,d=null,c=s;do{var p=c.lane;if((bo&p)===p)d!==null&&(d=d.next={lane:0,action:c.action,hasEagerState:c.hasEagerState,eagerState:c.eagerState,next:null}),o=c.hasEagerState?c.eagerState:e(o,c.action);else{var u={lane:p,action:c.action,hasEagerState:c.hasEagerState,eagerState:c.eagerState,next:null};d===null?(a=d=u,i=o):d=d.next=u,ge.lanes|=p,ko|=p}c=c.next}while(c!==null&&c!==s);d===null?i=o:d.next=a,zt(o,t.memoizedState)||(Fe=!0),t.memoizedState=o,t.baseState=i,t.baseQueue=d,r.lastRenderedState=o}if(e=r.interleaved,e!==null){n=e;do s=n.lane,ge.lanes|=s,ko|=s,n=n.next;while(n!==e)}else n===null&&(r.lanes=0);return[t.memoizedState,r.dispatch]}function Ol(e){var t=ut(),r=t.queue;if(r===null)throw Error(R(311));r.lastRenderedReducer=e;var o=r.dispatch,n=r.pending,s=t.memoizedState;if(n!==null){r.pending=null;var i=n=n.next;do s=e(s,i.action),i=i.next;while(i!==n);zt(s,t.memoizedState)||(Fe=!0),t.memoizedState=s,t.baseQueue===null&&(t.baseState=s),r.lastRenderedState=s}return[s,o]}function Qh(){}function Yh(e,t){var r=ge,o=ut(),n=t(),s=!zt(o.memoizedState,n);if(s&&(o.memoizedState=n,Fe=!0),o=o.queue,Xd(Xh.bind(null,r,o,e),[e]),o.getSnapshot!==t||s||Se!==null&&Se.memoizedState.tag&1){if(r.flags|=2048,Is(9,Zh.bind(null,r,o,n,t),void 0,null),Ie===null)throw Error(R(349));bo&30||Jh(r,t,n)}return n}function Jh(e,t,r){e.flags|=16384,e={getSnapshot:t,value:r},t=ge.updateQueue,t===null?(t={lastEffect:null,stores:null},ge.updateQueue=t,t.stores=[e]):(r=t.stores,r===null?t.stores=[e]:r.push(e))}function Zh(e,t,r,o){t.value=r,t.getSnapshot=o,ef(t)&&tf(e)}function Xh(e,t,r){return r(function(){ef(t)&&tf(e)})}function ef(e){var t=e.getSnapshot;e=e.value;try{var r=t();return!zt(e,r)}catch{return!0}}function tf(e){var t=cr(e,1);t!==null&&Tt(t,e,1,-1)}function Wp(e){var t=_t();return typeof e=="function"&&(e=e()),t.memoizedState=t.baseState=e,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:Ss,lastRenderedState:e},t.queue=e,e=e.dispatch=cA.bind(null,ge,e),[t.memoizedState,e]}function Is(e,t,r,o){return e={tag:e,create:t,destroy:r,deps:o,next:null},t=ge.updateQueue,t===null?(t={lastEffect:null,stores:null},ge.updateQueue=t,t.lastEffect=e.next=e):(r=t.lastEffect,r===null?t.lastEffect=e.next=e:(o=r.next,r.next=e,e.next=o,t.lastEffect=e)),e}function rf(){return ut().memoizedState}function zi(e,t,r,o){var n=_t();ge.flags|=e,n.memoizedState=Is(1|t,r,void 0,o===void 0?null:o)}function za(e,t,r,o){var n=ut();o=o===void 0?null:o;var s=void 0;if(ke!==null){var i=ke.memoizedState;if(s=i.destroy,o!==null&&Yd(o,i.deps)){n.memoizedState=Is(t,r,s,o);return}}ge.flags|=e,n.memoizedState=Is(1|t,r,s,o)}function jp(e,t){return zi(8390656,8,e,t)}function Xd(e,t){return za(2048,8,e,t)}function of(e,t){return za(4,2,e,t)}function nf(e,t){return za(4,4,e,t)}function sf(e,t){if(typeof t=="function")return e=e(),t(e),function(){t(null)};if(t!=null)return e=e(),t.current=e,function(){t.current=null}}function af(e,t,r){return r=r!=null?r.concat([e]):null,za(4,4,sf.bind(null,t,e),r)}function eu(){}function lf(e,t){var r=ut();t=t===void 0?null:t;var o=r.memoizedState;return o!==null&&t!==null&&Yd(t,o[1])?o[0]:(r.memoizedState=[e,t],e)}function cf(e,t){var r=ut();t=t===void 0?null:t;var o=r.memoizedState;return o!==null&&t!==null&&Yd(t,o[1])?o[0]:(e=e(),r.memoizedState=[e,t],e)}function df(e,t,r){return bo&21?(zt(r,t)||(r=mh(),ge.lanes|=r,ko|=r,e.baseState=!0),t):(e.baseState&&(e.baseState=!1,Fe=!0),e.memoizedState=r)}function aA(e,t){var r=re;re=r!==0&&4>r?r:4,e(!0);var o=ql.transition;ql.transition={};try{e(!1),t()}finally{re=r,ql.transition=o}}function uf(){return ut().memoizedState}function lA(e,t,r){var o=Ur(e);if(r={lane:o,action:r,hasEagerState:!1,eagerState:null,next:null},pf(e))gf(t,r);else if(r=Fh(e,t,r,o),r!==null){var n=Be();Tt(r,e,o,n),mf(r,t,o)}}function cA(e,t,r){var o=Ur(e),n={lane:o,action:r,hasEagerState:!1,eagerState:null,next:null};if(pf(e))gf(t,n);else{var s=e.alternate;if(e.lanes===0&&(s===null||s.lanes===0)&&(s=t.lastRenderedReducer,s!==null))try{var i=t.lastRenderedState,a=s(i,r);if(n.hasEagerState=!0,n.eagerState=a,zt(a,i)){var d=t.interleaved;d===null?(n.next=n,Gd(t)):(n.next=d.next,d.next=n),t.interleaved=n;return}}catch{}finally{}r=Fh(e,t,n,o),r!==null&&(n=Be(),Tt(r,e,o,n),mf(r,t,o))}}function pf(e){var t=e.alternate;return e===ge||t!==null&&t===ge}function gf(e,t){ns=oa=!0;var r=e.pending;r===null?t.next=t:(t.next=r.next,r.next=t),e.pending=t}function mf(e,t,r){if(r&4194240){var o=t.lanes;o&=e.pendingLanes,r|=o,t.lanes=r,Rd(e,r)}}var na={readContext:dt,useCallback:He,useContext:He,useEffect:He,useImperativeHandle:He,useInsertionEffect:He,useLayoutEffect:He,useMemo:He,useReducer:He,useRef:He,useState:He,useDebugValue:He,useDeferredValue:He,useTransition:He,useMutableSource:He,useSyncExternalStore:He,useId:He,unstable_isNewReconciler:!1},dA={readContext:dt,useCallback:function(e,t){return _t().memoizedState=[e,t===void 0?null:t],e},useContext:dt,useEffect:jp,useImperativeHandle:function(e,t,r){return r=r!=null?r.concat([e]):null,zi(4194308,4,sf.bind(null,t,e),r)},useLayoutEffect:function(e,t){return zi(4194308,4,e,t)},useInsertionEffect:function(e,t){return zi(4,2,e,t)},useMemo:function(e,t){var r=_t();return t=t===void 0?null:t,e=e(),r.memoizedState=[e,t],e},useReducer:function(e,t,r){var o=_t();return t=r!==void 0?r(t):t,o.memoizedState=o.baseState=t,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:t},o.queue=e,e=e.dispatch=lA.bind(null,ge,e),[o.memoizedState,e]},useRef:function(e){var t=_t();return e={current:e},t.memoizedState=e},useState:Wp,useDebugValue:eu,useDeferredValue:function(e){return _t().memoizedState=e},useTransition:function(){var e=Wp(!1),t=e[0];return e=aA.bind(null,e[1]),_t().memoizedState=e,[t,e]},useMutableSource:function(){},useSyncExternalStore:function(e,t,r){var o=ge,n=_t();if(ue){if(r===void 0)throw Error(R(407));r=r()}else{if(r=t(),Ie===null)throw Error(R(349));bo&30||Jh(o,t,r)}n.memoizedState=r;var s={value:r,getSnapshot:t};return n.queue=s,jp(Xh.bind(null,o,s,e),[e]),o.flags|=2048,Is(9,Zh.bind(null,o,s,r,t),void 0,null),r},useId:function(){var e=_t(),t=Ie.identifierPrefix;if(ue){var r=sr,o=nr;r=(o&~(1<<32-Dt(o)-1)).toString(32)+r,t=":"+t+"R"+r,r=As++,0<r&&(t+="H"+r.toString(32)),t+=":"}else r=iA++,t=":"+t+"r"+r.toString(32)+":";return e.memoizedState=t},unstable_isNewReconciler:!1},uA={readContext:dt,useCallback:lf,useContext:dt,useEffect:Xd,useImperativeHandle:af,useInsertionEffect:of,useLayoutEffect:nf,useMemo:cf,useReducer:Ll,useRef:rf,useState:function(){return Ll(Ss)},useDebugValue:eu,useDeferredValue:function(e){var t=ut();return df(t,ke.memoizedState,e)},useTransition:function(){var e=Ll(Ss)[0],t=ut().memoizedState;return[e,t]},useMutableSource:Qh,useSyncExternalStore:Yh,useId:uf,unstable_isNewReconciler:!1},pA={readContext:dt,useCallback:lf,useContext:dt,useEffect:Xd,useImperativeHandle:af,useInsertionEffect:of,useLayoutEffect:nf,useMemo:cf,useReducer:Ol,useRef:rf,useState:function(){return Ol(Ss)},useDebugValue:eu,useDeferredValue:function(e){var t=ut();return ke===null?t.memoizedState=e:df(t,ke.memoizedState,e)},useTransition:function(){var e=Ol(Ss)[0],t=ut().memoizedState;return[e,t]},useMutableSource:Qh,useSyncExternalStore:Yh,useId:uf,unstable_isNewReconciler:!1};function At(e,t){if(e&&e.defaultProps){t=he({},t),e=e.defaultProps;for(var r in e)t[r]===void 0&&(t[r]=e[r]);return t}return t}function Lc(e,t,r,o){t=e.memoizedState,r=r(o,t),r=r==null?t:he({},t,r),e.memoizedState=r,e.lanes===0&&(e.updateQueue.baseState=r)}var Ra={isMounted:function(e){return(e=e._reactInternals)?Co(e)===e:!1},enqueueSetState:function(e,t,r){e=e._reactInternals;var o=Be(),n=Ur(e),s=ir(o,n);s.payload=t,r!=null&&(s.callback=r),t=Wr(e,s,n),t!==null&&(Tt(t,e,n,o),Di(t,e,n))},enqueueReplaceState:function(e,t,r){e=e._reactInternals;var o=Be(),n=Ur(e),s=ir(o,n);s.tag=1,s.payload=t,r!=null&&(s.callback=r),t=Wr(e,s,n),t!==null&&(Tt(t,e,n,o),Di(t,e,n))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var r=Be(),o=Ur(e),n=ir(r,o);n.tag=2,t!=null&&(n.callback=t),t=Wr(e,n,o),t!==null&&(Tt(t,e,o,r),Di(t,e,o))}};function Up(e,t,r,o,n,s,i){return e=e.stateNode,typeof e.shouldComponentUpdate=="function"?e.shouldComponentUpdate(o,s,i):t.prototype&&t.prototype.isPureReactComponent?!fs(r,o)||!fs(n,s):!0}function hf(e,t,r){var o=!1,n=$r,s=t.contextType;return typeof s=="object"&&s!==null?s=dt(s):(n=$e(t)?yo:Le.current,o=t.contextTypes,s=(o=o!=null)?wn(e,n):$r),t=new t(r,s),e.memoizedState=t.state!==null&&t.state!==void 0?t.state:null,t.updater=Ra,e.stateNode=t,t._reactInternals=e,o&&(e=e.stateNode,e.__reactInternalMemoizedUnmaskedChildContext=n,e.__reactInternalMemoizedMaskedChildContext=s),t}function Gp(e,t,r,o){e=t.state,typeof t.componentWillReceiveProps=="function"&&t.componentWillReceiveProps(r,o),typeof t.UNSAFE_componentWillReceiveProps=="function"&&t.UNSAFE_componentWillReceiveProps(r,o),t.state!==e&&Ra.enqueueReplaceState(t,t.state,null)}function Oc(e,t,r,o){var n=e.stateNode;n.props=r,n.state=e.memoizedState,n.refs={},Fd(e);var s=t.contextType;typeof s=="object"&&s!==null?n.context=dt(s):(s=$e(t)?yo:Le.current,n.context=wn(e,s)),n.state=e.memoizedState,s=t.getDerivedStateFromProps,typeof s=="function"&&(Lc(e,t,s,r),n.state=e.memoizedState),typeof t.getDerivedStateFromProps=="function"||typeof n.getSnapshotBeforeUpdate=="function"||typeof n.UNSAFE_componentWillMount!="function"&&typeof n.componentWillMount!="function"||(t=n.state,typeof n.componentWillMount=="function"&&n.componentWillMount(),typeof n.UNSAFE_componentWillMount=="function"&&n.UNSAFE_componentWillMount(),t!==n.state&&Ra.enqueueReplaceState(n,n.state,null),ta(e,r,n,o),n.state=e.memoizedState),typeof n.componentDidMount=="function"&&(e.flags|=4194308)}function Sn(e,t){try{var r="",o=t;do r+=Vb(o),o=o.return;while(o);var n=r}catch(s){n=`
Error generating stack: `+s.message+`
`+s.stack}return{value:e,source:t,stack:n,digest:null}}function _l(e,t,r){return{value:e,source:null,stack:r??null,digest:t??null}}function _c(e,t){try{console.error(t.value)}catch(r){setTimeout(function(){throw r})}}var gA=typeof WeakMap=="function"?WeakMap:Map;function ff(e,t,r){r=ir(-1,r),r.tag=3,r.payload={element:null};var o=t.value;return r.callback=function(){ia||(ia=!0,Qc=o),_c(e,t)},r}function vf(e,t,r){r=ir(-1,r),r.tag=3;var o=e.type.getDerivedStateFromError;if(typeof o=="function"){var n=t.value;r.payload=function(){return o(n)},r.callback=function(){_c(e,t)}}var s=e.stateNode;return s!==null&&typeof s.componentDidCatch=="function"&&(r.callback=function(){_c(e,t),typeof o!="function"&&(jr===null?jr=new Set([this]):jr.add(this));var i=t.stack;this.componentDidCatch(t.value,{componentStack:i!==null?i:""})}),r}function Fp(e,t,r){var o=e.pingCache;if(o===null){o=e.pingCache=new gA;var n=new Set;o.set(t,n)}else n=o.get(t),n===void 0&&(n=new Set,o.set(t,n));n.has(r)||(n.add(r),e=xA.bind(null,e,t,r),t.then(e,e))}function Kp(e){do{var t;if((t=e.tag===13)&&(t=e.memoizedState,t=t!==null?t.dehydrated!==null:!0),t)return e;e=e.return}while(e!==null);return null}function $p(e,t,r,o,n){return e.mode&1?(e.flags|=65536,e.lanes=n,e):(e===t?e.flags|=65536:(e.flags|=128,r.flags|=131072,r.flags&=-52805,r.tag===1&&(r.alternate===null?r.tag=17:(t=ir(-1,1),t.tag=2,Wr(r,t,1))),r.lanes|=1),e)}var mA=gr.ReactCurrentOwner,Fe=!1;function _e(e,t,r,o){t.child=e===null?Gh(t,null,r,o):kn(t,e.child,r,o)}function Qp(e,t,r,o,n){r=r.render;var s=t.ref;return on(t,n),o=Jd(e,t,r,o,s,n),r=Zd(),e!==null&&!Fe?(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~n,dr(e,t,n)):(ue&&r&&_d(t),t.flags|=1,_e(e,t,o,n),t.child)}function Yp(e,t,r,o,n){if(e===null){var s=r.type;return typeof s=="function"&&!lu(s)&&s.defaultProps===void 0&&r.compare===null&&r.defaultProps===void 0?(t.tag=15,t.type=s,yf(e,t,s,o,n)):(e=Ei(r.type,null,o,t,t.mode,n),e.ref=t.ref,e.return=t,t.child=e)}if(s=e.child,!(e.lanes&n)){var i=s.memoizedProps;if(r=r.compare,r=r!==null?r:fs,r(i,o)&&e.ref===t.ref)return dr(e,t,n)}return t.flags|=1,e=Gr(s,o),e.ref=t.ref,e.return=t,t.child=e}function yf(e,t,r,o,n){if(e!==null){var s=e.memoizedProps;if(fs(s,o)&&e.ref===t.ref)if(Fe=!1,t.pendingProps=o=s,(e.lanes&n)!==0)e.flags&131072&&(Fe=!0);else return t.lanes=e.lanes,dr(e,t,n)}return Vc(e,t,r,o,n)}function wf(e,t,r){var o=t.pendingProps,n=o.children,s=e!==null?e.memoizedState:null;if(o.mode==="hidden")if(!(t.mode&1))t.memoizedState={baseLanes:0,cachePool:null,transitions:null},se(Zo,Ye),Ye|=r;else{if(!(r&1073741824))return e=s!==null?s.baseLanes|r:r,t.lanes=t.childLanes=1073741824,t.memoizedState={baseLanes:e,cachePool:null,transitions:null},t.updateQueue=null,se(Zo,Ye),Ye|=e,null;t.memoizedState={baseLanes:0,cachePool:null,transitions:null},o=s!==null?s.baseLanes:r,se(Zo,Ye),Ye|=o}else s!==null?(o=s.baseLanes|r,t.memoizedState=null):o=r,se(Zo,Ye),Ye|=o;return _e(e,t,n,r),t.child}function bf(e,t){var r=t.ref;(e===null&&r!==null||e!==null&&e.ref!==r)&&(t.flags|=512,t.flags|=2097152)}function Vc(e,t,r,o,n){var s=$e(r)?yo:Le.current;return s=wn(t,s),on(t,n),r=Jd(e,t,r,o,s,n),o=Zd(),e!==null&&!Fe?(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~n,dr(e,t,n)):(ue&&o&&_d(t),t.flags|=1,_e(e,t,r,n),t.child)}function Jp(e,t,r,o,n){if($e(r)){var s=!0;Yi(t)}else s=!1;if(on(t,n),t.stateNode===null)Ri(e,t),hf(t,r,o),Oc(t,r,o,n),o=!0;else if(e===null){var i=t.stateNode,a=t.memoizedProps;i.props=a;var d=i.context,c=r.contextType;typeof c=="object"&&c!==null?c=dt(c):(c=$e(r)?yo:Le.current,c=wn(t,c));var p=r.getDerivedStateFromProps,u=typeof p=="function"||typeof i.getSnapshotBeforeUpdate=="function";u||typeof i.UNSAFE_componentWillReceiveProps!="function"&&typeof i.componentWillReceiveProps!="function"||(a!==o||d!==c)&&Gp(t,i,o,c),xr=!1;var f=t.memoizedState;i.state=f,ta(t,o,i,n),d=t.memoizedState,a!==o||f!==d||Ke.current||xr?(typeof p=="function"&&(Lc(t,r,p,o),d=t.memoizedState),(a=xr||Up(t,r,a,o,f,d,c))?(u||typeof i.UNSAFE_componentWillMount!="function"&&typeof i.componentWillMount!="function"||(typeof i.componentWillMount=="function"&&i.componentWillMount(),typeof i.UNSAFE_componentWillMount=="function"&&i.UNSAFE_componentWillMount()),typeof i.componentDidMount=="function"&&(t.flags|=4194308)):(typeof i.componentDidMount=="function"&&(t.flags|=4194308),t.memoizedProps=o,t.memoizedState=d),i.props=o,i.state=d,i.context=c,o=a):(typeof i.componentDidMount=="function"&&(t.flags|=4194308),o=!1)}else{i=t.stateNode,Kh(e,t),a=t.memoizedProps,c=t.type===t.elementType?a:At(t.type,a),i.props=c,u=t.pendingProps,f=i.context,d=r.contextType,typeof d=="object"&&d!==null?d=dt(d):(d=$e(r)?yo:Le.current,d=wn(t,d));var g=r.getDerivedStateFromProps;(p=typeof g=="function"||typeof i.getSnapshotBeforeUpdate=="function")||typeof i.UNSAFE_componentWillReceiveProps!="function"&&typeof i.componentWillReceiveProps!="function"||(a!==u||f!==d)&&Gp(t,i,o,d),xr=!1,f=t.memoizedState,i.state=f,ta(t,o,i,n);var k=t.memoizedState;a!==u||f!==k||Ke.current||xr?(typeof g=="function"&&(Lc(t,r,g,o),k=t.memoizedState),(c=xr||Up(t,r,c,o,f,k,d)||!1)?(p||typeof i.UNSAFE_componentWillUpdate!="function"&&typeof i.componentWillUpdate!="function"||(typeof i.componentWillUpdate=="function"&&i.componentWillUpdate(o,k,d),typeof i.UNSAFE_componentWillUpdate=="function"&&i.UNSAFE_componentWillUpdate(o,k,d)),typeof i.componentDidUpdate=="function"&&(t.flags|=4),typeof i.getSnapshotBeforeUpdate=="function"&&(t.flags|=1024)):(typeof i.componentDidUpdate!="function"||a===e.memoizedProps&&f===e.memoizedState||(t.flags|=4),typeof i.getSnapshotBeforeUpdate!="function"||a===e.memoizedProps&&f===e.memoizedState||(t.flags|=1024),t.memoizedProps=o,t.memoizedState=k),i.props=o,i.state=k,i.context=d,o=c):(typeof i.componentDidUpdate!="function"||a===e.memoizedProps&&f===e.memoizedState||(t.flags|=4),typeof i.getSnapshotBeforeUpdate!="function"||a===e.memoizedProps&&f===e.memoizedState||(t.flags|=1024),o=!1)}return Bc(e,t,r,o,s,n)}function Bc(e,t,r,o,n,s){bf(e,t);var i=(t.flags&128)!==0;if(!o&&!i)return n&&qp(t,r,!1),dr(e,t,s);o=t.stateNode,mA.current=t;var a=i&&typeof r.getDerivedStateFromError!="function"?null:o.render();return t.flags|=1,e!==null&&i?(t.child=kn(t,e.child,null,s),t.child=kn(t,null,a,s)):_e(e,t,a,s),t.memoizedState=o.state,n&&qp(t,r,!0),t.child}function kf(e){var t=e.stateNode;t.pendingContext?Np(e,t.pendingContext,t.pendingContext!==t.context):t.context&&Np(e,t.context,!1),Kd(e,t.containerInfo)}function Zp(e,t,r,o,n){return bn(),Bd(n),t.flags|=256,_e(e,t,r,o),t.child}var Wc={dehydrated:null,treeContext:null,retryLane:0};function jc(e){return{baseLanes:e,cachePool:null,transitions:null}}function Af(e,t,r){var o=t.pendingProps,n=pe.current,s=!1,i=(t.flags&128)!==0,a;if((a=i)||(a=e!==null&&e.memoizedState===null?!1:(n&2)!==0),a?(s=!0,t.flags&=-129):(e===null||e.memoizedState!==null)&&(n|=1),se(pe,n&1),e===null)return Nc(t),e=t.memoizedState,e!==null&&(e=e.dehydrated,e!==null)?(t.mode&1?e.data==="$!"?t.lanes=8:t.lanes=1073741824:t.lanes=1,null):(i=o.children,e=o.fallback,s?(o=t.mode,s=t.child,i={mode:"hidden",children:i},!(o&1)&&s!==null?(s.childLanes=0,s.pendingProps=i):s=Ea(i,o,0,null),e=vo(e,o,r,null),s.return=t,e.return=t,s.sibling=e,t.child=s,t.child.memoizedState=jc(r),t.memoizedState=Wc,e):tu(t,i));if(n=e.memoizedState,n!==null&&(a=n.dehydrated,a!==null))return hA(e,t,i,o,a,n,r);if(s){s=o.fallback,i=t.mode,n=e.child,a=n.sibling;var d={mode:"hidden",children:o.children};return!(i&1)&&t.child!==n?(o=t.child,o.childLanes=0,o.pendingProps=d,t.deletions=null):(o=Gr(n,d),o.subtreeFlags=n.subtreeFlags&14680064),a!==null?s=Gr(a,s):(s=vo(s,i,r,null),s.flags|=2),s.return=t,o.return=t,o.sibling=s,t.child=o,o=s,s=t.child,i=e.child.memoizedState,i=i===null?jc(r):{baseLanes:i.baseLanes|r,cachePool:null,transitions:i.transitions},s.memoizedState=i,s.childLanes=e.childLanes&~r,t.memoizedState=Wc,o}return s=e.child,e=s.sibling,o=Gr(s,{mode:"visible",children:o.children}),!(t.mode&1)&&(o.lanes=r),o.return=t,o.sibling=null,e!==null&&(r=t.deletions,r===null?(t.deletions=[e],t.flags|=16):r.push(e)),t.child=o,t.memoizedState=null,o}function tu(e,t){return t=Ea({mode:"visible",children:t},e.mode,0,null),t.return=e,e.child=t}function di(e,t,r,o){return o!==null&&Bd(o),kn(t,e.child,null,r),e=tu(t,t.pendingProps.children),e.flags|=2,t.memoizedState=null,e}function hA(e,t,r,o,n,s,i){if(r)return t.flags&256?(t.flags&=-257,o=_l(Error(R(422))),di(e,t,i,o)):t.memoizedState!==null?(t.child=e.child,t.flags|=128,null):(s=o.fallback,n=t.mode,o=Ea({mode:"visible",children:o.children},n,0,null),s=vo(s,n,i,null),s.flags|=2,o.return=t,s.return=t,o.sibling=s,t.child=o,t.mode&1&&kn(t,e.child,null,i),t.child.memoizedState=jc(i),t.memoizedState=Wc,s);if(!(t.mode&1))return di(e,t,i,null);if(n.data==="$!"){if(o=n.nextSibling&&n.nextSibling.dataset,o)var a=o.dgst;return o=a,s=Error(R(419)),o=_l(s,o,void 0),di(e,t,i,o)}if(a=(i&e.childLanes)!==0,Fe||a){if(o=Ie,o!==null){switch(i&-i){case 4:n=2;break;case 16:n=8;break;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:n=32;break;case 536870912:n=268435456;break;default:n=0}n=n&(o.suspendedLanes|i)?0:n,n!==0&&n!==s.retryLane&&(s.retryLane=n,cr(e,n),Tt(o,e,n,-1))}return au(),o=_l(Error(R(421))),di(e,t,i,o)}return n.data==="$?"?(t.flags|=128,t.child=e.child,t=DA.bind(null,e),n._reactRetry=t,null):(e=s.treeContext,Ze=Br(n.nextSibling),Xe=t,ue=!0,xt=null,e!==null&&(it[at++]=nr,it[at++]=sr,it[at++]=wo,nr=e.id,sr=e.overflow,wo=t),t=tu(t,o.children),t.flags|=4096,t)}function Xp(e,t,r){e.lanes|=t;var o=e.alternate;o!==null&&(o.lanes|=t),qc(e.return,t,r)}function Vl(e,t,r,o,n){var s=e.memoizedState;s===null?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:o,tail:r,tailMode:n}:(s.isBackwards=t,s.rendering=null,s.renderingStartTime=0,s.last=o,s.tail=r,s.tailMode=n)}function Sf(e,t,r){var o=t.pendingProps,n=o.revealOrder,s=o.tail;if(_e(e,t,o.children,r),o=pe.current,o&2)o=o&1|2,t.flags|=128;else{if(e!==null&&e.flags&128)e:for(e=t.child;e!==null;){if(e.tag===13)e.memoizedState!==null&&Xp(e,r,t);else if(e.tag===19)Xp(e,r,t);else if(e.child!==null){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;e.sibling===null;){if(e.return===null||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}o&=1}if(se(pe,o),!(t.mode&1))t.memoizedState=null;else switch(n){case"forwards":for(r=t.child,n=null;r!==null;)e=r.alternate,e!==null&&ra(e)===null&&(n=r),r=r.sibling;r=n,r===null?(n=t.child,t.child=null):(n=r.sibling,r.sibling=null),Vl(t,!1,n,r,s);break;case"backwards":for(r=null,n=t.child,t.child=null;n!==null;){if(e=n.alternate,e!==null&&ra(e)===null){t.child=n;break}e=n.sibling,n.sibling=r,r=n,n=e}Vl(t,!0,r,null,s);break;case"together":Vl(t,!1,null,null,void 0);break;default:t.memoizedState=null}return t.child}function Ri(e,t){!(t.mode&1)&&e!==null&&(e.alternate=null,t.alternate=null,t.flags|=2)}function dr(e,t,r){if(e!==null&&(t.dependencies=e.dependencies),ko|=t.lanes,!(r&t.childLanes))return null;if(e!==null&&t.child!==e.child)throw Error(R(153));if(t.child!==null){for(e=t.child,r=Gr(e,e.pendingProps),t.child=r,r.return=t;e.sibling!==null;)e=e.sibling,r=r.sibling=Gr(e,e.pendingProps),r.return=t;r.sibling=null}return t.child}function fA(e,t,r){switch(t.tag){case 3:kf(t),bn();break;case 5:$h(t);break;case 1:$e(t.type)&&Yi(t);break;case 4:Kd(t,t.stateNode.containerInfo);break;case 10:var o=t.type._context,n=t.memoizedProps.value;se(Xi,o._currentValue),o._currentValue=n;break;case 13:if(o=t.memoizedState,o!==null)return o.dehydrated!==null?(se(pe,pe.current&1),t.flags|=128,null):r&t.child.childLanes?Af(e,t,r):(se(pe,pe.current&1),e=dr(e,t,r),e!==null?e.sibling:null);se(pe,pe.current&1);break;case 19:if(o=(r&t.childLanes)!==0,e.flags&128){if(o)return Sf(e,t,r);t.flags|=128}if(n=t.memoizedState,n!==null&&(n.rendering=null,n.tail=null,n.lastEffect=null),se(pe,pe.current),o)break;return null;case 22:case 23:return t.lanes=0,wf(e,t,r)}return dr(e,t,r)}var If,Uc,Pf,Cf;If=function(e,t){for(var r=t.child;r!==null;){if(r.tag===5||r.tag===6)e.appendChild(r.stateNode);else if(r.tag!==4&&r.child!==null){r.child.return=r,r=r.child;continue}if(r===t)break;for(;r.sibling===null;){if(r.return===null||r.return===t)return;r=r.return}r.sibling.return=r.return,r=r.sibling}};Uc=function(){};Pf=function(e,t,r,o){var n=e.memoizedProps;if(n!==o){e=t.stateNode,lo(Kt.current);var s=null;switch(r){case"input":n=pc(e,n),o=pc(e,o),s=[];break;case"select":n=he({},n,{value:void 0}),o=he({},o,{value:void 0}),s=[];break;case"textarea":n=hc(e,n),o=hc(e,o),s=[];break;default:typeof n.onClick!="function"&&typeof o.onClick=="function"&&(e.onclick=$i)}vc(r,o);var i;r=null;for(c in n)if(!o.hasOwnProperty(c)&&n.hasOwnProperty(c)&&n[c]!=null)if(c==="style"){var a=n[c];for(i in a)a.hasOwnProperty(i)&&(r||(r={}),r[i]="")}else c!=="dangerouslySetInnerHTML"&&c!=="children"&&c!=="suppressContentEditableWarning"&&c!=="suppressHydrationWarning"&&c!=="autoFocus"&&(cs.hasOwnProperty(c)?s||(s=[]):(s=s||[]).push(c,null));for(c in o){var d=o[c];if(a=n!=null?n[c]:void 0,o.hasOwnProperty(c)&&d!==a&&(d!=null||a!=null))if(c==="style")if(a){for(i in a)!a.hasOwnProperty(i)||d&&d.hasOwnProperty(i)||(r||(r={}),r[i]="");for(i in d)d.hasOwnProperty(i)&&a[i]!==d[i]&&(r||(r={}),r[i]=d[i])}else r||(s||(s=[]),s.push(c,r)),r=d;else c==="dangerouslySetInnerHTML"?(d=d?d.__html:void 0,a=a?a.__html:void 0,d!=null&&a!==d&&(s=s||[]).push(c,d)):c==="children"?typeof d!="string"&&typeof d!="number"||(s=s||[]).push(c,""+d):c!=="suppressContentEditableWarning"&&c!=="suppressHydrationWarning"&&(cs.hasOwnProperty(c)?(d!=null&&c==="onScroll"&&le("scroll",e),s||a===d||(s=[])):(s=s||[]).push(c,d))}r&&(s=s||[]).push("style",r);var c=s;(t.updateQueue=c)&&(t.flags|=4)}};Cf=function(e,t,r,o){r!==o&&(t.flags|=4)};function Gn(e,t){if(!ue)switch(e.tailMode){case"hidden":t=e.tail;for(var r=null;t!==null;)t.alternate!==null&&(r=t),t=t.sibling;r===null?e.tail=null:r.sibling=null;break;case"collapsed":r=e.tail;for(var o=null;r!==null;)r.alternate!==null&&(o=r),r=r.sibling;o===null?t||e.tail===null?e.tail=null:e.tail.sibling=null:o.sibling=null}}function Me(e){var t=e.alternate!==null&&e.alternate.child===e.child,r=0,o=0;if(t)for(var n=e.child;n!==null;)r|=n.lanes|n.childLanes,o|=n.subtreeFlags&14680064,o|=n.flags&14680064,n.return=e,n=n.sibling;else for(n=e.child;n!==null;)r|=n.lanes|n.childLanes,o|=n.subtreeFlags,o|=n.flags,n.return=e,n=n.sibling;return e.subtreeFlags|=o,e.childLanes=r,t}function vA(e,t,r){var o=t.pendingProps;switch(Vd(t),t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return Me(t),null;case 1:return $e(t.type)&&Qi(),Me(t),null;case 3:return o=t.stateNode,An(),ce(Ke),ce(Le),Qd(),o.pendingContext&&(o.context=o.pendingContext,o.pendingContext=null),(e===null||e.child===null)&&(li(t)?t.flags|=4:e===null||e.memoizedState.isDehydrated&&!(t.flags&256)||(t.flags|=1024,xt!==null&&(Zc(xt),xt=null))),Uc(e,t),Me(t),null;case 5:$d(t);var n=lo(ks.current);if(r=t.type,e!==null&&t.stateNode!=null)Pf(e,t,r,o,n),e.ref!==t.ref&&(t.flags|=512,t.flags|=2097152);else{if(!o){if(t.stateNode===null)throw Error(R(166));return Me(t),null}if(e=lo(Kt.current),li(t)){o=t.stateNode,r=t.type;var s=t.memoizedProps;switch(o[jt]=t,o[ws]=s,e=(t.mode&1)!==0,r){case"dialog":le("cancel",o),le("close",o);break;case"iframe":case"object":case"embed":le("load",o);break;case"video":case"audio":for(n=0;n<Zn.length;n++)le(Zn[n],o);break;case"source":le("error",o);break;case"img":case"image":case"link":le("error",o),le("load",o);break;case"details":le("toggle",o);break;case"input":lp(o,s),le("invalid",o);break;case"select":o._wrapperState={wasMultiple:!!s.multiple},le("invalid",o);break;case"textarea":dp(o,s),le("invalid",o)}vc(r,s),n=null;for(var i in s)if(s.hasOwnProperty(i)){var a=s[i];i==="children"?typeof a=="string"?o.textContent!==a&&(s.suppressHydrationWarning!==!0&&ai(o.textContent,a,e),n=["children",a]):typeof a=="number"&&o.textContent!==""+a&&(s.suppressHydrationWarning!==!0&&ai(o.textContent,a,e),n=["children",""+a]):cs.hasOwnProperty(i)&&a!=null&&i==="onScroll"&&le("scroll",o)}switch(r){case"input":Xs(o),cp(o,s,!0);break;case"textarea":Xs(o),up(o);break;case"select":case"option":break;default:typeof s.onClick=="function"&&(o.onclick=$i)}o=n,t.updateQueue=o,o!==null&&(t.flags|=4)}else{i=n.nodeType===9?n:n.ownerDocument,e==="http://www.w3.org/1999/xhtml"&&(e=Xm(r)),e==="http://www.w3.org/1999/xhtml"?r==="script"?(e=i.createElement("div"),e.innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):typeof o.is=="string"?e=i.createElement(r,{is:o.is}):(e=i.createElement(r),r==="select"&&(i=e,o.multiple?i.multiple=!0:o.size&&(i.size=o.size))):e=i.createElementNS(e,r),e[jt]=t,e[ws]=o,If(e,t,!1,!1),t.stateNode=e;e:{switch(i=yc(r,o),r){case"dialog":le("cancel",e),le("close",e),n=o;break;case"iframe":case"object":case"embed":le("load",e),n=o;break;case"video":case"audio":for(n=0;n<Zn.length;n++)le(Zn[n],e);n=o;break;case"source":le("error",e),n=o;break;case"img":case"image":case"link":le("error",e),le("load",e),n=o;break;case"details":le("toggle",e),n=o;break;case"input":lp(e,o),n=pc(e,o),le("invalid",e);break;case"option":n=o;break;case"select":e._wrapperState={wasMultiple:!!o.multiple},n=he({},o,{value:void 0}),le("invalid",e);break;case"textarea":dp(e,o),n=hc(e,o),le("invalid",e);break;default:n=o}vc(r,n),a=n;for(s in a)if(a.hasOwnProperty(s)){var d=a[s];s==="style"?rh(e,d):s==="dangerouslySetInnerHTML"?(d=d?d.__html:void 0,d!=null&&eh(e,d)):s==="children"?typeof d=="string"?(r!=="textarea"||d!=="")&&ds(e,d):typeof d=="number"&&ds(e,""+d):s!=="suppressContentEditableWarning"&&s!=="suppressHydrationWarning"&&s!=="autoFocus"&&(cs.hasOwnProperty(s)?d!=null&&s==="onScroll"&&le("scroll",e):d!=null&&Pd(e,s,d,i))}switch(r){case"input":Xs(e),cp(e,o,!1);break;case"textarea":Xs(e),up(e);break;case"option":o.value!=null&&e.setAttribute("value",""+Kr(o.value));break;case"select":e.multiple=!!o.multiple,s=o.value,s!=null?Xo(e,!!o.multiple,s,!1):o.defaultValue!=null&&Xo(e,!!o.multiple,o.defaultValue,!0);break;default:typeof n.onClick=="function"&&(e.onclick=$i)}switch(r){case"button":case"input":case"select":case"textarea":o=!!o.autoFocus;break e;case"img":o=!0;break e;default:o=!1}}o&&(t.flags|=4)}t.ref!==null&&(t.flags|=512,t.flags|=2097152)}return Me(t),null;case 6:if(e&&t.stateNode!=null)Cf(e,t,e.memoizedProps,o);else{if(typeof o!="string"&&t.stateNode===null)throw Error(R(166));if(r=lo(ks.current),lo(Kt.current),li(t)){if(o=t.stateNode,r=t.memoizedProps,o[jt]=t,(s=o.nodeValue!==r)&&(e=Xe,e!==null))switch(e.tag){case 3:ai(o.nodeValue,r,(e.mode&1)!==0);break;case 5:e.memoizedProps.suppressHydrationWarning!==!0&&ai(o.nodeValue,r,(e.mode&1)!==0)}s&&(t.flags|=4)}else o=(r.nodeType===9?r:r.ownerDocument).createTextNode(o),o[jt]=t,t.stateNode=o}return Me(t),null;case 13:if(ce(pe),o=t.memoizedState,e===null||e.memoizedState!==null&&e.memoizedState.dehydrated!==null){if(ue&&Ze!==null&&t.mode&1&&!(t.flags&128))jh(),bn(),t.flags|=98560,s=!1;else if(s=li(t),o!==null&&o.dehydrated!==null){if(e===null){if(!s)throw Error(R(318));if(s=t.memoizedState,s=s!==null?s.dehydrated:null,!s)throw Error(R(317));s[jt]=t}else bn(),!(t.flags&128)&&(t.memoizedState=null),t.flags|=4;Me(t),s=!1}else xt!==null&&(Zc(xt),xt=null),s=!0;if(!s)return t.flags&65536?t:null}return t.flags&128?(t.lanes=r,t):(o=o!==null,o!==(e!==null&&e.memoizedState!==null)&&o&&(t.child.flags|=8192,t.mode&1&&(e===null||pe.current&1?Ae===0&&(Ae=3):au())),t.updateQueue!==null&&(t.flags|=4),Me(t),null);case 4:return An(),Uc(e,t),e===null&&vs(t.stateNode.containerInfo),Me(t),null;case 10:return Ud(t.type._context),Me(t),null;case 17:return $e(t.type)&&Qi(),Me(t),null;case 19:if(ce(pe),s=t.memoizedState,s===null)return Me(t),null;if(o=(t.flags&128)!==0,i=s.rendering,i===null)if(o)Gn(s,!1);else{if(Ae!==0||e!==null&&e.flags&128)for(e=t.child;e!==null;){if(i=ra(e),i!==null){for(t.flags|=128,Gn(s,!1),o=i.updateQueue,o!==null&&(t.updateQueue=o,t.flags|=4),t.subtreeFlags=0,o=r,r=t.child;r!==null;)s=r,e=o,s.flags&=14680066,i=s.alternate,i===null?(s.childLanes=0,s.lanes=e,s.child=null,s.subtreeFlags=0,s.memoizedProps=null,s.memoizedState=null,s.updateQueue=null,s.dependencies=null,s.stateNode=null):(s.childLanes=i.childLanes,s.lanes=i.lanes,s.child=i.child,s.subtreeFlags=0,s.deletions=null,s.memoizedProps=i.memoizedProps,s.memoizedState=i.memoizedState,s.updateQueue=i.updateQueue,s.type=i.type,e=i.dependencies,s.dependencies=e===null?null:{lanes:e.lanes,firstContext:e.firstContext}),r=r.sibling;return se(pe,pe.current&1|2),t.child}e=e.sibling}s.tail!==null&&ye()>In&&(t.flags|=128,o=!0,Gn(s,!1),t.lanes=4194304)}else{if(!o)if(e=ra(i),e!==null){if(t.flags|=128,o=!0,r=e.updateQueue,r!==null&&(t.updateQueue=r,t.flags|=4),Gn(s,!0),s.tail===null&&s.tailMode==="hidden"&&!i.alternate&&!ue)return Me(t),null}else 2*ye()-s.renderingStartTime>In&&r!==1073741824&&(t.flags|=128,o=!0,Gn(s,!1),t.lanes=4194304);s.isBackwards?(i.sibling=t.child,t.child=i):(r=s.last,r!==null?r.sibling=i:t.child=i,s.last=i)}return s.tail!==null?(t=s.tail,s.rendering=t,s.tail=t.sibling,s.renderingStartTime=ye(),t.sibling=null,r=pe.current,se(pe,o?r&1|2:r&1),t):(Me(t),null);case 22:case 23:return iu(),o=t.memoizedState!==null,e!==null&&e.memoizedState!==null!==o&&(t.flags|=8192),o&&t.mode&1?Ye&1073741824&&(Me(t),t.subtreeFlags&6&&(t.flags|=8192)):Me(t),null;case 24:return null;case 25:return null}throw Error(R(156,t.tag))}function yA(e,t){switch(Vd(t),t.tag){case 1:return $e(t.type)&&Qi(),e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 3:return An(),ce(Ke),ce(Le),Qd(),e=t.flags,e&65536&&!(e&128)?(t.flags=e&-65537|128,t):null;case 5:return $d(t),null;case 13:if(ce(pe),e=t.memoizedState,e!==null&&e.dehydrated!==null){if(t.alternate===null)throw Error(R(340));bn()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 19:return ce(pe),null;case 4:return An(),null;case 10:return Ud(t.type._context),null;case 22:case 23:return iu(),null;case 24:return null;default:return null}}var ui=!1,qe=!1,wA=typeof WeakSet=="function"?WeakSet:Set,q=null;function Jo(e,t){var r=e.ref;if(r!==null)if(typeof r=="function")try{r(null)}catch(o){ve(e,t,o)}else r.current=null}function Gc(e,t,r){try{r()}catch(o){ve(e,t,o)}}var eg=!1;function bA(e,t){if(Dc=Gi,e=zh(),Od(e)){if("selectionStart"in e)var r={start:e.selectionStart,end:e.selectionEnd};else e:{r=(r=e.ownerDocument)&&r.defaultView||window;var o=r.getSelection&&r.getSelection();if(o&&o.rangeCount!==0){r=o.anchorNode;var n=o.anchorOffset,s=o.focusNode;o=o.focusOffset;try{r.nodeType,s.nodeType}catch{r=null;break e}var i=0,a=-1,d=-1,c=0,p=0,u=e,f=null;t:for(;;){for(var g;u!==r||n!==0&&u.nodeType!==3||(a=i+n),u!==s||o!==0&&u.nodeType!==3||(d=i+o),u.nodeType===3&&(i+=u.nodeValue.length),(g=u.firstChild)!==null;)f=u,u=g;for(;;){if(u===e)break t;if(f===r&&++c===n&&(a=i),f===s&&++p===o&&(d=i),(g=u.nextSibling)!==null)break;u=f,f=u.parentNode}u=g}r=a===-1||d===-1?null:{start:a,end:d}}else r=null}r=r||{start:0,end:0}}else r=null;for(Tc={focusedElem:e,selectionRange:r},Gi=!1,q=t;q!==null;)if(t=q,e=t.child,(t.subtreeFlags&1028)!==0&&e!==null)e.return=t,q=e;else for(;q!==null;){t=q;try{var k=t.alternate;if(t.flags&1024)switch(t.tag){case 0:case 11:case 15:break;case 1:if(k!==null){var y=k.memoizedProps,b=k.memoizedState,v=t.stateNode,h=v.getSnapshotBeforeUpdate(t.elementType===t.type?y:At(t.type,y),b);v.__reactInternalSnapshotBeforeUpdate=h}break;case 3:var w=t.stateNode.containerInfo;w.nodeType===1?w.textContent="":w.nodeType===9&&w.documentElement&&w.removeChild(w.documentElement);break;case 5:case 6:case 4:case 17:break;default:throw Error(R(163))}}catch(A){ve(t,t.return,A)}if(e=t.sibling,e!==null){e.return=t.return,q=e;break}q=t.return}return k=eg,eg=!1,k}function ss(e,t,r){var o=t.updateQueue;if(o=o!==null?o.lastEffect:null,o!==null){var n=o=o.next;do{if((n.tag&e)===e){var s=n.destroy;n.destroy=void 0,s!==void 0&&Gc(t,r,s)}n=n.next}while(n!==o)}}function Ha(e,t){if(t=t.updateQueue,t=t!==null?t.lastEffect:null,t!==null){var r=t=t.next;do{if((r.tag&e)===e){var o=r.create;r.destroy=o()}r=r.next}while(r!==t)}}function Fc(e){var t=e.ref;if(t!==null){var r=e.stateNode;switch(e.tag){case 5:e=r;break;default:e=r}typeof t=="function"?t(e):t.current=e}}function xf(e){var t=e.alternate;t!==null&&(e.alternate=null,xf(t)),e.child=null,e.deletions=null,e.sibling=null,e.tag===5&&(t=e.stateNode,t!==null&&(delete t[jt],delete t[ws],delete t[Hc],delete t[rA],delete t[oA])),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}function Df(e){return e.tag===5||e.tag===3||e.tag===4}function tg(e){e:for(;;){for(;e.sibling===null;){if(e.return===null||Df(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;e.tag!==5&&e.tag!==6&&e.tag!==18;){if(e.flags&2||e.child===null||e.tag===4)continue e;e.child.return=e,e=e.child}if(!(e.flags&2))return e.stateNode}}function Kc(e,t,r){var o=e.tag;if(o===5||o===6)e=e.stateNode,t?r.nodeType===8?r.parentNode.insertBefore(e,t):r.insertBefore(e,t):(r.nodeType===8?(t=r.parentNode,t.insertBefore(e,r)):(t=r,t.appendChild(e)),r=r._reactRootContainer,r!=null||t.onclick!==null||(t.onclick=$i));else if(o!==4&&(e=e.child,e!==null))for(Kc(e,t,r),e=e.sibling;e!==null;)Kc(e,t,r),e=e.sibling}function $c(e,t,r){var o=e.tag;if(o===5||o===6)e=e.stateNode,t?r.insertBefore(e,t):r.appendChild(e);else if(o!==4&&(e=e.child,e!==null))for($c(e,t,r),e=e.sibling;e!==null;)$c(e,t,r),e=e.sibling}var Ce=null,Ct=!1;function kr(e,t,r){for(r=r.child;r!==null;)Tf(e,t,r),r=r.sibling}function Tf(e,t,r){if(Ft&&typeof Ft.onCommitFiberUnmount=="function")try{Ft.onCommitFiberUnmount(Ia,r)}catch{}switch(r.tag){case 5:qe||Jo(r,t);case 6:var o=Ce,n=Ct;Ce=null,kr(e,t,r),Ce=o,Ct=n,Ce!==null&&(Ct?(e=Ce,r=r.stateNode,e.nodeType===8?e.parentNode.removeChild(r):e.removeChild(r)):Ce.removeChild(r.stateNode));break;case 18:Ce!==null&&(Ct?(e=Ce,r=r.stateNode,e.nodeType===8?Ml(e.parentNode,r):e.nodeType===1&&Ml(e,r),ms(e)):Ml(Ce,r.stateNode));break;case 4:o=Ce,n=Ct,Ce=r.stateNode.containerInfo,Ct=!0,kr(e,t,r),Ce=o,Ct=n;break;case 0:case 11:case 14:case 15:if(!qe&&(o=r.updateQueue,o!==null&&(o=o.lastEffect,o!==null))){n=o=o.next;do{var s=n,i=s.destroy;s=s.tag,i!==void 0&&(s&2||s&4)&&Gc(r,t,i),n=n.next}while(n!==o)}kr(e,t,r);break;case 1:if(!qe&&(Jo(r,t),o=r.stateNode,typeof o.componentWillUnmount=="function"))try{o.props=r.memoizedProps,o.state=r.memoizedState,o.componentWillUnmount()}catch(a){ve(r,t,a)}kr(e,t,r);break;case 21:kr(e,t,r);break;case 22:r.mode&1?(qe=(o=qe)||r.memoizedState!==null,kr(e,t,r),qe=o):kr(e,t,r);break;default:kr(e,t,r)}}function rg(e){var t=e.updateQueue;if(t!==null){e.updateQueue=null;var r=e.stateNode;r===null&&(r=e.stateNode=new wA),t.forEach(function(o){var n=TA.bind(null,e,o);r.has(o)||(r.add(o),o.then(n,n))})}}function bt(e,t){var r=t.deletions;if(r!==null)for(var o=0;o<r.length;o++){var n=r[o];try{var s=e,i=t,a=i;e:for(;a!==null;){switch(a.tag){case 5:Ce=a.stateNode,Ct=!1;break e;case 3:Ce=a.stateNode.containerInfo,Ct=!0;break e;case 4:Ce=a.stateNode.containerInfo,Ct=!0;break e}a=a.return}if(Ce===null)throw Error(R(160));Tf(s,i,n),Ce=null,Ct=!1;var d=n.alternate;d!==null&&(d.return=null),n.return=null}catch(c){ve(n,t,c)}}if(t.subtreeFlags&12854)for(t=t.child;t!==null;)zf(t,e),t=t.sibling}function zf(e,t){var r=e.alternate,o=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:if(bt(t,e),Ot(e),o&4){try{ss(3,e,e.return),Ha(3,e)}catch(y){ve(e,e.return,y)}try{ss(5,e,e.return)}catch(y){ve(e,e.return,y)}}break;case 1:bt(t,e),Ot(e),o&512&&r!==null&&Jo(r,r.return);break;case 5:if(bt(t,e),Ot(e),o&512&&r!==null&&Jo(r,r.return),e.flags&32){var n=e.stateNode;try{ds(n,"")}catch(y){ve(e,e.return,y)}}if(o&4&&(n=e.stateNode,n!=null)){var s=e.memoizedProps,i=r!==null?r.memoizedProps:s,a=e.type,d=e.updateQueue;if(e.updateQueue=null,d!==null)try{a==="input"&&s.type==="radio"&&s.name!=null&&Jm(n,s),yc(a,i);var c=yc(a,s);for(i=0;i<d.length;i+=2){var p=d[i],u=d[i+1];p==="style"?rh(n,u):p==="dangerouslySetInnerHTML"?eh(n,u):p==="children"?ds(n,u):Pd(n,p,u,c)}switch(a){case"input":gc(n,s);break;case"textarea":Zm(n,s);break;case"select":var f=n._wrapperState.wasMultiple;n._wrapperState.wasMultiple=!!s.multiple;var g=s.value;g!=null?Xo(n,!!s.multiple,g,!1):f!==!!s.multiple&&(s.defaultValue!=null?Xo(n,!!s.multiple,s.defaultValue,!0):Xo(n,!!s.multiple,s.multiple?[]:"",!1))}n[ws]=s}catch(y){ve(e,e.return,y)}}break;case 6:if(bt(t,e),Ot(e),o&4){if(e.stateNode===null)throw Error(R(162));n=e.stateNode,s=e.memoizedProps;try{n.nodeValue=s}catch(y){ve(e,e.return,y)}}break;case 3:if(bt(t,e),Ot(e),o&4&&r!==null&&r.memoizedState.isDehydrated)try{ms(t.containerInfo)}catch(y){ve(e,e.return,y)}break;case 4:bt(t,e),Ot(e);break;case 13:bt(t,e),Ot(e),n=e.child,n.flags&8192&&(s=n.memoizedState!==null,n.stateNode.isHidden=s,!s||n.alternate!==null&&n.alternate.memoizedState!==null||(nu=ye())),o&4&&rg(e);break;case 22:if(p=r!==null&&r.memoizedState!==null,e.mode&1?(qe=(c=qe)||p,bt(t,e),qe=c):bt(t,e),Ot(e),o&8192){if(c=e.memoizedState!==null,(e.stateNode.isHidden=c)&&!p&&e.mode&1)for(q=e,p=e.child;p!==null;){for(u=q=p;q!==null;){switch(f=q,g=f.child,f.tag){case 0:case 11:case 14:case 15:ss(4,f,f.return);break;case 1:Jo(f,f.return);var k=f.stateNode;if(typeof k.componentWillUnmount=="function"){o=f,r=f.return;try{t=o,k.props=t.memoizedProps,k.state=t.memoizedState,k.componentWillUnmount()}catch(y){ve(o,r,y)}}break;case 5:Jo(f,f.return);break;case 22:if(f.memoizedState!==null){ng(u);continue}}g!==null?(g.return=f,q=g):ng(u)}p=p.sibling}e:for(p=null,u=e;;){if(u.tag===5){if(p===null){p=u;try{n=u.stateNode,c?(s=n.style,typeof s.setProperty=="function"?s.setProperty("display","none","important"):s.display="none"):(a=u.stateNode,d=u.memoizedProps.style,i=d!=null&&d.hasOwnProperty("display")?d.display:null,a.style.display=th("display",i))}catch(y){ve(e,e.return,y)}}}else if(u.tag===6){if(p===null)try{u.stateNode.nodeValue=c?"":u.memoizedProps}catch(y){ve(e,e.return,y)}}else if((u.tag!==22&&u.tag!==23||u.memoizedState===null||u===e)&&u.child!==null){u.child.return=u,u=u.child;continue}if(u===e)break e;for(;u.sibling===null;){if(u.return===null||u.return===e)break e;p===u&&(p=null),u=u.return}p===u&&(p=null),u.sibling.return=u.return,u=u.sibling}}break;case 19:bt(t,e),Ot(e),o&4&&rg(e);break;case 21:break;default:bt(t,e),Ot(e)}}function Ot(e){var t=e.flags;if(t&2){try{e:{for(var r=e.return;r!==null;){if(Df(r)){var o=r;break e}r=r.return}throw Error(R(160))}switch(o.tag){case 5:var n=o.stateNode;o.flags&32&&(ds(n,""),o.flags&=-33);var s=tg(e);$c(e,s,n);break;case 3:case 4:var i=o.stateNode.containerInfo,a=tg(e);Kc(e,a,i);break;default:throw Error(R(161))}}catch(d){ve(e,e.return,d)}e.flags&=-3}t&4096&&(e.flags&=-4097)}function kA(e,t,r){q=e,Rf(e)}function Rf(e,t,r){for(var o=(e.mode&1)!==0;q!==null;){var n=q,s=n.child;if(n.tag===22&&o){var i=n.memoizedState!==null||ui;if(!i){var a=n.alternate,d=a!==null&&a.memoizedState!==null||qe;a=ui;var c=qe;if(ui=i,(qe=d)&&!c)for(q=n;q!==null;)i=q,d=i.child,i.tag===22&&i.memoizedState!==null?sg(n):d!==null?(d.return=i,q=d):sg(n);for(;s!==null;)q=s,Rf(s),s=s.sibling;q=n,ui=a,qe=c}og(e)}else n.subtreeFlags&8772&&s!==null?(s.return=n,q=s):og(e)}}function og(e){for(;q!==null;){var t=q;if(t.flags&8772){var r=t.alternate;try{if(t.flags&8772)switch(t.tag){case 0:case 11:case 15:qe||Ha(5,t);break;case 1:var o=t.stateNode;if(t.flags&4&&!qe)if(r===null)o.componentDidMount();else{var n=t.elementType===t.type?r.memoizedProps:At(t.type,r.memoizedProps);o.componentDidUpdate(n,r.memoizedState,o.__reactInternalSnapshotBeforeUpdate)}var s=t.updateQueue;s!==null&&Bp(t,s,o);break;case 3:var i=t.updateQueue;if(i!==null){if(r=null,t.child!==null)switch(t.child.tag){case 5:r=t.child.stateNode;break;case 1:r=t.child.stateNode}Bp(t,i,r)}break;case 5:var a=t.stateNode;if(r===null&&t.flags&4){r=a;var d=t.memoizedProps;switch(t.type){case"button":case"input":case"select":case"textarea":d.autoFocus&&r.focus();break;case"img":d.src&&(r.src=d.src)}}break;case 6:break;case 4:break;case 12:break;case 13:if(t.memoizedState===null){var c=t.alternate;if(c!==null){var p=c.memoizedState;if(p!==null){var u=p.dehydrated;u!==null&&ms(u)}}}break;case 19:case 17:case 21:case 22:case 23:case 25:break;default:throw Error(R(163))}qe||t.flags&512&&Fc(t)}catch(f){ve(t,t.return,f)}}if(t===e){q=null;break}if(r=t.sibling,r!==null){r.return=t.return,q=r;break}q=t.return}}function ng(e){for(;q!==null;){var t=q;if(t===e){q=null;break}var r=t.sibling;if(r!==null){r.return=t.return,q=r;break}q=t.return}}function sg(e){for(;q!==null;){var t=q;try{switch(t.tag){case 0:case 11:case 15:var r=t.return;try{Ha(4,t)}catch(d){ve(t,r,d)}break;case 1:var o=t.stateNode;if(typeof o.componentDidMount=="function"){var n=t.return;try{o.componentDidMount()}catch(d){ve(t,n,d)}}var s=t.return;try{Fc(t)}catch(d){ve(t,s,d)}break;case 5:var i=t.return;try{Fc(t)}catch(d){ve(t,i,d)}}}catch(d){ve(t,t.return,d)}if(t===e){q=null;break}var a=t.sibling;if(a!==null){a.return=t.return,q=a;break}q=t.return}}var AA=Math.ceil,sa=gr.ReactCurrentDispatcher,ru=gr.ReactCurrentOwner,ct=gr.ReactCurrentBatchConfig,X=0,Ie=null,be=null,xe=0,Ye=0,Zo=Zr(0),Ae=0,Ps=null,ko=0,Ma=0,ou=0,is=null,Ge=null,nu=0,In=1/0,tr=null,ia=!1,Qc=null,jr=null,pi=!1,qr=null,aa=0,as=0,Yc=null,Hi=-1,Mi=0;function Be(){return X&6?ye():Hi!==-1?Hi:Hi=ye()}function Ur(e){return e.mode&1?X&2&&xe!==0?xe&-xe:sA.transition!==null?(Mi===0&&(Mi=mh()),Mi):(e=re,e!==0||(e=window.event,e=e===void 0?16:kh(e.type)),e):1}function Tt(e,t,r,o){if(50<as)throw as=0,Yc=null,Error(R(185));Os(e,r,o),(!(X&2)||e!==Ie)&&(e===Ie&&(!(X&2)&&(Ma|=r),Ae===4&&Tr(e,xe)),Qe(e,o),r===1&&X===0&&!(t.mode&1)&&(In=ye()+500,Ta&&Xr()))}function Qe(e,t){var r=e.callbackNode;sk(e,t);var o=Ui(e,e===Ie?xe:0);if(o===0)r!==null&&mp(r),e.callbackNode=null,e.callbackPriority=0;else if(t=o&-o,e.callbackPriority!==t){if(r!=null&&mp(r),t===1)e.tag===0?nA(ig.bind(null,e)):Vh(ig.bind(null,e)),eA(function(){!(X&6)&&Xr()}),r=null;else{switch(hh(o)){case 1:r=zd;break;case 4:r=ph;break;case 16:r=ji;break;case 536870912:r=gh;break;default:r=ji}r=_f(r,Hf.bind(null,e))}e.callbackPriority=t,e.callbackNode=r}}function Hf(e,t){if(Hi=-1,Mi=0,X&6)throw Error(R(327));var r=e.callbackNode;if(nn()&&e.callbackNode!==r)return null;var o=Ui(e,e===Ie?xe:0);if(o===0)return null;if(o&30||o&e.expiredLanes||t)t=la(e,o);else{t=o;var n=X;X|=2;var s=Ef();(Ie!==e||xe!==t)&&(tr=null,In=ye()+500,fo(e,t));do try{PA();break}catch(a){Mf(e,a)}while(!0);jd(),sa.current=s,X=n,be!==null?t=0:(Ie=null,xe=0,t=Ae)}if(t!==0){if(t===2&&(n=Sc(e),n!==0&&(o=n,t=Jc(e,n))),t===1)throw r=Ps,fo(e,0),Tr(e,o),Qe(e,ye()),r;if(t===6)Tr(e,o);else{if(n=e.current.alternate,!(o&30)&&!SA(n)&&(t=la(e,o),t===2&&(s=Sc(e),s!==0&&(o=s,t=Jc(e,s))),t===1))throw r=Ps,fo(e,0),Tr(e,o),Qe(e,ye()),r;switch(e.finishedWork=n,e.finishedLanes=o,t){case 0:case 1:throw Error(R(345));case 2:so(e,Ge,tr);break;case 3:if(Tr(e,o),(o&130023424)===o&&(t=nu+500-ye(),10<t)){if(Ui(e,0)!==0)break;if(n=e.suspendedLanes,(n&o)!==o){Be(),e.pingedLanes|=e.suspendedLanes&n;break}e.timeoutHandle=Rc(so.bind(null,e,Ge,tr),t);break}so(e,Ge,tr);break;case 4:if(Tr(e,o),(o&4194240)===o)break;for(t=e.eventTimes,n=-1;0<o;){var i=31-Dt(o);s=1<<i,i=t[i],i>n&&(n=i),o&=~s}if(o=n,o=ye()-o,o=(120>o?120:480>o?480:1080>o?1080:1920>o?1920:3e3>o?3e3:4320>o?4320:1960*AA(o/1960))-o,10<o){e.timeoutHandle=Rc(so.bind(null,e,Ge,tr),o);break}so(e,Ge,tr);break;case 5:so(e,Ge,tr);break;default:throw Error(R(329))}}}return Qe(e,ye()),e.callbackNode===r?Hf.bind(null,e):null}function Jc(e,t){var r=is;return e.current.memoizedState.isDehydrated&&(fo(e,t).flags|=256),e=la(e,t),e!==2&&(t=Ge,Ge=r,t!==null&&Zc(t)),e}function Zc(e){Ge===null?Ge=e:Ge.push.apply(Ge,e)}function SA(e){for(var t=e;;){if(t.flags&16384){var r=t.updateQueue;if(r!==null&&(r=r.stores,r!==null))for(var o=0;o<r.length;o++){var n=r[o],s=n.getSnapshot;n=n.value;try{if(!zt(s(),n))return!1}catch{return!1}}}if(r=t.child,t.subtreeFlags&16384&&r!==null)r.return=t,t=r;else{if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}function Tr(e,t){for(t&=~ou,t&=~Ma,e.suspendedLanes|=t,e.pingedLanes&=~t,e=e.expirationTimes;0<t;){var r=31-Dt(t),o=1<<r;e[r]=-1,t&=~o}}function ig(e){if(X&6)throw Error(R(327));nn();var t=Ui(e,0);if(!(t&1))return Qe(e,ye()),null;var r=la(e,t);if(e.tag!==0&&r===2){var o=Sc(e);o!==0&&(t=o,r=Jc(e,o))}if(r===1)throw r=Ps,fo(e,0),Tr(e,t),Qe(e,ye()),r;if(r===6)throw Error(R(345));return e.finishedWork=e.current.alternate,e.finishedLanes=t,so(e,Ge,tr),Qe(e,ye()),null}function su(e,t){var r=X;X|=1;try{return e(t)}finally{X=r,X===0&&(In=ye()+500,Ta&&Xr())}}function Ao(e){qr!==null&&qr.tag===0&&!(X&6)&&nn();var t=X;X|=1;var r=ct.transition,o=re;try{if(ct.transition=null,re=1,e)return e()}finally{re=o,ct.transition=r,X=t,!(X&6)&&Xr()}}function iu(){Ye=Zo.current,ce(Zo)}function fo(e,t){e.finishedWork=null,e.finishedLanes=0;var r=e.timeoutHandle;if(r!==-1&&(e.timeoutHandle=-1,Xk(r)),be!==null)for(r=be.return;r!==null;){var o=r;switch(Vd(o),o.tag){case 1:o=o.type.childContextTypes,o!=null&&Qi();break;case 3:An(),ce(Ke),ce(Le),Qd();break;case 5:$d(o);break;case 4:An();break;case 13:ce(pe);break;case 19:ce(pe);break;case 10:Ud(o.type._context);break;case 22:case 23:iu()}r=r.return}if(Ie=e,be=e=Gr(e.current,null),xe=Ye=t,Ae=0,Ps=null,ou=Ma=ko=0,Ge=is=null,ao!==null){for(t=0;t<ao.length;t++)if(r=ao[t],o=r.interleaved,o!==null){r.interleaved=null;var n=o.next,s=r.pending;if(s!==null){var i=s.next;s.next=n,o.next=i}r.pending=o}ao=null}return e}function Mf(e,t){do{var r=be;try{if(jd(),Ti.current=na,oa){for(var o=ge.memoizedState;o!==null;){var n=o.queue;n!==null&&(n.pending=null),o=o.next}oa=!1}if(bo=0,Se=ke=ge=null,ns=!1,As=0,ru.current=null,r===null||r.return===null){Ae=1,Ps=t,be=null;break}e:{var s=e,i=r.return,a=r,d=t;if(t=xe,a.flags|=32768,d!==null&&typeof d=="object"&&typeof d.then=="function"){var c=d,p=a,u=p.tag;if(!(p.mode&1)&&(u===0||u===11||u===15)){var f=p.alternate;f?(p.updateQueue=f.updateQueue,p.memoizedState=f.memoizedState,p.lanes=f.lanes):(p.updateQueue=null,p.memoizedState=null)}var g=Kp(i);if(g!==null){g.flags&=-257,$p(g,i,a,s,t),g.mode&1&&Fp(s,c,t),t=g,d=c;var k=t.updateQueue;if(k===null){var y=new Set;y.add(d),t.updateQueue=y}else k.add(d);break e}else{if(!(t&1)){Fp(s,c,t),au();break e}d=Error(R(426))}}else if(ue&&a.mode&1){var b=Kp(i);if(b!==null){!(b.flags&65536)&&(b.flags|=256),$p(b,i,a,s,t),Bd(Sn(d,a));break e}}s=d=Sn(d,a),Ae!==4&&(Ae=2),is===null?is=[s]:is.push(s),s=i;do{switch(s.tag){case 3:s.flags|=65536,t&=-t,s.lanes|=t;var v=ff(s,d,t);Vp(s,v);break e;case 1:a=d;var h=s.type,w=s.stateNode;if(!(s.flags&128)&&(typeof h.getDerivedStateFromError=="function"||w!==null&&typeof w.componentDidCatch=="function"&&(jr===null||!jr.has(w)))){s.flags|=65536,t&=-t,s.lanes|=t;var A=vf(s,a,t);Vp(s,A);break e}}s=s.return}while(s!==null)}qf(r)}catch(S){t=S,be===r&&r!==null&&(be=r=r.return);continue}break}while(!0)}function Ef(){var e=sa.current;return sa.current=na,e===null?na:e}function au(){(Ae===0||Ae===3||Ae===2)&&(Ae=4),Ie===null||!(ko&268435455)&&!(Ma&268435455)||Tr(Ie,xe)}function la(e,t){var r=X;X|=2;var o=Ef();(Ie!==e||xe!==t)&&(tr=null,fo(e,t));do try{IA();break}catch(n){Mf(e,n)}while(!0);if(jd(),X=r,sa.current=o,be!==null)throw Error(R(261));return Ie=null,xe=0,Ae}function IA(){for(;be!==null;)Nf(be)}function PA(){for(;be!==null&&!Yb();)Nf(be)}function Nf(e){var t=Of(e.alternate,e,Ye);e.memoizedProps=e.pendingProps,t===null?qf(e):be=t,ru.current=null}function qf(e){var t=e;do{var r=t.alternate;if(e=t.return,t.flags&32768){if(r=yA(r,t),r!==null){r.flags&=32767,be=r;return}if(e!==null)e.flags|=32768,e.subtreeFlags=0,e.deletions=null;else{Ae=6,be=null;return}}else if(r=vA(r,t,Ye),r!==null){be=r;return}if(t=t.sibling,t!==null){be=t;return}be=t=e}while(t!==null);Ae===0&&(Ae=5)}function so(e,t,r){var o=re,n=ct.transition;try{ct.transition=null,re=1,CA(e,t,r,o)}finally{ct.transition=n,re=o}return null}function CA(e,t,r,o){do nn();while(qr!==null);if(X&6)throw Error(R(327));r=e.finishedWork;var n=e.finishedLanes;if(r===null)return null;if(e.finishedWork=null,e.finishedLanes=0,r===e.current)throw Error(R(177));e.callbackNode=null,e.callbackPriority=0;var s=r.lanes|r.childLanes;if(ik(e,s),e===Ie&&(be=Ie=null,xe=0),!(r.subtreeFlags&2064)&&!(r.flags&2064)||pi||(pi=!0,_f(ji,function(){return nn(),null})),s=(r.flags&15990)!==0,r.subtreeFlags&15990||s){s=ct.transition,ct.transition=null;var i=re;re=1;var a=X;X|=4,ru.current=null,bA(e,r),zf(r,e),Fk(Tc),Gi=!!Dc,Tc=Dc=null,e.current=r,kA(r),Jb(),X=a,re=i,ct.transition=s}else e.current=r;if(pi&&(pi=!1,qr=e,aa=n),s=e.pendingLanes,s===0&&(jr=null),ek(r.stateNode),Qe(e,ye()),t!==null)for(o=e.onRecoverableError,r=0;r<t.length;r++)n=t[r],o(n.value,{componentStack:n.stack,digest:n.digest});if(ia)throw ia=!1,e=Qc,Qc=null,e;return aa&1&&e.tag!==0&&nn(),s=e.pendingLanes,s&1?e===Yc?as++:(as=0,Yc=e):as=0,Xr(),null}function nn(){if(qr!==null){var e=hh(aa),t=ct.transition,r=re;try{if(ct.transition=null,re=16>e?16:e,qr===null)var o=!1;else{if(e=qr,qr=null,aa=0,X&6)throw Error(R(331));var n=X;for(X|=4,q=e.current;q!==null;){var s=q,i=s.child;if(q.flags&16){var a=s.deletions;if(a!==null){for(var d=0;d<a.length;d++){var c=a[d];for(q=c;q!==null;){var p=q;switch(p.tag){case 0:case 11:case 15:ss(8,p,s)}var u=p.child;if(u!==null)u.return=p,q=u;else for(;q!==null;){p=q;var f=p.sibling,g=p.return;if(xf(p),p===c){q=null;break}if(f!==null){f.return=g,q=f;break}q=g}}}var k=s.alternate;if(k!==null){var y=k.child;if(y!==null){k.child=null;do{var b=y.sibling;y.sibling=null,y=b}while(y!==null)}}q=s}}if(s.subtreeFlags&2064&&i!==null)i.return=s,q=i;else e:for(;q!==null;){if(s=q,s.flags&2048)switch(s.tag){case 0:case 11:case 15:ss(9,s,s.return)}var v=s.sibling;if(v!==null){v.return=s.return,q=v;break e}q=s.return}}var h=e.current;for(q=h;q!==null;){i=q;var w=i.child;if(i.subtreeFlags&2064&&w!==null)w.return=i,q=w;else e:for(i=h;q!==null;){if(a=q,a.flags&2048)try{switch(a.tag){case 0:case 11:case 15:Ha(9,a)}}catch(S){ve(a,a.return,S)}if(a===i){q=null;break e}var A=a.sibling;if(A!==null){A.return=a.return,q=A;break e}q=a.return}}if(X=n,Xr(),Ft&&typeof Ft.onPostCommitFiberRoot=="function")try{Ft.onPostCommitFiberRoot(Ia,e)}catch{}o=!0}return o}finally{re=r,ct.transition=t}}return!1}function ag(e,t,r){t=Sn(r,t),t=ff(e,t,1),e=Wr(e,t,1),t=Be(),e!==null&&(Os(e,1,t),Qe(e,t))}function ve(e,t,r){if(e.tag===3)ag(e,e,r);else for(;t!==null;){if(t.tag===3){ag(t,e,r);break}else if(t.tag===1){var o=t.stateNode;if(typeof t.type.getDerivedStateFromError=="function"||typeof o.componentDidCatch=="function"&&(jr===null||!jr.has(o))){e=Sn(r,e),e=vf(t,e,1),t=Wr(t,e,1),e=Be(),t!==null&&(Os(t,1,e),Qe(t,e));break}}t=t.return}}function xA(e,t,r){var o=e.pingCache;o!==null&&o.delete(t),t=Be(),e.pingedLanes|=e.suspendedLanes&r,Ie===e&&(xe&r)===r&&(Ae===4||Ae===3&&(xe&130023424)===xe&&500>ye()-nu?fo(e,0):ou|=r),Qe(e,t)}function Lf(e,t){t===0&&(e.mode&1?(t=ri,ri<<=1,!(ri&130023424)&&(ri=4194304)):t=1);var r=Be();e=cr(e,t),e!==null&&(Os(e,t,r),Qe(e,r))}function DA(e){var t=e.memoizedState,r=0;t!==null&&(r=t.retryLane),Lf(e,r)}function TA(e,t){var r=0;switch(e.tag){case 13:var o=e.stateNode,n=e.memoizedState;n!==null&&(r=n.retryLane);break;case 19:o=e.stateNode;break;default:throw Error(R(314))}o!==null&&o.delete(t),Lf(e,r)}var Of;Of=function(e,t,r){if(e!==null)if(e.memoizedProps!==t.pendingProps||Ke.current)Fe=!0;else{if(!(e.lanes&r)&&!(t.flags&128))return Fe=!1,fA(e,t,r);Fe=!!(e.flags&131072)}else Fe=!1,ue&&t.flags&1048576&&Bh(t,Zi,t.index);switch(t.lanes=0,t.tag){case 2:var o=t.type;Ri(e,t),e=t.pendingProps;var n=wn(t,Le.current);on(t,r),n=Jd(null,t,o,e,n,r);var s=Zd();return t.flags|=1,typeof n=="object"&&n!==null&&typeof n.render=="function"&&n.$$typeof===void 0?(t.tag=1,t.memoizedState=null,t.updateQueue=null,$e(o)?(s=!0,Yi(t)):s=!1,t.memoizedState=n.state!==null&&n.state!==void 0?n.state:null,Fd(t),n.updater=Ra,t.stateNode=n,n._reactInternals=t,Oc(t,o,e,r),t=Bc(null,t,o,!0,s,r)):(t.tag=0,ue&&s&&_d(t),_e(null,t,n,r),t=t.child),t;case 16:o=t.elementType;e:{switch(Ri(e,t),e=t.pendingProps,n=o._init,o=n(o._payload),t.type=o,n=t.tag=RA(o),e=At(o,e),n){case 0:t=Vc(null,t,o,e,r);break e;case 1:t=Jp(null,t,o,e,r);break e;case 11:t=Qp(null,t,o,e,r);break e;case 14:t=Yp(null,t,o,At(o.type,e),r);break e}throw Error(R(306,o,""))}return t;case 0:return o=t.type,n=t.pendingProps,n=t.elementType===o?n:At(o,n),Vc(e,t,o,n,r);case 1:return o=t.type,n=t.pendingProps,n=t.elementType===o?n:At(o,n),Jp(e,t,o,n,r);case 3:e:{if(kf(t),e===null)throw Error(R(387));o=t.pendingProps,s=t.memoizedState,n=s.element,Kh(e,t),ta(t,o,null,r);var i=t.memoizedState;if(o=i.element,s.isDehydrated)if(s={element:o,isDehydrated:!1,cache:i.cache,pendingSuspenseBoundaries:i.pendingSuspenseBoundaries,transitions:i.transitions},t.updateQueue.baseState=s,t.memoizedState=s,t.flags&256){n=Sn(Error(R(423)),t),t=Zp(e,t,o,r,n);break e}else if(o!==n){n=Sn(Error(R(424)),t),t=Zp(e,t,o,r,n);break e}else for(Ze=Br(t.stateNode.containerInfo.firstChild),Xe=t,ue=!0,xt=null,r=Gh(t,null,o,r),t.child=r;r;)r.flags=r.flags&-3|4096,r=r.sibling;else{if(bn(),o===n){t=dr(e,t,r);break e}_e(e,t,o,r)}t=t.child}return t;case 5:return $h(t),e===null&&Nc(t),o=t.type,n=t.pendingProps,s=e!==null?e.memoizedProps:null,i=n.children,zc(o,n)?i=null:s!==null&&zc(o,s)&&(t.flags|=32),bf(e,t),_e(e,t,i,r),t.child;case 6:return e===null&&Nc(t),null;case 13:return Af(e,t,r);case 4:return Kd(t,t.stateNode.containerInfo),o=t.pendingProps,e===null?t.child=kn(t,null,o,r):_e(e,t,o,r),t.child;case 11:return o=t.type,n=t.pendingProps,n=t.elementType===o?n:At(o,n),Qp(e,t,o,n,r);case 7:return _e(e,t,t.pendingProps,r),t.child;case 8:return _e(e,t,t.pendingProps.children,r),t.child;case 12:return _e(e,t,t.pendingProps.children,r),t.child;case 10:e:{if(o=t.type._context,n=t.pendingProps,s=t.memoizedProps,i=n.value,se(Xi,o._currentValue),o._currentValue=i,s!==null)if(zt(s.value,i)){if(s.children===n.children&&!Ke.current){t=dr(e,t,r);break e}}else for(s=t.child,s!==null&&(s.return=t);s!==null;){var a=s.dependencies;if(a!==null){i=s.child;for(var d=a.firstContext;d!==null;){if(d.context===o){if(s.tag===1){d=ir(-1,r&-r),d.tag=2;var c=s.updateQueue;if(c!==null){c=c.shared;var p=c.pending;p===null?d.next=d:(d.next=p.next,p.next=d),c.pending=d}}s.lanes|=r,d=s.alternate,d!==null&&(d.lanes|=r),qc(s.return,r,t),a.lanes|=r;break}d=d.next}}else if(s.tag===10)i=s.type===t.type?null:s.child;else if(s.tag===18){if(i=s.return,i===null)throw Error(R(341));i.lanes|=r,a=i.alternate,a!==null&&(a.lanes|=r),qc(i,r,t),i=s.sibling}else i=s.child;if(i!==null)i.return=s;else for(i=s;i!==null;){if(i===t){i=null;break}if(s=i.sibling,s!==null){s.return=i.return,i=s;break}i=i.return}s=i}_e(e,t,n.children,r),t=t.child}return t;case 9:return n=t.type,o=t.pendingProps.children,on(t,r),n=dt(n),o=o(n),t.flags|=1,_e(e,t,o,r),t.child;case 14:return o=t.type,n=At(o,t.pendingProps),n=At(o.type,n),Yp(e,t,o,n,r);case 15:return yf(e,t,t.type,t.pendingProps,r);case 17:return o=t.type,n=t.pendingProps,n=t.elementType===o?n:At(o,n),Ri(e,t),t.tag=1,$e(o)?(e=!0,Yi(t)):e=!1,on(t,r),hf(t,o,n),Oc(t,o,n,r),Bc(null,t,o,!0,e,r);case 19:return Sf(e,t,r);case 22:return wf(e,t,r)}throw Error(R(156,t.tag))};function _f(e,t){return uh(e,t)}function zA(e,t,r,o){this.tag=e,this.key=r,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=o,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function lt(e,t,r,o){return new zA(e,t,r,o)}function lu(e){return e=e.prototype,!(!e||!e.isReactComponent)}function RA(e){if(typeof e=="function")return lu(e)?1:0;if(e!=null){if(e=e.$$typeof,e===xd)return 11;if(e===Dd)return 14}return 2}function Gr(e,t){var r=e.alternate;return r===null?(r=lt(e.tag,t,e.key,e.mode),r.elementType=e.elementType,r.type=e.type,r.stateNode=e.stateNode,r.alternate=e,e.alternate=r):(r.pendingProps=t,r.type=e.type,r.flags=0,r.subtreeFlags=0,r.deletions=null),r.flags=e.flags&14680064,r.childLanes=e.childLanes,r.lanes=e.lanes,r.child=e.child,r.memoizedProps=e.memoizedProps,r.memoizedState=e.memoizedState,r.updateQueue=e.updateQueue,t=e.dependencies,r.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext},r.sibling=e.sibling,r.index=e.index,r.ref=e.ref,r}function Ei(e,t,r,o,n,s){var i=2;if(o=e,typeof e=="function")lu(e)&&(i=1);else if(typeof e=="string")i=5;else e:switch(e){case Wo:return vo(r.children,n,s,t);case Cd:i=8,n|=8;break;case lc:return e=lt(12,r,t,n|2),e.elementType=lc,e.lanes=s,e;case cc:return e=lt(13,r,t,n),e.elementType=cc,e.lanes=s,e;case dc:return e=lt(19,r,t,n),e.elementType=dc,e.lanes=s,e;case $m:return Ea(r,n,s,t);default:if(typeof e=="object"&&e!==null)switch(e.$$typeof){case Fm:i=10;break e;case Km:i=9;break e;case xd:i=11;break e;case Dd:i=14;break e;case Cr:i=16,o=null;break e}throw Error(R(130,e==null?e:typeof e,""))}return t=lt(i,r,t,n),t.elementType=e,t.type=o,t.lanes=s,t}function vo(e,t,r,o){return e=lt(7,e,o,t),e.lanes=r,e}function Ea(e,t,r,o){return e=lt(22,e,o,t),e.elementType=$m,e.lanes=r,e.stateNode={isHidden:!1},e}function Bl(e,t,r){return e=lt(6,e,null,t),e.lanes=r,e}function Wl(e,t,r){return t=lt(4,e.children!==null?e.children:[],e.key,t),t.lanes=r,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function HA(e,t,r,o,n){this.tag=t,this.containerInfo=e,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.pendingContext=this.context=null,this.callbackPriority=0,this.eventTimes=Al(0),this.expirationTimes=Al(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=Al(0),this.identifierPrefix=o,this.onRecoverableError=n,this.mutableSourceEagerHydrationData=null}function cu(e,t,r,o,n,s,i,a,d){return e=new HA(e,t,r,a,d),t===1?(t=1,s===!0&&(t|=8)):t=0,s=lt(3,null,null,t),e.current=s,s.stateNode=e,s.memoizedState={element:o,isDehydrated:r,cache:null,transitions:null,pendingSuspenseBoundaries:null},Fd(s),e}function MA(e,t,r){var o=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:Bo,key:o==null?null:""+o,children:e,containerInfo:t,implementation:r}}function Vf(e){if(!e)return $r;e=e._reactInternals;e:{if(Co(e)!==e||e.tag!==1)throw Error(R(170));var t=e;do{switch(t.tag){case 3:t=t.stateNode.context;break e;case 1:if($e(t.type)){t=t.stateNode.__reactInternalMemoizedMergedChildContext;break e}}t=t.return}while(t!==null);throw Error(R(171))}if(e.tag===1){var r=e.type;if($e(r))return _h(e,r,t)}return t}function Bf(e,t,r,o,n,s,i,a,d){return e=cu(r,o,!0,e,n,s,i,a,d),e.context=Vf(null),r=e.current,o=Be(),n=Ur(r),s=ir(o,n),s.callback=t??null,Wr(r,s,n),e.current.lanes=n,Os(e,n,o),Qe(e,o),e}function Na(e,t,r,o){var n=t.current,s=Be(),i=Ur(n);return r=Vf(r),t.context===null?t.context=r:t.pendingContext=r,t=ir(s,i),t.payload={element:e},o=o===void 0?null:o,o!==null&&(t.callback=o),e=Wr(n,t,i),e!==null&&(Tt(e,n,i,s),Di(e,n,i)),i}function ca(e){if(e=e.current,!e.child)return null;switch(e.child.tag){case 5:return e.child.stateNode;default:return e.child.stateNode}}function lg(e,t){if(e=e.memoizedState,e!==null&&e.dehydrated!==null){var r=e.retryLane;e.retryLane=r!==0&&r<t?r:t}}function du(e,t){lg(e,t),(e=e.alternate)&&lg(e,t)}function EA(){return null}var Wf=typeof reportError=="function"?reportError:function(e){console.error(e)};function uu(e){this._internalRoot=e}qa.prototype.render=uu.prototype.render=function(e){var t=this._internalRoot;if(t===null)throw Error(R(409));Na(e,t,null,null)};qa.prototype.unmount=uu.prototype.unmount=function(){var e=this._internalRoot;if(e!==null){this._internalRoot=null;var t=e.containerInfo;Ao(function(){Na(null,e,null,null)}),t[lr]=null}};function qa(e){this._internalRoot=e}qa.prototype.unstable_scheduleHydration=function(e){if(e){var t=yh();e={blockedOn:null,target:e,priority:t};for(var r=0;r<Dr.length&&t!==0&&t<Dr[r].priority;r++);Dr.splice(r,0,e),r===0&&bh(e)}};function pu(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11)}function La(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11&&(e.nodeType!==8||e.nodeValue!==" react-mount-point-unstable "))}function cg(){}function NA(e,t,r,o,n){if(n){if(typeof o=="function"){var s=o;o=function(){var c=ca(i);s.call(c)}}var i=Bf(t,o,e,0,null,!1,!1,"",cg);return e._reactRootContainer=i,e[lr]=i.current,vs(e.nodeType===8?e.parentNode:e),Ao(),i}for(;n=e.lastChild;)e.removeChild(n);if(typeof o=="function"){var a=o;o=function(){var c=ca(d);a.call(c)}}var d=cu(e,0,!1,null,null,!1,!1,"",cg);return e._reactRootContainer=d,e[lr]=d.current,vs(e.nodeType===8?e.parentNode:e),Ao(function(){Na(t,d,r,o)}),d}function Oa(e,t,r,o,n){var s=r._reactRootContainer;if(s){var i=s;if(typeof n=="function"){var a=n;n=function(){var d=ca(i);a.call(d)}}Na(t,i,e,n)}else i=NA(r,t,e,n,o);return ca(i)}fh=function(e){switch(e.tag){case 3:var t=e.stateNode;if(t.current.memoizedState.isDehydrated){var r=Jn(t.pendingLanes);r!==0&&(Rd(t,r|1),Qe(t,ye()),!(X&6)&&(In=ye()+500,Xr()))}break;case 13:Ao(function(){var o=cr(e,1);if(o!==null){var n=Be();Tt(o,e,1,n)}}),du(e,1)}};Hd=function(e){if(e.tag===13){var t=cr(e,134217728);if(t!==null){var r=Be();Tt(t,e,134217728,r)}du(e,134217728)}};vh=function(e){if(e.tag===13){var t=Ur(e),r=cr(e,t);if(r!==null){var o=Be();Tt(r,e,t,o)}du(e,t)}};yh=function(){return re};wh=function(e,t){var r=re;try{return re=e,t()}finally{re=r}};bc=function(e,t,r){switch(t){case"input":if(gc(e,r),t=r.name,r.type==="radio"&&t!=null){for(r=e;r.parentNode;)r=r.parentNode;for(r=r.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<r.length;t++){var o=r[t];if(o!==e&&o.form===e.form){var n=Da(o);if(!n)throw Error(R(90));Ym(o),gc(o,n)}}}break;case"textarea":Zm(e,r);break;case"select":t=r.value,t!=null&&Xo(e,!!r.multiple,t,!1)}};sh=su;ih=Ao;var qA={usingClientEntryPoint:!1,Events:[Vs,Fo,Da,oh,nh,su]},Fn={findFiberByHostInstance:io,bundleType:0,version:"18.3.1",rendererPackageName:"react-dom"},LA={bundleType:Fn.bundleType,version:Fn.version,rendererPackageName:Fn.rendererPackageName,rendererConfig:Fn.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setErrorHandler:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:gr.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return e=ch(e),e===null?null:e.stateNode},findFiberByHostInstance:Fn.findFiberByHostInstance||EA,findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null,reconcilerVersion:"18.3.1-next-f1338f8080-20240426"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var gi=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!gi.isDisabled&&gi.supportsFiber)try{Ia=gi.inject(LA),Ft=gi}catch{}}rt.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=qA;rt.createPortal=function(e,t){var r=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!pu(t))throw Error(R(200));return MA(e,t,null,r)};rt.createRoot=function(e,t){if(!pu(e))throw Error(R(299));var r=!1,o="",n=Wf;return t!=null&&(t.unstable_strictMode===!0&&(r=!0),t.identifierPrefix!==void 0&&(o=t.identifierPrefix),t.onRecoverableError!==void 0&&(n=t.onRecoverableError)),t=cu(e,1,!1,null,null,r,!1,o,n),e[lr]=t.current,vs(e.nodeType===8?e.parentNode:e),new uu(t)};rt.findDOMNode=function(e){if(e==null)return null;if(e.nodeType===1)return e;var t=e._reactInternals;if(t===void 0)throw typeof e.render=="function"?Error(R(188)):(e=Object.keys(e).join(","),Error(R(268,e)));return e=ch(t),e=e===null?null:e.stateNode,e};rt.flushSync=function(e){return Ao(e)};rt.hydrate=function(e,t,r){if(!La(t))throw Error(R(200));return Oa(null,e,t,!0,r)};rt.hydrateRoot=function(e,t,r){if(!pu(e))throw Error(R(405));var o=r!=null&&r.hydratedSources||null,n=!1,s="",i=Wf;if(r!=null&&(r.unstable_strictMode===!0&&(n=!0),r.identifierPrefix!==void 0&&(s=r.identifierPrefix),r.onRecoverableError!==void 0&&(i=r.onRecoverableError)),t=Bf(t,null,e,1,r??null,n,!1,s,i),e[lr]=t.current,vs(e),o)for(e=0;e<o.length;e++)r=o[e],n=r._getVersion,n=n(r._source),t.mutableSourceEagerHydrationData==null?t.mutableSourceEagerHydrationData=[r,n]:t.mutableSourceEagerHydrationData.push(r,n);return new qa(t)};rt.render=function(e,t,r){if(!La(t))throw Error(R(200));return Oa(null,e,t,!1,r)};rt.unmountComponentAtNode=function(e){if(!La(e))throw Error(R(40));return e._reactRootContainer?(Ao(function(){Oa(null,null,e,!1,function(){e._reactRootContainer=null,e[lr]=null})}),!0):!1};rt.unstable_batchedUpdates=su;rt.unstable_renderSubtreeIntoContainer=function(e,t,r,o){if(!La(r))throw Error(R(200));if(e==null||e._reactInternals===void 0)throw Error(R(38));return Oa(e,t,r,!1,o)};rt.version="18.3.1-next-f1338f8080-20240426";function jf(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(jf)}catch(e){console.error(e)}}jf(),Wm.exports=rt;var Ws=Wm.exports;const Uf=zm(Ws);var Gf,dg=Ws;Gf=dg.createRoot,dg.hydrateRoot;const OA=1,_A=1e6;let jl=0;function VA(){return jl=(jl+1)%Number.MAX_SAFE_INTEGER,jl.toString()}const Ul=new Map,ug=e=>{if(Ul.has(e))return;const t=setTimeout(()=>{Ul.delete(e),ls({type:"REMOVE_TOAST",toastId:e})},_A);Ul.set(e,t)},BA=(e,t)=>{switch(t.type){case"ADD_TOAST":return{...e,toasts:[t.toast,...e.toasts].slice(0,OA)};case"UPDATE_TOAST":return{...e,toasts:e.toasts.map(r=>r.id===t.toast.id?{...r,...t.toast}:r)};case"DISMISS_TOAST":{const{toastId:r}=t;return r?ug(r):e.toasts.forEach(o=>{ug(o.id)}),{...e,toasts:e.toasts.map(o=>o.id===r||r===void 0?{...o,open:!1}:o)}}case"REMOVE_TOAST":return t.toastId===void 0?{...e,toasts:[]}:{...e,toasts:e.toasts.filter(r=>r.id!==t.toastId)}}},Ni=[];let qi={toasts:[]};function ls(e){qi=BA(qi,e),Ni.forEach(t=>{t(qi)})}function WA({...e}){const t=VA(),r=n=>ls({type:"UPDATE_TOAST",toast:{...n,id:t}}),o=()=>ls({type:"DISMISS_TOAST",toastId:t});return ls({type:"ADD_TOAST",toast:{...e,id:t,open:!0,onOpenChange:n=>{n||o()}}}),{id:t,dismiss:o,update:r}}function jA(){const[e,t]=m.useState(qi);return m.useEffect(()=>(Ni.push(t),()=>{const r=Ni.indexOf(t);r>-1&&Ni.splice(r,1)}),[e]),{...e,toast:WA,dismiss:r=>ls({type:"DISMISS_TOAST",toastId:r})}}function ne(e,t,{checkForDefaultPrevented:r=!0}={}){return function(n){if(e==null||e(n),r===!1||!n.defaultPrevented)return t==null?void 0:t(n)}}function pg(e,t){if(typeof e=="function")return e(t);e!=null&&(e.current=t)}function Ff(...e){return t=>{let r=!1;const o=e.map(n=>{const s=pg(n,t);return!r&&typeof s=="function"&&(r=!0),s});if(r)return()=>{for(let n=0;n<o.length;n++){const s=o[n];typeof s=="function"?s():pg(e[n],null)}}}}function Te(...e){return m.useCallback(Ff(...e),e)}function UA(e,t){const r=m.createContext(t),o=s=>{const{children:i,...a}=s,d=m.useMemo(()=>a,Object.values(a));return l.jsx(r.Provider,{value:d,children:i})};o.displayName=e+"Provider";function n(s){const i=m.useContext(r);if(i)return i;if(t!==void 0)return t;throw new Error(`\`${s}\` must be used within \`${e}\``)}return[o,n]}function xo(e,t=[]){let r=[];function o(s,i){const a=m.createContext(i),d=r.length;r=[...r,i];const c=u=>{var v;const{scope:f,children:g,...k}=u,y=((v=f==null?void 0:f[e])==null?void 0:v[d])||a,b=m.useMemo(()=>k,Object.values(k));return l.jsx(y.Provider,{value:b,children:g})};c.displayName=s+"Provider";function p(u,f){var y;const g=((y=f==null?void 0:f[e])==null?void 0:y[d])||a,k=m.useContext(g);if(k)return k;if(i!==void 0)return i;throw new Error(`\`${u}\` must be used within \`${s}\``)}return[c,p]}const n=()=>{const s=r.map(i=>m.createContext(i));return function(a){const d=(a==null?void 0:a[e])||s;return m.useMemo(()=>({[`__scope${e}`]:{...a,[e]:d}}),[a,d])}};return n.scopeName=e,[o,GA(n,...t)]}function GA(...e){const t=e[0];if(e.length===1)return t;const r=()=>{const o=e.map(n=>({useScope:n(),scopeName:n.scopeName}));return function(s){const i=o.reduce((a,{useScope:d,scopeName:c})=>{const u=d(s)[`__scope${c}`];return{...a,...u}},{});return m.useMemo(()=>({[`__scope${t.scopeName}`]:i}),[i])}};return r.scopeName=t.scopeName,r}function Cs(e){const t=FA(e),r=m.forwardRef((o,n)=>{const{children:s,...i}=o,a=m.Children.toArray(s),d=a.find($A);if(d){const c=d.props.children,p=a.map(u=>u===d?m.Children.count(c)>1?m.Children.only(null):m.isValidElement(c)?c.props.children:null:u);return l.jsx(t,{...i,ref:n,children:m.isValidElement(c)?m.cloneElement(c,void 0,p):null})}return l.jsx(t,{...i,ref:n,children:s})});return r.displayName=`${e}.Slot`,r}var Rn=Cs("Slot");function FA(e){const t=m.forwardRef((r,o)=>{const{children:n,...s}=r;if(m.isValidElement(n)){const i=YA(n),a=QA(s,n.props);return n.type!==m.Fragment&&(a.ref=o?Ff(o,i):i),m.cloneElement(n,a)}return m.Children.count(n)>1?m.Children.only(null):null});return t.displayName=`${e}.SlotClone`,t}var Kf=Symbol("radix.slottable");function KA(e){const t=({children:r})=>l.jsx(l.Fragment,{children:r});return t.displayName=`${e}.Slottable`,t.__radixId=Kf,t}function $A(e){return m.isValidElement(e)&&typeof e.type=="function"&&"__radixId"in e.type&&e.type.__radixId===Kf}function QA(e,t){const r={...t};for(const o in t){const n=e[o],s=t[o];/^on[A-Z]/.test(o)?n&&s?r[o]=(...a)=>{const d=s(...a);return n(...a),d}:n&&(r[o]=n):o==="style"?r[o]={...n,...s}:o==="className"&&(r[o]=[n,s].filter(Boolean).join(" "))}return{...e,...r}}function YA(e){var o,n;let t=(o=Object.getOwnPropertyDescriptor(e.props,"ref"))==null?void 0:o.get,r=t&&"isReactWarning"in t&&t.isReactWarning;return r?e.ref:(t=(n=Object.getOwnPropertyDescriptor(e,"ref"))==null?void 0:n.get,r=t&&"isReactWarning"in t&&t.isReactWarning,r?e.props.ref:e.props.ref||e.ref)}function $f(e){const t=e+"CollectionProvider",[r,o]=xo(t),[n,s]=r(t,{collectionRef:{current:null},itemMap:new Map}),i=y=>{const{scope:b,children:v}=y,h=z.useRef(null),w=z.useRef(new Map).current;return l.jsx(n,{scope:b,itemMap:w,collectionRef:h,children:v})};i.displayName=t;const a=e+"CollectionSlot",d=Cs(a),c=z.forwardRef((y,b)=>{const{scope:v,children:h}=y,w=s(a,v),A=Te(b,w.collectionRef);return l.jsx(d,{ref:A,children:h})});c.displayName=a;const p=e+"CollectionItemSlot",u="data-radix-collection-item",f=Cs(p),g=z.forwardRef((y,b)=>{const{scope:v,children:h,...w}=y,A=z.useRef(null),S=Te(b,A),I=s(p,v);return z.useEffect(()=>(I.itemMap.set(A,{ref:A,...w}),()=>void I.itemMap.delete(A))),l.jsx(f,{[u]:"",ref:S,children:h})});g.displayName=p;function k(y){const b=s(e+"CollectionConsumer",y);return z.useCallback(()=>{const h=b.collectionRef.current;if(!h)return[];const w=Array.from(h.querySelectorAll(`[${u}]`));return Array.from(b.itemMap.values()).sort((I,P)=>w.indexOf(I.ref.current)-w.indexOf(P.ref.current))},[b.collectionRef,b.itemMap])}return[{Provider:i,Slot:c,ItemSlot:g},k,o]}var JA=["a","button","div","form","h2","h3","img","input","label","li","nav","ol","p","select","span","svg","ul"],ie=JA.reduce((e,t)=>{const r=Cs(`Primitive.${t}`),o=m.forwardRef((n,s)=>{const{asChild:i,...a}=n,d=i?r:t;return typeof window<"u"&&(window[Symbol.for("radix-ui")]=!0),l.jsx(d,{...a,ref:s})});return o.displayName=`Primitive.${t}`,{...e,[t]:o}},{});function Qf(e,t){e&&Ws.flushSync(()=>e.dispatchEvent(t))}function Qt(e){const t=m.useRef(e);return m.useEffect(()=>{t.current=e}),m.useMemo(()=>(...r)=>{var o;return(o=t.current)==null?void 0:o.call(t,...r)},[])}function ZA(e,t=globalThis==null?void 0:globalThis.document){const r=Qt(e);m.useEffect(()=>{const o=n=>{n.key==="Escape"&&r(n)};return t.addEventListener("keydown",o,{capture:!0}),()=>t.removeEventListener("keydown",o,{capture:!0})},[r,t])}var XA="DismissableLayer",Xc="dismissableLayer.update",eS="dismissableLayer.pointerDownOutside",tS="dismissableLayer.focusOutside",gg,Yf=m.createContext({layers:new Set,layersWithOutsidePointerEventsDisabled:new Set,branches:new Set}),_a=m.forwardRef((e,t)=>{const{disableOutsidePointerEvents:r=!1,onEscapeKeyDown:o,onPointerDownOutside:n,onFocusOutside:s,onInteractOutside:i,onDismiss:a,...d}=e,c=m.useContext(Yf),[p,u]=m.useState(null),f=(p==null?void 0:p.ownerDocument)??(globalThis==null?void 0:globalThis.document),[,g]=m.useState({}),k=Te(t,P=>u(P)),y=Array.from(c.layers),[b]=[...c.layersWithOutsidePointerEventsDisabled].slice(-1),v=y.indexOf(b),h=p?y.indexOf(p):-1,w=c.layersWithOutsidePointerEventsDisabled.size>0,A=h>=v,S=oS(P=>{const D=P.target,M=[...c.branches].some(H=>H.contains(D));!A||M||(n==null||n(P),i==null||i(P),P.defaultPrevented||a==null||a())},f),I=nS(P=>{const D=P.target;[...c.branches].some(H=>H.contains(D))||(s==null||s(P),i==null||i(P),P.defaultPrevented||a==null||a())},f);return ZA(P=>{h===c.layers.size-1&&(o==null||o(P),!P.defaultPrevented&&a&&(P.preventDefault(),a()))},f),m.useEffect(()=>{if(p)return r&&(c.layersWithOutsidePointerEventsDisabled.size===0&&(gg=f.body.style.pointerEvents,f.body.style.pointerEvents="none"),c.layersWithOutsidePointerEventsDisabled.add(p)),c.layers.add(p),mg(),()=>{r&&c.layersWithOutsidePointerEventsDisabled.size===1&&(f.body.style.pointerEvents=gg)}},[p,f,r,c]),m.useEffect(()=>()=>{p&&(c.layers.delete(p),c.layersWithOutsidePointerEventsDisabled.delete(p),mg())},[p,c]),m.useEffect(()=>{const P=()=>g({});return document.addEventListener(Xc,P),()=>document.removeEventListener(Xc,P)},[]),l.jsx(ie.div,{...d,ref:k,style:{pointerEvents:w?A?"auto":"none":void 0,...e.style},onFocusCapture:ne(e.onFocusCapture,I.onFocusCapture),onBlurCapture:ne(e.onBlurCapture,I.onBlurCapture),onPointerDownCapture:ne(e.onPointerDownCapture,S.onPointerDownCapture)})});_a.displayName=XA;var rS="DismissableLayerBranch",Jf=m.forwardRef((e,t)=>{const r=m.useContext(Yf),o=m.useRef(null),n=Te(t,o);return m.useEffect(()=>{const s=o.current;if(s)return r.branches.add(s),()=>{r.branches.delete(s)}},[r.branches]),l.jsx(ie.div,{...e,ref:n})});Jf.displayName=rS;function oS(e,t=globalThis==null?void 0:globalThis.document){const r=Qt(e),o=m.useRef(!1),n=m.useRef(()=>{});return m.useEffect(()=>{const s=a=>{if(a.target&&!o.current){let d=function(){Zf(eS,r,c,{discrete:!0})};const c={originalEvent:a};a.pointerType==="touch"?(t.removeEventListener("click",n.current),n.current=d,t.addEventListener("click",n.current,{once:!0})):d()}else t.removeEventListener("click",n.current);o.current=!1},i=window.setTimeout(()=>{t.addEventListener("pointerdown",s)},0);return()=>{window.clearTimeout(i),t.removeEventListener("pointerdown",s),t.removeEventListener("click",n.current)}},[t,r]),{onPointerDownCapture:()=>o.current=!0}}function nS(e,t=globalThis==null?void 0:globalThis.document){const r=Qt(e),o=m.useRef(!1);return m.useEffect(()=>{const n=s=>{s.target&&!o.current&&Zf(tS,r,{originalEvent:s},{discrete:!1})};return t.addEventListener("focusin",n),()=>t.removeEventListener("focusin",n)},[t,r]),{onFocusCapture:()=>o.current=!0,onBlurCapture:()=>o.current=!1}}function mg(){const e=new CustomEvent(Xc);document.dispatchEvent(e)}function Zf(e,t,r,{discrete:o}){const n=r.originalEvent.target,s=new CustomEvent(e,{bubbles:!1,cancelable:!0,detail:r});t&&n.addEventListener(e,t,{once:!0}),o?Qf(n,s):n.dispatchEvent(s)}var sS=_a,iS=Jf,Yt=globalThis!=null&&globalThis.document?m.useLayoutEffect:()=>{},aS="Portal",gu=m.forwardRef((e,t)=>{var a;const{container:r,...o}=e,[n,s]=m.useState(!1);Yt(()=>s(!0),[]);const i=r||n&&((a=globalThis==null?void 0:globalThis.document)==null?void 0:a.body);return i?Uf.createPortal(l.jsx(ie.div,{...o,ref:t}),i):null});gu.displayName=aS;function lS(e,t){return m.useReducer((r,o)=>t[r][o]??r,e)}var Do=e=>{const{present:t,children:r}=e,o=cS(t),n=typeof r=="function"?r({present:o.isPresent}):m.Children.only(r),s=Te(o.ref,dS(n));return typeof r=="function"||o.isPresent?m.cloneElement(n,{ref:s}):null};Do.displayName="Presence";function cS(e){const[t,r]=m.useState(),o=m.useRef(null),n=m.useRef(e),s=m.useRef("none"),i=e?"mounted":"unmounted",[a,d]=lS(i,{mounted:{UNMOUNT:"unmounted",ANIMATION_OUT:"unmountSuspended"},unmountSuspended:{MOUNT:"mounted",ANIMATION_END:"unmounted"},unmounted:{MOUNT:"mounted"}});return m.useEffect(()=>{const c=mi(o.current);s.current=a==="mounted"?c:"none"},[a]),Yt(()=>{const c=o.current,p=n.current;if(p!==e){const f=s.current,g=mi(c);e?d("MOUNT"):g==="none"||(c==null?void 0:c.display)==="none"?d("UNMOUNT"):d(p&&f!==g?"ANIMATION_OUT":"UNMOUNT"),n.current=e}},[e,d]),Yt(()=>{if(t){let c;const p=t.ownerDocument.defaultView??window,u=g=>{const y=mi(o.current).includes(g.animationName);if(g.target===t&&y&&(d("ANIMATION_END"),!n.current)){const b=t.style.animationFillMode;t.style.animationFillMode="forwards",c=p.setTimeout(()=>{t.style.animationFillMode==="forwards"&&(t.style.animationFillMode=b)})}},f=g=>{g.target===t&&(s.current=mi(o.current))};return t.addEventListener("animationstart",f),t.addEventListener("animationcancel",u),t.addEventListener("animationend",u),()=>{p.clearTimeout(c),t.removeEventListener("animationstart",f),t.removeEventListener("animationcancel",u),t.removeEventListener("animationend",u)}}else d("ANIMATION_END")},[t,d]),{isPresent:["mounted","unmountSuspended"].includes(a),ref:m.useCallback(c=>{o.current=c?getComputedStyle(c):null,r(c)},[])}}function mi(e){return(e==null?void 0:e.animationName)||"none"}function dS(e){var o,n;let t=(o=Object.getOwnPropertyDescriptor(e.props,"ref"))==null?void 0:o.get,r=t&&"isReactWarning"in t&&t.isReactWarning;return r?e.ref:(t=(n=Object.getOwnPropertyDescriptor(e,"ref"))==null?void 0:n.get,r=t&&"isReactWarning"in t&&t.isReactWarning,r?e.props.ref:e.props.ref||e.ref)}var uS=Ad[" useInsertionEffect ".trim().toString()]||Yt;function Hn({prop:e,defaultProp:t,onChange:r=()=>{},caller:o}){const[n,s,i]=pS({defaultProp:t,onChange:r}),a=e!==void 0,d=a?e:n;{const p=m.useRef(e!==void 0);m.useEffect(()=>{const u=p.current;u!==a&&console.warn(`${o} is changing from ${u?"controlled":"uncontrolled"} to ${a?"controlled":"uncontrolled"}. Components should not switch from controlled to uncontrolled (or vice versa). Decide between using a controlled or uncontrolled value for the lifetime of the component.`),p.current=a},[a,o])}const c=m.useCallback(p=>{var u;if(a){const f=gS(p)?p(e):p;f!==e&&((u=i.current)==null||u.call(i,f))}else s(p)},[a,e,s,i]);return[d,c]}function pS({defaultProp:e,onChange:t}){const[r,o]=m.useState(e),n=m.useRef(r),s=m.useRef(t);return uS(()=>{s.current=t},[t]),m.useEffect(()=>{var i;n.current!==r&&((i=s.current)==null||i.call(s,r),n.current=r)},[r,n]),[r,o,s]}function gS(e){return typeof e=="function"}var mS=Object.freeze({position:"absolute",border:0,width:1,height:1,padding:0,margin:-1,overflow:"hidden",clip:"rect(0, 0, 0, 0)",whiteSpace:"nowrap",wordWrap:"normal"}),hS="VisuallyHidden",Va=m.forwardRef((e,t)=>l.jsx(ie.span,{...e,ref:t,style:{...mS,...e.style}}));Va.displayName=hS;var fS=Va,mu="ToastProvider",[hu,vS,yS]=$f("Toast"),[Xf,pD]=xo("Toast",[yS]),[wS,Ba]=Xf(mu),ev=e=>{const{__scopeToast:t,label:r="Notification",duration:o=5e3,swipeDirection:n="right",swipeThreshold:s=50,children:i}=e,[a,d]=m.useState(null),[c,p]=m.useState(0),u=m.useRef(!1),f=m.useRef(!1);return r.trim()||console.error(`Invalid prop \`label\` supplied to \`${mu}\`. Expected non-empty \`string\`.`),l.jsx(hu.Provider,{scope:t,children:l.jsx(wS,{scope:t,label:r,duration:o,swipeDirection:n,swipeThreshold:s,toastCount:c,viewport:a,onViewportChange:d,onToastAdd:m.useCallback(()=>p(g=>g+1),[]),onToastRemove:m.useCallback(()=>p(g=>g-1),[]),isFocusedToastEscapeKeyDownRef:u,isClosePausedRef:f,children:i})})};ev.displayName=mu;var tv="ToastViewport",bS=["F8"],ed="toast.viewportPause",td="toast.viewportResume",rv=m.forwardRef((e,t)=>{const{__scopeToast:r,hotkey:o=bS,label:n="Notifications ({hotkey})",...s}=e,i=Ba(tv,r),a=vS(r),d=m.useRef(null),c=m.useRef(null),p=m.useRef(null),u=m.useRef(null),f=Te(t,u,i.onViewportChange),g=o.join("+").replace(/Key/g,"").replace(/Digit/g,""),k=i.toastCount>0;m.useEffect(()=>{const b=v=>{var w;o.length!==0&&o.every(A=>v[A]||v.code===A)&&((w=u.current)==null||w.focus())};return document.addEventListener("keydown",b),()=>document.removeEventListener("keydown",b)},[o]),m.useEffect(()=>{const b=d.current,v=u.current;if(k&&b&&v){const h=()=>{if(!i.isClosePausedRef.current){const I=new CustomEvent(ed);v.dispatchEvent(I),i.isClosePausedRef.current=!0}},w=()=>{if(i.isClosePausedRef.current){const I=new CustomEvent(td);v.dispatchEvent(I),i.isClosePausedRef.current=!1}},A=I=>{!b.contains(I.relatedTarget)&&w()},S=()=>{b.contains(document.activeElement)||w()};return b.addEventListener("focusin",h),b.addEventListener("focusout",A),b.addEventListener("pointermove",h),b.addEventListener("pointerleave",S),window.addEventListener("blur",h),window.addEventListener("focus",w),()=>{b.removeEventListener("focusin",h),b.removeEventListener("focusout",A),b.removeEventListener("pointermove",h),b.removeEventListener("pointerleave",S),window.removeEventListener("blur",h),window.removeEventListener("focus",w)}}},[k,i.isClosePausedRef]);const y=m.useCallback(({tabbingDirection:b})=>{const h=a().map(w=>{const A=w.ref.current,S=[A,...MS(A)];return b==="forwards"?S:S.reverse()});return(b==="forwards"?h.reverse():h).flat()},[a]);return m.useEffect(()=>{const b=u.current;if(b){const v=h=>{var S,I,P;const w=h.altKey||h.ctrlKey||h.metaKey;if(h.key==="Tab"&&!w){const D=document.activeElement,M=h.shiftKey;if(h.target===b&&M){(S=c.current)==null||S.focus();return}const L=y({tabbingDirection:M?"backwards":"forwards"}),$=L.findIndex(E=>E===D);Gl(L.slice($+1))?h.preventDefault():M?(I=c.current)==null||I.focus():(P=p.current)==null||P.focus()}};return b.addEventListener("keydown",v),()=>b.removeEventListener("keydown",v)}},[a,y]),l.jsxs(iS,{ref:d,role:"region","aria-label":n.replace("{hotkey}",g),tabIndex:-1,style:{pointerEvents:k?void 0:"none"},children:[k&&l.jsx(rd,{ref:c,onFocusFromOutsideViewport:()=>{const b=y({tabbingDirection:"forwards"});Gl(b)}}),l.jsx(hu.Slot,{scope:r,children:l.jsx(ie.ol,{tabIndex:-1,...s,ref:f})}),k&&l.jsx(rd,{ref:p,onFocusFromOutsideViewport:()=>{const b=y({tabbingDirection:"backwards"});Gl(b)}})]})});rv.displayName=tv;var ov="ToastFocusProxy",rd=m.forwardRef((e,t)=>{const{__scopeToast:r,onFocusFromOutsideViewport:o,...n}=e,s=Ba(ov,r);return l.jsx(Va,{"aria-hidden":!0,tabIndex:0,...n,ref:t,style:{position:"fixed"},onFocus:i=>{var c;const a=i.relatedTarget;!((c=s.viewport)!=null&&c.contains(a))&&o()}})});rd.displayName=ov;var js="Toast",kS="toast.swipeStart",AS="toast.swipeMove",SS="toast.swipeCancel",IS="toast.swipeEnd",nv=m.forwardRef((e,t)=>{const{forceMount:r,open:o,defaultOpen:n,onOpenChange:s,...i}=e,[a,d]=Hn({prop:o,defaultProp:n??!0,onChange:s,caller:js});return l.jsx(Do,{present:r||a,children:l.jsx(xS,{open:a,...i,ref:t,onClose:()=>d(!1),onPause:Qt(e.onPause),onResume:Qt(e.onResume),onSwipeStart:ne(e.onSwipeStart,c=>{c.currentTarget.setAttribute("data-swipe","start")}),onSwipeMove:ne(e.onSwipeMove,c=>{const{x:p,y:u}=c.detail.delta;c.currentTarget.setAttribute("data-swipe","move"),c.currentTarget.style.setProperty("--radix-toast-swipe-move-x",`${p}px`),c.currentTarget.style.setProperty("--radix-toast-swipe-move-y",`${u}px`)}),onSwipeCancel:ne(e.onSwipeCancel,c=>{c.currentTarget.setAttribute("data-swipe","cancel"),c.currentTarget.style.removeProperty("--radix-toast-swipe-move-x"),c.currentTarget.style.removeProperty("--radix-toast-swipe-move-y"),c.currentTarget.style.removeProperty("--radix-toast-swipe-end-x"),c.currentTarget.style.removeProperty("--radix-toast-swipe-end-y")}),onSwipeEnd:ne(e.onSwipeEnd,c=>{const{x:p,y:u}=c.detail.delta;c.currentTarget.setAttribute("data-swipe","end"),c.currentTarget.style.removeProperty("--radix-toast-swipe-move-x"),c.currentTarget.style.removeProperty("--radix-toast-swipe-move-y"),c.currentTarget.style.setProperty("--radix-toast-swipe-end-x",`${p}px`),c.currentTarget.style.setProperty("--radix-toast-swipe-end-y",`${u}px`),d(!1)})})})});nv.displayName=js;var[PS,CS]=Xf(js,{onClose(){}}),xS=m.forwardRef((e,t)=>{const{__scopeToast:r,type:o="foreground",duration:n,open:s,onClose:i,onEscapeKeyDown:a,onPause:d,onResume:c,onSwipeStart:p,onSwipeMove:u,onSwipeCancel:f,onSwipeEnd:g,...k}=e,y=Ba(js,r),[b,v]=m.useState(null),h=Te(t,E=>v(E)),w=m.useRef(null),A=m.useRef(null),S=n||y.duration,I=m.useRef(0),P=m.useRef(S),D=m.useRef(0),{onToastAdd:M,onToastRemove:H}=y,V=Qt(()=>{var J;(b==null?void 0:b.contains(document.activeElement))&&((J=y.viewport)==null||J.focus()),i()}),L=m.useCallback(E=>{!E||E===1/0||(window.clearTimeout(D.current),I.current=new Date().getTime(),D.current=window.setTimeout(V,E))},[V]);m.useEffect(()=>{const E=y.viewport;if(E){const J=()=>{L(P.current),c==null||c()},B=()=>{const U=new Date().getTime()-I.current;P.current=P.current-U,window.clearTimeout(D.current),d==null||d()};return E.addEventListener(ed,B),E.addEventListener(td,J),()=>{E.removeEventListener(ed,B),E.removeEventListener(td,J)}}},[y.viewport,S,d,c,L]),m.useEffect(()=>{s&&!y.isClosePausedRef.current&&L(S)},[s,S,y.isClosePausedRef,L]),m.useEffect(()=>(M(),()=>H()),[M,H]);const $=m.useMemo(()=>b?uv(b):null,[b]);return y.viewport?l.jsxs(l.Fragment,{children:[$&&l.jsx(DS,{__scopeToast:r,role:"status","aria-live":o==="foreground"?"assertive":"polite","aria-atomic":!0,children:$}),l.jsx(PS,{scope:r,onClose:V,children:Ws.createPortal(l.jsx(hu.ItemSlot,{scope:r,children:l.jsx(sS,{asChild:!0,onEscapeKeyDown:ne(a,()=>{y.isFocusedToastEscapeKeyDownRef.current||V(),y.isFocusedToastEscapeKeyDownRef.current=!1}),children:l.jsx(ie.li,{role:"status","aria-live":"off","aria-atomic":!0,tabIndex:0,"data-state":s?"open":"closed","data-swipe-direction":y.swipeDirection,...k,ref:h,style:{userSelect:"none",touchAction:"none",...e.style},onKeyDown:ne(e.onKeyDown,E=>{E.key==="Escape"&&(a==null||a(E.nativeEvent),E.nativeEvent.defaultPrevented||(y.isFocusedToastEscapeKeyDownRef.current=!0,V()))}),onPointerDown:ne(e.onPointerDown,E=>{E.button===0&&(w.current={x:E.clientX,y:E.clientY})}),onPointerMove:ne(e.onPointerMove,E=>{if(!w.current)return;const J=E.clientX-w.current.x,B=E.clientY-w.current.y,U=!!A.current,x=["left","right"].includes(y.swipeDirection),T=["left","up"].includes(y.swipeDirection)?Math.min:Math.max,N=x?T(0,J):0,G=x?0:T(0,B),_=E.pointerType==="touch"?10:2,Q={x:N,y:G},Z={originalEvent:E,delta:Q};U?(A.current=Q,hi(AS,u,Z,{discrete:!1})):hg(Q,y.swipeDirection,_)?(A.current=Q,hi(kS,p,Z,{discrete:!1}),E.target.setPointerCapture(E.pointerId)):(Math.abs(J)>_||Math.abs(B)>_)&&(w.current=null)}),onPointerUp:ne(e.onPointerUp,E=>{const J=A.current,B=E.target;if(B.hasPointerCapture(E.pointerId)&&B.releasePointerCapture(E.pointerId),A.current=null,w.current=null,J){const U=E.currentTarget,x={originalEvent:E,delta:J};hg(J,y.swipeDirection,y.swipeThreshold)?hi(IS,g,x,{discrete:!0}):hi(SS,f,x,{discrete:!0}),U.addEventListener("click",T=>T.preventDefault(),{once:!0})}})})})}),y.viewport)})]}):null}),DS=e=>{const{__scopeToast:t,children:r,...o}=e,n=Ba(js,t),[s,i]=m.useState(!1),[a,d]=m.useState(!1);return RS(()=>i(!0)),m.useEffect(()=>{const c=window.setTimeout(()=>d(!0),1e3);return()=>window.clearTimeout(c)},[]),a?null:l.jsx(gu,{asChild:!0,children:l.jsx(Va,{...o,children:s&&l.jsxs(l.Fragment,{children:[n.label," ",r]})})})},TS="ToastTitle",sv=m.forwardRef((e,t)=>{const{__scopeToast:r,...o}=e;return l.jsx(ie.div,{...o,ref:t})});sv.displayName=TS;var zS="ToastDescription",iv=m.forwardRef((e,t)=>{const{__scopeToast:r,...o}=e;return l.jsx(ie.div,{...o,ref:t})});iv.displayName=zS;var av="ToastAction",lv=m.forwardRef((e,t)=>{const{altText:r,...o}=e;return r.trim()?l.jsx(dv,{altText:r,asChild:!0,children:l.jsx(fu,{...o,ref:t})}):(console.error(`Invalid prop \`altText\` supplied to \`${av}\`. Expected non-empty \`string\`.`),null)});lv.displayName=av;var cv="ToastClose",fu=m.forwardRef((e,t)=>{const{__scopeToast:r,...o}=e,n=CS(cv,r);return l.jsx(dv,{asChild:!0,children:l.jsx(ie.button,{type:"button",...o,ref:t,onClick:ne(e.onClick,n.onClose)})})});fu.displayName=cv;var dv=m.forwardRef((e,t)=>{const{__scopeToast:r,altText:o,...n}=e;return l.jsx(ie.div,{"data-radix-toast-announce-exclude":"","data-radix-toast-announce-alt":o||void 0,...n,ref:t})});function uv(e){const t=[];return Array.from(e.childNodes).forEach(o=>{if(o.nodeType===o.TEXT_NODE&&o.textContent&&t.push(o.textContent),HS(o)){const n=o.ariaHidden||o.hidden||o.style.display==="none",s=o.dataset.radixToastAnnounceExclude==="";if(!n)if(s){const i=o.dataset.radixToastAnnounceAlt;i&&t.push(i)}else t.push(...uv(o))}}),t}function hi(e,t,r,{discrete:o}){const n=r.originalEvent.currentTarget,s=new CustomEvent(e,{bubbles:!0,cancelable:!0,detail:r});t&&n.addEventListener(e,t,{once:!0}),o?Qf(n,s):n.dispatchEvent(s)}var hg=(e,t,r=0)=>{const o=Math.abs(e.x),n=Math.abs(e.y),s=o>n;return t==="left"||t==="right"?s&&o>r:!s&&n>r};function RS(e=()=>{}){const t=Qt(e);Yt(()=>{let r=0,o=0;return r=window.requestAnimationFrame(()=>o=window.requestAnimationFrame(t)),()=>{window.cancelAnimationFrame(r),window.cancelAnimationFrame(o)}},[t])}function HS(e){return e.nodeType===e.ELEMENT_NODE}function MS(e){const t=[],r=document.createTreeWalker(e,NodeFilter.SHOW_ELEMENT,{acceptNode:o=>{const n=o.tagName==="INPUT"&&o.type==="hidden";return o.disabled||o.hidden||n?NodeFilter.FILTER_SKIP:o.tabIndex>=0?NodeFilter.FILTER_ACCEPT:NodeFilter.FILTER_SKIP}});for(;r.nextNode();)t.push(r.currentNode);return t}function Gl(e){const t=document.activeElement;return e.some(r=>r===t?!0:(r.focus(),document.activeElement!==t))}var ES=ev,pv=rv,gv=nv,mv=sv,hv=iv,fv=lv,vv=fu;function yv(e){var t,r,o="";if(typeof e=="string"||typeof e=="number")o+=e;else if(typeof e=="object")if(Array.isArray(e)){var n=e.length;for(t=0;t<n;t++)e[t]&&(r=yv(e[t]))&&(o&&(o+=" "),o+=r)}else for(r in e)e[r]&&(o&&(o+=" "),o+=r);return o}function wv(){for(var e,t,r=0,o="",n=arguments.length;r<n;r++)(e=arguments[r])&&(t=yv(e))&&(o&&(o+=" "),o+=t);return o}const fg=e=>typeof e=="boolean"?`${e}`:e===0?"0":e,vg=wv,Wa=(e,t)=>r=>{var o;if((t==null?void 0:t.variants)==null)return vg(e,r==null?void 0:r.class,r==null?void 0:r.className);const{variants:n,defaultVariants:s}=t,i=Object.keys(n).map(c=>{const p=r==null?void 0:r[c],u=s==null?void 0:s[c];if(p===null)return null;const f=fg(p)||fg(u);return n[c][f]}),a=r&&Object.entries(r).reduce((c,p)=>{let[u,f]=p;return f===void 0||(c[u]=f),c},{}),d=t==null||(o=t.compoundVariants)===null||o===void 0?void 0:o.reduce((c,p)=>{let{class:u,className:f,...g}=p;return Object.entries(g).every(k=>{let[y,b]=k;return Array.isArray(b)?b.includes({...s,...a}[y]):{...s,...a}[y]===b})?[...c,u,f]:c},[]);return vg(e,i,d,r==null?void 0:r.class,r==null?void 0:r.className)};/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const NS=e=>e.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase(),bv=(...e)=>e.filter((t,r,o)=>!!t&&t.trim()!==""&&o.indexOf(t)===r).join(" ").trim();/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */var qS={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const LS=m.forwardRef(({color:e="currentColor",size:t=24,strokeWidth:r=2,absoluteStrokeWidth:o,className:n="",children:s,iconNode:i,...a},d)=>m.createElement("svg",{ref:d,...qS,width:t,height:t,stroke:e,strokeWidth:o?Number(r)*24/Number(t):r,className:bv("lucide",n),...a},[...i.map(([c,p])=>m.createElement(c,p)),...Array.isArray(s)?s:[s]]));/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const K=(e,t)=>{const r=m.forwardRef(({className:o,...n},s)=>m.createElement(LS,{ref:s,iconNode:t,className:bv(`lucide-${NS(e)}`,o),...n}));return r.displayName=`${e}`,r};/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const vu=K("Activity",[["path",{d:"M22 12h-2.48a2 2 0 0 0-1.93 1.46l-2.35 8.36a.25.25 0 0 1-.48 0L9.24 2.18a.25.25 0 0 0-.48 0l-2.35 8.36A2 2 0 0 1 4.49 12H2",key:"169zse"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Fl=K("ArrowRight",[["path",{d:"M5 12h14",key:"1ays0h"}],["path",{d:"m12 5 7 7-7 7",key:"xquz4c"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const od=K("BookOpen",[["path",{d:"M12 7v14",key:"1akyts"}],["path",{d:"M3 18a1 1 0 0 1-1-1V4a1 1 0 0 1 1-1h5a4 4 0 0 1 4 4 4 4 0 0 1 4-4h5a1 1 0 0 1 1 1v13a1 1 0 0 1-1 1h-6a3 3 0 0 0-3 3 3 3 0 0 0-3-3z",key:"ruj8y"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const OS=K("Book",[["path",{d:"M4 19.5v-15A2.5 2.5 0 0 1 6.5 2H19a1 1 0 0 1 1 1v18a1 1 0 0 1-1 1H6.5a1 1 0 0 1 0-5H20",key:"k3hazp"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const pt=K("Bookmark",[["path",{d:"m19 21-7-4-7 4V5a2 2 0 0 1 2-2h10a2 2 0 0 1 2 2v16z",key:"1fy3hk"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const xs=K("Box",[["path",{d:"M21 8a2 2 0 0 0-1-1.73l-7-4a2 2 0 0 0-2 0l-7 4A2 2 0 0 0 3 8v8a2 2 0 0 0 1 1.73l7 4a2 2 0 0 0 2 0l7-4A2 2 0 0 0 21 16Z",key:"hh9hay"}],["path",{d:"m3.3 7 8.7 5 8.7-5",key:"g66t2b"}],["path",{d:"M12 22V12",key:"d0xqtd"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const kv=K("Boxes",[["path",{d:"M2.97 12.92A2 2 0 0 0 2 14.63v3.24a2 2 0 0 0 .97 1.71l3 1.8a2 2 0 0 0 2.06 0L12 19v-5.5l-5-3-4.03 2.42Z",key:"lc1i9w"}],["path",{d:"m7 16.5-4.74-2.85",key:"1o9zyk"}],["path",{d:"m7 16.5 5-3",key:"va8pkn"}],["path",{d:"M7 16.5v5.17",key:"jnp8gn"}],["path",{d:"M12 13.5V19l3.97 2.38a2 2 0 0 0 2.06 0l3-1.8a2 2 0 0 0 .97-1.71v-3.24a2 2 0 0 0-.97-1.71L17 10.5l-5 3Z",key:"8zsnat"}],["path",{d:"m17 16.5-5-3",key:"8arw3v"}],["path",{d:"m17 16.5 4.74-2.85",key:"8rfmw"}],["path",{d:"M17 16.5v5.17",key:"k6z78m"}],["path",{d:"M7.97 4.42A2 2 0 0 0 7 6.13v4.37l5 3 5-3V6.13a2 2 0 0 0-.97-1.71l-3-1.8a2 2 0 0 0-2.06 0l-3 1.8Z",key:"1xygjf"}],["path",{d:"M12 8 7.26 5.15",key:"1vbdud"}],["path",{d:"m12 8 4.74-2.85",key:"3rx089"}],["path",{d:"M12 13.5V8",key:"1io7kd"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const _S=K("Bug",[["path",{d:"m8 2 1.88 1.88",key:"fmnt4t"}],["path",{d:"M14.12 3.88 16 2",key:"qol33r"}],["path",{d:"M9 7.13v-1a3.003 3.003 0 1 1 6 0v1",key:"d7y7pr"}],["path",{d:"M12 20c-3.3 0-6-2.7-6-6v-3a4 4 0 0 1 4-4h4a4 4 0 0 1 4 4v3c0 3.3-2.7 6-6 6",key:"xs1cw7"}],["path",{d:"M12 20v-9",key:"1qisl0"}],["path",{d:"M6.53 9C4.6 8.8 3 7.1 3 5",key:"32zzws"}],["path",{d:"M6 13H2",key:"82j7cp"}],["path",{d:"M3 21c0-2.1 1.7-3.9 3.8-4",key:"4p0ekp"}],["path",{d:"M20.97 5c0 2.1-1.6 3.8-3.5 4",key:"18gb23"}],["path",{d:"M22 13h-4",key:"1jl80f"}],["path",{d:"M17.2 17c2.1.1 3.8 1.9 3.8 4",key:"k3fwyw"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const gt=K("ChevronDown",[["path",{d:"m6 9 6 6 6-6",key:"qrunsl"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const VS=K("CircleCheckBig",[["path",{d:"M21.801 10A10 10 0 1 1 17 3.335",key:"yps3ct"}],["path",{d:"m9 11 3 3L22 4",key:"1pflzl"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const ja=K("Cloud",[["path",{d:"M17.5 19H9a7 7 0 1 1 6.71-9h1.79a4.5 4.5 0 1 1 0 9Z",key:"p7xjir"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Us=K("Code",[["polyline",{points:"16 18 22 12 16 6",key:"z7tu5w"}],["polyline",{points:"8 6 2 12 8 18",key:"1eg1df"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Ua=K("Container",[["path",{d:"M22 7.7c0-.6-.4-1.2-.8-1.5l-6.3-3.9a1.72 1.72 0 0 0-1.7 0l-10.3 6c-.5.2-.9.8-.9 1.4v6.6c0 .5.4 1.2.8 1.5l6.3 3.9a1.72 1.72 0 0 0 1.7 0l10.3-6c.5-.3.9-1 .9-1.5Z",key:"1t2lqe"}],["path",{d:"M10 21.9V14L2.1 9.1",key:"o7czzq"}],["path",{d:"m10 14 11.9-6.9",key:"zm5e20"}],["path",{d:"M14 19.8v-8.1",key:"159ecu"}],["path",{d:"M18 17.5V9.4",key:"11uown"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const BS=K("Cpu",[["rect",{width:"16",height:"16",x:"4",y:"4",rx:"2",key:"14l7u7"}],["rect",{width:"6",height:"6",x:"9",y:"9",rx:"1",key:"5aljv4"}],["path",{d:"M15 2v2",key:"13l42r"}],["path",{d:"M15 20v2",key:"15mkzm"}],["path",{d:"M2 15h2",key:"1gxd5l"}],["path",{d:"M2 9h2",key:"1bbxkp"}],["path",{d:"M20 15h2",key:"19e6y8"}],["path",{d:"M20 9h2",key:"19tzq7"}],["path",{d:"M9 2v2",key:"165o2o"}],["path",{d:"M9 20v2",key:"i2bqo8"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const yg=K("Database",[["ellipse",{cx:"12",cy:"5",rx:"9",ry:"3",key:"msslwz"}],["path",{d:"M3 5V19A9 3 0 0 0 21 19V5",key:"1wlel7"}],["path",{d:"M3 12A9 3 0 0 0 21 12",key:"mv7ke4"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const yu=K("DollarSign",[["line",{x1:"12",x2:"12",y1:"2",y2:"22",key:"7eqyqh"}],["path",{d:"M17 5H9.5a3.5 3.5 0 0 0 0 7h5a3.5 3.5 0 0 1 0 7H6",key:"1b0p4s"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const wg=K("Gauge",[["path",{d:"m12 14 4-4",key:"9kzdfg"}],["path",{d:"M3.34 19a10 10 0 1 1 17.32 0",key:"19p75a"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const wu=K("GitBranch",[["line",{x1:"6",x2:"6",y1:"3",y2:"15",key:"17qcm7"}],["circle",{cx:"18",cy:"6",r:"3",key:"1h7g24"}],["circle",{cx:"6",cy:"18",r:"3",key:"fqmcym"}],["path",{d:"M18 9a9 9 0 0 1-9 9",key:"n2h4wq"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const WS=K("Github",[["path",{d:"M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4",key:"tonef"}],["path",{d:"M9 18c-4.51 2-5-2-7-2",key:"9comsn"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const jS=K("Globe",[["circle",{cx:"12",cy:"12",r:"10",key:"1mglay"}],["path",{d:"M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20",key:"13o1zl"}],["path",{d:"M2 12h20",key:"9i4pu4"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const US=K("Grid3x3",[["rect",{width:"18",height:"18",x:"3",y:"3",rx:"2",key:"afitv7"}],["path",{d:"M3 9h18",key:"1pudct"}],["path",{d:"M3 15h18",key:"5xshup"}],["path",{d:"M9 3v18",key:"fh3hqa"}],["path",{d:"M15 3v18",key:"14nvp0"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const GS=K("HardDrive",[["line",{x1:"22",x2:"2",y1:"12",y2:"12",key:"1y58io"}],["path",{d:"M5.45 5.11 2 12v6a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2v-6l-3.45-6.89A2 2 0 0 0 16.76 4H7.24a2 2 0 0 0-1.79 1.11z",key:"oot6mr"}],["line",{x1:"6",x2:"6.01",y1:"16",y2:"16",key:"sgf278"}],["line",{x1:"10",x2:"10.01",y1:"16",y2:"16",key:"1l4acy"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const FS=K("House",[["path",{d:"M15 21v-8a1 1 0 0 0-1-1h-4a1 1 0 0 0-1 1v8",key:"5wwlr5"}],["path",{d:"M3 10a2 2 0 0 1 .709-1.528l7-5.999a2 2 0 0 1 2.582 0l7 5.999A2 2 0 0 1 21 10v9a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z",key:"1d0kgt"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const KS=K("Key",[["path",{d:"m15.5 7.5 2.3 2.3a1 1 0 0 0 1.4 0l2.1-2.1a1 1 0 0 0 0-1.4L19 4",key:"g0fldk"}],["path",{d:"m21 2-9.6 9.6",key:"1j0ho8"}],["circle",{cx:"7.5",cy:"15.5",r:"5.5",key:"yqb3hr"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const bg=K("Layers",[["path",{d:"m12.83 2.18a2 2 0 0 0-1.66 0L2.6 6.08a1 1 0 0 0 0 1.83l8.58 3.91a2 2 0 0 0 1.66 0l8.58-3.9a1 1 0 0 0 0-1.83Z",key:"8b97xw"}],["path",{d:"m22 17.65-9.17 4.16a2 2 0 0 1-1.66 0L2 17.65",key:"dd6zsq"}],["path",{d:"m22 12.65-9.17 4.16a2 2 0 0 1-1.66 0L2 12.65",key:"ep9fru"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const kg=K("Monitor",[["rect",{width:"20",height:"14",x:"2",y:"3",rx:"2",key:"48i651"}],["line",{x1:"8",x2:"16",y1:"21",y2:"21",key:"1svkeh"}],["line",{x1:"12",x2:"12",y1:"17",y2:"21",key:"vw1qmm"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const $S=K("Moon",[["path",{d:"M12 3a6 6 0 0 0 9 9 9 9 0 1 1-9-9Z",key:"a7tn18"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const QS=K("Network",[["rect",{x:"16",y:"16",width:"6",height:"6",rx:"1",key:"4q2zg0"}],["rect",{x:"2",y:"16",width:"6",height:"6",rx:"1",key:"8cvhb9"}],["rect",{x:"9",y:"2",width:"6",height:"6",rx:"1",key:"1egb70"}],["path",{d:"M5 16v-3a1 1 0 0 1 1-1h12a1 1 0 0 1 1 1v3",key:"1jsf9p"}],["path",{d:"M12 12V8",key:"2874zd"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const YS=K("PanelLeft",[["rect",{width:"18",height:"18",x:"3",y:"3",rx:"2",key:"afitv7"}],["path",{d:"M9 3v18",key:"fh3hqa"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const JS=K("Search",[["circle",{cx:"11",cy:"11",r:"8",key:"4ej97u"}],["path",{d:"m21 21-4.3-4.3",key:"1qie3q"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Ga=K("Server",[["rect",{width:"20",height:"8",x:"2",y:"2",rx:"2",ry:"2",key:"ngkwjq"}],["rect",{width:"20",height:"8",x:"2",y:"14",rx:"2",ry:"2",key:"iecqi9"}],["line",{x1:"6",x2:"6.01",y1:"6",y2:"6",key:"16zg32"}],["line",{x1:"6",x2:"6.01",y1:"18",y2:"18",key:"nzw8ys"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const ZS=K("Settings",[["path",{d:"M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 1 1.72v.51a2 2 0 0 1-1 1.74l-.15.09a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l.22-.39a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1-1-1.74v-.5a2 2 0 0 1 1-1.74l.15-.09a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z",key:"1qme2f"}],["circle",{cx:"12",cy:"12",r:"3",key:"1v7zrd"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Ag=K("ShieldCheck",[["path",{d:"M20 13c0 5-3.5 7.5-7.66 8.95a1 1 0 0 1-.67-.01C7.5 20.5 4 18 4 13V6a1 1 0 0 1 1-1c2 0 4.5-1.2 6.24-2.72a1.17 1.17 0 0 1 1.52 0C14.51 3.81 17 5 19 5a1 1 0 0 1 1 1z",key:"oel41y"}],["path",{d:"m9 12 2 2 4-4",key:"dzmm74"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const XS=K("Sun",[["circle",{cx:"12",cy:"12",r:"4",key:"4exip2"}],["path",{d:"M12 2v2",key:"tus03m"}],["path",{d:"M12 20v2",key:"1lh1kg"}],["path",{d:"m4.93 4.93 1.41 1.41",key:"149t6j"}],["path",{d:"m17.66 17.66 1.41 1.41",key:"ptbguv"}],["path",{d:"M2 12h2",key:"1t8f8n"}],["path",{d:"M20 12h2",key:"1q8mjw"}],["path",{d:"m6.34 17.66-1.41 1.41",key:"1m8zz5"}],["path",{d:"m19.07 4.93-1.41 1.41",key:"1shlcs"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const da=K("Terminal",[["polyline",{points:"4 17 10 11 4 5",key:"akl6gq"}],["line",{x1:"12",x2:"20",y1:"19",y2:"19",key:"q2wloq"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const eI=K("TriangleAlert",[["path",{d:"m21.73 18-8-14a2 2 0 0 0-3.48 0l-8 14A2 2 0 0 0 4 21h16a2 2 0 0 0 1.73-3",key:"wmoenq"}],["path",{d:"M12 9v4",key:"juzpu7"}],["path",{d:"M12 17h.01",key:"p32p05"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Av=K("Users",[["path",{d:"M16 21v-2a4 4 0 0 0-4-4H6a4 4 0 0 0-4 4v2",key:"1yyitq"}],["circle",{cx:"9",cy:"7",r:"4",key:"nufk8"}],["path",{d:"M22 21v-2a4 4 0 0 0-3-3.87",key:"kshegd"}],["path",{d:"M16 3.13a4 4 0 0 1 0 7.75",key:"1da9ce"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const Kl=K("Wrench",[["path",{d:"M14.7 6.3a1 1 0 0 0 0 1.4l1.6 1.6a1 1 0 0 0 1.4 0l3.77-3.77a6 6 0 0 1-7.94 7.94l-6.91 6.91a2.12 2.12 0 0 1-3-3l6.91-6.91a6 6 0 0 1 7.94-7.94l-3.76 3.76z",key:"cbrjhi"}]]);/**
 * @license lucide-react v0.462.0 - ISC
 *
 * This source code is licensed under the ISC license.
 * See the LICENSE file in the root directory of this source tree.
 */const bu=K("X",[["path",{d:"M18 6 6 18",key:"1bl5f8"}],["path",{d:"m6 6 12 12",key:"d8bk6v"}]]),ku="-",tI=e=>{const t=oI(e),{conflictingClassGroups:r,conflictingClassGroupModifiers:o}=e;return{getClassGroupId:i=>{const a=i.split(ku);return a[0]===""&&a.length!==1&&a.shift(),Sv(a,t)||rI(i)},getConflictingClassGroupIds:(i,a)=>{const d=r[i]||[];return a&&o[i]?[...d,...o[i]]:d}}},Sv=(e,t)=>{var i;if(e.length===0)return t.classGroupId;const r=e[0],o=t.nextPart.get(r),n=o?Sv(e.slice(1),o):void 0;if(n)return n;if(t.validators.length===0)return;const s=e.join(ku);return(i=t.validators.find(({validator:a})=>a(s)))==null?void 0:i.classGroupId},Sg=/^\[(.+)\]$/,rI=e=>{if(Sg.test(e)){const t=Sg.exec(e)[1],r=t==null?void 0:t.substring(0,t.indexOf(":"));if(r)return"arbitrary.."+r}},oI=e=>{const{theme:t,prefix:r}=e,o={nextPart:new Map,validators:[]};return sI(Object.entries(e.classGroups),r).forEach(([s,i])=>{nd(i,o,s,t)}),o},nd=(e,t,r,o)=>{e.forEach(n=>{if(typeof n=="string"){const s=n===""?t:Ig(t,n);s.classGroupId=r;return}if(typeof n=="function"){if(nI(n)){nd(n(o),t,r,o);return}t.validators.push({validator:n,classGroupId:r});return}Object.entries(n).forEach(([s,i])=>{nd(i,Ig(t,s),r,o)})})},Ig=(e,t)=>{let r=e;return t.split(ku).forEach(o=>{r.nextPart.has(o)||r.nextPart.set(o,{nextPart:new Map,validators:[]}),r=r.nextPart.get(o)}),r},nI=e=>e.isThemeGetter,sI=(e,t)=>t?e.map(([r,o])=>{const n=o.map(s=>typeof s=="string"?t+s:typeof s=="object"?Object.fromEntries(Object.entries(s).map(([i,a])=>[t+i,a])):s);return[r,n]}):e,iI=e=>{if(e<1)return{get:()=>{},set:()=>{}};let t=0,r=new Map,o=new Map;const n=(s,i)=>{r.set(s,i),t++,t>e&&(t=0,o=r,r=new Map)};return{get(s){let i=r.get(s);if(i!==void 0)return i;if((i=o.get(s))!==void 0)return n(s,i),i},set(s,i){r.has(s)?r.set(s,i):n(s,i)}}},Iv="!",aI=e=>{const{separator:t,experimentalParseClassName:r}=e,o=t.length===1,n=t[0],s=t.length,i=a=>{const d=[];let c=0,p=0,u;for(let b=0;b<a.length;b++){let v=a[b];if(c===0){if(v===n&&(o||a.slice(b,b+s)===t)){d.push(a.slice(p,b)),p=b+s;continue}if(v==="/"){u=b;continue}}v==="["?c++:v==="]"&&c--}const f=d.length===0?a:a.substring(p),g=f.startsWith(Iv),k=g?f.substring(1):f,y=u&&u>p?u-p:void 0;return{modifiers:d,hasImportantModifier:g,baseClassName:k,maybePostfixModifierPosition:y}};return r?a=>r({className:a,parseClassName:i}):i},lI=e=>{if(e.length<=1)return e;const t=[];let r=[];return e.forEach(o=>{o[0]==="["?(t.push(...r.sort(),o),r=[]):r.push(o)}),t.push(...r.sort()),t},cI=e=>({cache:iI(e.cacheSize),parseClassName:aI(e),...tI(e)}),dI=/\s+/,uI=(e,t)=>{const{parseClassName:r,getClassGroupId:o,getConflictingClassGroupIds:n}=t,s=[],i=e.trim().split(dI);let a="";for(let d=i.length-1;d>=0;d-=1){const c=i[d],{modifiers:p,hasImportantModifier:u,baseClassName:f,maybePostfixModifierPosition:g}=r(c);let k=!!g,y=o(k?f.substring(0,g):f);if(!y){if(!k){a=c+(a.length>0?" "+a:a);continue}if(y=o(f),!y){a=c+(a.length>0?" "+a:a);continue}k=!1}const b=lI(p).join(":"),v=u?b+Iv:b,h=v+y;if(s.includes(h))continue;s.push(h);const w=n(y,k);for(let A=0;A<w.length;++A){const S=w[A];s.push(v+S)}a=c+(a.length>0?" "+a:a)}return a};function pI(){let e=0,t,r,o="";for(;e<arguments.length;)(t=arguments[e++])&&(r=Pv(t))&&(o&&(o+=" "),o+=r);return o}const Pv=e=>{if(typeof e=="string")return e;let t,r="";for(let o=0;o<e.length;o++)e[o]&&(t=Pv(e[o]))&&(r&&(r+=" "),r+=t);return r};function gI(e,...t){let r,o,n,s=i;function i(d){const c=t.reduce((p,u)=>u(p),e());return r=cI(c),o=r.cache.get,n=r.cache.set,s=a,a(d)}function a(d){const c=o(d);if(c)return c;const p=uI(d,r);return n(d,p),p}return function(){return s(pI.apply(null,arguments))}}const ae=e=>{const t=r=>r[e]||[];return t.isThemeGetter=!0,t},Cv=/^\[(?:([a-z-]+):)?(.+)\]$/i,mI=/^\d+\/\d+$/,hI=new Set(["px","full","screen"]),fI=/^(\d+(\.\d+)?)?(xs|sm|md|lg|xl)$/,vI=/\d+(%|px|r?em|[sdl]?v([hwib]|min|max)|pt|pc|in|cm|mm|cap|ch|ex|r?lh|cq(w|h|i|b|min|max))|\b(calc|min|max|clamp)\(.+\)|^0$/,yI=/^(rgba?|hsla?|hwb|(ok)?(lab|lch))\(.+\)$/,wI=/^(inset_)?-?((\d+)?\.?(\d+)[a-z]+|0)_-?((\d+)?\.?(\d+)[a-z]+|0)/,bI=/^(url|image|image-set|cross-fade|element|(repeating-)?(linear|radial|conic)-gradient)\(.+\)$/,Xt=e=>sn(e)||hI.has(e)||mI.test(e),Ar=e=>Mn(e,"length",DI),sn=e=>!!e&&!Number.isNaN(Number(e)),$l=e=>Mn(e,"number",sn),Kn=e=>!!e&&Number.isInteger(Number(e)),kI=e=>e.endsWith("%")&&sn(e.slice(0,-1)),F=e=>Cv.test(e),Sr=e=>fI.test(e),AI=new Set(["length","size","percentage"]),SI=e=>Mn(e,AI,xv),II=e=>Mn(e,"position",xv),PI=new Set(["image","url"]),CI=e=>Mn(e,PI,zI),xI=e=>Mn(e,"",TI),$n=()=>!0,Mn=(e,t,r)=>{const o=Cv.exec(e);return o?o[1]?typeof t=="string"?o[1]===t:t.has(o[1]):r(o[2]):!1},DI=e=>vI.test(e)&&!yI.test(e),xv=()=>!1,TI=e=>wI.test(e),zI=e=>bI.test(e),RI=()=>{const e=ae("colors"),t=ae("spacing"),r=ae("blur"),o=ae("brightness"),n=ae("borderColor"),s=ae("borderRadius"),i=ae("borderSpacing"),a=ae("borderWidth"),d=ae("contrast"),c=ae("grayscale"),p=ae("hueRotate"),u=ae("invert"),f=ae("gap"),g=ae("gradientColorStops"),k=ae("gradientColorStopPositions"),y=ae("inset"),b=ae("margin"),v=ae("opacity"),h=ae("padding"),w=ae("saturate"),A=ae("scale"),S=ae("sepia"),I=ae("skew"),P=ae("space"),D=ae("translate"),M=()=>["auto","contain","none"],H=()=>["auto","hidden","clip","visible","scroll"],V=()=>["auto",F,t],L=()=>[F,t],$=()=>["",Xt,Ar],E=()=>["auto",sn,F],J=()=>["bottom","center","left","left-bottom","left-top","right","right-bottom","right-top","top"],B=()=>["solid","dashed","dotted","double","none"],U=()=>["normal","multiply","screen","overlay","darken","lighten","color-dodge","color-burn","hard-light","soft-light","difference","exclusion","hue","saturation","color","luminosity"],x=()=>["start","end","center","between","around","evenly","stretch"],T=()=>["","0",F],N=()=>["auto","avoid","all","avoid-page","page","left","right","column"],G=()=>[sn,F];return{cacheSize:500,separator:":",theme:{colors:[$n],spacing:[Xt,Ar],blur:["none","",Sr,F],brightness:G(),borderColor:[e],borderRadius:["none","","full",Sr,F],borderSpacing:L(),borderWidth:$(),contrast:G(),grayscale:T(),hueRotate:G(),invert:T(),gap:L(),gradientColorStops:[e],gradientColorStopPositions:[kI,Ar],inset:V(),margin:V(),opacity:G(),padding:L(),saturate:G(),scale:G(),sepia:T(),skew:G(),space:L(),translate:L()},classGroups:{aspect:[{aspect:["auto","square","video",F]}],container:["container"],columns:[{columns:[Sr]}],"break-after":[{"break-after":N()}],"break-before":[{"break-before":N()}],"break-inside":[{"break-inside":["auto","avoid","avoid-page","avoid-column"]}],"box-decoration":[{"box-decoration":["slice","clone"]}],box:[{box:["border","content"]}],display:["block","inline-block","inline","flex","inline-flex","table","inline-table","table-caption","table-cell","table-column","table-column-group","table-footer-group","table-header-group","table-row-group","table-row","flow-root","grid","inline-grid","contents","list-item","hidden"],float:[{float:["right","left","none","start","end"]}],clear:[{clear:["left","right","both","none","start","end"]}],isolation:["isolate","isolation-auto"],"object-fit":[{object:["contain","cover","fill","none","scale-down"]}],"object-position":[{object:[...J(),F]}],overflow:[{overflow:H()}],"overflow-x":[{"overflow-x":H()}],"overflow-y":[{"overflow-y":H()}],overscroll:[{overscroll:M()}],"overscroll-x":[{"overscroll-x":M()}],"overscroll-y":[{"overscroll-y":M()}],position:["static","fixed","absolute","relative","sticky"],inset:[{inset:[y]}],"inset-x":[{"inset-x":[y]}],"inset-y":[{"inset-y":[y]}],start:[{start:[y]}],end:[{end:[y]}],top:[{top:[y]}],right:[{right:[y]}],bottom:[{bottom:[y]}],left:[{left:[y]}],visibility:["visible","invisible","collapse"],z:[{z:["auto",Kn,F]}],basis:[{basis:V()}],"flex-direction":[{flex:["row","row-reverse","col","col-reverse"]}],"flex-wrap":[{flex:["wrap","wrap-reverse","nowrap"]}],flex:[{flex:["1","auto","initial","none",F]}],grow:[{grow:T()}],shrink:[{shrink:T()}],order:[{order:["first","last","none",Kn,F]}],"grid-cols":[{"grid-cols":[$n]}],"col-start-end":[{col:["auto",{span:["full",Kn,F]},F]}],"col-start":[{"col-start":E()}],"col-end":[{"col-end":E()}],"grid-rows":[{"grid-rows":[$n]}],"row-start-end":[{row:["auto",{span:[Kn,F]},F]}],"row-start":[{"row-start":E()}],"row-end":[{"row-end":E()}],"grid-flow":[{"grid-flow":["row","col","dense","row-dense","col-dense"]}],"auto-cols":[{"auto-cols":["auto","min","max","fr",F]}],"auto-rows":[{"auto-rows":["auto","min","max","fr",F]}],gap:[{gap:[f]}],"gap-x":[{"gap-x":[f]}],"gap-y":[{"gap-y":[f]}],"justify-content":[{justify:["normal",...x()]}],"justify-items":[{"justify-items":["start","end","center","stretch"]}],"justify-self":[{"justify-self":["auto","start","end","center","stretch"]}],"align-content":[{content:["normal",...x(),"baseline"]}],"align-items":[{items:["start","end","center","baseline","stretch"]}],"align-self":[{self:["auto","start","end","center","stretch","baseline"]}],"place-content":[{"place-content":[...x(),"baseline"]}],"place-items":[{"place-items":["start","end","center","baseline","stretch"]}],"place-self":[{"place-self":["auto","start","end","center","stretch"]}],p:[{p:[h]}],px:[{px:[h]}],py:[{py:[h]}],ps:[{ps:[h]}],pe:[{pe:[h]}],pt:[{pt:[h]}],pr:[{pr:[h]}],pb:[{pb:[h]}],pl:[{pl:[h]}],m:[{m:[b]}],mx:[{mx:[b]}],my:[{my:[b]}],ms:[{ms:[b]}],me:[{me:[b]}],mt:[{mt:[b]}],mr:[{mr:[b]}],mb:[{mb:[b]}],ml:[{ml:[b]}],"space-x":[{"space-x":[P]}],"space-x-reverse":["space-x-reverse"],"space-y":[{"space-y":[P]}],"space-y-reverse":["space-y-reverse"],w:[{w:["auto","min","max","fit","svw","lvw","dvw",F,t]}],"min-w":[{"min-w":[F,t,"min","max","fit"]}],"max-w":[{"max-w":[F,t,"none","full","min","max","fit","prose",{screen:[Sr]},Sr]}],h:[{h:[F,t,"auto","min","max","fit","svh","lvh","dvh"]}],"min-h":[{"min-h":[F,t,"min","max","fit","svh","lvh","dvh"]}],"max-h":[{"max-h":[F,t,"min","max","fit","svh","lvh","dvh"]}],size:[{size:[F,t,"auto","min","max","fit"]}],"font-size":[{text:["base",Sr,Ar]}],"font-smoothing":["antialiased","subpixel-antialiased"],"font-style":["italic","not-italic"],"font-weight":[{font:["thin","extralight","light","normal","medium","semibold","bold","extrabold","black",$l]}],"font-family":[{font:[$n]}],"fvn-normal":["normal-nums"],"fvn-ordinal":["ordinal"],"fvn-slashed-zero":["slashed-zero"],"fvn-figure":["lining-nums","oldstyle-nums"],"fvn-spacing":["proportional-nums","tabular-nums"],"fvn-fraction":["diagonal-fractions","stacked-fractions"],tracking:[{tracking:["tighter","tight","normal","wide","wider","widest",F]}],"line-clamp":[{"line-clamp":["none",sn,$l]}],leading:[{leading:["none","tight","snug","normal","relaxed","loose",Xt,F]}],"list-image":[{"list-image":["none",F]}],"list-style-type":[{list:["none","disc","decimal",F]}],"list-style-position":[{list:["inside","outside"]}],"placeholder-color":[{placeholder:[e]}],"placeholder-opacity":[{"placeholder-opacity":[v]}],"text-alignment":[{text:["left","center","right","justify","start","end"]}],"text-color":[{text:[e]}],"text-opacity":[{"text-opacity":[v]}],"text-decoration":["underline","overline","line-through","no-underline"],"text-decoration-style":[{decoration:[...B(),"wavy"]}],"text-decoration-thickness":[{decoration:["auto","from-font",Xt,Ar]}],"underline-offset":[{"underline-offset":["auto",Xt,F]}],"text-decoration-color":[{decoration:[e]}],"text-transform":["uppercase","lowercase","capitalize","normal-case"],"text-overflow":["truncate","text-ellipsis","text-clip"],"text-wrap":[{text:["wrap","nowrap","balance","pretty"]}],indent:[{indent:L()}],"vertical-align":[{align:["baseline","top","middle","bottom","text-top","text-bottom","sub","super",F]}],whitespace:[{whitespace:["normal","nowrap","pre","pre-line","pre-wrap","break-spaces"]}],break:[{break:["normal","words","all","keep"]}],hyphens:[{hyphens:["none","manual","auto"]}],content:[{content:["none",F]}],"bg-attachment":[{bg:["fixed","local","scroll"]}],"bg-clip":[{"bg-clip":["border","padding","content","text"]}],"bg-opacity":[{"bg-opacity":[v]}],"bg-origin":[{"bg-origin":["border","padding","content"]}],"bg-position":[{bg:[...J(),II]}],"bg-repeat":[{bg:["no-repeat",{repeat:["","x","y","round","space"]}]}],"bg-size":[{bg:["auto","cover","contain",SI]}],"bg-image":[{bg:["none",{"gradient-to":["t","tr","r","br","b","bl","l","tl"]},CI]}],"bg-color":[{bg:[e]}],"gradient-from-pos":[{from:[k]}],"gradient-via-pos":[{via:[k]}],"gradient-to-pos":[{to:[k]}],"gradient-from":[{from:[g]}],"gradient-via":[{via:[g]}],"gradient-to":[{to:[g]}],rounded:[{rounded:[s]}],"rounded-s":[{"rounded-s":[s]}],"rounded-e":[{"rounded-e":[s]}],"rounded-t":[{"rounded-t":[s]}],"rounded-r":[{"rounded-r":[s]}],"rounded-b":[{"rounded-b":[s]}],"rounded-l":[{"rounded-l":[s]}],"rounded-ss":[{"rounded-ss":[s]}],"rounded-se":[{"rounded-se":[s]}],"rounded-ee":[{"rounded-ee":[s]}],"rounded-es":[{"rounded-es":[s]}],"rounded-tl":[{"rounded-tl":[s]}],"rounded-tr":[{"rounded-tr":[s]}],"rounded-br":[{"rounded-br":[s]}],"rounded-bl":[{"rounded-bl":[s]}],"border-w":[{border:[a]}],"border-w-x":[{"border-x":[a]}],"border-w-y":[{"border-y":[a]}],"border-w-s":[{"border-s":[a]}],"border-w-e":[{"border-e":[a]}],"border-w-t":[{"border-t":[a]}],"border-w-r":[{"border-r":[a]}],"border-w-b":[{"border-b":[a]}],"border-w-l":[{"border-l":[a]}],"border-opacity":[{"border-opacity":[v]}],"border-style":[{border:[...B(),"hidden"]}],"divide-x":[{"divide-x":[a]}],"divide-x-reverse":["divide-x-reverse"],"divide-y":[{"divide-y":[a]}],"divide-y-reverse":["divide-y-reverse"],"divide-opacity":[{"divide-opacity":[v]}],"divide-style":[{divide:B()}],"border-color":[{border:[n]}],"border-color-x":[{"border-x":[n]}],"border-color-y":[{"border-y":[n]}],"border-color-s":[{"border-s":[n]}],"border-color-e":[{"border-e":[n]}],"border-color-t":[{"border-t":[n]}],"border-color-r":[{"border-r":[n]}],"border-color-b":[{"border-b":[n]}],"border-color-l":[{"border-l":[n]}],"divide-color":[{divide:[n]}],"outline-style":[{outline:["",...B()]}],"outline-offset":[{"outline-offset":[Xt,F]}],"outline-w":[{outline:[Xt,Ar]}],"outline-color":[{outline:[e]}],"ring-w":[{ring:$()}],"ring-w-inset":["ring-inset"],"ring-color":[{ring:[e]}],"ring-opacity":[{"ring-opacity":[v]}],"ring-offset-w":[{"ring-offset":[Xt,Ar]}],"ring-offset-color":[{"ring-offset":[e]}],shadow:[{shadow:["","inner","none",Sr,xI]}],"shadow-color":[{shadow:[$n]}],opacity:[{opacity:[v]}],"mix-blend":[{"mix-blend":[...U(),"plus-lighter","plus-darker"]}],"bg-blend":[{"bg-blend":U()}],filter:[{filter:["","none"]}],blur:[{blur:[r]}],brightness:[{brightness:[o]}],contrast:[{contrast:[d]}],"drop-shadow":[{"drop-shadow":["","none",Sr,F]}],grayscale:[{grayscale:[c]}],"hue-rotate":[{"hue-rotate":[p]}],invert:[{invert:[u]}],saturate:[{saturate:[w]}],sepia:[{sepia:[S]}],"backdrop-filter":[{"backdrop-filter":["","none"]}],"backdrop-blur":[{"backdrop-blur":[r]}],"backdrop-brightness":[{"backdrop-brightness":[o]}],"backdrop-contrast":[{"backdrop-contrast":[d]}],"backdrop-grayscale":[{"backdrop-grayscale":[c]}],"backdrop-hue-rotate":[{"backdrop-hue-rotate":[p]}],"backdrop-invert":[{"backdrop-invert":[u]}],"backdrop-opacity":[{"backdrop-opacity":[v]}],"backdrop-saturate":[{"backdrop-saturate":[w]}],"backdrop-sepia":[{"backdrop-sepia":[S]}],"border-collapse":[{border:["collapse","separate"]}],"border-spacing":[{"border-spacing":[i]}],"border-spacing-x":[{"border-spacing-x":[i]}],"border-spacing-y":[{"border-spacing-y":[i]}],"table-layout":[{table:["auto","fixed"]}],caption:[{caption:["top","bottom"]}],transition:[{transition:["none","all","","colors","opacity","shadow","transform",F]}],duration:[{duration:G()}],ease:[{ease:["linear","in","out","in-out",F]}],delay:[{delay:G()}],animate:[{animate:["none","spin","ping","pulse","bounce",F]}],transform:[{transform:["","gpu","none"]}],scale:[{scale:[A]}],"scale-x":[{"scale-x":[A]}],"scale-y":[{"scale-y":[A]}],rotate:[{rotate:[Kn,F]}],"translate-x":[{"translate-x":[D]}],"translate-y":[{"translate-y":[D]}],"skew-x":[{"skew-x":[I]}],"skew-y":[{"skew-y":[I]}],"transform-origin":[{origin:["center","top","top-right","right","bottom-right","bottom","bottom-left","left","top-left",F]}],accent:[{accent:["auto",e]}],appearance:[{appearance:["none","auto"]}],cursor:[{cursor:["auto","default","pointer","wait","text","move","help","not-allowed","none","context-menu","progress","cell","crosshair","vertical-text","alias","copy","no-drop","grab","grabbing","all-scroll","col-resize","row-resize","n-resize","e-resize","s-resize","w-resize","ne-resize","nw-resize","se-resize","sw-resize","ew-resize","ns-resize","nesw-resize","nwse-resize","zoom-in","zoom-out",F]}],"caret-color":[{caret:[e]}],"pointer-events":[{"pointer-events":["none","auto"]}],resize:[{resize:["none","y","x",""]}],"scroll-behavior":[{scroll:["auto","smooth"]}],"scroll-m":[{"scroll-m":L()}],"scroll-mx":[{"scroll-mx":L()}],"scroll-my":[{"scroll-my":L()}],"scroll-ms":[{"scroll-ms":L()}],"scroll-me":[{"scroll-me":L()}],"scroll-mt":[{"scroll-mt":L()}],"scroll-mr":[{"scroll-mr":L()}],"scroll-mb":[{"scroll-mb":L()}],"scroll-ml":[{"scroll-ml":L()}],"scroll-p":[{"scroll-p":L()}],"scroll-px":[{"scroll-px":L()}],"scroll-py":[{"scroll-py":L()}],"scroll-ps":[{"scroll-ps":L()}],"scroll-pe":[{"scroll-pe":L()}],"scroll-pt":[{"scroll-pt":L()}],"scroll-pr":[{"scroll-pr":L()}],"scroll-pb":[{"scroll-pb":L()}],"scroll-pl":[{"scroll-pl":L()}],"snap-align":[{snap:["start","end","center","align-none"]}],"snap-stop":[{snap:["normal","always"]}],"snap-type":[{snap:["none","x","y","both"]}],"snap-strictness":[{snap:["mandatory","proximity"]}],touch:[{touch:["auto","none","manipulation"]}],"touch-x":[{"touch-pan":["x","left","right"]}],"touch-y":[{"touch-pan":["y","up","down"]}],"touch-pz":["touch-pinch-zoom"],select:[{select:["none","text","all","auto"]}],"will-change":[{"will-change":["auto","scroll","contents","transform",F]}],fill:[{fill:[e,"none"]}],"stroke-w":[{stroke:[Xt,Ar,$l]}],stroke:[{stroke:[e,"none"]}],sr:["sr-only","not-sr-only"],"forced-color-adjust":[{"forced-color-adjust":["auto","none"]}]},conflictingClassGroups:{overflow:["overflow-x","overflow-y"],overscroll:["overscroll-x","overscroll-y"],inset:["inset-x","inset-y","start","end","top","right","bottom","left"],"inset-x":["right","left"],"inset-y":["top","bottom"],flex:["basis","grow","shrink"],gap:["gap-x","gap-y"],p:["px","py","ps","pe","pt","pr","pb","pl"],px:["pr","pl"],py:["pt","pb"],m:["mx","my","ms","me","mt","mr","mb","ml"],mx:["mr","ml"],my:["mt","mb"],size:["w","h"],"font-size":["leading"],"fvn-normal":["fvn-ordinal","fvn-slashed-zero","fvn-figure","fvn-spacing","fvn-fraction"],"fvn-ordinal":["fvn-normal"],"fvn-slashed-zero":["fvn-normal"],"fvn-figure":["fvn-normal"],"fvn-spacing":["fvn-normal"],"fvn-fraction":["fvn-normal"],"line-clamp":["display","overflow"],rounded:["rounded-s","rounded-e","rounded-t","rounded-r","rounded-b","rounded-l","rounded-ss","rounded-se","rounded-ee","rounded-es","rounded-tl","rounded-tr","rounded-br","rounded-bl"],"rounded-s":["rounded-ss","rounded-es"],"rounded-e":["rounded-se","rounded-ee"],"rounded-t":["rounded-tl","rounded-tr"],"rounded-r":["rounded-tr","rounded-br"],"rounded-b":["rounded-br","rounded-bl"],"rounded-l":["rounded-tl","rounded-bl"],"border-spacing":["border-spacing-x","border-spacing-y"],"border-w":["border-w-s","border-w-e","border-w-t","border-w-r","border-w-b","border-w-l"],"border-w-x":["border-w-r","border-w-l"],"border-w-y":["border-w-t","border-w-b"],"border-color":["border-color-s","border-color-e","border-color-t","border-color-r","border-color-b","border-color-l"],"border-color-x":["border-color-r","border-color-l"],"border-color-y":["border-color-t","border-color-b"],"scroll-m":["scroll-mx","scroll-my","scroll-ms","scroll-me","scroll-mt","scroll-mr","scroll-mb","scroll-ml"],"scroll-mx":["scroll-mr","scroll-ml"],"scroll-my":["scroll-mt","scroll-mb"],"scroll-p":["scroll-px","scroll-py","scroll-ps","scroll-pe","scroll-pt","scroll-pr","scroll-pb","scroll-pl"],"scroll-px":["scroll-pr","scroll-pl"],"scroll-py":["scroll-pt","scroll-pb"],touch:["touch-x","touch-y","touch-pz"],"touch-x":["touch"],"touch-y":["touch"],"touch-pz":["touch"]},conflictingClassGroupModifiers:{"font-size":["leading"]}}},HI=gI(RI);function O(...e){return HI(wv(e))}const MI=ES,Dv=m.forwardRef(({className:e,...t},r)=>l.jsx(pv,{ref:r,className:O("fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",e),...t}));Dv.displayName=pv.displayName;const EI=Wa("group pointer-events-auto relative flex w-full items-center justify-between space-x-4 overflow-hidden rounded-md border p-6 pr-8 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",{variants:{variant:{default:"border bg-background text-foreground",destructive:"destructive group border-destructive bg-destructive text-destructive-foreground"}},defaultVariants:{variant:"default"}}),Tv=m.forwardRef(({className:e,variant:t,...r},o)=>l.jsx(gv,{ref:o,className:O(EI({variant:t}),e),...r}));Tv.displayName=gv.displayName;const NI=m.forwardRef(({className:e,...t},r)=>l.jsx(fv,{ref:r,className:O("inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium ring-offset-background transition-colors group-[.destructive]:border-muted/40 hover:bg-secondary group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 group-[.destructive]:focus:ring-destructive disabled:pointer-events-none disabled:opacity-50",e),...t}));NI.displayName=fv.displayName;const zv=m.forwardRef(({className:e,...t},r)=>l.jsx(vv,{ref:r,className:O("absolute right-2 top-2 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity group-hover:opacity-100 group-[.destructive]:text-red-300 hover:text-foreground group-[.destructive]:hover:text-red-50 focus:opacity-100 focus:outline-none focus:ring-2 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",e),"toast-close":"",...t,children:l.jsx(bu,{className:"h-4 w-4"})}));zv.displayName=vv.displayName;const Rv=m.forwardRef(({className:e,...t},r)=>l.jsx(mv,{ref:r,className:O("text-sm font-semibold",e),...t}));Rv.displayName=mv.displayName;const Hv=m.forwardRef(({className:e,...t},r)=>l.jsx(hv,{ref:r,className:O("text-sm opacity-90",e),...t}));Hv.displayName=hv.displayName;function qI(){const{toasts:e}=jA();return l.jsxs(MI,{children:[e.map(function({id:t,title:r,description:o,action:n,...s}){return l.jsxs(Tv,{...s,children:[l.jsxs("div",{className:"grid gap-1",children:[r&&l.jsx(Rv,{children:r}),o&&l.jsx(Hv,{children:o})]}),n,l.jsx(zv,{})]},t)}),l.jsx(Dv,{})]})}var Pg=["light","dark"],LI="(prefers-color-scheme: dark)",OI=m.createContext(void 0),_I={setTheme:e=>{},themes:[]},VI=()=>{var e;return(e=m.useContext(OI))!=null?e:_I};m.memo(({forcedTheme:e,storageKey:t,attribute:r,enableSystem:o,enableColorScheme:n,defaultTheme:s,value:i,attrs:a,nonce:d})=>{let c=s==="system",p=r==="class"?`var d=document.documentElement,c=d.classList;${`c.remove(${a.map(k=>`'${k}'`).join(",")})`};`:`var d=document.documentElement,n='${r}',s='setAttribute';`,u=n?Pg.includes(s)&&s?`if(e==='light'||e==='dark'||!e)d.style.colorScheme=e||'${s}'`:"if(e==='light'||e==='dark')d.style.colorScheme=e":"",f=(k,y=!1,b=!0)=>{let v=i?i[k]:k,h=y?k+"|| ''":`'${v}'`,w="";return n&&b&&!y&&Pg.includes(k)&&(w+=`d.style.colorScheme = '${k}';`),r==="class"?y||v?w+=`c.add(${h})`:w+="null":v&&(w+=`d[s](n,${h})`),w},g=e?`!function(){${p}${f(e)}}()`:o?`!function(){try{${p}var e=localStorage.getItem('${t}');if('system'===e||(!e&&${c})){var t='${LI}',m=window.matchMedia(t);if(m.media!==t||m.matches){${f("dark")}}else{${f("light")}}}else if(e){${i?`var x=${JSON.stringify(i)};`:""}${f(i?"x[e]":"e",!0)}}${c?"":"else{"+f(s,!1,!1)+"}"}${u}}catch(e){}}()`:`!function(){try{${p}var e=localStorage.getItem('${t}');if(e){${i?`var x=${JSON.stringify(i)};`:""}${f(i?"x[e]":"e",!0)}}else{${f(s,!1,!1)};}${u}}catch(t){}}();`;return m.createElement("script",{nonce:d,dangerouslySetInnerHTML:{__html:g}})});var BI=e=>{switch(e){case"success":return UI;case"info":return FI;case"warning":return GI;case"error":return KI;default:return null}},WI=Array(12).fill(0),jI=({visible:e,className:t})=>z.createElement("div",{className:["sonner-loading-wrapper",t].filter(Boolean).join(" "),"data-visible":e},z.createElement("div",{className:"sonner-spinner"},WI.map((r,o)=>z.createElement("div",{className:"sonner-loading-bar",key:`spinner-bar-${o}`})))),UI=z.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 20 20",fill:"currentColor",height:"20",width:"20"},z.createElement("path",{fillRule:"evenodd",d:"M10 18a8 8 0 100-16 8 8 0 000 16zm3.857-9.809a.75.75 0 00-1.214-.882l-3.483 4.79-1.88-1.88a.75.75 0 10-1.06 1.061l2.5 2.5a.75.75 0 001.137-.089l4-5.5z",clipRule:"evenodd"})),GI=z.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 24 24",fill:"currentColor",height:"20",width:"20"},z.createElement("path",{fillRule:"evenodd",d:"M9.401 3.003c1.155-2 4.043-2 5.197 0l7.355 12.748c1.154 2-.29 4.5-2.599 4.5H4.645c-2.309 0-3.752-2.5-2.598-4.5L9.4 3.003zM12 8.25a.75.75 0 01.75.75v3.75a.75.75 0 01-1.5 0V9a.75.75 0 01.75-.75zm0 8.25a.75.75 0 100-1.5.75.75 0 000 1.5z",clipRule:"evenodd"})),FI=z.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 20 20",fill:"currentColor",height:"20",width:"20"},z.createElement("path",{fillRule:"evenodd",d:"M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7-4a1 1 0 11-2 0 1 1 0 012 0zM9 9a.75.75 0 000 1.5h.253a.25.25 0 01.244.304l-.459 2.066A1.75 1.75 0 0010.747 15H11a.75.75 0 000-1.5h-.253a.25.25 0 01-.244-.304l.459-2.066A1.75 1.75 0 009.253 9H9z",clipRule:"evenodd"})),KI=z.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 20 20",fill:"currentColor",height:"20",width:"20"},z.createElement("path",{fillRule:"evenodd",d:"M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-8-5a.75.75 0 01.75.75v4.5a.75.75 0 01-1.5 0v-4.5A.75.75 0 0110 5zm0 10a1 1 0 100-2 1 1 0 000 2z",clipRule:"evenodd"})),$I=z.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"12",height:"12",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"1.5",strokeLinecap:"round",strokeLinejoin:"round"},z.createElement("line",{x1:"18",y1:"6",x2:"6",y2:"18"}),z.createElement("line",{x1:"6",y1:"6",x2:"18",y2:"18"})),QI=()=>{let[e,t]=z.useState(document.hidden);return z.useEffect(()=>{let r=()=>{t(document.hidden)};return document.addEventListener("visibilitychange",r),()=>window.removeEventListener("visibilitychange",r)},[]),e},sd=1,YI=class{constructor(){this.subscribe=e=>(this.subscribers.push(e),()=>{let t=this.subscribers.indexOf(e);this.subscribers.splice(t,1)}),this.publish=e=>{this.subscribers.forEach(t=>t(e))},this.addToast=e=>{this.publish(e),this.toasts=[...this.toasts,e]},this.create=e=>{var t;let{message:r,...o}=e,n=typeof(e==null?void 0:e.id)=="number"||((t=e.id)==null?void 0:t.length)>0?e.id:sd++,s=this.toasts.find(a=>a.id===n),i=e.dismissible===void 0?!0:e.dismissible;return this.dismissedToasts.has(n)&&this.dismissedToasts.delete(n),s?this.toasts=this.toasts.map(a=>a.id===n?(this.publish({...a,...e,id:n,title:r}),{...a,...e,id:n,dismissible:i,title:r}):a):this.addToast({title:r,...o,dismissible:i,id:n}),n},this.dismiss=e=>(this.dismissedToasts.add(e),e||this.toasts.forEach(t=>{this.subscribers.forEach(r=>r({id:t.id,dismiss:!0}))}),this.subscribers.forEach(t=>t({id:e,dismiss:!0})),e),this.message=(e,t)=>this.create({...t,message:e}),this.error=(e,t)=>this.create({...t,message:e,type:"error"}),this.success=(e,t)=>this.create({...t,type:"success",message:e}),this.info=(e,t)=>this.create({...t,type:"info",message:e}),this.warning=(e,t)=>this.create({...t,type:"warning",message:e}),this.loading=(e,t)=>this.create({...t,type:"loading",message:e}),this.promise=(e,t)=>{if(!t)return;let r;t.loading!==void 0&&(r=this.create({...t,promise:e,type:"loading",message:t.loading,description:typeof t.description!="function"?t.description:void 0}));let o=e instanceof Promise?e:e(),n=r!==void 0,s,i=o.then(async d=>{if(s=["resolve",d],z.isValidElement(d))n=!1,this.create({id:r,type:"default",message:d});else if(ZI(d)&&!d.ok){n=!1;let c=typeof t.error=="function"?await t.error(`HTTP error! status: ${d.status}`):t.error,p=typeof t.description=="function"?await t.description(`HTTP error! status: ${d.status}`):t.description;this.create({id:r,type:"error",message:c,description:p})}else if(t.success!==void 0){n=!1;let c=typeof t.success=="function"?await t.success(d):t.success,p=typeof t.description=="function"?await t.description(d):t.description;this.create({id:r,type:"success",message:c,description:p})}}).catch(async d=>{if(s=["reject",d],t.error!==void 0){n=!1;let c=typeof t.error=="function"?await t.error(d):t.error,p=typeof t.description=="function"?await t.description(d):t.description;this.create({id:r,type:"error",message:c,description:p})}}).finally(()=>{var d;n&&(this.dismiss(r),r=void 0),(d=t.finally)==null||d.call(t)}),a=()=>new Promise((d,c)=>i.then(()=>s[0]==="reject"?c(s[1]):d(s[1])).catch(c));return typeof r!="string"&&typeof r!="number"?{unwrap:a}:Object.assign(r,{unwrap:a})},this.custom=(e,t)=>{let r=(t==null?void 0:t.id)||sd++;return this.create({jsx:e(r),id:r,...t}),r},this.getActiveToasts=()=>this.toasts.filter(e=>!this.dismissedToasts.has(e.id)),this.subscribers=[],this.toasts=[],this.dismissedToasts=new Set}},Ue=new YI,JI=(e,t)=>{let r=(t==null?void 0:t.id)||sd++;return Ue.addToast({title:e,...t,id:r}),r},ZI=e=>e&&typeof e=="object"&&"ok"in e&&typeof e.ok=="boolean"&&"status"in e&&typeof e.status=="number",XI=JI,eP=()=>Ue.toasts,tP=()=>Ue.getActiveToasts();Object.assign(XI,{success:Ue.success,info:Ue.info,warning:Ue.warning,error:Ue.error,custom:Ue.custom,message:Ue.message,promise:Ue.promise,dismiss:Ue.dismiss,loading:Ue.loading},{getHistory:eP,getToasts:tP});function rP(e,{insertAt:t}={}){if(typeof document>"u")return;let r=document.head||document.getElementsByTagName("head")[0],o=document.createElement("style");o.type="text/css",t==="top"&&r.firstChild?r.insertBefore(o,r.firstChild):r.appendChild(o),o.styleSheet?o.styleSheet.cssText=e:o.appendChild(document.createTextNode(e))}rP(`:where(html[dir="ltr"]),:where([data-sonner-toaster][dir="ltr"]){--toast-icon-margin-start: -3px;--toast-icon-margin-end: 4px;--toast-svg-margin-start: -1px;--toast-svg-margin-end: 0px;--toast-button-margin-start: auto;--toast-button-margin-end: 0;--toast-close-button-start: 0;--toast-close-button-end: unset;--toast-close-button-transform: translate(-35%, -35%)}:where(html[dir="rtl"]),:where([data-sonner-toaster][dir="rtl"]){--toast-icon-margin-start: 4px;--toast-icon-margin-end: -3px;--toast-svg-margin-start: 0px;--toast-svg-margin-end: -1px;--toast-button-margin-start: 0;--toast-button-margin-end: auto;--toast-close-button-start: unset;--toast-close-button-end: 0;--toast-close-button-transform: translate(35%, -35%)}:where([data-sonner-toaster]){position:fixed;width:var(--width);font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;--gray1: hsl(0, 0%, 99%);--gray2: hsl(0, 0%, 97.3%);--gray3: hsl(0, 0%, 95.1%);--gray4: hsl(0, 0%, 93%);--gray5: hsl(0, 0%, 90.9%);--gray6: hsl(0, 0%, 88.7%);--gray7: hsl(0, 0%, 85.8%);--gray8: hsl(0, 0%, 78%);--gray9: hsl(0, 0%, 56.1%);--gray10: hsl(0, 0%, 52.3%);--gray11: hsl(0, 0%, 43.5%);--gray12: hsl(0, 0%, 9%);--border-radius: 8px;box-sizing:border-box;padding:0;margin:0;list-style:none;outline:none;z-index:999999999;transition:transform .4s ease}:where([data-sonner-toaster][data-lifted="true"]){transform:translateY(-10px)}@media (hover: none) and (pointer: coarse){:where([data-sonner-toaster][data-lifted="true"]){transform:none}}:where([data-sonner-toaster][data-x-position="right"]){right:var(--offset-right)}:where([data-sonner-toaster][data-x-position="left"]){left:var(--offset-left)}:where([data-sonner-toaster][data-x-position="center"]){left:50%;transform:translate(-50%)}:where([data-sonner-toaster][data-y-position="top"]){top:var(--offset-top)}:where([data-sonner-toaster][data-y-position="bottom"]){bottom:var(--offset-bottom)}:where([data-sonner-toast]){--y: translateY(100%);--lift-amount: calc(var(--lift) * var(--gap));z-index:var(--z-index);position:absolute;opacity:0;transform:var(--y);filter:blur(0);touch-action:none;transition:transform .4s,opacity .4s,height .4s,box-shadow .2s;box-sizing:border-box;outline:none;overflow-wrap:anywhere}:where([data-sonner-toast][data-styled="true"]){padding:16px;background:var(--normal-bg);border:1px solid var(--normal-border);color:var(--normal-text);border-radius:var(--border-radius);box-shadow:0 4px 12px #0000001a;width:var(--width);font-size:13px;display:flex;align-items:center;gap:6px}:where([data-sonner-toast]:focus-visible){box-shadow:0 4px 12px #0000001a,0 0 0 2px #0003}:where([data-sonner-toast][data-y-position="top"]){top:0;--y: translateY(-100%);--lift: 1;--lift-amount: calc(1 * var(--gap))}:where([data-sonner-toast][data-y-position="bottom"]){bottom:0;--y: translateY(100%);--lift: -1;--lift-amount: calc(var(--lift) * var(--gap))}:where([data-sonner-toast]) :where([data-description]){font-weight:400;line-height:1.4;color:inherit}:where([data-sonner-toast]) :where([data-title]){font-weight:500;line-height:1.5;color:inherit}:where([data-sonner-toast]) :where([data-icon]){display:flex;height:16px;width:16px;position:relative;justify-content:flex-start;align-items:center;flex-shrink:0;margin-left:var(--toast-icon-margin-start);margin-right:var(--toast-icon-margin-end)}:where([data-sonner-toast][data-promise="true"]) :where([data-icon])>svg{opacity:0;transform:scale(.8);transform-origin:center;animation:sonner-fade-in .3s ease forwards}:where([data-sonner-toast]) :where([data-icon])>*{flex-shrink:0}:where([data-sonner-toast]) :where([data-icon]) svg{margin-left:var(--toast-svg-margin-start);margin-right:var(--toast-svg-margin-end)}:where([data-sonner-toast]) :where([data-content]){display:flex;flex-direction:column;gap:2px}[data-sonner-toast][data-styled=true] [data-button]{border-radius:4px;padding-left:8px;padding-right:8px;height:24px;font-size:12px;color:var(--normal-bg);background:var(--normal-text);margin-left:var(--toast-button-margin-start);margin-right:var(--toast-button-margin-end);border:none;cursor:pointer;outline:none;display:flex;align-items:center;flex-shrink:0;transition:opacity .4s,box-shadow .2s}:where([data-sonner-toast]) :where([data-button]):focus-visible{box-shadow:0 0 0 2px #0006}:where([data-sonner-toast]) :where([data-button]):first-of-type{margin-left:var(--toast-button-margin-start);margin-right:var(--toast-button-margin-end)}:where([data-sonner-toast]) :where([data-cancel]){color:var(--normal-text);background:rgba(0,0,0,.08)}:where([data-sonner-toast][data-theme="dark"]) :where([data-cancel]){background:rgba(255,255,255,.3)}:where([data-sonner-toast]) :where([data-close-button]){position:absolute;left:var(--toast-close-button-start);right:var(--toast-close-button-end);top:0;height:20px;width:20px;display:flex;justify-content:center;align-items:center;padding:0;color:var(--gray12);border:1px solid var(--gray4);transform:var(--toast-close-button-transform);border-radius:50%;cursor:pointer;z-index:1;transition:opacity .1s,background .2s,border-color .2s}[data-sonner-toast] [data-close-button]{background:var(--gray1)}:where([data-sonner-toast]) :where([data-close-button]):focus-visible{box-shadow:0 4px 12px #0000001a,0 0 0 2px #0003}:where([data-sonner-toast]) :where([data-disabled="true"]){cursor:not-allowed}:where([data-sonner-toast]):hover :where([data-close-button]):hover{background:var(--gray2);border-color:var(--gray5)}:where([data-sonner-toast][data-swiping="true"]):before{content:"";position:absolute;left:-50%;right:-50%;height:100%;z-index:-1}:where([data-sonner-toast][data-y-position="top"][data-swiping="true"]):before{bottom:50%;transform:scaleY(3) translateY(50%)}:where([data-sonner-toast][data-y-position="bottom"][data-swiping="true"]):before{top:50%;transform:scaleY(3) translateY(-50%)}:where([data-sonner-toast][data-swiping="false"][data-removed="true"]):before{content:"";position:absolute;inset:0;transform:scaleY(2)}:where([data-sonner-toast]):after{content:"";position:absolute;left:0;height:calc(var(--gap) + 1px);bottom:100%;width:100%}:where([data-sonner-toast][data-mounted="true"]){--y: translateY(0);opacity:1}:where([data-sonner-toast][data-expanded="false"][data-front="false"]){--scale: var(--toasts-before) * .05 + 1;--y: translateY(calc(var(--lift-amount) * var(--toasts-before))) scale(calc(-1 * var(--scale)));height:var(--front-toast-height)}:where([data-sonner-toast])>*{transition:opacity .4s}:where([data-sonner-toast][data-expanded="false"][data-front="false"][data-styled="true"])>*{opacity:0}:where([data-sonner-toast][data-visible="false"]){opacity:0;pointer-events:none}:where([data-sonner-toast][data-mounted="true"][data-expanded="true"]){--y: translateY(calc(var(--lift) * var(--offset)));height:var(--initial-height)}:where([data-sonner-toast][data-removed="true"][data-front="true"][data-swipe-out="false"]){--y: translateY(calc(var(--lift) * -100%));opacity:0}:where([data-sonner-toast][data-removed="true"][data-front="false"][data-swipe-out="false"][data-expanded="true"]){--y: translateY(calc(var(--lift) * var(--offset) + var(--lift) * -100%));opacity:0}:where([data-sonner-toast][data-removed="true"][data-front="false"][data-swipe-out="false"][data-expanded="false"]){--y: translateY(40%);opacity:0;transition:transform .5s,opacity .2s}:where([data-sonner-toast][data-removed="true"][data-front="false"]):before{height:calc(var(--initial-height) + 20%)}[data-sonner-toast][data-swiping=true]{transform:var(--y) translateY(var(--swipe-amount-y, 0px)) translate(var(--swipe-amount-x, 0px));transition:none}[data-sonner-toast][data-swiped=true]{user-select:none}[data-sonner-toast][data-swipe-out=true][data-y-position=bottom],[data-sonner-toast][data-swipe-out=true][data-y-position=top]{animation-duration:.2s;animation-timing-function:ease-out;animation-fill-mode:forwards}[data-sonner-toast][data-swipe-out=true][data-swipe-direction=left]{animation-name:swipe-out-left}[data-sonner-toast][data-swipe-out=true][data-swipe-direction=right]{animation-name:swipe-out-right}[data-sonner-toast][data-swipe-out=true][data-swipe-direction=up]{animation-name:swipe-out-up}[data-sonner-toast][data-swipe-out=true][data-swipe-direction=down]{animation-name:swipe-out-down}@keyframes swipe-out-left{0%{transform:var(--y) translate(var(--swipe-amount-x));opacity:1}to{transform:var(--y) translate(calc(var(--swipe-amount-x) - 100%));opacity:0}}@keyframes swipe-out-right{0%{transform:var(--y) translate(var(--swipe-amount-x));opacity:1}to{transform:var(--y) translate(calc(var(--swipe-amount-x) + 100%));opacity:0}}@keyframes swipe-out-up{0%{transform:var(--y) translateY(var(--swipe-amount-y));opacity:1}to{transform:var(--y) translateY(calc(var(--swipe-amount-y) - 100%));opacity:0}}@keyframes swipe-out-down{0%{transform:var(--y) translateY(var(--swipe-amount-y));opacity:1}to{transform:var(--y) translateY(calc(var(--swipe-amount-y) + 100%));opacity:0}}@media (max-width: 600px){[data-sonner-toaster]{position:fixed;right:var(--mobile-offset-right);left:var(--mobile-offset-left);width:100%}[data-sonner-toaster][dir=rtl]{left:calc(var(--mobile-offset-left) * -1)}[data-sonner-toaster] [data-sonner-toast]{left:0;right:0;width:calc(100% - var(--mobile-offset-left) * 2)}[data-sonner-toaster][data-x-position=left]{left:var(--mobile-offset-left)}[data-sonner-toaster][data-y-position=bottom]{bottom:var(--mobile-offset-bottom)}[data-sonner-toaster][data-y-position=top]{top:var(--mobile-offset-top)}[data-sonner-toaster][data-x-position=center]{left:var(--mobile-offset-left);right:var(--mobile-offset-right);transform:none}}[data-sonner-toaster][data-theme=light]{--normal-bg: #fff;--normal-border: var(--gray4);--normal-text: var(--gray12);--success-bg: hsl(143, 85%, 96%);--success-border: hsl(145, 92%, 91%);--success-text: hsl(140, 100%, 27%);--info-bg: hsl(208, 100%, 97%);--info-border: hsl(221, 91%, 91%);--info-text: hsl(210, 92%, 45%);--warning-bg: hsl(49, 100%, 97%);--warning-border: hsl(49, 91%, 91%);--warning-text: hsl(31, 92%, 45%);--error-bg: hsl(359, 100%, 97%);--error-border: hsl(359, 100%, 94%);--error-text: hsl(360, 100%, 45%)}[data-sonner-toaster][data-theme=light] [data-sonner-toast][data-invert=true]{--normal-bg: #000;--normal-border: hsl(0, 0%, 20%);--normal-text: var(--gray1)}[data-sonner-toaster][data-theme=dark] [data-sonner-toast][data-invert=true]{--normal-bg: #fff;--normal-border: var(--gray3);--normal-text: var(--gray12)}[data-sonner-toaster][data-theme=dark]{--normal-bg: #000;--normal-bg-hover: hsl(0, 0%, 12%);--normal-border: hsl(0, 0%, 20%);--normal-border-hover: hsl(0, 0%, 25%);--normal-text: var(--gray1);--success-bg: hsl(150, 100%, 6%);--success-border: hsl(147, 100%, 12%);--success-text: hsl(150, 86%, 65%);--info-bg: hsl(215, 100%, 6%);--info-border: hsl(223, 100%, 12%);--info-text: hsl(216, 87%, 65%);--warning-bg: hsl(64, 100%, 6%);--warning-border: hsl(60, 100%, 12%);--warning-text: hsl(46, 87%, 65%);--error-bg: hsl(358, 76%, 10%);--error-border: hsl(357, 89%, 16%);--error-text: hsl(358, 100%, 81%)}[data-sonner-toaster][data-theme=dark] [data-sonner-toast] [data-close-button]{background:var(--normal-bg);border-color:var(--normal-border);color:var(--normal-text)}[data-sonner-toaster][data-theme=dark] [data-sonner-toast] [data-close-button]:hover{background:var(--normal-bg-hover);border-color:var(--normal-border-hover)}[data-rich-colors=true][data-sonner-toast][data-type=success],[data-rich-colors=true][data-sonner-toast][data-type=success] [data-close-button]{background:var(--success-bg);border-color:var(--success-border);color:var(--success-text)}[data-rich-colors=true][data-sonner-toast][data-type=info],[data-rich-colors=true][data-sonner-toast][data-type=info] [data-close-button]{background:var(--info-bg);border-color:var(--info-border);color:var(--info-text)}[data-rich-colors=true][data-sonner-toast][data-type=warning],[data-rich-colors=true][data-sonner-toast][data-type=warning] [data-close-button]{background:var(--warning-bg);border-color:var(--warning-border);color:var(--warning-text)}[data-rich-colors=true][data-sonner-toast][data-type=error],[data-rich-colors=true][data-sonner-toast][data-type=error] [data-close-button]{background:var(--error-bg);border-color:var(--error-border);color:var(--error-text)}.sonner-loading-wrapper{--size: 16px;height:var(--size);width:var(--size);position:absolute;inset:0;z-index:10}.sonner-loading-wrapper[data-visible=false]{transform-origin:center;animation:sonner-fade-out .2s ease forwards}.sonner-spinner{position:relative;top:50%;left:50%;height:var(--size);width:var(--size)}.sonner-loading-bar{animation:sonner-spin 1.2s linear infinite;background:var(--gray11);border-radius:6px;height:8%;left:-10%;position:absolute;top:-3.9%;width:24%}.sonner-loading-bar:nth-child(1){animation-delay:-1.2s;transform:rotate(.0001deg) translate(146%)}.sonner-loading-bar:nth-child(2){animation-delay:-1.1s;transform:rotate(30deg) translate(146%)}.sonner-loading-bar:nth-child(3){animation-delay:-1s;transform:rotate(60deg) translate(146%)}.sonner-loading-bar:nth-child(4){animation-delay:-.9s;transform:rotate(90deg) translate(146%)}.sonner-loading-bar:nth-child(5){animation-delay:-.8s;transform:rotate(120deg) translate(146%)}.sonner-loading-bar:nth-child(6){animation-delay:-.7s;transform:rotate(150deg) translate(146%)}.sonner-loading-bar:nth-child(7){animation-delay:-.6s;transform:rotate(180deg) translate(146%)}.sonner-loading-bar:nth-child(8){animation-delay:-.5s;transform:rotate(210deg) translate(146%)}.sonner-loading-bar:nth-child(9){animation-delay:-.4s;transform:rotate(240deg) translate(146%)}.sonner-loading-bar:nth-child(10){animation-delay:-.3s;transform:rotate(270deg) translate(146%)}.sonner-loading-bar:nth-child(11){animation-delay:-.2s;transform:rotate(300deg) translate(146%)}.sonner-loading-bar:nth-child(12){animation-delay:-.1s;transform:rotate(330deg) translate(146%)}@keyframes sonner-fade-in{0%{opacity:0;transform:scale(.8)}to{opacity:1;transform:scale(1)}}@keyframes sonner-fade-out{0%{opacity:1;transform:scale(1)}to{opacity:0;transform:scale(.8)}}@keyframes sonner-spin{0%{opacity:1}to{opacity:.15}}@media (prefers-reduced-motion){[data-sonner-toast],[data-sonner-toast]>*,.sonner-loading-bar{transition:none!important;animation:none!important}}.sonner-loader{position:absolute;top:50%;left:50%;transform:translate(-50%,-50%);transform-origin:center;transition:opacity .2s,transform .2s}.sonner-loader[data-visible=false]{opacity:0;transform:scale(.8) translate(-50%,-50%)}
`);function fi(e){return e.label!==void 0}var oP=3,nP="32px",sP="16px",Cg=4e3,iP=356,aP=14,lP=20,cP=200;function kt(...e){return e.filter(Boolean).join(" ")}function dP(e){let[t,r]=e.split("-"),o=[];return t&&o.push(t),r&&o.push(r),o}var uP=e=>{var t,r,o,n,s,i,a,d,c,p,u;let{invert:f,toast:g,unstyled:k,interacting:y,setHeights:b,visibleToasts:v,heights:h,index:w,toasts:A,expanded:S,removeToast:I,defaultRichColors:P,closeButton:D,style:M,cancelButtonStyle:H,actionButtonStyle:V,className:L="",descriptionClassName:$="",duration:E,position:J,gap:B,loadingIcon:U,expandByDefault:x,classNames:T,icons:N,closeButtonAriaLabel:G="Close toast",pauseWhenPageIsHidden:_}=e,[Q,Z]=z.useState(null),[we,ze]=z.useState(null),[te,Ro]=z.useState(!1),[hr,to]=z.useState(!1),[fr,Ho]=z.useState(!1),[vr,Ks]=z.useState(!1),[pl,$s]=z.useState(!1),[gl,Ln]=z.useState(0),[Mo,Qu]=z.useState(0),On=z.useRef(g.duration||E||Cg),Yu=z.useRef(null),ro=z.useRef(null),ob=w===0,nb=w+1<=v,nt=g.type,Eo=g.dismissible!==!1,sb=g.className||"",ib=g.descriptionClassName||"",Qs=z.useMemo(()=>h.findIndex(W=>W.toastId===g.id)||0,[h,g.id]),ab=z.useMemo(()=>{var W;return(W=g.closeButton)!=null?W:D},[g.closeButton,D]),Ju=z.useMemo(()=>g.duration||E||Cg,[g.duration,E]),ml=z.useRef(0),No=z.useRef(0),Zu=z.useRef(0),qo=z.useRef(null),[lb,cb]=J.split("-"),Xu=z.useMemo(()=>h.reduce((W,oe,de)=>de>=Qs?W:W+oe.height,0),[h,Qs]),ep=QI(),db=g.invert||f,hl=nt==="loading";No.current=z.useMemo(()=>Qs*B+Xu,[Qs,Xu]),z.useEffect(()=>{On.current=Ju},[Ju]),z.useEffect(()=>{Ro(!0)},[]),z.useEffect(()=>{let W=ro.current;if(W){let oe=W.getBoundingClientRect().height;return Qu(oe),b(de=>[{toastId:g.id,height:oe,position:g.position},...de]),()=>b(de=>de.filter(vt=>vt.toastId!==g.id))}},[b,g.id]),z.useLayoutEffect(()=>{if(!te)return;let W=ro.current,oe=W.style.height;W.style.height="auto";let de=W.getBoundingClientRect().height;W.style.height=oe,Qu(de),b(vt=>vt.find(yt=>yt.toastId===g.id)?vt.map(yt=>yt.toastId===g.id?{...yt,height:de}:yt):[{toastId:g.id,height:de,position:g.position},...vt])},[te,g.title,g.description,b,g.id]);let yr=z.useCallback(()=>{to(!0),Ln(No.current),b(W=>W.filter(oe=>oe.toastId!==g.id)),setTimeout(()=>{I(g)},cP)},[g,I,b,No]);z.useEffect(()=>{if(g.promise&&nt==="loading"||g.duration===1/0||g.type==="loading")return;let W;return S||y||_&&ep?(()=>{if(Zu.current<ml.current){let oe=new Date().getTime()-ml.current;On.current=On.current-oe}Zu.current=new Date().getTime()})():On.current!==1/0&&(ml.current=new Date().getTime(),W=setTimeout(()=>{var oe;(oe=g.onAutoClose)==null||oe.call(g,g),yr()},On.current)),()=>clearTimeout(W)},[S,y,g,nt,_,ep,yr]),z.useEffect(()=>{g.delete&&yr()},[yr,g.delete]);function ub(){var W,oe,de;return N!=null&&N.loading?z.createElement("div",{className:kt(T==null?void 0:T.loader,(W=g==null?void 0:g.classNames)==null?void 0:W.loader,"sonner-loader"),"data-visible":nt==="loading"},N.loading):U?z.createElement("div",{className:kt(T==null?void 0:T.loader,(oe=g==null?void 0:g.classNames)==null?void 0:oe.loader,"sonner-loader"),"data-visible":nt==="loading"},U):z.createElement(jI,{className:kt(T==null?void 0:T.loader,(de=g==null?void 0:g.classNames)==null?void 0:de.loader),visible:nt==="loading"})}return z.createElement("li",{tabIndex:0,ref:ro,className:kt(L,sb,T==null?void 0:T.toast,(t=g==null?void 0:g.classNames)==null?void 0:t.toast,T==null?void 0:T.default,T==null?void 0:T[nt],(r=g==null?void 0:g.classNames)==null?void 0:r[nt]),"data-sonner-toast":"","data-rich-colors":(o=g.richColors)!=null?o:P,"data-styled":!(g.jsx||g.unstyled||k),"data-mounted":te,"data-promise":!!g.promise,"data-swiped":pl,"data-removed":hr,"data-visible":nb,"data-y-position":lb,"data-x-position":cb,"data-index":w,"data-front":ob,"data-swiping":fr,"data-dismissible":Eo,"data-type":nt,"data-invert":db,"data-swipe-out":vr,"data-swipe-direction":we,"data-expanded":!!(S||x&&te),style:{"--index":w,"--toasts-before":w,"--z-index":A.length-w,"--offset":`${hr?gl:No.current}px`,"--initial-height":x?"auto":`${Mo}px`,...M,...g.style},onDragEnd:()=>{Ho(!1),Z(null),qo.current=null},onPointerDown:W=>{hl||!Eo||(Yu.current=new Date,Ln(No.current),W.target.setPointerCapture(W.pointerId),W.target.tagName!=="BUTTON"&&(Ho(!0),qo.current={x:W.clientX,y:W.clientY}))},onPointerUp:()=>{var W,oe,de,vt;if(vr||!Eo)return;qo.current=null;let yt=Number(((W=ro.current)==null?void 0:W.style.getPropertyValue("--swipe-amount-x").replace("px",""))||0),wr=Number(((oe=ro.current)==null?void 0:oe.style.getPropertyValue("--swipe-amount-y").replace("px",""))||0),oo=new Date().getTime()-((de=Yu.current)==null?void 0:de.getTime()),wt=Q==="x"?yt:wr,br=Math.abs(wt)/oo;if(Math.abs(wt)>=lP||br>.11){Ln(No.current),(vt=g.onDismiss)==null||vt.call(g,g),ze(Q==="x"?yt>0?"right":"left":wr>0?"down":"up"),yr(),Ks(!0),$s(!1);return}Ho(!1),Z(null)},onPointerMove:W=>{var oe,de,vt,yt;if(!qo.current||!Eo||((oe=window.getSelection())==null?void 0:oe.toString().length)>0)return;let wr=W.clientY-qo.current.y,oo=W.clientX-qo.current.x,wt=(de=e.swipeDirections)!=null?de:dP(J);!Q&&(Math.abs(oo)>1||Math.abs(wr)>1)&&Z(Math.abs(oo)>Math.abs(wr)?"x":"y");let br={x:0,y:0};Q==="y"?(wt.includes("top")||wt.includes("bottom"))&&(wt.includes("top")&&wr<0||wt.includes("bottom")&&wr>0)&&(br.y=wr):Q==="x"&&(wt.includes("left")||wt.includes("right"))&&(wt.includes("left")&&oo<0||wt.includes("right")&&oo>0)&&(br.x=oo),(Math.abs(br.x)>0||Math.abs(br.y)>0)&&$s(!0),(vt=ro.current)==null||vt.style.setProperty("--swipe-amount-x",`${br.x}px`),(yt=ro.current)==null||yt.style.setProperty("--swipe-amount-y",`${br.y}px`)}},ab&&!g.jsx?z.createElement("button",{"aria-label":G,"data-disabled":hl,"data-close-button":!0,onClick:hl||!Eo?()=>{}:()=>{var W;yr(),(W=g.onDismiss)==null||W.call(g,g)},className:kt(T==null?void 0:T.closeButton,(n=g==null?void 0:g.classNames)==null?void 0:n.closeButton)},(s=N==null?void 0:N.close)!=null?s:$I):null,g.jsx||m.isValidElement(g.title)?g.jsx?g.jsx:typeof g.title=="function"?g.title():g.title:z.createElement(z.Fragment,null,nt||g.icon||g.promise?z.createElement("div",{"data-icon":"",className:kt(T==null?void 0:T.icon,(i=g==null?void 0:g.classNames)==null?void 0:i.icon)},g.promise||g.type==="loading"&&!g.icon?g.icon||ub():null,g.type!=="loading"?g.icon||(N==null?void 0:N[nt])||BI(nt):null):null,z.createElement("div",{"data-content":"",className:kt(T==null?void 0:T.content,(a=g==null?void 0:g.classNames)==null?void 0:a.content)},z.createElement("div",{"data-title":"",className:kt(T==null?void 0:T.title,(d=g==null?void 0:g.classNames)==null?void 0:d.title)},typeof g.title=="function"?g.title():g.title),g.description?z.createElement("div",{"data-description":"",className:kt($,ib,T==null?void 0:T.description,(c=g==null?void 0:g.classNames)==null?void 0:c.description)},typeof g.description=="function"?g.description():g.description):null),m.isValidElement(g.cancel)?g.cancel:g.cancel&&fi(g.cancel)?z.createElement("button",{"data-button":!0,"data-cancel":!0,style:g.cancelButtonStyle||H,onClick:W=>{var oe,de;fi(g.cancel)&&Eo&&((de=(oe=g.cancel).onClick)==null||de.call(oe,W),yr())},className:kt(T==null?void 0:T.cancelButton,(p=g==null?void 0:g.classNames)==null?void 0:p.cancelButton)},g.cancel.label):null,m.isValidElement(g.action)?g.action:g.action&&fi(g.action)?z.createElement("button",{"data-button":!0,"data-action":!0,style:g.actionButtonStyle||V,onClick:W=>{var oe,de;fi(g.action)&&((de=(oe=g.action).onClick)==null||de.call(oe,W),!W.defaultPrevented&&yr())},className:kt(T==null?void 0:T.actionButton,(u=g==null?void 0:g.classNames)==null?void 0:u.actionButton)},g.action.label):null))};function xg(){if(typeof window>"u"||typeof document>"u")return"ltr";let e=document.documentElement.getAttribute("dir");return e==="auto"||!e?window.getComputedStyle(document.documentElement).direction:e}function pP(e,t){let r={};return[e,t].forEach((o,n)=>{let s=n===1,i=s?"--mobile-offset":"--offset",a=s?sP:nP;function d(c){["top","right","bottom","left"].forEach(p=>{r[`${i}-${p}`]=typeof c=="number"?`${c}px`:c})}typeof o=="number"||typeof o=="string"?d(o):typeof o=="object"?["top","right","bottom","left"].forEach(c=>{o[c]===void 0?r[`${i}-${c}`]=a:r[`${i}-${c}`]=typeof o[c]=="number"?`${o[c]}px`:o[c]}):d(a)}),r}var gP=m.forwardRef(function(e,t){let{invert:r,position:o="bottom-right",hotkey:n=["altKey","KeyT"],expand:s,closeButton:i,className:a,offset:d,mobileOffset:c,theme:p="light",richColors:u,duration:f,style:g,visibleToasts:k=oP,toastOptions:y,dir:b=xg(),gap:v=aP,loadingIcon:h,icons:w,containerAriaLabel:A="Notifications",pauseWhenPageIsHidden:S}=e,[I,P]=z.useState([]),D=z.useMemo(()=>Array.from(new Set([o].concat(I.filter(_=>_.position).map(_=>_.position)))),[I,o]),[M,H]=z.useState([]),[V,L]=z.useState(!1),[$,E]=z.useState(!1),[J,B]=z.useState(p!=="system"?p:typeof window<"u"&&window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),U=z.useRef(null),x=n.join("+").replace(/Key/g,"").replace(/Digit/g,""),T=z.useRef(null),N=z.useRef(!1),G=z.useCallback(_=>{P(Q=>{var Z;return(Z=Q.find(we=>we.id===_.id))!=null&&Z.delete||Ue.dismiss(_.id),Q.filter(({id:we})=>we!==_.id)})},[]);return z.useEffect(()=>Ue.subscribe(_=>{if(_.dismiss){P(Q=>Q.map(Z=>Z.id===_.id?{...Z,delete:!0}:Z));return}setTimeout(()=>{Uf.flushSync(()=>{P(Q=>{let Z=Q.findIndex(we=>we.id===_.id);return Z!==-1?[...Q.slice(0,Z),{...Q[Z],..._},...Q.slice(Z+1)]:[_,...Q]})})})}),[]),z.useEffect(()=>{if(p!=="system"){B(p);return}if(p==="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?B("dark"):B("light")),typeof window>"u")return;let _=window.matchMedia("(prefers-color-scheme: dark)");try{_.addEventListener("change",({matches:Q})=>{B(Q?"dark":"light")})}catch{_.addListener(({matches:Z})=>{try{B(Z?"dark":"light")}catch(we){console.error(we)}})}},[p]),z.useEffect(()=>{I.length<=1&&L(!1)},[I]),z.useEffect(()=>{let _=Q=>{var Z,we;n.every(ze=>Q[ze]||Q.code===ze)&&(L(!0),(Z=U.current)==null||Z.focus()),Q.code==="Escape"&&(document.activeElement===U.current||(we=U.current)!=null&&we.contains(document.activeElement))&&L(!1)};return document.addEventListener("keydown",_),()=>document.removeEventListener("keydown",_)},[n]),z.useEffect(()=>{if(U.current)return()=>{T.current&&(T.current.focus({preventScroll:!0}),T.current=null,N.current=!1)}},[U.current]),z.createElement("section",{ref:t,"aria-label":`${A} ${x}`,tabIndex:-1,"aria-live":"polite","aria-relevant":"additions text","aria-atomic":"false",suppressHydrationWarning:!0},D.map((_,Q)=>{var Z;let[we,ze]=_.split("-");return I.length?z.createElement("ol",{key:_,dir:b==="auto"?xg():b,tabIndex:-1,ref:U,className:a,"data-sonner-toaster":!0,"data-theme":J,"data-y-position":we,"data-lifted":V&&I.length>1&&!s,"data-x-position":ze,style:{"--front-toast-height":`${((Z=M[0])==null?void 0:Z.height)||0}px`,"--width":`${iP}px`,"--gap":`${v}px`,...g,...pP(d,c)},onBlur:te=>{N.current&&!te.currentTarget.contains(te.relatedTarget)&&(N.current=!1,T.current&&(T.current.focus({preventScroll:!0}),T.current=null))},onFocus:te=>{te.target instanceof HTMLElement&&te.target.dataset.dismissible==="false"||N.current||(N.current=!0,T.current=te.relatedTarget)},onMouseEnter:()=>L(!0),onMouseMove:()=>L(!0),onMouseLeave:()=>{$||L(!1)},onDragEnd:()=>L(!1),onPointerDown:te=>{te.target instanceof HTMLElement&&te.target.dataset.dismissible==="false"||E(!0)},onPointerUp:()=>E(!1)},I.filter(te=>!te.position&&Q===0||te.position===_).map((te,Ro)=>{var hr,to;return z.createElement(uP,{key:te.id,icons:w,index:Ro,toast:te,defaultRichColors:u,duration:(hr=y==null?void 0:y.duration)!=null?hr:f,className:y==null?void 0:y.className,descriptionClassName:y==null?void 0:y.descriptionClassName,invert:r,visibleToasts:k,closeButton:(to=y==null?void 0:y.closeButton)!=null?to:i,interacting:$,position:_,style:y==null?void 0:y.style,unstyled:y==null?void 0:y.unstyled,classNames:y==null?void 0:y.classNames,cancelButtonStyle:y==null?void 0:y.cancelButtonStyle,actionButtonStyle:y==null?void 0:y.actionButtonStyle,removeToast:G,toasts:I.filter(fr=>fr.position==te.position),heights:M.filter(fr=>fr.position==te.position),setHeights:H,expandByDefault:s,gap:v,loadingIcon:h,expanded:V,pauseWhenPageIsHidden:S,swipeDirections:e.swipeDirections})})):null}))});const mP=({...e})=>{const{theme:t="system"}=VI();return l.jsx(gP,{theme:t,className:"toaster group",toastOptions:{classNames:{toast:"group toast group-[.toaster]:bg-background group-[.toaster]:text-foreground group-[.toaster]:border-border group-[.toaster]:shadow-lg",description:"group-[.toast]:text-muted-foreground",actionButton:"group-[.toast]:bg-primary group-[.toast]:text-primary-foreground",cancelButton:"group-[.toast]:bg-muted group-[.toast]:text-muted-foreground"}},...e})};var hP=Ad[" useId ".trim().toString()]||(()=>{}),fP=0;function an(e){const[t,r]=m.useState(hP());return Yt(()=>{r(o=>o??String(fP++))},[e]),t?`radix-${t}`:""}const vP=["top","right","bottom","left"],Qr=Math.min,Je=Math.max,ua=Math.round,vi=Math.floor,$t=e=>({x:e,y:e}),yP={left:"right",right:"left",bottom:"top",top:"bottom"},wP={start:"end",end:"start"};function id(e,t,r){return Je(e,Qr(t,r))}function ur(e,t){return typeof e=="function"?e(t):e}function pr(e){return e.split("-")[0]}function En(e){return e.split("-")[1]}function Au(e){return e==="x"?"y":"x"}function Su(e){return e==="y"?"height":"width"}const bP=new Set(["top","bottom"]);function Gt(e){return bP.has(pr(e))?"y":"x"}function Iu(e){return Au(Gt(e))}function kP(e,t,r){r===void 0&&(r=!1);const o=En(e),n=Iu(e),s=Su(n);let i=n==="x"?o===(r?"end":"start")?"right":"left":o==="start"?"bottom":"top";return t.reference[s]>t.floating[s]&&(i=pa(i)),[i,pa(i)]}function AP(e){const t=pa(e);return[ad(e),t,ad(t)]}function ad(e){return e.replace(/start|end/g,t=>wP[t])}const Dg=["left","right"],Tg=["right","left"],SP=["top","bottom"],IP=["bottom","top"];function PP(e,t,r){switch(e){case"top":case"bottom":return r?t?Tg:Dg:t?Dg:Tg;case"left":case"right":return t?SP:IP;default:return[]}}function CP(e,t,r,o){const n=En(e);let s=PP(pr(e),r==="start",o);return n&&(s=s.map(i=>i+"-"+n),t&&(s=s.concat(s.map(ad)))),s}function pa(e){return e.replace(/left|right|bottom|top/g,t=>yP[t])}function xP(e){return{top:0,right:0,bottom:0,left:0,...e}}function Mv(e){return typeof e!="number"?xP(e):{top:e,right:e,bottom:e,left:e}}function ga(e){const{x:t,y:r,width:o,height:n}=e;return{width:o,height:n,top:r,left:t,right:t+o,bottom:r+n,x:t,y:r}}function zg(e,t,r){let{reference:o,floating:n}=e;const s=Gt(t),i=Iu(t),a=Su(i),d=pr(t),c=s==="y",p=o.x+o.width/2-n.width/2,u=o.y+o.height/2-n.height/2,f=o[a]/2-n[a]/2;let g;switch(d){case"top":g={x:p,y:o.y-n.height};break;case"bottom":g={x:p,y:o.y+o.height};break;case"right":g={x:o.x+o.width,y:u};break;case"left":g={x:o.x-n.width,y:u};break;default:g={x:o.x,y:o.y}}switch(En(t)){case"start":g[i]-=f*(r&&c?-1:1);break;case"end":g[i]+=f*(r&&c?-1:1);break}return g}const DP=async(e,t,r)=>{const{placement:o="bottom",strategy:n="absolute",middleware:s=[],platform:i}=r,a=s.filter(Boolean),d=await(i.isRTL==null?void 0:i.isRTL(t));let c=await i.getElementRects({reference:e,floating:t,strategy:n}),{x:p,y:u}=zg(c,o,d),f=o,g={},k=0;for(let y=0;y<a.length;y++){const{name:b,fn:v}=a[y],{x:h,y:w,data:A,reset:S}=await v({x:p,y:u,initialPlacement:o,placement:f,strategy:n,middlewareData:g,rects:c,platform:i,elements:{reference:e,floating:t}});p=h??p,u=w??u,g={...g,[b]:{...g[b],...A}},S&&k<=50&&(k++,typeof S=="object"&&(S.placement&&(f=S.placement),S.rects&&(c=S.rects===!0?await i.getElementRects({reference:e,floating:t,strategy:n}):S.rects),{x:p,y:u}=zg(c,f,d)),y=-1)}return{x:p,y:u,placement:f,strategy:n,middlewareData:g}};async function Ds(e,t){var r;t===void 0&&(t={});const{x:o,y:n,platform:s,rects:i,elements:a,strategy:d}=e,{boundary:c="clippingAncestors",rootBoundary:p="viewport",elementContext:u="floating",altBoundary:f=!1,padding:g=0}=ur(t,e),k=Mv(g),b=a[f?u==="floating"?"reference":"floating":u],v=ga(await s.getClippingRect({element:(r=await(s.isElement==null?void 0:s.isElement(b)))==null||r?b:b.contextElement||await(s.getDocumentElement==null?void 0:s.getDocumentElement(a.floating)),boundary:c,rootBoundary:p,strategy:d})),h=u==="floating"?{x:o,y:n,width:i.floating.width,height:i.floating.height}:i.reference,w=await(s.getOffsetParent==null?void 0:s.getOffsetParent(a.floating)),A=await(s.isElement==null?void 0:s.isElement(w))?await(s.getScale==null?void 0:s.getScale(w))||{x:1,y:1}:{x:1,y:1},S=ga(s.convertOffsetParentRelativeRectToViewportRelativeRect?await s.convertOffsetParentRelativeRectToViewportRelativeRect({elements:a,rect:h,offsetParent:w,strategy:d}):h);return{top:(v.top-S.top+k.top)/A.y,bottom:(S.bottom-v.bottom+k.bottom)/A.y,left:(v.left-S.left+k.left)/A.x,right:(S.right-v.right+k.right)/A.x}}const TP=e=>({name:"arrow",options:e,async fn(t){const{x:r,y:o,placement:n,rects:s,platform:i,elements:a,middlewareData:d}=t,{element:c,padding:p=0}=ur(e,t)||{};if(c==null)return{};const u=Mv(p),f={x:r,y:o},g=Iu(n),k=Su(g),y=await i.getDimensions(c),b=g==="y",v=b?"top":"left",h=b?"bottom":"right",w=b?"clientHeight":"clientWidth",A=s.reference[k]+s.reference[g]-f[g]-s.floating[k],S=f[g]-s.reference[g],I=await(i.getOffsetParent==null?void 0:i.getOffsetParent(c));let P=I?I[w]:0;(!P||!await(i.isElement==null?void 0:i.isElement(I)))&&(P=a.floating[w]||s.floating[k]);const D=A/2-S/2,M=P/2-y[k]/2-1,H=Qr(u[v],M),V=Qr(u[h],M),L=H,$=P-y[k]-V,E=P/2-y[k]/2+D,J=id(L,E,$),B=!d.arrow&&En(n)!=null&&E!==J&&s.reference[k]/2-(E<L?H:V)-y[k]/2<0,U=B?E<L?E-L:E-$:0;return{[g]:f[g]+U,data:{[g]:J,centerOffset:E-J-U,...B&&{alignmentOffset:U}},reset:B}}}),zP=function(e){return e===void 0&&(e={}),{name:"flip",options:e,async fn(t){var r,o;const{placement:n,middlewareData:s,rects:i,initialPlacement:a,platform:d,elements:c}=t,{mainAxis:p=!0,crossAxis:u=!0,fallbackPlacements:f,fallbackStrategy:g="bestFit",fallbackAxisSideDirection:k="none",flipAlignment:y=!0,...b}=ur(e,t);if((r=s.arrow)!=null&&r.alignmentOffset)return{};const v=pr(n),h=Gt(a),w=pr(a)===a,A=await(d.isRTL==null?void 0:d.isRTL(c.floating)),S=f||(w||!y?[pa(a)]:AP(a)),I=k!=="none";!f&&I&&S.push(...CP(a,y,k,A));const P=[a,...S],D=await Ds(t,b),M=[];let H=((o=s.flip)==null?void 0:o.overflows)||[];if(p&&M.push(D[v]),u){const E=kP(n,i,A);M.push(D[E[0]],D[E[1]])}if(H=[...H,{placement:n,overflows:M}],!M.every(E=>E<=0)){var V,L;const E=(((V=s.flip)==null?void 0:V.index)||0)+1,J=P[E];if(J&&(!(u==="alignment"?h!==Gt(J):!1)||H.every(x=>x.overflows[0]>0&&Gt(x.placement)===h)))return{data:{index:E,overflows:H},reset:{placement:J}};let B=(L=H.filter(U=>U.overflows[0]<=0).sort((U,x)=>U.overflows[1]-x.overflows[1])[0])==null?void 0:L.placement;if(!B)switch(g){case"bestFit":{var $;const U=($=H.filter(x=>{if(I){const T=Gt(x.placement);return T===h||T==="y"}return!0}).map(x=>[x.placement,x.overflows.filter(T=>T>0).reduce((T,N)=>T+N,0)]).sort((x,T)=>x[1]-T[1])[0])==null?void 0:$[0];U&&(B=U);break}case"initialPlacement":B=a;break}if(n!==B)return{reset:{placement:B}}}return{}}}};function Rg(e,t){return{top:e.top-t.height,right:e.right-t.width,bottom:e.bottom-t.height,left:e.left-t.width}}function Hg(e){return vP.some(t=>e[t]>=0)}const RP=function(e){return e===void 0&&(e={}),{name:"hide",options:e,async fn(t){const{rects:r}=t,{strategy:o="referenceHidden",...n}=ur(e,t);switch(o){case"referenceHidden":{const s=await Ds(t,{...n,elementContext:"reference"}),i=Rg(s,r.reference);return{data:{referenceHiddenOffsets:i,referenceHidden:Hg(i)}}}case"escaped":{const s=await Ds(t,{...n,altBoundary:!0}),i=Rg(s,r.floating);return{data:{escapedOffsets:i,escaped:Hg(i)}}}default:return{}}}}},Ev=new Set(["left","top"]);async function HP(e,t){const{placement:r,platform:o,elements:n}=e,s=await(o.isRTL==null?void 0:o.isRTL(n.floating)),i=pr(r),a=En(r),d=Gt(r)==="y",c=Ev.has(i)?-1:1,p=s&&d?-1:1,u=ur(t,e);let{mainAxis:f,crossAxis:g,alignmentAxis:k}=typeof u=="number"?{mainAxis:u,crossAxis:0,alignmentAxis:null}:{mainAxis:u.mainAxis||0,crossAxis:u.crossAxis||0,alignmentAxis:u.alignmentAxis};return a&&typeof k=="number"&&(g=a==="end"?k*-1:k),d?{x:g*p,y:f*c}:{x:f*c,y:g*p}}const MP=function(e){return e===void 0&&(e=0),{name:"offset",options:e,async fn(t){var r,o;const{x:n,y:s,placement:i,middlewareData:a}=t,d=await HP(t,e);return i===((r=a.offset)==null?void 0:r.placement)&&(o=a.arrow)!=null&&o.alignmentOffset?{}:{x:n+d.x,y:s+d.y,data:{...d,placement:i}}}}},EP=function(e){return e===void 0&&(e={}),{name:"shift",options:e,async fn(t){const{x:r,y:o,placement:n}=t,{mainAxis:s=!0,crossAxis:i=!1,limiter:a={fn:b=>{let{x:v,y:h}=b;return{x:v,y:h}}},...d}=ur(e,t),c={x:r,y:o},p=await Ds(t,d),u=Gt(pr(n)),f=Au(u);let g=c[f],k=c[u];if(s){const b=f==="y"?"top":"left",v=f==="y"?"bottom":"right",h=g+p[b],w=g-p[v];g=id(h,g,w)}if(i){const b=u==="y"?"top":"left",v=u==="y"?"bottom":"right",h=k+p[b],w=k-p[v];k=id(h,k,w)}const y=a.fn({...t,[f]:g,[u]:k});return{...y,data:{x:y.x-r,y:y.y-o,enabled:{[f]:s,[u]:i}}}}}},NP=function(e){return e===void 0&&(e={}),{options:e,fn(t){const{x:r,y:o,placement:n,rects:s,middlewareData:i}=t,{offset:a=0,mainAxis:d=!0,crossAxis:c=!0}=ur(e,t),p={x:r,y:o},u=Gt(n),f=Au(u);let g=p[f],k=p[u];const y=ur(a,t),b=typeof y=="number"?{mainAxis:y,crossAxis:0}:{mainAxis:0,crossAxis:0,...y};if(d){const w=f==="y"?"height":"width",A=s.reference[f]-s.floating[w]+b.mainAxis,S=s.reference[f]+s.reference[w]-b.mainAxis;g<A?g=A:g>S&&(g=S)}if(c){var v,h;const w=f==="y"?"width":"height",A=Ev.has(pr(n)),S=s.reference[u]-s.floating[w]+(A&&((v=i.offset)==null?void 0:v[u])||0)+(A?0:b.crossAxis),I=s.reference[u]+s.reference[w]+(A?0:((h=i.offset)==null?void 0:h[u])||0)-(A?b.crossAxis:0);k<S?k=S:k>I&&(k=I)}return{[f]:g,[u]:k}}}},qP=function(e){return e===void 0&&(e={}),{name:"size",options:e,async fn(t){var r,o;const{placement:n,rects:s,platform:i,elements:a}=t,{apply:d=()=>{},...c}=ur(e,t),p=await Ds(t,c),u=pr(n),f=En(n),g=Gt(n)==="y",{width:k,height:y}=s.floating;let b,v;u==="top"||u==="bottom"?(b=u,v=f===(await(i.isRTL==null?void 0:i.isRTL(a.floating))?"start":"end")?"left":"right"):(v=u,b=f==="end"?"top":"bottom");const h=y-p.top-p.bottom,w=k-p.left-p.right,A=Qr(y-p[b],h),S=Qr(k-p[v],w),I=!t.middlewareData.shift;let P=A,D=S;if((r=t.middlewareData.shift)!=null&&r.enabled.x&&(D=w),(o=t.middlewareData.shift)!=null&&o.enabled.y&&(P=h),I&&!f){const H=Je(p.left,0),V=Je(p.right,0),L=Je(p.top,0),$=Je(p.bottom,0);g?D=k-2*(H!==0||V!==0?H+V:Je(p.left,p.right)):P=y-2*(L!==0||$!==0?L+$:Je(p.top,p.bottom))}await d({...t,availableWidth:D,availableHeight:P});const M=await i.getDimensions(a.floating);return k!==M.width||y!==M.height?{reset:{rects:!0}}:{}}}};function Fa(){return typeof window<"u"}function Nn(e){return Nv(e)?(e.nodeName||"").toLowerCase():"#document"}function et(e){var t;return(e==null||(t=e.ownerDocument)==null?void 0:t.defaultView)||window}function Zt(e){var t;return(t=(Nv(e)?e.ownerDocument:e.document)||window.document)==null?void 0:t.documentElement}function Nv(e){return Fa()?e instanceof Node||e instanceof et(e).Node:!1}function Rt(e){return Fa()?e instanceof Element||e instanceof et(e).Element:!1}function Jt(e){return Fa()?e instanceof HTMLElement||e instanceof et(e).HTMLElement:!1}function Mg(e){return!Fa()||typeof ShadowRoot>"u"?!1:e instanceof ShadowRoot||e instanceof et(e).ShadowRoot}const LP=new Set(["inline","contents"]);function Gs(e){const{overflow:t,overflowX:r,overflowY:o,display:n}=Ht(e);return/auto|scroll|overlay|hidden|clip/.test(t+o+r)&&!LP.has(n)}const OP=new Set(["table","td","th"]);function _P(e){return OP.has(Nn(e))}const VP=[":popover-open",":modal"];function Ka(e){return VP.some(t=>{try{return e.matches(t)}catch{return!1}})}const BP=["transform","translate","scale","rotate","perspective"],WP=["transform","translate","scale","rotate","perspective","filter"],jP=["paint","layout","strict","content"];function Pu(e){const t=Cu(),r=Rt(e)?Ht(e):e;return BP.some(o=>r[o]?r[o]!=="none":!1)||(r.containerType?r.containerType!=="normal":!1)||!t&&(r.backdropFilter?r.backdropFilter!=="none":!1)||!t&&(r.filter?r.filter!=="none":!1)||WP.some(o=>(r.willChange||"").includes(o))||jP.some(o=>(r.contain||"").includes(o))}function UP(e){let t=Yr(e);for(;Jt(t)&&!Pn(t);){if(Pu(t))return t;if(Ka(t))return null;t=Yr(t)}return null}function Cu(){return typeof CSS>"u"||!CSS.supports?!1:CSS.supports("-webkit-backdrop-filter","none")}const GP=new Set(["html","body","#document"]);function Pn(e){return GP.has(Nn(e))}function Ht(e){return et(e).getComputedStyle(e)}function $a(e){return Rt(e)?{scrollLeft:e.scrollLeft,scrollTop:e.scrollTop}:{scrollLeft:e.scrollX,scrollTop:e.scrollY}}function Yr(e){if(Nn(e)==="html")return e;const t=e.assignedSlot||e.parentNode||Mg(e)&&e.host||Zt(e);return Mg(t)?t.host:t}function qv(e){const t=Yr(e);return Pn(t)?e.ownerDocument?e.ownerDocument.body:e.body:Jt(t)&&Gs(t)?t:qv(t)}function Ts(e,t,r){var o;t===void 0&&(t=[]),r===void 0&&(r=!0);const n=qv(e),s=n===((o=e.ownerDocument)==null?void 0:o.body),i=et(n);if(s){const a=ld(i);return t.concat(i,i.visualViewport||[],Gs(n)?n:[],a&&r?Ts(a):[])}return t.concat(n,Ts(n,[],r))}function ld(e){return e.parent&&Object.getPrototypeOf(e.parent)?e.frameElement:null}function Lv(e){const t=Ht(e);let r=parseFloat(t.width)||0,o=parseFloat(t.height)||0;const n=Jt(e),s=n?e.offsetWidth:r,i=n?e.offsetHeight:o,a=ua(r)!==s||ua(o)!==i;return a&&(r=s,o=i),{width:r,height:o,$:a}}function xu(e){return Rt(e)?e:e.contextElement}function ln(e){const t=xu(e);if(!Jt(t))return $t(1);const r=t.getBoundingClientRect(),{width:o,height:n,$:s}=Lv(t);let i=(s?ua(r.width):r.width)/o,a=(s?ua(r.height):r.height)/n;return(!i||!Number.isFinite(i))&&(i=1),(!a||!Number.isFinite(a))&&(a=1),{x:i,y:a}}const FP=$t(0);function Ov(e){const t=et(e);return!Cu()||!t.visualViewport?FP:{x:t.visualViewport.offsetLeft,y:t.visualViewport.offsetTop}}function KP(e,t,r){return t===void 0&&(t=!1),!r||t&&r!==et(e)?!1:t}function So(e,t,r,o){t===void 0&&(t=!1),r===void 0&&(r=!1);const n=e.getBoundingClientRect(),s=xu(e);let i=$t(1);t&&(o?Rt(o)&&(i=ln(o)):i=ln(e));const a=KP(s,r,o)?Ov(s):$t(0);let d=(n.left+a.x)/i.x,c=(n.top+a.y)/i.y,p=n.width/i.x,u=n.height/i.y;if(s){const f=et(s),g=o&&Rt(o)?et(o):o;let k=f,y=ld(k);for(;y&&o&&g!==k;){const b=ln(y),v=y.getBoundingClientRect(),h=Ht(y),w=v.left+(y.clientLeft+parseFloat(h.paddingLeft))*b.x,A=v.top+(y.clientTop+parseFloat(h.paddingTop))*b.y;d*=b.x,c*=b.y,p*=b.x,u*=b.y,d+=w,c+=A,k=et(y),y=ld(k)}}return ga({width:p,height:u,x:d,y:c})}function Du(e,t){const r=$a(e).scrollLeft;return t?t.left+r:So(Zt(e)).left+r}function _v(e,t,r){r===void 0&&(r=!1);const o=e.getBoundingClientRect(),n=o.left+t.scrollLeft-(r?0:Du(e,o)),s=o.top+t.scrollTop;return{x:n,y:s}}function $P(e){let{elements:t,rect:r,offsetParent:o,strategy:n}=e;const s=n==="fixed",i=Zt(o),a=t?Ka(t.floating):!1;if(o===i||a&&s)return r;let d={scrollLeft:0,scrollTop:0},c=$t(1);const p=$t(0),u=Jt(o);if((u||!u&&!s)&&((Nn(o)!=="body"||Gs(i))&&(d=$a(o)),Jt(o))){const g=So(o);c=ln(o),p.x=g.x+o.clientLeft,p.y=g.y+o.clientTop}const f=i&&!u&&!s?_v(i,d,!0):$t(0);return{width:r.width*c.x,height:r.height*c.y,x:r.x*c.x-d.scrollLeft*c.x+p.x+f.x,y:r.y*c.y-d.scrollTop*c.y+p.y+f.y}}function QP(e){return Array.from(e.getClientRects())}function YP(e){const t=Zt(e),r=$a(e),o=e.ownerDocument.body,n=Je(t.scrollWidth,t.clientWidth,o.scrollWidth,o.clientWidth),s=Je(t.scrollHeight,t.clientHeight,o.scrollHeight,o.clientHeight);let i=-r.scrollLeft+Du(e);const a=-r.scrollTop;return Ht(o).direction==="rtl"&&(i+=Je(t.clientWidth,o.clientWidth)-n),{width:n,height:s,x:i,y:a}}function JP(e,t){const r=et(e),o=Zt(e),n=r.visualViewport;let s=o.clientWidth,i=o.clientHeight,a=0,d=0;if(n){s=n.width,i=n.height;const c=Cu();(!c||c&&t==="fixed")&&(a=n.offsetLeft,d=n.offsetTop)}return{width:s,height:i,x:a,y:d}}const ZP=new Set(["absolute","fixed"]);function XP(e,t){const r=So(e,!0,t==="fixed"),o=r.top+e.clientTop,n=r.left+e.clientLeft,s=Jt(e)?ln(e):$t(1),i=e.clientWidth*s.x,a=e.clientHeight*s.y,d=n*s.x,c=o*s.y;return{width:i,height:a,x:d,y:c}}function Eg(e,t,r){let o;if(t==="viewport")o=JP(e,r);else if(t==="document")o=YP(Zt(e));else if(Rt(t))o=XP(t,r);else{const n=Ov(e);o={x:t.x-n.x,y:t.y-n.y,width:t.width,height:t.height}}return ga(o)}function Vv(e,t){const r=Yr(e);return r===t||!Rt(r)||Pn(r)?!1:Ht(r).position==="fixed"||Vv(r,t)}function eC(e,t){const r=t.get(e);if(r)return r;let o=Ts(e,[],!1).filter(a=>Rt(a)&&Nn(a)!=="body"),n=null;const s=Ht(e).position==="fixed";let i=s?Yr(e):e;for(;Rt(i)&&!Pn(i);){const a=Ht(i),d=Pu(i);!d&&a.position==="fixed"&&(n=null),(s?!d&&!n:!d&&a.position==="static"&&!!n&&ZP.has(n.position)||Gs(i)&&!d&&Vv(e,i))?o=o.filter(p=>p!==i):n=a,i=Yr(i)}return t.set(e,o),o}function tC(e){let{element:t,boundary:r,rootBoundary:o,strategy:n}=e;const i=[...r==="clippingAncestors"?Ka(t)?[]:eC(t,this._c):[].concat(r),o],a=i[0],d=i.reduce((c,p)=>{const u=Eg(t,p,n);return c.top=Je(u.top,c.top),c.right=Qr(u.right,c.right),c.bottom=Qr(u.bottom,c.bottom),c.left=Je(u.left,c.left),c},Eg(t,a,n));return{width:d.right-d.left,height:d.bottom-d.top,x:d.left,y:d.top}}function rC(e){const{width:t,height:r}=Lv(e);return{width:t,height:r}}function oC(e,t,r){const o=Jt(t),n=Zt(t),s=r==="fixed",i=So(e,!0,s,t);let a={scrollLeft:0,scrollTop:0};const d=$t(0);function c(){d.x=Du(n)}if(o||!o&&!s)if((Nn(t)!=="body"||Gs(n))&&(a=$a(t)),o){const g=So(t,!0,s,t);d.x=g.x+t.clientLeft,d.y=g.y+t.clientTop}else n&&c();s&&!o&&n&&c();const p=n&&!o&&!s?_v(n,a):$t(0),u=i.left+a.scrollLeft-d.x-p.x,f=i.top+a.scrollTop-d.y-p.y;return{x:u,y:f,width:i.width,height:i.height}}function Ql(e){return Ht(e).position==="static"}function Ng(e,t){if(!Jt(e)||Ht(e).position==="fixed")return null;if(t)return t(e);let r=e.offsetParent;return Zt(e)===r&&(r=r.ownerDocument.body),r}function Bv(e,t){const r=et(e);if(Ka(e))return r;if(!Jt(e)){let n=Yr(e);for(;n&&!Pn(n);){if(Rt(n)&&!Ql(n))return n;n=Yr(n)}return r}let o=Ng(e,t);for(;o&&_P(o)&&Ql(o);)o=Ng(o,t);return o&&Pn(o)&&Ql(o)&&!Pu(o)?r:o||UP(e)||r}const nC=async function(e){const t=this.getOffsetParent||Bv,r=this.getDimensions,o=await r(e.floating);return{reference:oC(e.reference,await t(e.floating),e.strategy),floating:{x:0,y:0,width:o.width,height:o.height}}};function sC(e){return Ht(e).direction==="rtl"}const iC={convertOffsetParentRelativeRectToViewportRelativeRect:$P,getDocumentElement:Zt,getClippingRect:tC,getOffsetParent:Bv,getElementRects:nC,getClientRects:QP,getDimensions:rC,getScale:ln,isElement:Rt,isRTL:sC};function Wv(e,t){return e.x===t.x&&e.y===t.y&&e.width===t.width&&e.height===t.height}function aC(e,t){let r=null,o;const n=Zt(e);function s(){var a;clearTimeout(o),(a=r)==null||a.disconnect(),r=null}function i(a,d){a===void 0&&(a=!1),d===void 0&&(d=1),s();const c=e.getBoundingClientRect(),{left:p,top:u,width:f,height:g}=c;if(a||t(),!f||!g)return;const k=vi(u),y=vi(n.clientWidth-(p+f)),b=vi(n.clientHeight-(u+g)),v=vi(p),w={rootMargin:-k+"px "+-y+"px "+-b+"px "+-v+"px",threshold:Je(0,Qr(1,d))||1};let A=!0;function S(I){const P=I[0].intersectionRatio;if(P!==d){if(!A)return i();P?i(!1,P):o=setTimeout(()=>{i(!1,1e-7)},1e3)}P===1&&!Wv(c,e.getBoundingClientRect())&&i(),A=!1}try{r=new IntersectionObserver(S,{...w,root:n.ownerDocument})}catch{r=new IntersectionObserver(S,w)}r.observe(e)}return i(!0),s}function lC(e,t,r,o){o===void 0&&(o={});const{ancestorScroll:n=!0,ancestorResize:s=!0,elementResize:i=typeof ResizeObserver=="function",layoutShift:a=typeof IntersectionObserver=="function",animationFrame:d=!1}=o,c=xu(e),p=n||s?[...c?Ts(c):[],...Ts(t)]:[];p.forEach(v=>{n&&v.addEventListener("scroll",r,{passive:!0}),s&&v.addEventListener("resize",r)});const u=c&&a?aC(c,r):null;let f=-1,g=null;i&&(g=new ResizeObserver(v=>{let[h]=v;h&&h.target===c&&g&&(g.unobserve(t),cancelAnimationFrame(f),f=requestAnimationFrame(()=>{var w;(w=g)==null||w.observe(t)})),r()}),c&&!d&&g.observe(c),g.observe(t));let k,y=d?So(e):null;d&&b();function b(){const v=So(e);y&&!Wv(y,v)&&r(),y=v,k=requestAnimationFrame(b)}return r(),()=>{var v;p.forEach(h=>{n&&h.removeEventListener("scroll",r),s&&h.removeEventListener("resize",r)}),u==null||u(),(v=g)==null||v.disconnect(),g=null,d&&cancelAnimationFrame(k)}}const cC=MP,dC=EP,uC=zP,pC=qP,gC=RP,qg=TP,mC=NP,hC=(e,t,r)=>{const o=new Map,n={platform:iC,...r},s={...n.platform,_c:o};return DP(e,t,{...n,platform:s})};var fC=typeof document<"u",vC=function(){},Li=fC?m.useLayoutEffect:vC;function ma(e,t){if(e===t)return!0;if(typeof e!=typeof t)return!1;if(typeof e=="function"&&e.toString()===t.toString())return!0;let r,o,n;if(e&&t&&typeof e=="object"){if(Array.isArray(e)){if(r=e.length,r!==t.length)return!1;for(o=r;o--!==0;)if(!ma(e[o],t[o]))return!1;return!0}if(n=Object.keys(e),r=n.length,r!==Object.keys(t).length)return!1;for(o=r;o--!==0;)if(!{}.hasOwnProperty.call(t,n[o]))return!1;for(o=r;o--!==0;){const s=n[o];if(!(s==="_owner"&&e.$$typeof)&&!ma(e[s],t[s]))return!1}return!0}return e!==e&&t!==t}function jv(e){return typeof window>"u"?1:(e.ownerDocument.defaultView||window).devicePixelRatio||1}function Lg(e,t){const r=jv(e);return Math.round(t*r)/r}function Yl(e){const t=m.useRef(e);return Li(()=>{t.current=e}),t}function yC(e){e===void 0&&(e={});const{placement:t="bottom",strategy:r="absolute",middleware:o=[],platform:n,elements:{reference:s,floating:i}={},transform:a=!0,whileElementsMounted:d,open:c}=e,[p,u]=m.useState({x:0,y:0,strategy:r,placement:t,middlewareData:{},isPositioned:!1}),[f,g]=m.useState(o);ma(f,o)||g(o);const[k,y]=m.useState(null),[b,v]=m.useState(null),h=m.useCallback(x=>{x!==I.current&&(I.current=x,y(x))},[]),w=m.useCallback(x=>{x!==P.current&&(P.current=x,v(x))},[]),A=s||k,S=i||b,I=m.useRef(null),P=m.useRef(null),D=m.useRef(p),M=d!=null,H=Yl(d),V=Yl(n),L=Yl(c),$=m.useCallback(()=>{if(!I.current||!P.current)return;const x={placement:t,strategy:r,middleware:f};V.current&&(x.platform=V.current),hC(I.current,P.current,x).then(T=>{const N={...T,isPositioned:L.current!==!1};E.current&&!ma(D.current,N)&&(D.current=N,Ws.flushSync(()=>{u(N)}))})},[f,t,r,V,L]);Li(()=>{c===!1&&D.current.isPositioned&&(D.current.isPositioned=!1,u(x=>({...x,isPositioned:!1})))},[c]);const E=m.useRef(!1);Li(()=>(E.current=!0,()=>{E.current=!1}),[]),Li(()=>{if(A&&(I.current=A),S&&(P.current=S),A&&S){if(H.current)return H.current(A,S,$);$()}},[A,S,$,H,M]);const J=m.useMemo(()=>({reference:I,floating:P,setReference:h,setFloating:w}),[h,w]),B=m.useMemo(()=>({reference:A,floating:S}),[A,S]),U=m.useMemo(()=>{const x={position:r,left:0,top:0};if(!B.floating)return x;const T=Lg(B.floating,p.x),N=Lg(B.floating,p.y);return a?{...x,transform:"translate("+T+"px, "+N+"px)",...jv(B.floating)>=1.5&&{willChange:"transform"}}:{position:r,left:T,top:N}},[r,a,B.floating,p.x,p.y]);return m.useMemo(()=>({...p,update:$,refs:J,elements:B,floatingStyles:U}),[p,$,J,B,U])}const wC=e=>{function t(r){return{}.hasOwnProperty.call(r,"current")}return{name:"arrow",options:e,fn(r){const{element:o,padding:n}=typeof e=="function"?e(r):e;return o&&t(o)?o.current!=null?qg({element:o.current,padding:n}).fn(r):{}:o?qg({element:o,padding:n}).fn(r):{}}}},bC=(e,t)=>({...cC(e),options:[e,t]}),kC=(e,t)=>({...dC(e),options:[e,t]}),AC=(e,t)=>({...mC(e),options:[e,t]}),SC=(e,t)=>({...uC(e),options:[e,t]}),IC=(e,t)=>({...pC(e),options:[e,t]}),PC=(e,t)=>({...gC(e),options:[e,t]}),CC=(e,t)=>({...wC(e),options:[e,t]});var xC="Arrow",Uv=m.forwardRef((e,t)=>{const{children:r,width:o=10,height:n=5,...s}=e;return l.jsx(ie.svg,{...s,ref:t,width:o,height:n,viewBox:"0 0 30 10",preserveAspectRatio:"none",children:e.asChild?r:l.jsx("polygon",{points:"0,0 30,0 15,10"})})});Uv.displayName=xC;var DC=Uv;function TC(e){const[t,r]=m.useState(void 0);return Yt(()=>{if(e){r({width:e.offsetWidth,height:e.offsetHeight});const o=new ResizeObserver(n=>{if(!Array.isArray(n)||!n.length)return;const s=n[0];let i,a;if("borderBoxSize"in s){const d=s.borderBoxSize,c=Array.isArray(d)?d[0]:d;i=c.inlineSize,a=c.blockSize}else i=e.offsetWidth,a=e.offsetHeight;r({width:i,height:a})});return o.observe(e,{box:"border-box"}),()=>o.unobserve(e)}else r(void 0)},[e]),t}var Tu="Popper",[Gv,Fv]=xo(Tu),[zC,Kv]=Gv(Tu),$v=e=>{const{__scopePopper:t,children:r}=e,[o,n]=m.useState(null);return l.jsx(zC,{scope:t,anchor:o,onAnchorChange:n,children:r})};$v.displayName=Tu;var Qv="PopperAnchor",Yv=m.forwardRef((e,t)=>{const{__scopePopper:r,virtualRef:o,...n}=e,s=Kv(Qv,r),i=m.useRef(null),a=Te(t,i);return m.useEffect(()=>{s.onAnchorChange((o==null?void 0:o.current)||i.current)}),o?null:l.jsx(ie.div,{...n,ref:a})});Yv.displayName=Qv;var zu="PopperContent",[RC,HC]=Gv(zu),Jv=m.forwardRef((e,t)=>{var te,Ro,hr,to,fr,Ho;const{__scopePopper:r,side:o="bottom",sideOffset:n=0,align:s="center",alignOffset:i=0,arrowPadding:a=0,avoidCollisions:d=!0,collisionBoundary:c=[],collisionPadding:p=0,sticky:u="partial",hideWhenDetached:f=!1,updatePositionStrategy:g="optimized",onPlaced:k,...y}=e,b=Kv(zu,r),[v,h]=m.useState(null),w=Te(t,vr=>h(vr)),[A,S]=m.useState(null),I=TC(A),P=(I==null?void 0:I.width)??0,D=(I==null?void 0:I.height)??0,M=o+(s!=="center"?"-"+s:""),H=typeof p=="number"?p:{top:0,right:0,bottom:0,left:0,...p},V=Array.isArray(c)?c:[c],L=V.length>0,$={padding:H,boundary:V.filter(EC),altBoundary:L},{refs:E,floatingStyles:J,placement:B,isPositioned:U,middlewareData:x}=yC({strategy:"fixed",placement:M,whileElementsMounted:(...vr)=>lC(...vr,{animationFrame:g==="always"}),elements:{reference:b.anchor},middleware:[bC({mainAxis:n+D,alignmentAxis:i}),d&&kC({mainAxis:!0,crossAxis:!1,limiter:u==="partial"?AC():void 0,...$}),d&&SC({...$}),IC({...$,apply:({elements:vr,rects:Ks,availableWidth:pl,availableHeight:$s})=>{const{width:gl,height:Ln}=Ks.reference,Mo=vr.floating.style;Mo.setProperty("--radix-popper-available-width",`${pl}px`),Mo.setProperty("--radix-popper-available-height",`${$s}px`),Mo.setProperty("--radix-popper-anchor-width",`${gl}px`),Mo.setProperty("--radix-popper-anchor-height",`${Ln}px`)}}),A&&CC({element:A,padding:a}),NC({arrowWidth:P,arrowHeight:D}),f&&PC({strategy:"referenceHidden",...$})]}),[T,N]=ey(B),G=Qt(k);Yt(()=>{U&&(G==null||G())},[U,G]);const _=(te=x.arrow)==null?void 0:te.x,Q=(Ro=x.arrow)==null?void 0:Ro.y,Z=((hr=x.arrow)==null?void 0:hr.centerOffset)!==0,[we,ze]=m.useState();return Yt(()=>{v&&ze(window.getComputedStyle(v).zIndex)},[v]),l.jsx("div",{ref:E.setFloating,"data-radix-popper-content-wrapper":"",style:{...J,transform:U?J.transform:"translate(0, -200%)",minWidth:"max-content",zIndex:we,"--radix-popper-transform-origin":[(to=x.transformOrigin)==null?void 0:to.x,(fr=x.transformOrigin)==null?void 0:fr.y].join(" "),...((Ho=x.hide)==null?void 0:Ho.referenceHidden)&&{visibility:"hidden",pointerEvents:"none"}},dir:e.dir,children:l.jsx(RC,{scope:r,placedSide:T,onArrowChange:S,arrowX:_,arrowY:Q,shouldHideArrow:Z,children:l.jsx(ie.div,{"data-side":T,"data-align":N,...y,ref:w,style:{...y.style,animation:U?void 0:"none"}})})})});Jv.displayName=zu;var Zv="PopperArrow",MC={top:"bottom",right:"left",bottom:"top",left:"right"},Xv=m.forwardRef(function(t,r){const{__scopePopper:o,...n}=t,s=HC(Zv,o),i=MC[s.placedSide];return l.jsx("span",{ref:s.onArrowChange,style:{position:"absolute",left:s.arrowX,top:s.arrowY,[i]:0,transformOrigin:{top:"",right:"0 0",bottom:"center 0",left:"100% 0"}[s.placedSide],transform:{top:"translateY(100%)",right:"translateY(50%) rotate(90deg) translateX(-50%)",bottom:"rotate(180deg)",left:"translateY(50%) rotate(-90deg) translateX(50%)"}[s.placedSide],visibility:s.shouldHideArrow?"hidden":void 0},children:l.jsx(DC,{...n,ref:r,style:{...n.style,display:"block"}})})});Xv.displayName=Zv;function EC(e){return e!==null}var NC=e=>({name:"transformOrigin",options:e,fn(t){var b,v,h;const{placement:r,rects:o,middlewareData:n}=t,i=((b=n.arrow)==null?void 0:b.centerOffset)!==0,a=i?0:e.arrowWidth,d=i?0:e.arrowHeight,[c,p]=ey(r),u={start:"0%",center:"50%",end:"100%"}[p],f=(((v=n.arrow)==null?void 0:v.x)??0)+a/2,g=(((h=n.arrow)==null?void 0:h.y)??0)+d/2;let k="",y="";return c==="bottom"?(k=i?u:`${f}px`,y=`${-d}px`):c==="top"?(k=i?u:`${f}px`,y=`${o.floating.height+d}px`):c==="right"?(k=`${-d}px`,y=i?u:`${g}px`):c==="left"&&(k=`${o.floating.width+d}px`,y=i?u:`${g}px`),{data:{x:k,y}}}});function ey(e){const[t,r="center"]=e.split("-");return[t,r]}var qC=$v,LC=Yv,OC=Jv,_C=Xv,[Qa,gD]=xo("Tooltip",[Fv]),Ya=Fv(),ty="TooltipProvider",VC=700,cd="tooltip.open",[BC,Ru]=Qa(ty),ry=e=>{const{__scopeTooltip:t,delayDuration:r=VC,skipDelayDuration:o=300,disableHoverableContent:n=!1,children:s}=e,i=m.useRef(!0),a=m.useRef(!1),d=m.useRef(0);return m.useEffect(()=>{const c=d.current;return()=>window.clearTimeout(c)},[]),l.jsx(BC,{scope:t,isOpenDelayedRef:i,delayDuration:r,onOpen:m.useCallback(()=>{window.clearTimeout(d.current),i.current=!1},[]),onClose:m.useCallback(()=>{window.clearTimeout(d.current),d.current=window.setTimeout(()=>i.current=!0,o)},[o]),isPointerInTransitRef:a,onPointerInTransitChange:m.useCallback(c=>{a.current=c},[]),disableHoverableContent:n,children:s})};ry.displayName=ty;var zs="Tooltip",[WC,Ja]=Qa(zs),oy=e=>{const{__scopeTooltip:t,children:r,open:o,defaultOpen:n,onOpenChange:s,disableHoverableContent:i,delayDuration:a}=e,d=Ru(zs,e.__scopeTooltip),c=Ya(t),[p,u]=m.useState(null),f=an(),g=m.useRef(0),k=i??d.disableHoverableContent,y=a??d.delayDuration,b=m.useRef(!1),[v,h]=Hn({prop:o,defaultProp:n??!1,onChange:P=>{P?(d.onOpen(),document.dispatchEvent(new CustomEvent(cd))):d.onClose(),s==null||s(P)},caller:zs}),w=m.useMemo(()=>v?b.current?"delayed-open":"instant-open":"closed",[v]),A=m.useCallback(()=>{window.clearTimeout(g.current),g.current=0,b.current=!1,h(!0)},[h]),S=m.useCallback(()=>{window.clearTimeout(g.current),g.current=0,h(!1)},[h]),I=m.useCallback(()=>{window.clearTimeout(g.current),g.current=window.setTimeout(()=>{b.current=!0,h(!0),g.current=0},y)},[y,h]);return m.useEffect(()=>()=>{g.current&&(window.clearTimeout(g.current),g.current=0)},[]),l.jsx(qC,{...c,children:l.jsx(WC,{scope:t,contentId:f,open:v,stateAttribute:w,trigger:p,onTriggerChange:u,onTriggerEnter:m.useCallback(()=>{d.isOpenDelayedRef.current?I():A()},[d.isOpenDelayedRef,I,A]),onTriggerLeave:m.useCallback(()=>{k?S():(window.clearTimeout(g.current),g.current=0)},[S,k]),onOpen:A,onClose:S,disableHoverableContent:k,children:r})})};oy.displayName=zs;var dd="TooltipTrigger",ny=m.forwardRef((e,t)=>{const{__scopeTooltip:r,...o}=e,n=Ja(dd,r),s=Ru(dd,r),i=Ya(r),a=m.useRef(null),d=Te(t,a,n.onTriggerChange),c=m.useRef(!1),p=m.useRef(!1),u=m.useCallback(()=>c.current=!1,[]);return m.useEffect(()=>()=>document.removeEventListener("pointerup",u),[u]),l.jsx(LC,{asChild:!0,...i,children:l.jsx(ie.button,{"aria-describedby":n.open?n.contentId:void 0,"data-state":n.stateAttribute,...o,ref:d,onPointerMove:ne(e.onPointerMove,f=>{f.pointerType!=="touch"&&!p.current&&!s.isPointerInTransitRef.current&&(n.onTriggerEnter(),p.current=!0)}),onPointerLeave:ne(e.onPointerLeave,()=>{n.onTriggerLeave(),p.current=!1}),onPointerDown:ne(e.onPointerDown,()=>{n.open&&n.onClose(),c.current=!0,document.addEventListener("pointerup",u,{once:!0})}),onFocus:ne(e.onFocus,()=>{c.current||n.onOpen()}),onBlur:ne(e.onBlur,n.onClose),onClick:ne(e.onClick,n.onClose)})})});ny.displayName=dd;var jC="TooltipPortal",[mD,UC]=Qa(jC,{forceMount:void 0}),Cn="TooltipContent",sy=m.forwardRef((e,t)=>{const r=UC(Cn,e.__scopeTooltip),{forceMount:o=r.forceMount,side:n="top",...s}=e,i=Ja(Cn,e.__scopeTooltip);return l.jsx(Do,{present:o||i.open,children:i.disableHoverableContent?l.jsx(iy,{side:n,...s,ref:t}):l.jsx(GC,{side:n,...s,ref:t})})}),GC=m.forwardRef((e,t)=>{const r=Ja(Cn,e.__scopeTooltip),o=Ru(Cn,e.__scopeTooltip),n=m.useRef(null),s=Te(t,n),[i,a]=m.useState(null),{trigger:d,onClose:c}=r,p=n.current,{onPointerInTransitChange:u}=o,f=m.useCallback(()=>{a(null),u(!1)},[u]),g=m.useCallback((k,y)=>{const b=k.currentTarget,v={x:k.clientX,y:k.clientY},h=YC(v,b.getBoundingClientRect()),w=JC(v,h),A=ZC(y.getBoundingClientRect()),S=ex([...w,...A]);a(S),u(!0)},[u]);return m.useEffect(()=>()=>f(),[f]),m.useEffect(()=>{if(d&&p){const k=b=>g(b,p),y=b=>g(b,d);return d.addEventListener("pointerleave",k),p.addEventListener("pointerleave",y),()=>{d.removeEventListener("pointerleave",k),p.removeEventListener("pointerleave",y)}}},[d,p,g,f]),m.useEffect(()=>{if(i){const k=y=>{const b=y.target,v={x:y.clientX,y:y.clientY},h=(d==null?void 0:d.contains(b))||(p==null?void 0:p.contains(b)),w=!XC(v,i);h?f():w&&(f(),c())};return document.addEventListener("pointermove",k),()=>document.removeEventListener("pointermove",k)}},[d,p,i,c,f]),l.jsx(iy,{...e,ref:s})}),[FC,KC]=Qa(zs,{isInside:!1}),$C=KA("TooltipContent"),iy=m.forwardRef((e,t)=>{const{__scopeTooltip:r,children:o,"aria-label":n,onEscapeKeyDown:s,onPointerDownOutside:i,...a}=e,d=Ja(Cn,r),c=Ya(r),{onClose:p}=d;return m.useEffect(()=>(document.addEventListener(cd,p),()=>document.removeEventListener(cd,p)),[p]),m.useEffect(()=>{if(d.trigger){const u=f=>{const g=f.target;g!=null&&g.contains(d.trigger)&&p()};return window.addEventListener("scroll",u,{capture:!0}),()=>window.removeEventListener("scroll",u,{capture:!0})}},[d.trigger,p]),l.jsx(_a,{asChild:!0,disableOutsidePointerEvents:!1,onEscapeKeyDown:s,onPointerDownOutside:i,onFocusOutside:u=>u.preventDefault(),onDismiss:p,children:l.jsxs(OC,{"data-state":d.stateAttribute,...c,...a,ref:t,style:{...a.style,"--radix-tooltip-content-transform-origin":"var(--radix-popper-transform-origin)","--radix-tooltip-content-available-width":"var(--radix-popper-available-width)","--radix-tooltip-content-available-height":"var(--radix-popper-available-height)","--radix-tooltip-trigger-width":"var(--radix-popper-anchor-width)","--radix-tooltip-trigger-height":"var(--radix-popper-anchor-height)"},children:[l.jsx($C,{children:o}),l.jsx(FC,{scope:r,isInside:!0,children:l.jsx(fS,{id:d.contentId,role:"tooltip",children:n||o})})]})})});sy.displayName=Cn;var ay="TooltipArrow",QC=m.forwardRef((e,t)=>{const{__scopeTooltip:r,...o}=e,n=Ya(r);return KC(ay,r).isInside?null:l.jsx(_C,{...n,...o,ref:t})});QC.displayName=ay;function YC(e,t){const r=Math.abs(t.top-e.y),o=Math.abs(t.bottom-e.y),n=Math.abs(t.right-e.x),s=Math.abs(t.left-e.x);switch(Math.min(r,o,n,s)){case s:return"left";case n:return"right";case r:return"top";case o:return"bottom";default:throw new Error("unreachable")}}function JC(e,t,r=5){const o=[];switch(t){case"top":o.push({x:e.x-r,y:e.y+r},{x:e.x+r,y:e.y+r});break;case"bottom":o.push({x:e.x-r,y:e.y-r},{x:e.x+r,y:e.y-r});break;case"left":o.push({x:e.x+r,y:e.y-r},{x:e.x+r,y:e.y+r});break;case"right":o.push({x:e.x-r,y:e.y-r},{x:e.x-r,y:e.y+r});break}return o}function ZC(e){const{top:t,right:r,bottom:o,left:n}=e;return[{x:n,y:t},{x:r,y:t},{x:r,y:o},{x:n,y:o}]}function XC(e,t){const{x:r,y:o}=e;let n=!1;for(let s=0,i=t.length-1;s<t.length;i=s++){const a=t[s],d=t[i],c=a.x,p=a.y,u=d.x,f=d.y;p>o!=f>o&&r<(u-c)*(o-p)/(f-p)+c&&(n=!n)}return n}function ex(e){const t=e.slice();return t.sort((r,o)=>r.x<o.x?-1:r.x>o.x?1:r.y<o.y?-1:r.y>o.y?1:0),tx(t)}function tx(e){if(e.length<=1)return e.slice();const t=[];for(let o=0;o<e.length;o++){const n=e[o];for(;t.length>=2;){const s=t[t.length-1],i=t[t.length-2];if((s.x-i.x)*(n.y-i.y)>=(s.y-i.y)*(n.x-i.x))t.pop();else break}t.push(n)}t.pop();const r=[];for(let o=e.length-1;o>=0;o--){const n=e[o];for(;r.length>=2;){const s=r[r.length-1],i=r[r.length-2];if((s.x-i.x)*(n.y-i.y)>=(s.y-i.y)*(n.x-i.x))r.pop();else break}r.push(n)}return r.pop(),t.length===1&&r.length===1&&t[0].x===r[0].x&&t[0].y===r[0].y?t:t.concat(r)}var rx=ry,ox=oy,nx=ny,ly=sy;const cy=rx,sx=ox,ix=nx,dy=m.forwardRef(({className:e,sideOffset:t=4,...r},o)=>l.jsx(ly,{ref:o,sideOffset:t,className:O("z-50 overflow-hidden rounded-md border bg-popover px-3 py-1.5 text-sm text-popover-foreground shadow-md animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",e),...r}));dy.displayName=ly.displayName;var Za=class{constructor(){this.listeners=new Set,this.subscribe=this.subscribe.bind(this)}subscribe(e){return this.listeners.add(e),this.onSubscribe(),()=>{this.listeners.delete(e),this.onUnsubscribe()}}hasListeners(){return this.listeners.size>0}onSubscribe(){}onUnsubscribe(){}},Xa=typeof window>"u"||"Deno"in globalThis;function St(){}function ax(e,t){return typeof e=="function"?e(t):e}function lx(e){return typeof e=="number"&&e>=0&&e!==1/0}function cx(e,t){return Math.max(e+(t||0)-Date.now(),0)}function ud(e,t){return typeof e=="function"?e(t):e}function dx(e,t){return typeof e=="function"?e(t):e}function Og(e,t){const{type:r="all",exact:o,fetchStatus:n,predicate:s,queryKey:i,stale:a}=e;if(i){if(o){if(t.queryHash!==Hu(i,t.options))return!1}else if(!Hs(t.queryKey,i))return!1}if(r!=="all"){const d=t.isActive();if(r==="active"&&!d||r==="inactive"&&d)return!1}return!(typeof a=="boolean"&&t.isStale()!==a||n&&n!==t.state.fetchStatus||s&&!s(t))}function _g(e,t){const{exact:r,status:o,predicate:n,mutationKey:s}=e;if(s){if(!t.options.mutationKey)return!1;if(r){if(Rs(t.options.mutationKey)!==Rs(s))return!1}else if(!Hs(t.options.mutationKey,s))return!1}return!(o&&t.state.status!==o||n&&!n(t))}function Hu(e,t){return((t==null?void 0:t.queryKeyHashFn)||Rs)(e)}function Rs(e){return JSON.stringify(e,(t,r)=>pd(r)?Object.keys(r).sort().reduce((o,n)=>(o[n]=r[n],o),{}):r)}function Hs(e,t){return e===t?!0:typeof e!=typeof t?!1:e&&t&&typeof e=="object"&&typeof t=="object"?Object.keys(t).every(r=>Hs(e[r],t[r])):!1}function uy(e,t){if(e===t)return e;const r=Vg(e)&&Vg(t);if(r||pd(e)&&pd(t)){const o=r?e:Object.keys(e),n=o.length,s=r?t:Object.keys(t),i=s.length,a=r?[]:{},d=new Set(o);let c=0;for(let p=0;p<i;p++){const u=r?p:s[p];(!r&&d.has(u)||r)&&e[u]===void 0&&t[u]===void 0?(a[u]=void 0,c++):(a[u]=uy(e[u],t[u]),a[u]===e[u]&&e[u]!==void 0&&c++)}return n===i&&c===n?e:a}return t}function Vg(e){return Array.isArray(e)&&e.length===Object.keys(e).length}function pd(e){if(!Bg(e))return!1;const t=e.constructor;if(t===void 0)return!0;const r=t.prototype;return!(!Bg(r)||!r.hasOwnProperty("isPrototypeOf")||Object.getPrototypeOf(e)!==Object.prototype)}function Bg(e){return Object.prototype.toString.call(e)==="[object Object]"}function ux(e){return new Promise(t=>{setTimeout(t,e)})}function px(e,t,r){return typeof r.structuralSharing=="function"?r.structuralSharing(e,t):r.structuralSharing!==!1?uy(e,t):t}function gx(e,t,r=0){const o=[...e,t];return r&&o.length>r?o.slice(1):o}function mx(e,t,r=0){const o=[t,...e];return r&&o.length>r?o.slice(0,-1):o}var Mu=Symbol();function py(e,t){return!e.queryFn&&(t!=null&&t.initialPromise)?()=>t.initialPromise:!e.queryFn||e.queryFn===Mu?()=>Promise.reject(new Error(`Missing queryFn: '${e.queryHash}'`)):e.queryFn}var co,zr,dn,Am,hx=(Am=class extends Za{constructor(){super();ee(this,co);ee(this,zr);ee(this,dn);j(this,dn,t=>{if(!Xa&&window.addEventListener){const r=()=>t();return window.addEventListener("visibilitychange",r,!1),()=>{window.removeEventListener("visibilitychange",r)}}})}onSubscribe(){C(this,zr)||this.setEventListener(C(this,dn))}onUnsubscribe(){var t;this.hasListeners()||((t=C(this,zr))==null||t.call(this),j(this,zr,void 0))}setEventListener(t){var r;j(this,dn,t),(r=C(this,zr))==null||r.call(this),j(this,zr,t(o=>{typeof o=="boolean"?this.setFocused(o):this.onFocus()}))}setFocused(t){C(this,co)!==t&&(j(this,co,t),this.onFocus())}onFocus(){const t=this.isFocused();this.listeners.forEach(r=>{r(t)})}isFocused(){var t;return typeof C(this,co)=="boolean"?C(this,co):((t=globalThis.document)==null?void 0:t.visibilityState)!=="hidden"}},co=new WeakMap,zr=new WeakMap,dn=new WeakMap,Am),gy=new hx,un,Rr,pn,Sm,fx=(Sm=class extends Za{constructor(){super();ee(this,un,!0);ee(this,Rr);ee(this,pn);j(this,pn,t=>{if(!Xa&&window.addEventListener){const r=()=>t(!0),o=()=>t(!1);return window.addEventListener("online",r,!1),window.addEventListener("offline",o,!1),()=>{window.removeEventListener("online",r),window.removeEventListener("offline",o)}}})}onSubscribe(){C(this,Rr)||this.setEventListener(C(this,pn))}onUnsubscribe(){var t;this.hasListeners()||((t=C(this,Rr))==null||t.call(this),j(this,Rr,void 0))}setEventListener(t){var r;j(this,pn,t),(r=C(this,Rr))==null||r.call(this),j(this,Rr,t(this.setOnline.bind(this)))}setOnline(t){C(this,un)!==t&&(j(this,un,t),this.listeners.forEach(o=>{o(t)}))}isOnline(){return C(this,un)}},un=new WeakMap,Rr=new WeakMap,pn=new WeakMap,Sm),ha=new fx;function vx(){let e,t;const r=new Promise((n,s)=>{e=n,t=s});r.status="pending",r.catch(()=>{});function o(n){Object.assign(r,n),delete r.resolve,delete r.reject}return r.resolve=n=>{o({status:"fulfilled",value:n}),e(n)},r.reject=n=>{o({status:"rejected",reason:n}),t(n)},r}function yx(e){return Math.min(1e3*2**e,3e4)}function my(e){return(e??"online")==="online"?ha.isOnline():!0}var hy=class extends Error{constructor(e){super("CancelledError"),this.revert=e==null?void 0:e.revert,this.silent=e==null?void 0:e.silent}};function Jl(e){return e instanceof hy}function fy(e){let t=!1,r=0,o=!1,n;const s=vx(),i=y=>{var b;o||(f(new hy(y)),(b=e.abort)==null||b.call(e))},a=()=>{t=!0},d=()=>{t=!1},c=()=>gy.isFocused()&&(e.networkMode==="always"||ha.isOnline())&&e.canRun(),p=()=>my(e.networkMode)&&e.canRun(),u=y=>{var b;o||(o=!0,(b=e.onSuccess)==null||b.call(e,y),n==null||n(),s.resolve(y))},f=y=>{var b;o||(o=!0,(b=e.onError)==null||b.call(e,y),n==null||n(),s.reject(y))},g=()=>new Promise(y=>{var b;n=v=>{(o||c())&&y(v)},(b=e.onPause)==null||b.call(e)}).then(()=>{var y;n=void 0,o||(y=e.onContinue)==null||y.call(e)}),k=()=>{if(o)return;let y;const b=r===0?e.initialPromise:void 0;try{y=b??e.fn()}catch(v){y=Promise.reject(v)}Promise.resolve(y).then(u).catch(v=>{var I;if(o)return;const h=e.retry??(Xa?0:3),w=e.retryDelay??yx,A=typeof w=="function"?w(r,v):w,S=h===!0||typeof h=="number"&&r<h||typeof h=="function"&&h(r,v);if(t||!S){f(v);return}r++,(I=e.onFail)==null||I.call(e,r,v),ux(A).then(()=>c()?void 0:g()).then(()=>{t?f(v):k()})})};return{promise:s,cancel:i,continue:()=>(n==null||n(),s),cancelRetry:a,continueRetry:d,canStart:p,start:()=>(p()?k():g().then(k),s)}}var wx=e=>setTimeout(e,0);function bx(){let e=[],t=0,r=a=>{a()},o=a=>{a()},n=wx;const s=a=>{t?e.push(a):n(()=>{r(a)})},i=()=>{const a=e;e=[],a.length&&n(()=>{o(()=>{a.forEach(d=>{r(d)})})})};return{batch:a=>{let d;t++;try{d=a()}finally{t--,t||i()}return d},batchCalls:a=>(...d)=>{s(()=>{a(...d)})},schedule:s,setNotifyFunction:a=>{r=a},setBatchNotifyFunction:a=>{o=a},setScheduler:a=>{n=a}}}var Ve=bx(),uo,Im,vy=(Im=class{constructor(){ee(this,uo)}destroy(){this.clearGcTimeout()}scheduleGc(){this.clearGcTimeout(),lx(this.gcTime)&&j(this,uo,setTimeout(()=>{this.optionalRemove()},this.gcTime))}updateGcTime(e){this.gcTime=Math.max(this.gcTime||0,e??(Xa?1/0:5*60*1e3))}clearGcTimeout(){C(this,uo)&&(clearTimeout(C(this,uo)),j(this,uo,void 0))}},uo=new WeakMap,Im),gn,po,st,go,Ne,Ns,mo,It,er,Pm,kx=(Pm=class extends vy{constructor(t){super();ee(this,It);ee(this,gn);ee(this,po);ee(this,st);ee(this,go);ee(this,Ne);ee(this,Ns);ee(this,mo);j(this,mo,!1),j(this,Ns,t.defaultOptions),this.setOptions(t.options),this.observers=[],j(this,go,t.client),j(this,st,C(this,go).getQueryCache()),this.queryKey=t.queryKey,this.queryHash=t.queryHash,j(this,gn,Sx(this.options)),this.state=t.state??C(this,gn),this.scheduleGc()}get meta(){return this.options.meta}get promise(){var t;return(t=C(this,Ne))==null?void 0:t.promise}setOptions(t){this.options={...C(this,Ns),...t},this.updateGcTime(this.options.gcTime)}optionalRemove(){!this.observers.length&&this.state.fetchStatus==="idle"&&C(this,st).remove(this)}setData(t,r){const o=px(this.state.data,t,this.options);return Re(this,It,er).call(this,{data:o,type:"success",dataUpdatedAt:r==null?void 0:r.updatedAt,manual:r==null?void 0:r.manual}),o}setState(t,r){Re(this,It,er).call(this,{type:"setState",state:t,setStateOptions:r})}cancel(t){var o,n;const r=(o=C(this,Ne))==null?void 0:o.promise;return(n=C(this,Ne))==null||n.cancel(t),r?r.then(St).catch(St):Promise.resolve()}destroy(){super.destroy(),this.cancel({silent:!0})}reset(){this.destroy(),this.setState(C(this,gn))}isActive(){return this.observers.some(t=>dx(t.options.enabled,this)!==!1)}isDisabled(){return this.getObserversCount()>0?!this.isActive():this.options.queryFn===Mu||this.state.dataUpdateCount+this.state.errorUpdateCount===0}isStatic(){return this.getObserversCount()>0?this.observers.some(t=>ud(t.options.staleTime,this)==="static"):!1}isStale(){return this.getObserversCount()>0?this.observers.some(t=>t.getCurrentResult().isStale):this.state.data===void 0||this.state.isInvalidated}isStaleByTime(t=0){return this.state.data===void 0?!0:t==="static"?!1:this.state.isInvalidated?!0:!cx(this.state.dataUpdatedAt,t)}onFocus(){var r;const t=this.observers.find(o=>o.shouldFetchOnWindowFocus());t==null||t.refetch({cancelRefetch:!1}),(r=C(this,Ne))==null||r.continue()}onOnline(){var r;const t=this.observers.find(o=>o.shouldFetchOnReconnect());t==null||t.refetch({cancelRefetch:!1}),(r=C(this,Ne))==null||r.continue()}addObserver(t){this.observers.includes(t)||(this.observers.push(t),this.clearGcTimeout(),C(this,st).notify({type:"observerAdded",query:this,observer:t}))}removeObserver(t){this.observers.includes(t)&&(this.observers=this.observers.filter(r=>r!==t),this.observers.length||(C(this,Ne)&&(C(this,mo)?C(this,Ne).cancel({revert:!0}):C(this,Ne).cancelRetry()),this.scheduleGc()),C(this,st).notify({type:"observerRemoved",query:this,observer:t}))}getObserversCount(){return this.observers.length}invalidate(){this.state.isInvalidated||Re(this,It,er).call(this,{type:"invalidate"})}fetch(t,r){var c,p,u;if(this.state.fetchStatus!=="idle"){if(this.state.data!==void 0&&(r!=null&&r.cancelRefetch))this.cancel({silent:!0});else if(C(this,Ne))return C(this,Ne).continueRetry(),C(this,Ne).promise}if(t&&this.setOptions(t),!this.options.queryFn){const f=this.observers.find(g=>g.options.queryFn);f&&this.setOptions(f.options)}const o=new AbortController,n=f=>{Object.defineProperty(f,"signal",{enumerable:!0,get:()=>(j(this,mo,!0),o.signal)})},s=()=>{const f=py(this.options,r),k=(()=>{const y={client:C(this,go),queryKey:this.queryKey,meta:this.meta};return n(y),y})();return j(this,mo,!1),this.options.persister?this.options.persister(f,k,this):f(k)},a=(()=>{const f={fetchOptions:r,options:this.options,queryKey:this.queryKey,client:C(this,go),state:this.state,fetchFn:s};return n(f),f})();(c=this.options.behavior)==null||c.onFetch(a,this),j(this,po,this.state),(this.state.fetchStatus==="idle"||this.state.fetchMeta!==((p=a.fetchOptions)==null?void 0:p.meta))&&Re(this,It,er).call(this,{type:"fetch",meta:(u=a.fetchOptions)==null?void 0:u.meta});const d=f=>{var g,k,y,b;Jl(f)&&f.silent||Re(this,It,er).call(this,{type:"error",error:f}),Jl(f)||((k=(g=C(this,st).config).onError)==null||k.call(g,f,this),(b=(y=C(this,st).config).onSettled)==null||b.call(y,this.state.data,f,this)),this.scheduleGc()};return j(this,Ne,fy({initialPromise:r==null?void 0:r.initialPromise,fn:a.fetchFn,abort:o.abort.bind(o),onSuccess:f=>{var g,k,y,b;if(f===void 0){d(new Error(`${this.queryHash} data is undefined`));return}try{this.setData(f)}catch(v){d(v);return}(k=(g=C(this,st).config).onSuccess)==null||k.call(g,f,this),(b=(y=C(this,st).config).onSettled)==null||b.call(y,f,this.state.error,this),this.scheduleGc()},onError:d,onFail:(f,g)=>{Re(this,It,er).call(this,{type:"failed",failureCount:f,error:g})},onPause:()=>{Re(this,It,er).call(this,{type:"pause"})},onContinue:()=>{Re(this,It,er).call(this,{type:"continue"})},retry:a.options.retry,retryDelay:a.options.retryDelay,networkMode:a.options.networkMode,canRun:()=>!0})),C(this,Ne).start()}},gn=new WeakMap,po=new WeakMap,st=new WeakMap,go=new WeakMap,Ne=new WeakMap,Ns=new WeakMap,mo=new WeakMap,It=new WeakSet,er=function(t){const r=o=>{switch(t.type){case"failed":return{...o,fetchFailureCount:t.failureCount,fetchFailureReason:t.error};case"pause":return{...o,fetchStatus:"paused"};case"continue":return{...o,fetchStatus:"fetching"};case"fetch":return{...o,...Ax(o.data,this.options),fetchMeta:t.meta??null};case"success":return j(this,po,void 0),{...o,data:t.data,dataUpdateCount:o.dataUpdateCount+1,dataUpdatedAt:t.dataUpdatedAt??Date.now(),error:null,isInvalidated:!1,status:"success",...!t.manual&&{fetchStatus:"idle",fetchFailureCount:0,fetchFailureReason:null}};case"error":const n=t.error;return Jl(n)&&n.revert&&C(this,po)?{...C(this,po),fetchStatus:"idle"}:{...o,error:n,errorUpdateCount:o.errorUpdateCount+1,errorUpdatedAt:Date.now(),fetchFailureCount:o.fetchFailureCount+1,fetchFailureReason:n,fetchStatus:"idle",status:"error"};case"invalidate":return{...o,isInvalidated:!0};case"setState":return{...o,...t.state}}};this.state=r(this.state),Ve.batch(()=>{this.observers.forEach(o=>{o.onQueryUpdate()}),C(this,st).notify({query:this,type:"updated",action:t})})},Pm);function Ax(e,t){return{fetchFailureCount:0,fetchFailureReason:null,fetchStatus:my(t.networkMode)?"fetching":"paused",...e===void 0&&{error:null,status:"pending"}}}function Sx(e){const t=typeof e.initialData=="function"?e.initialData():e.initialData,r=t!==void 0,o=r?typeof e.initialDataUpdatedAt=="function"?e.initialDataUpdatedAt():e.initialDataUpdatedAt:0;return{data:t,dataUpdateCount:0,dataUpdatedAt:r?o??Date.now():0,error:null,errorUpdateCount:0,errorUpdatedAt:0,fetchFailureCount:0,fetchFailureReason:null,fetchMeta:null,isInvalidated:!1,status:r?"success":"pending",fetchStatus:"idle"}}var Vt,Cm,Ix=(Cm=class extends Za{constructor(t={}){super();ee(this,Vt);this.config=t,j(this,Vt,new Map)}build(t,r,o){const n=r.queryKey,s=r.queryHash??Hu(n,r);let i=this.get(s);return i||(i=new kx({client:t,queryKey:n,queryHash:s,options:t.defaultQueryOptions(r),state:o,defaultOptions:t.getQueryDefaults(n)}),this.add(i)),i}add(t){C(this,Vt).has(t.queryHash)||(C(this,Vt).set(t.queryHash,t),this.notify({type:"added",query:t}))}remove(t){const r=C(this,Vt).get(t.queryHash);r&&(t.destroy(),r===t&&C(this,Vt).delete(t.queryHash),this.notify({type:"removed",query:t}))}clear(){Ve.batch(()=>{this.getAll().forEach(t=>{this.remove(t)})})}get(t){return C(this,Vt).get(t)}getAll(){return[...C(this,Vt).values()]}find(t){const r={exact:!0,...t};return this.getAll().find(o=>Og(r,o))}findAll(t={}){const r=this.getAll();return Object.keys(t).length>0?r.filter(o=>Og(t,o)):r}notify(t){Ve.batch(()=>{this.listeners.forEach(r=>{r(t)})})}onFocus(){Ve.batch(()=>{this.getAll().forEach(t=>{t.onFocus()})})}onOnline(){Ve.batch(()=>{this.getAll().forEach(t=>{t.onOnline()})})}},Vt=new WeakMap,Cm),Bt,Oe,ho,Wt,Ir,xm,Px=(xm=class extends vy{constructor(t){super();ee(this,Wt);ee(this,Bt);ee(this,Oe);ee(this,ho);this.mutationId=t.mutationId,j(this,Oe,t.mutationCache),j(this,Bt,[]),this.state=t.state||Cx(),this.setOptions(t.options),this.scheduleGc()}setOptions(t){this.options=t,this.updateGcTime(this.options.gcTime)}get meta(){return this.options.meta}addObserver(t){C(this,Bt).includes(t)||(C(this,Bt).push(t),this.clearGcTimeout(),C(this,Oe).notify({type:"observerAdded",mutation:this,observer:t}))}removeObserver(t){j(this,Bt,C(this,Bt).filter(r=>r!==t)),this.scheduleGc(),C(this,Oe).notify({type:"observerRemoved",mutation:this,observer:t})}optionalRemove(){C(this,Bt).length||(this.state.status==="pending"?this.scheduleGc():C(this,Oe).remove(this))}continue(){var t;return((t=C(this,ho))==null?void 0:t.continue())??this.execute(this.state.variables)}async execute(t){var s,i,a,d,c,p,u,f,g,k,y,b,v,h,w,A,S,I,P,D;const r=()=>{Re(this,Wt,Ir).call(this,{type:"continue"})};j(this,ho,fy({fn:()=>this.options.mutationFn?this.options.mutationFn(t):Promise.reject(new Error("No mutationFn found")),onFail:(M,H)=>{Re(this,Wt,Ir).call(this,{type:"failed",failureCount:M,error:H})},onPause:()=>{Re(this,Wt,Ir).call(this,{type:"pause"})},onContinue:r,retry:this.options.retry??0,retryDelay:this.options.retryDelay,networkMode:this.options.networkMode,canRun:()=>C(this,Oe).canRun(this)}));const o=this.state.status==="pending",n=!C(this,ho).canStart();try{if(o)r();else{Re(this,Wt,Ir).call(this,{type:"pending",variables:t,isPaused:n}),await((i=(s=C(this,Oe).config).onMutate)==null?void 0:i.call(s,t,this));const H=await((d=(a=this.options).onMutate)==null?void 0:d.call(a,t));H!==this.state.context&&Re(this,Wt,Ir).call(this,{type:"pending",context:H,variables:t,isPaused:n})}const M=await C(this,ho).start();return await((p=(c=C(this,Oe).config).onSuccess)==null?void 0:p.call(c,M,t,this.state.context,this)),await((f=(u=this.options).onSuccess)==null?void 0:f.call(u,M,t,this.state.context)),await((k=(g=C(this,Oe).config).onSettled)==null?void 0:k.call(g,M,null,this.state.variables,this.state.context,this)),await((b=(y=this.options).onSettled)==null?void 0:b.call(y,M,null,t,this.state.context)),Re(this,Wt,Ir).call(this,{type:"success",data:M}),M}catch(M){try{throw await((h=(v=C(this,Oe).config).onError)==null?void 0:h.call(v,M,t,this.state.context,this)),await((A=(w=this.options).onError)==null?void 0:A.call(w,M,t,this.state.context)),await((I=(S=C(this,Oe).config).onSettled)==null?void 0:I.call(S,void 0,M,this.state.variables,this.state.context,this)),await((D=(P=this.options).onSettled)==null?void 0:D.call(P,void 0,M,t,this.state.context)),M}finally{Re(this,Wt,Ir).call(this,{type:"error",error:M})}}finally{C(this,Oe).runNext(this)}}},Bt=new WeakMap,Oe=new WeakMap,ho=new WeakMap,Wt=new WeakSet,Ir=function(t){const r=o=>{switch(t.type){case"failed":return{...o,failureCount:t.failureCount,failureReason:t.error};case"pause":return{...o,isPaused:!0};case"continue":return{...o,isPaused:!1};case"pending":return{...o,context:t.context,data:void 0,failureCount:0,failureReason:null,error:null,isPaused:t.isPaused,status:"pending",variables:t.variables,submittedAt:Date.now()};case"success":return{...o,data:t.data,failureCount:0,failureReason:null,error:null,status:"success",isPaused:!1};case"error":return{...o,data:void 0,error:t.error,failureCount:o.failureCount+1,failureReason:t.error,isPaused:!1,status:"error"}}};this.state=r(this.state),Ve.batch(()=>{C(this,Bt).forEach(o=>{o.onMutationUpdate(t)}),C(this,Oe).notify({mutation:this,type:"updated",action:t})})},xm);function Cx(){return{context:void 0,data:void 0,error:null,failureCount:0,failureReason:null,isPaused:!1,status:"idle",variables:void 0,submittedAt:0}}var or,Pt,qs,Dm,xx=(Dm=class extends Za{constructor(t={}){super();ee(this,or);ee(this,Pt);ee(this,qs);this.config=t,j(this,or,new Set),j(this,Pt,new Map),j(this,qs,0)}build(t,r,o){const n=new Px({mutationCache:this,mutationId:++Ys(this,qs)._,options:t.defaultMutationOptions(r),state:o});return this.add(n),n}add(t){C(this,or).add(t);const r=yi(t);if(typeof r=="string"){const o=C(this,Pt).get(r);o?o.push(t):C(this,Pt).set(r,[t])}this.notify({type:"added",mutation:t})}remove(t){if(C(this,or).delete(t)){const r=yi(t);if(typeof r=="string"){const o=C(this,Pt).get(r);if(o)if(o.length>1){const n=o.indexOf(t);n!==-1&&o.splice(n,1)}else o[0]===t&&C(this,Pt).delete(r)}}this.notify({type:"removed",mutation:t})}canRun(t){const r=yi(t);if(typeof r=="string"){const o=C(this,Pt).get(r),n=o==null?void 0:o.find(s=>s.state.status==="pending");return!n||n===t}else return!0}runNext(t){var o;const r=yi(t);if(typeof r=="string"){const n=(o=C(this,Pt).get(r))==null?void 0:o.find(s=>s!==t&&s.state.isPaused);return(n==null?void 0:n.continue())??Promise.resolve()}else return Promise.resolve()}clear(){Ve.batch(()=>{C(this,or).forEach(t=>{this.notify({type:"removed",mutation:t})}),C(this,or).clear(),C(this,Pt).clear()})}getAll(){return Array.from(C(this,or))}find(t){const r={exact:!0,...t};return this.getAll().find(o=>_g(r,o))}findAll(t={}){return this.getAll().filter(r=>_g(t,r))}notify(t){Ve.batch(()=>{this.listeners.forEach(r=>{r(t)})})}resumePausedMutations(){const t=this.getAll().filter(r=>r.state.isPaused);return Ve.batch(()=>Promise.all(t.map(r=>r.continue().catch(St))))}},or=new WeakMap,Pt=new WeakMap,qs=new WeakMap,Dm);function yi(e){var t;return(t=e.options.scope)==null?void 0:t.id}function Wg(e){return{onFetch:(t,r)=>{var p,u,f,g,k;const o=t.options,n=(f=(u=(p=t.fetchOptions)==null?void 0:p.meta)==null?void 0:u.fetchMore)==null?void 0:f.direction,s=((g=t.state.data)==null?void 0:g.pages)||[],i=((k=t.state.data)==null?void 0:k.pageParams)||[];let a={pages:[],pageParams:[]},d=0;const c=async()=>{let y=!1;const b=w=>{Object.defineProperty(w,"signal",{enumerable:!0,get:()=>(t.signal.aborted?y=!0:t.signal.addEventListener("abort",()=>{y=!0}),t.signal)})},v=py(t.options,t.fetchOptions),h=async(w,A,S)=>{if(y)return Promise.reject();if(A==null&&w.pages.length)return Promise.resolve(w);const P=(()=>{const V={client:t.client,queryKey:t.queryKey,pageParam:A,direction:S?"backward":"forward",meta:t.options.meta};return b(V),V})(),D=await v(P),{maxPages:M}=t.options,H=S?mx:gx;return{pages:H(w.pages,D,M),pageParams:H(w.pageParams,A,M)}};if(n&&s.length){const w=n==="backward",A=w?Dx:jg,S={pages:s,pageParams:i},I=A(o,S);a=await h(S,I,w)}else{const w=e??s.length;do{const A=d===0?i[0]??o.initialPageParam:jg(o,a);if(d>0&&A==null)break;a=await h(a,A),d++}while(d<w)}return a};t.options.persister?t.fetchFn=()=>{var y,b;return(b=(y=t.options).persister)==null?void 0:b.call(y,c,{client:t.client,queryKey:t.queryKey,meta:t.options.meta,signal:t.signal},r)}:t.fetchFn=c}}}function jg(e,{pages:t,pageParams:r}){const o=t.length-1;return t.length>0?e.getNextPageParam(t[o],t,r[o],r):void 0}function Dx(e,{pages:t,pageParams:r}){var o;return t.length>0?(o=e.getPreviousPageParam)==null?void 0:o.call(e,t[0],t,r[0],r):void 0}var fe,Hr,Mr,mn,hn,Er,fn,vn,Tm,Tx=(Tm=class{constructor(e={}){ee(this,fe);ee(this,Hr);ee(this,Mr);ee(this,mn);ee(this,hn);ee(this,Er);ee(this,fn);ee(this,vn);j(this,fe,e.queryCache||new Ix),j(this,Hr,e.mutationCache||new xx),j(this,Mr,e.defaultOptions||{}),j(this,mn,new Map),j(this,hn,new Map),j(this,Er,0)}mount(){Ys(this,Er)._++,C(this,Er)===1&&(j(this,fn,gy.subscribe(async e=>{e&&(await this.resumePausedMutations(),C(this,fe).onFocus())})),j(this,vn,ha.subscribe(async e=>{e&&(await this.resumePausedMutations(),C(this,fe).onOnline())})))}unmount(){var e,t;Ys(this,Er)._--,C(this,Er)===0&&((e=C(this,fn))==null||e.call(this),j(this,fn,void 0),(t=C(this,vn))==null||t.call(this),j(this,vn,void 0))}isFetching(e){return C(this,fe).findAll({...e,fetchStatus:"fetching"}).length}isMutating(e){return C(this,Hr).findAll({...e,status:"pending"}).length}getQueryData(e){var r;const t=this.defaultQueryOptions({queryKey:e});return(r=C(this,fe).get(t.queryHash))==null?void 0:r.state.data}ensureQueryData(e){const t=this.defaultQueryOptions(e),r=C(this,fe).build(this,t),o=r.state.data;return o===void 0?this.fetchQuery(e):(e.revalidateIfStale&&r.isStaleByTime(ud(t.staleTime,r))&&this.prefetchQuery(t),Promise.resolve(o))}getQueriesData(e){return C(this,fe).findAll(e).map(({queryKey:t,state:r})=>{const o=r.data;return[t,o]})}setQueryData(e,t,r){const o=this.defaultQueryOptions({queryKey:e}),n=C(this,fe).get(o.queryHash),s=n==null?void 0:n.state.data,i=ax(t,s);if(i!==void 0)return C(this,fe).build(this,o).setData(i,{...r,manual:!0})}setQueriesData(e,t,r){return Ve.batch(()=>C(this,fe).findAll(e).map(({queryKey:o})=>[o,this.setQueryData(o,t,r)]))}getQueryState(e){var r;const t=this.defaultQueryOptions({queryKey:e});return(r=C(this,fe).get(t.queryHash))==null?void 0:r.state}removeQueries(e){const t=C(this,fe);Ve.batch(()=>{t.findAll(e).forEach(r=>{t.remove(r)})})}resetQueries(e,t){const r=C(this,fe);return Ve.batch(()=>(r.findAll(e).forEach(o=>{o.reset()}),this.refetchQueries({type:"active",...e},t)))}cancelQueries(e,t={}){const r={revert:!0,...t},o=Ve.batch(()=>C(this,fe).findAll(e).map(n=>n.cancel(r)));return Promise.all(o).then(St).catch(St)}invalidateQueries(e,t={}){return Ve.batch(()=>(C(this,fe).findAll(e).forEach(r=>{r.invalidate()}),(e==null?void 0:e.refetchType)==="none"?Promise.resolve():this.refetchQueries({...e,type:(e==null?void 0:e.refetchType)??(e==null?void 0:e.type)??"active"},t)))}refetchQueries(e,t={}){const r={...t,cancelRefetch:t.cancelRefetch??!0},o=Ve.batch(()=>C(this,fe).findAll(e).filter(n=>!n.isDisabled()&&!n.isStatic()).map(n=>{let s=n.fetch(void 0,r);return r.throwOnError||(s=s.catch(St)),n.state.fetchStatus==="paused"?Promise.resolve():s}));return Promise.all(o).then(St)}fetchQuery(e){const t=this.defaultQueryOptions(e);t.retry===void 0&&(t.retry=!1);const r=C(this,fe).build(this,t);return r.isStaleByTime(ud(t.staleTime,r))?r.fetch(t):Promise.resolve(r.state.data)}prefetchQuery(e){return this.fetchQuery(e).then(St).catch(St)}fetchInfiniteQuery(e){return e.behavior=Wg(e.pages),this.fetchQuery(e)}prefetchInfiniteQuery(e){return this.fetchInfiniteQuery(e).then(St).catch(St)}ensureInfiniteQueryData(e){return e.behavior=Wg(e.pages),this.ensureQueryData(e)}resumePausedMutations(){return ha.isOnline()?C(this,Hr).resumePausedMutations():Promise.resolve()}getQueryCache(){return C(this,fe)}getMutationCache(){return C(this,Hr)}getDefaultOptions(){return C(this,Mr)}setDefaultOptions(e){j(this,Mr,e)}setQueryDefaults(e,t){C(this,mn).set(Rs(e),{queryKey:e,defaultOptions:t})}getQueryDefaults(e){const t=[...C(this,mn).values()],r={};return t.forEach(o=>{Hs(e,o.queryKey)&&Object.assign(r,o.defaultOptions)}),r}setMutationDefaults(e,t){C(this,hn).set(Rs(e),{mutationKey:e,defaultOptions:t})}getMutationDefaults(e){const t=[...C(this,hn).values()],r={};return t.forEach(o=>{Hs(e,o.mutationKey)&&Object.assign(r,o.defaultOptions)}),r}defaultQueryOptions(e){if(e._defaulted)return e;const t={...C(this,Mr).queries,...this.getQueryDefaults(e.queryKey),...e,_defaulted:!0};return t.queryHash||(t.queryHash=Hu(t.queryKey,t)),t.refetchOnReconnect===void 0&&(t.refetchOnReconnect=t.networkMode!=="always"),t.throwOnError===void 0&&(t.throwOnError=!!t.suspense),!t.networkMode&&t.persister&&(t.networkMode="offlineFirst"),t.queryFn===Mu&&(t.enabled=!1),t}defaultMutationOptions(e){return e!=null&&e._defaulted?e:{...C(this,Mr).mutations,...(e==null?void 0:e.mutationKey)&&this.getMutationDefaults(e.mutationKey),...e,_defaulted:!0}}clear(){C(this,fe).clear(),C(this,Hr).clear()}},fe=new WeakMap,Hr=new WeakMap,Mr=new WeakMap,mn=new WeakMap,hn=new WeakMap,Er=new WeakMap,fn=new WeakMap,vn=new WeakMap,Tm),zx=m.createContext(void 0),Rx=({client:e,children:t})=>(m.useEffect(()=>(e.mount(),()=>{e.unmount()}),[e]),l.jsx(zx.Provider,{value:e,children:t}));/**
 * @remix-run/router v1.23.0
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function Ms(){return Ms=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var r=arguments[t];for(var o in r)Object.prototype.hasOwnProperty.call(r,o)&&(e[o]=r[o])}return e},Ms.apply(this,arguments)}var Lr;(function(e){e.Pop="POP",e.Push="PUSH",e.Replace="REPLACE"})(Lr||(Lr={}));const Ug="popstate";function Hx(e){e===void 0&&(e={});function t(n,s){let{pathname:i="/",search:a="",hash:d=""}=To(n.location.hash.substr(1));return!i.startsWith("/")&&!i.startsWith(".")&&(i="/"+i),gd("",{pathname:i,search:a,hash:d},s.state&&s.state.usr||null,s.state&&s.state.key||"default")}function r(n,s){let i=n.document.querySelector("base"),a="";if(i&&i.getAttribute("href")){let d=n.location.href,c=d.indexOf("#");a=c===-1?d:d.slice(0,c)}return a+"#"+(typeof s=="string"?s:fa(s))}function o(n,s){Eu(n.pathname.charAt(0)==="/","relative pathnames are not supported in hash history.push("+JSON.stringify(s)+")")}return Ex(t,r,o,e)}function me(e,t){if(e===!1||e===null||typeof e>"u")throw new Error(t)}function Eu(e,t){if(!e){typeof console<"u"&&console.warn(t);try{throw new Error(t)}catch{}}}function Mx(){return Math.random().toString(36).substr(2,8)}function Gg(e,t){return{usr:e.state,key:e.key,idx:t}}function gd(e,t,r,o){return r===void 0&&(r=null),Ms({pathname:typeof e=="string"?e:e.pathname,search:"",hash:""},typeof t=="string"?To(t):t,{state:r,key:t&&t.key||o||Mx()})}function fa(e){let{pathname:t="/",search:r="",hash:o=""}=e;return r&&r!=="?"&&(t+=r.charAt(0)==="?"?r:"?"+r),o&&o!=="#"&&(t+=o.charAt(0)==="#"?o:"#"+o),t}function To(e){let t={};if(e){let r=e.indexOf("#");r>=0&&(t.hash=e.substr(r),e=e.substr(0,r));let o=e.indexOf("?");o>=0&&(t.search=e.substr(o),e=e.substr(0,o)),e&&(t.pathname=e)}return t}function Ex(e,t,r,o){o===void 0&&(o={});let{window:n=document.defaultView,v5Compat:s=!1}=o,i=n.history,a=Lr.Pop,d=null,c=p();c==null&&(c=0,i.replaceState(Ms({},i.state,{idx:c}),""));function p(){return(i.state||{idx:null}).idx}function u(){a=Lr.Pop;let b=p(),v=b==null?null:b-c;c=b,d&&d({action:a,location:y.location,delta:v})}function f(b,v){a=Lr.Push;let h=gd(y.location,b,v);r&&r(h,b),c=p()+1;let w=Gg(h,c),A=y.createHref(h);try{i.pushState(w,"",A)}catch(S){if(S instanceof DOMException&&S.name==="DataCloneError")throw S;n.location.assign(A)}s&&d&&d({action:a,location:y.location,delta:1})}function g(b,v){a=Lr.Replace;let h=gd(y.location,b,v);r&&r(h,b),c=p();let w=Gg(h,c),A=y.createHref(h);i.replaceState(w,"",A),s&&d&&d({action:a,location:y.location,delta:0})}function k(b){let v=n.location.origin!=="null"?n.location.origin:n.location.href,h=typeof b=="string"?b:fa(b);return h=h.replace(/ $/,"%20"),me(v,"No window.location.(origin|href) available to create URL for href: "+h),new URL(h,v)}let y={get action(){return a},get location(){return e(n,i)},listen(b){if(d)throw new Error("A history only accepts one active listener");return n.addEventListener(Ug,u),d=b,()=>{n.removeEventListener(Ug,u),d=null}},createHref(b){return t(n,b)},createURL:k,encodeLocation(b){let v=k(b);return{pathname:v.pathname,search:v.search,hash:v.hash}},push:f,replace:g,go(b){return i.go(b)}};return y}var Fg;(function(e){e.data="data",e.deferred="deferred",e.redirect="redirect",e.error="error"})(Fg||(Fg={}));function Nx(e,t,r){return r===void 0&&(r="/"),qx(e,t,r,!1)}function qx(e,t,r,o){let n=typeof t=="string"?To(t):t,s=xn(n.pathname||"/",r);if(s==null)return null;let i=yy(e);Lx(i);let a=null;for(let d=0;a==null&&d<i.length;++d){let c=$x(s);a=Fx(i[d],c,o)}return a}function yy(e,t,r,o){t===void 0&&(t=[]),r===void 0&&(r=[]),o===void 0&&(o="");let n=(s,i,a)=>{let d={relativePath:a===void 0?s.path||"":a,caseSensitive:s.caseSensitive===!0,childrenIndex:i,route:s};d.relativePath.startsWith("/")&&(me(d.relativePath.startsWith(o),'Absolute route path "'+d.relativePath+'" nested under path '+('"'+o+'" is not valid. An absolute child route path ')+"must start with the combined path of all its parent routes."),d.relativePath=d.relativePath.slice(o.length));let c=Fr([o,d.relativePath]),p=r.concat(d);s.children&&s.children.length>0&&(me(s.index!==!0,"Index routes must not have child routes. Please remove "+('all child routes from route path "'+c+'".')),yy(s.children,t,p,c)),!(s.path==null&&!s.index)&&t.push({path:c,score:Ux(c,s.index),routesMeta:p})};return e.forEach((s,i)=>{var a;if(s.path===""||!((a=s.path)!=null&&a.includes("?")))n(s,i);else for(let d of wy(s.path))n(s,i,d)}),t}function wy(e){let t=e.split("/");if(t.length===0)return[];let[r,...o]=t,n=r.endsWith("?"),s=r.replace(/\?$/,"");if(o.length===0)return n?[s,""]:[s];let i=wy(o.join("/")),a=[];return a.push(...i.map(d=>d===""?s:[s,d].join("/"))),n&&a.push(...i),a.map(d=>e.startsWith("/")&&d===""?"/":d)}function Lx(e){e.sort((t,r)=>t.score!==r.score?r.score-t.score:Gx(t.routesMeta.map(o=>o.childrenIndex),r.routesMeta.map(o=>o.childrenIndex)))}const Ox=/^:[\w-]+$/,_x=3,Vx=2,Bx=1,Wx=10,jx=-2,Kg=e=>e==="*";function Ux(e,t){let r=e.split("/"),o=r.length;return r.some(Kg)&&(o+=jx),t&&(o+=Vx),r.filter(n=>!Kg(n)).reduce((n,s)=>n+(Ox.test(s)?_x:s===""?Bx:Wx),o)}function Gx(e,t){return e.length===t.length&&e.slice(0,-1).every((o,n)=>o===t[n])?e[e.length-1]-t[t.length-1]:0}function Fx(e,t,r){let{routesMeta:o}=e,n={},s="/",i=[];for(let a=0;a<o.length;++a){let d=o[a],c=a===o.length-1,p=s==="/"?t:t.slice(s.length)||"/",u=va({path:d.relativePath,caseSensitive:d.caseSensitive,end:c},p),f=d.route;if(!u&&c&&r&&!o[o.length-1].route.index&&(u=va({path:d.relativePath,caseSensitive:d.caseSensitive,end:!1},p)),!u)return null;Object.assign(n,u.params),i.push({params:n,pathname:Fr([s,u.pathname]),pathnameBase:Zx(Fr([s,u.pathnameBase])),route:f}),u.pathnameBase!=="/"&&(s=Fr([s,u.pathnameBase]))}return i}function va(e,t){typeof e=="string"&&(e={path:e,caseSensitive:!1,end:!0});let[r,o]=Kx(e.path,e.caseSensitive,e.end),n=t.match(r);if(!n)return null;let s=n[0],i=s.replace(/(.)\/+$/,"$1"),a=n.slice(1);return{params:o.reduce((c,p,u)=>{let{paramName:f,isOptional:g}=p;if(f==="*"){let y=a[u]||"";i=s.slice(0,s.length-y.length).replace(/(.)\/+$/,"$1")}const k=a[u];return g&&!k?c[f]=void 0:c[f]=(k||"").replace(/%2F/g,"/"),c},{}),pathname:s,pathnameBase:i,pattern:e}}function Kx(e,t,r){t===void 0&&(t=!1),r===void 0&&(r=!0),Eu(e==="*"||!e.endsWith("*")||e.endsWith("/*"),'Route path "'+e+'" will be treated as if it were '+('"'+e.replace(/\*$/,"/*")+'" because the `*` character must ')+"always follow a `/` in the pattern. To get rid of this warning, "+('please change the route path to "'+e.replace(/\*$/,"/*")+'".'));let o=[],n="^"+e.replace(/\/*\*?$/,"").replace(/^\/*/,"/").replace(/[\\.*+^${}|()[\]]/g,"\\$&").replace(/\/:([\w-]+)(\?)?/g,(i,a,d)=>(o.push({paramName:a,isOptional:d!=null}),d?"/?([^\\/]+)?":"/([^\\/]+)"));return e.endsWith("*")?(o.push({paramName:"*"}),n+=e==="*"||e==="/*"?"(.*)$":"(?:\\/(.+)|\\/*)$"):r?n+="\\/*$":e!==""&&e!=="/"&&(n+="(?:(?=\\/|$))"),[new RegExp(n,t?void 0:"i"),o]}function $x(e){try{return e.split("/").map(t=>decodeURIComponent(t).replace(/\//g,"%2F")).join("/")}catch(t){return Eu(!1,'The URL path "'+e+'" could not be decoded because it is is a malformed URL segment. This is probably due to a bad percent '+("encoding ("+t+").")),e}}function xn(e,t){if(t==="/")return e;if(!e.toLowerCase().startsWith(t.toLowerCase()))return null;let r=t.endsWith("/")?t.length-1:t.length,o=e.charAt(r);return o&&o!=="/"?null:e.slice(r)||"/"}function Qx(e,t){t===void 0&&(t="/");let{pathname:r,search:o="",hash:n=""}=typeof e=="string"?To(e):e;return{pathname:r?r.startsWith("/")?r:Yx(r,t):t,search:Xx(o),hash:e1(n)}}function Yx(e,t){let r=t.replace(/\/+$/,"").split("/");return e.split("/").forEach(n=>{n===".."?r.length>1&&r.pop():n!=="."&&r.push(n)}),r.length>1?r.join("/"):"/"}function Zl(e,t,r,o){return"Cannot include a '"+e+"' character in a manually specified "+("`to."+t+"` field ["+JSON.stringify(o)+"].  Please separate it out to the ")+("`to."+r+"` field. Alternatively you may provide the full path as ")+'a string in <Link to="..."> and the router will parse it for you.'}function Jx(e){return e.filter((t,r)=>r===0||t.route.path&&t.route.path.length>0)}function Nu(e,t){let r=Jx(e);return t?r.map((o,n)=>n===r.length-1?o.pathname:o.pathnameBase):r.map(o=>o.pathnameBase)}function qu(e,t,r,o){o===void 0&&(o=!1);let n;typeof e=="string"?n=To(e):(n=Ms({},e),me(!n.pathname||!n.pathname.includes("?"),Zl("?","pathname","search",n)),me(!n.pathname||!n.pathname.includes("#"),Zl("#","pathname","hash",n)),me(!n.search||!n.search.includes("#"),Zl("#","search","hash",n)));let s=e===""||n.pathname==="",i=s?"/":n.pathname,a;if(i==null)a=r;else{let u=t.length-1;if(!o&&i.startsWith("..")){let f=i.split("/");for(;f[0]==="..";)f.shift(),u-=1;n.pathname=f.join("/")}a=u>=0?t[u]:"/"}let d=Qx(n,a),c=i&&i!=="/"&&i.endsWith("/"),p=(s||i===".")&&r.endsWith("/");return!d.pathname.endsWith("/")&&(c||p)&&(d.pathname+="/"),d}const Fr=e=>e.join("/").replace(/\/\/+/g,"/"),Zx=e=>e.replace(/\/+$/,"").replace(/^\/*/,"/"),Xx=e=>!e||e==="?"?"":e.startsWith("?")?e:"?"+e,e1=e=>!e||e==="#"?"":e.startsWith("#")?e:"#"+e;function t1(e){return e!=null&&typeof e.status=="number"&&typeof e.statusText=="string"&&typeof e.internal=="boolean"&&"data"in e}const by=["post","put","patch","delete"];new Set(by);const r1=["get",...by];new Set(r1);/**
 * React Router v6.30.1
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function Es(){return Es=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var r=arguments[t];for(var o in r)Object.prototype.hasOwnProperty.call(r,o)&&(e[o]=r[o])}return e},Es.apply(this,arguments)}const el=m.createContext(null),ky=m.createContext(null),mr=m.createContext(null),tl=m.createContext(null),eo=m.createContext({outlet:null,matches:[],isDataRoute:!1}),Ay=m.createContext(null);function o1(e,t){let{relative:r}=t===void 0?{}:t;qn()||me(!1);let{basename:o,navigator:n}=m.useContext(mr),{hash:s,pathname:i,search:a}=rl(e,{relative:r}),d=i;return o!=="/"&&(d=i==="/"?o:Fr([o,i])),n.createHref({pathname:d,search:a,hash:s})}function qn(){return m.useContext(tl)!=null}function zo(){return qn()||me(!1),m.useContext(tl).location}function Sy(e){m.useContext(mr).static||m.useLayoutEffect(e)}function Iy(){let{isDataRoute:e}=m.useContext(eo);return e?f1():n1()}function n1(){qn()||me(!1);let e=m.useContext(el),{basename:t,future:r,navigator:o}=m.useContext(mr),{matches:n}=m.useContext(eo),{pathname:s}=zo(),i=JSON.stringify(Nu(n,r.v7_relativeSplatPath)),a=m.useRef(!1);return Sy(()=>{a.current=!0}),m.useCallback(function(c,p){if(p===void 0&&(p={}),!a.current)return;if(typeof c=="number"){o.go(c);return}let u=qu(c,JSON.parse(i),s,p.relative==="path");e==null&&t!=="/"&&(u.pathname=u.pathname==="/"?t:Fr([t,u.pathname])),(p.replace?o.replace:o.push)(u,p.state,p)},[t,o,i,s,e])}function rl(e,t){let{relative:r}=t===void 0?{}:t,{future:o}=m.useContext(mr),{matches:n}=m.useContext(eo),{pathname:s}=zo(),i=JSON.stringify(Nu(n,o.v7_relativeSplatPath));return m.useMemo(()=>qu(e,JSON.parse(i),s,r==="path"),[e,i,s,r])}function s1(e,t){return i1(e,t)}function i1(e,t,r,o){qn()||me(!1);let{navigator:n}=m.useContext(mr),{matches:s}=m.useContext(eo),i=s[s.length-1],a=i?i.params:{};i&&i.pathname;let d=i?i.pathnameBase:"/";i&&i.route;let c=zo(),p;if(t){var u;let b=typeof t=="string"?To(t):t;d==="/"||(u=b.pathname)!=null&&u.startsWith(d)||me(!1),p=b}else p=c;let f=p.pathname||"/",g=f;if(d!=="/"){let b=d.replace(/^\//,"").split("/");g="/"+f.replace(/^\//,"").split("/").slice(b.length).join("/")}let k=Nx(e,{pathname:g}),y=u1(k&&k.map(b=>Object.assign({},b,{params:Object.assign({},a,b.params),pathname:Fr([d,n.encodeLocation?n.encodeLocation(b.pathname).pathname:b.pathname]),pathnameBase:b.pathnameBase==="/"?d:Fr([d,n.encodeLocation?n.encodeLocation(b.pathnameBase).pathname:b.pathnameBase])})),s,r,o);return t&&y?m.createElement(tl.Provider,{value:{location:Es({pathname:"/",search:"",hash:"",state:null,key:"default"},p),navigationType:Lr.Pop}},y):y}function a1(){let e=h1(),t=t1(e)?e.status+" "+e.statusText:e instanceof Error?e.message:JSON.stringify(e),r=e instanceof Error?e.stack:null,n={padding:"0.5rem",backgroundColor:"rgba(200,200,200, 0.5)"};return m.createElement(m.Fragment,null,m.createElement("h2",null,"Unexpected Application Error!"),m.createElement("h3",{style:{fontStyle:"italic"}},t),r?m.createElement("pre",{style:n},r):null,null)}const l1=m.createElement(a1,null);class c1 extends m.Component{constructor(t){super(t),this.state={location:t.location,revalidation:t.revalidation,error:t.error}}static getDerivedStateFromError(t){return{error:t}}static getDerivedStateFromProps(t,r){return r.location!==t.location||r.revalidation!=="idle"&&t.revalidation==="idle"?{error:t.error,location:t.location,revalidation:t.revalidation}:{error:t.error!==void 0?t.error:r.error,location:r.location,revalidation:t.revalidation||r.revalidation}}componentDidCatch(t,r){console.error("React Router caught the following error during render",t,r)}render(){return this.state.error!==void 0?m.createElement(eo.Provider,{value:this.props.routeContext},m.createElement(Ay.Provider,{value:this.state.error,children:this.props.component})):this.props.children}}function d1(e){let{routeContext:t,match:r,children:o}=e,n=m.useContext(el);return n&&n.static&&n.staticContext&&(r.route.errorElement||r.route.ErrorBoundary)&&(n.staticContext._deepestRenderedBoundaryId=r.route.id),m.createElement(eo.Provider,{value:t},o)}function u1(e,t,r,o){var n;if(t===void 0&&(t=[]),r===void 0&&(r=null),o===void 0&&(o=null),e==null){var s;if(!r)return null;if(r.errors)e=r.matches;else if((s=o)!=null&&s.v7_partialHydration&&t.length===0&&!r.initialized&&r.matches.length>0)e=r.matches;else return null}let i=e,a=(n=r)==null?void 0:n.errors;if(a!=null){let p=i.findIndex(u=>u.route.id&&(a==null?void 0:a[u.route.id])!==void 0);p>=0||me(!1),i=i.slice(0,Math.min(i.length,p+1))}let d=!1,c=-1;if(r&&o&&o.v7_partialHydration)for(let p=0;p<i.length;p++){let u=i[p];if((u.route.HydrateFallback||u.route.hydrateFallbackElement)&&(c=p),u.route.id){let{loaderData:f,errors:g}=r,k=u.route.loader&&f[u.route.id]===void 0&&(!g||g[u.route.id]===void 0);if(u.route.lazy||k){d=!0,c>=0?i=i.slice(0,c+1):i=[i[0]];break}}}return i.reduceRight((p,u,f)=>{let g,k=!1,y=null,b=null;r&&(g=a&&u.route.id?a[u.route.id]:void 0,y=u.route.errorElement||l1,d&&(c<0&&f===0?(k=!0,b=null):c===f&&(k=!0,b=u.route.hydrateFallbackElement||null)));let v=t.concat(i.slice(0,f+1)),h=()=>{let w;return g?w=y:k?w=b:u.route.Component?w=m.createElement(u.route.Component,null):u.route.element?w=u.route.element:w=p,m.createElement(d1,{match:u,routeContext:{outlet:p,matches:v,isDataRoute:r!=null},children:w})};return r&&(u.route.ErrorBoundary||u.route.errorElement||f===0)?m.createElement(c1,{location:r.location,revalidation:r.revalidation,component:y,error:g,children:h(),routeContext:{outlet:null,matches:v,isDataRoute:!0}}):h()},null)}var Py=function(e){return e.UseBlocker="useBlocker",e.UseRevalidator="useRevalidator",e.UseNavigateStable="useNavigate",e}(Py||{}),ya=function(e){return e.UseBlocker="useBlocker",e.UseLoaderData="useLoaderData",e.UseActionData="useActionData",e.UseRouteError="useRouteError",e.UseNavigation="useNavigation",e.UseRouteLoaderData="useRouteLoaderData",e.UseMatches="useMatches",e.UseRevalidator="useRevalidator",e.UseNavigateStable="useNavigate",e.UseRouteId="useRouteId",e}(ya||{});function p1(e){let t=m.useContext(el);return t||me(!1),t}function g1(e){let t=m.useContext(ky);return t||me(!1),t}function m1(e){let t=m.useContext(eo);return t||me(!1),t}function Cy(e){let t=m1(),r=t.matches[t.matches.length-1];return r.route.id||me(!1),r.route.id}function h1(){var e;let t=m.useContext(Ay),r=g1(ya.UseRouteError),o=Cy(ya.UseRouteError);return t!==void 0?t:(e=r.errors)==null?void 0:e[o]}function f1(){let{router:e}=p1(Py.UseNavigateStable),t=Cy(ya.UseNavigateStable),r=m.useRef(!1);return Sy(()=>{r.current=!0}),m.useCallback(function(n,s){s===void 0&&(s={}),r.current&&(typeof n=="number"?e.navigate(n):e.navigate(n,Es({fromRouteId:t},s)))},[e,t])}function v1(e,t){e==null||e.v7_startTransition,e==null||e.v7_relativeSplatPath}function $g(e){let{to:t,replace:r,state:o,relative:n}=e;qn()||me(!1);let{future:s,static:i}=m.useContext(mr),{matches:a}=m.useContext(eo),{pathname:d}=zo(),c=Iy(),p=qu(t,Nu(a,s.v7_relativeSplatPath),d,n==="path"),u=JSON.stringify(p);return m.useEffect(()=>c(JSON.parse(u),{replace:r,state:o,relative:n}),[c,u,n,r,o]),null}function Ee(e){me(!1)}function y1(e){let{basename:t="/",children:r=null,location:o,navigationType:n=Lr.Pop,navigator:s,static:i=!1,future:a}=e;qn()&&me(!1);let d=t.replace(/^\/*/,"/"),c=m.useMemo(()=>({basename:d,navigator:s,static:i,future:Es({v7_relativeSplatPath:!1},a)}),[d,a,s,i]);typeof o=="string"&&(o=To(o));let{pathname:p="/",search:u="",hash:f="",state:g=null,key:k="default"}=o,y=m.useMemo(()=>{let b=xn(p,d);return b==null?null:{location:{pathname:b,search:u,hash:f,state:g,key:k},navigationType:n}},[d,p,u,f,g,k,n]);return y==null?null:m.createElement(mr.Provider,{value:c},m.createElement(tl.Provider,{children:r,value:y}))}function w1(e){let{children:t,location:r}=e;return s1(md(t),r)}new Promise(()=>{});function md(e,t){t===void 0&&(t=[]);let r=[];return m.Children.forEach(e,(o,n)=>{if(!m.isValidElement(o))return;let s=[...t,n];if(o.type===m.Fragment){r.push.apply(r,md(o.props.children,s));return}o.type!==Ee&&me(!1),!o.props.index||!o.props.children||me(!1);let i={id:o.props.id||s.join("-"),caseSensitive:o.props.caseSensitive,element:o.props.element,Component:o.props.Component,index:o.props.index,path:o.props.path,loader:o.props.loader,action:o.props.action,errorElement:o.props.errorElement,ErrorBoundary:o.props.ErrorBoundary,hasErrorBoundary:o.props.ErrorBoundary!=null||o.props.errorElement!=null,shouldRevalidate:o.props.shouldRevalidate,handle:o.props.handle,lazy:o.props.lazy};o.props.children&&(i.children=md(o.props.children,s)),r.push(i)}),r}/**
 * React Router DOM v6.30.1
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function wa(){return wa=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var r=arguments[t];for(var o in r)Object.prototype.hasOwnProperty.call(r,o)&&(e[o]=r[o])}return e},wa.apply(this,arguments)}function xy(e,t){if(e==null)return{};var r={},o=Object.keys(e),n,s;for(s=0;s<o.length;s++)n=o[s],!(t.indexOf(n)>=0)&&(r[n]=e[n]);return r}function b1(e){return!!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey)}function k1(e,t){return e.button===0&&(!t||t==="_self")&&!b1(e)}const A1=["onClick","relative","reloadDocument","replace","state","target","to","preventScrollReset","viewTransition"],S1=["aria-current","caseSensitive","className","end","style","to","viewTransition","children"],I1="6";try{window.__reactRouterVersion=I1}catch{}const P1=m.createContext({isTransitioning:!1}),C1="startTransition",Qg=Ad[C1];function x1(e){let{basename:t,children:r,future:o,window:n}=e,s=m.useRef();s.current==null&&(s.current=Hx({window:n,v5Compat:!0}));let i=s.current,[a,d]=m.useState({action:i.action,location:i.location}),{v7_startTransition:c}=o||{},p=m.useCallback(u=>{c&&Qg?Qg(()=>d(u)):d(u)},[d,c]);return m.useLayoutEffect(()=>i.listen(p),[i,p]),m.useEffect(()=>v1(o),[o]),m.createElement(y1,{basename:t,children:r,location:a.location,navigationType:a.action,navigator:i,future:o})}const D1=typeof window<"u"&&typeof window.document<"u"&&typeof window.document.createElement<"u",T1=/^(?:[a-z][a-z0-9+.-]*:|\/\/)/i,ba=m.forwardRef(function(t,r){let{onClick:o,relative:n,reloadDocument:s,replace:i,state:a,target:d,to:c,preventScrollReset:p,viewTransition:u}=t,f=xy(t,A1),{basename:g}=m.useContext(mr),k,y=!1;if(typeof c=="string"&&T1.test(c)&&(k=c,D1))try{let w=new URL(window.location.href),A=c.startsWith("//")?new URL(w.protocol+c):new URL(c),S=xn(A.pathname,g);A.origin===w.origin&&S!=null?c=S+A.search+A.hash:y=!0}catch{}let b=o1(c,{relative:n}),v=H1(c,{replace:i,state:a,target:d,preventScrollReset:p,relative:n,viewTransition:u});function h(w){o&&o(w),w.defaultPrevented||v(w)}return m.createElement("a",wa({},f,{href:k||b,onClick:y||s?o:h,ref:r,target:d}))}),z1=m.forwardRef(function(t,r){let{"aria-current":o="page",caseSensitive:n=!1,className:s="",end:i=!1,style:a,to:d,viewTransition:c,children:p}=t,u=xy(t,S1),f=rl(d,{relative:u.relative}),g=zo(),k=m.useContext(ky),{navigator:y,basename:b}=m.useContext(mr),v=k!=null&&M1(f)&&c===!0,h=y.encodeLocation?y.encodeLocation(f).pathname:f.pathname,w=g.pathname,A=k&&k.navigation&&k.navigation.location?k.navigation.location.pathname:null;n||(w=w.toLowerCase(),A=A?A.toLowerCase():null,h=h.toLowerCase()),A&&b&&(A=xn(A,b)||A);const S=h!=="/"&&h.endsWith("/")?h.length-1:h.length;let I=w===h||!i&&w.startsWith(h)&&w.charAt(S)==="/",P=A!=null&&(A===h||!i&&A.startsWith(h)&&A.charAt(h.length)==="/"),D={isActive:I,isPending:P,isTransitioning:v},M=I?o:void 0,H;typeof s=="function"?H=s(D):H=[s,I?"active":null,P?"pending":null,v?"transitioning":null].filter(Boolean).join(" ");let V=typeof a=="function"?a(D):a;return m.createElement(ba,wa({},u,{"aria-current":M,className:H,ref:r,style:V,to:d,viewTransition:c}),typeof p=="function"?p(D):p)});var hd;(function(e){e.UseScrollRestoration="useScrollRestoration",e.UseSubmit="useSubmit",e.UseSubmitFetcher="useSubmitFetcher",e.UseFetcher="useFetcher",e.useViewTransitionState="useViewTransitionState"})(hd||(hd={}));var Yg;(function(e){e.UseFetcher="useFetcher",e.UseFetchers="useFetchers",e.UseScrollRestoration="useScrollRestoration"})(Yg||(Yg={}));function R1(e){let t=m.useContext(el);return t||me(!1),t}function H1(e,t){let{target:r,replace:o,state:n,preventScrollReset:s,relative:i,viewTransition:a}=t===void 0?{}:t,d=Iy(),c=zo(),p=rl(e,{relative:i});return m.useCallback(u=>{if(k1(u,r)){u.preventDefault();let f=o!==void 0?o:fa(c)===fa(p);d(e,{replace:f,state:n,preventScrollReset:s,relative:i,viewTransition:a})}},[c,d,p,o,n,r,e,s,i,a])}function M1(e,t){t===void 0&&(t={});let r=m.useContext(P1);r==null&&me(!1);let{basename:o}=R1(hd.useViewTransitionState),n=rl(e,{relative:t.relative});if(!r.isTransitioning)return!1;let s=xn(r.currentLocation.pathname,o)||r.currentLocation.pathname,i=xn(r.nextLocation.pathname,o)||r.nextLocation.pathname;return va(n.pathname,i)!=null||va(n.pathname,s)!=null}const E1={theme:"system",setTheme:()=>null},Dy=m.createContext(E1);function N1({children:e,defaultTheme:t="system",storageKey:r="vite-ui-theme",...o}){const[n,s]=m.useState(()=>localStorage.getItem(r)||t);m.useEffect(()=>{const a=window.document.documentElement;if(a.classList.remove("light","dark"),n==="system"){const d=window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light";a.classList.add(d);return}a.classList.add(n)},[n]);const i={theme:n,setTheme:a=>{localStorage.setItem(r,a),s(a)}};return l.jsx(Dy.Provider,{...o,value:i,children:e})}const q1=()=>{const e=m.useContext(Dy);if(e===void 0)throw new Error("useTheme must be used within a ThemeProvider");return e},Xl=768;function L1(){const[e,t]=m.useState(void 0);return m.useEffect(()=>{const r=window.matchMedia(`(max-width: ${Xl-1}px)`),o=()=>{t(window.innerWidth<Xl)};return r.addEventListener("change",o),t(window.innerWidth<Xl),()=>r.removeEventListener("change",o)},[]),!!e}const O1=Wa("inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0",{variants:{variant:{default:"bg-primary text-primary-foreground hover:bg-primary/90",destructive:"bg-destructive text-destructive-foreground hover:bg-destructive/90",outline:"border border-input bg-background hover:bg-accent hover:text-accent-foreground",secondary:"bg-secondary text-secondary-foreground hover:bg-secondary/80",ghost:"hover:bg-accent hover:text-accent-foreground",link:"text-primary underline-offset-4 hover:underline"},size:{default:"h-10 px-4 py-2",sm:"h-9 rounded-md px-3",lg:"h-11 rounded-md px-8",icon:"h-10 w-10"}},defaultVariants:{variant:"default",size:"default"}}),Pe=m.forwardRef(({className:e,variant:t,size:r,asChild:o=!1,...n},s)=>{const i=o?Rn:"button";return l.jsx(i,{className:O(O1({variant:t,size:r,className:e})),ref:s,...n})});Pe.displayName="Button";const Lu=m.forwardRef(({className:e,type:t,...r},o)=>l.jsx("input",{type:t,className:O("flex h-10 w-full rounded-md border border-input bg-background px-3 py-2 text-base ring-offset-background file:border-0 file:bg-transparent file:text-sm file:font-medium file:text-foreground placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 md:text-sm",e),ref:o,...r}));Lu.displayName="Input";var _1="Separator",Jg="horizontal",V1=["horizontal","vertical"],Ty=m.forwardRef((e,t)=>{const{decorative:r,orientation:o=Jg,...n}=e,s=B1(o)?o:Jg,a=r?{role:"none"}:{"aria-orientation":s==="vertical"?s:void 0,role:"separator"};return l.jsx(ie.div,{"data-orientation":s,...a,...n,ref:t})});Ty.displayName=_1;function B1(e){return V1.includes(e)}var zy=Ty;const Ry=m.forwardRef(({className:e,orientation:t="horizontal",decorative:r=!0,...o},n)=>l.jsx(zy,{ref:n,decorative:r,orientation:t,className:O("shrink-0 bg-border",t==="horizontal"?"h-[1px] w-full":"h-full w-[1px]",e),...o}));Ry.displayName=zy.displayName;var ec="focusScope.autoFocusOnMount",tc="focusScope.autoFocusOnUnmount",Zg={bubbles:!1,cancelable:!0},W1="FocusScope",Hy=m.forwardRef((e,t)=>{const{loop:r=!1,trapped:o=!1,onMountAutoFocus:n,onUnmountAutoFocus:s,...i}=e,[a,d]=m.useState(null),c=Qt(n),p=Qt(s),u=m.useRef(null),f=Te(t,y=>d(y)),g=m.useRef({paused:!1,pause(){this.paused=!0},resume(){this.paused=!1}}).current;m.useEffect(()=>{if(o){let y=function(w){if(g.paused||!a)return;const A=w.target;a.contains(A)?u.current=A:Pr(u.current,{select:!0})},b=function(w){if(g.paused||!a)return;const A=w.relatedTarget;A!==null&&(a.contains(A)||Pr(u.current,{select:!0}))},v=function(w){if(document.activeElement===document.body)for(const S of w)S.removedNodes.length>0&&Pr(a)};document.addEventListener("focusin",y),document.addEventListener("focusout",b);const h=new MutationObserver(v);return a&&h.observe(a,{childList:!0,subtree:!0}),()=>{document.removeEventListener("focusin",y),document.removeEventListener("focusout",b),h.disconnect()}}},[o,a,g.paused]),m.useEffect(()=>{if(a){em.add(g);const y=document.activeElement;if(!a.contains(y)){const v=new CustomEvent(ec,Zg);a.addEventListener(ec,c),a.dispatchEvent(v),v.defaultPrevented||(j1($1(My(a)),{select:!0}),document.activeElement===y&&Pr(a))}return()=>{a.removeEventListener(ec,c),setTimeout(()=>{const v=new CustomEvent(tc,Zg);a.addEventListener(tc,p),a.dispatchEvent(v),v.defaultPrevented||Pr(y??document.body,{select:!0}),a.removeEventListener(tc,p),em.remove(g)},0)}}},[a,c,p,g]);const k=m.useCallback(y=>{if(!r&&!o||g.paused)return;const b=y.key==="Tab"&&!y.altKey&&!y.ctrlKey&&!y.metaKey,v=document.activeElement;if(b&&v){const h=y.currentTarget,[w,A]=U1(h);w&&A?!y.shiftKey&&v===A?(y.preventDefault(),r&&Pr(w,{select:!0})):y.shiftKey&&v===w&&(y.preventDefault(),r&&Pr(A,{select:!0})):v===h&&y.preventDefault()}},[r,o,g.paused]);return l.jsx(ie.div,{tabIndex:-1,...i,ref:f,onKeyDown:k})});Hy.displayName=W1;function j1(e,{select:t=!1}={}){const r=document.activeElement;for(const o of e)if(Pr(o,{select:t}),document.activeElement!==r)return}function U1(e){const t=My(e),r=Xg(t,e),o=Xg(t.reverse(),e);return[r,o]}function My(e){const t=[],r=document.createTreeWalker(e,NodeFilter.SHOW_ELEMENT,{acceptNode:o=>{const n=o.tagName==="INPUT"&&o.type==="hidden";return o.disabled||o.hidden||n?NodeFilter.FILTER_SKIP:o.tabIndex>=0?NodeFilter.FILTER_ACCEPT:NodeFilter.FILTER_SKIP}});for(;r.nextNode();)t.push(r.currentNode);return t}function Xg(e,t){for(const r of e)if(!G1(r,{upTo:t}))return r}function G1(e,{upTo:t}){if(getComputedStyle(e).visibility==="hidden")return!0;for(;e;){if(t!==void 0&&e===t)return!1;if(getComputedStyle(e).display==="none")return!0;e=e.parentElement}return!1}function F1(e){return e instanceof HTMLInputElement&&"select"in e}function Pr(e,{select:t=!1}={}){if(e&&e.focus){const r=document.activeElement;e.focus({preventScroll:!0}),e!==r&&F1(e)&&t&&e.select()}}var em=K1();function K1(){let e=[];return{add(t){const r=e[0];t!==r&&(r==null||r.pause()),e=tm(e,t),e.unshift(t)},remove(t){var r;e=tm(e,t),(r=e[0])==null||r.resume()}}}function tm(e,t){const r=[...e],o=r.indexOf(t);return o!==-1&&r.splice(o,1),r}function $1(e){return e.filter(t=>t.tagName!=="A")}var rc=0;function Q1(){m.useEffect(()=>{const e=document.querySelectorAll("[data-radix-focus-guard]");return document.body.insertAdjacentElement("afterbegin",e[0]??rm()),document.body.insertAdjacentElement("beforeend",e[1]??rm()),rc++,()=>{rc===1&&document.querySelectorAll("[data-radix-focus-guard]").forEach(t=>t.remove()),rc--}},[])}function rm(){const e=document.createElement("span");return e.setAttribute("data-radix-focus-guard",""),e.tabIndex=0,e.style.outline="none",e.style.opacity="0",e.style.position="fixed",e.style.pointerEvents="none",e}var Ut=function(){return Ut=Object.assign||function(t){for(var r,o=1,n=arguments.length;o<n;o++){r=arguments[o];for(var s in r)Object.prototype.hasOwnProperty.call(r,s)&&(t[s]=r[s])}return t},Ut.apply(this,arguments)};function Ey(e,t){var r={};for(var o in e)Object.prototype.hasOwnProperty.call(e,o)&&t.indexOf(o)<0&&(r[o]=e[o]);if(e!=null&&typeof Object.getOwnPropertySymbols=="function")for(var n=0,o=Object.getOwnPropertySymbols(e);n<o.length;n++)t.indexOf(o[n])<0&&Object.prototype.propertyIsEnumerable.call(e,o[n])&&(r[o[n]]=e[o[n]]);return r}function Y1(e,t,r){if(r||arguments.length===2)for(var o=0,n=t.length,s;o<n;o++)(s||!(o in t))&&(s||(s=Array.prototype.slice.call(t,0,o)),s[o]=t[o]);return e.concat(s||Array.prototype.slice.call(t))}var Oi="right-scroll-bar-position",_i="width-before-scroll-bar",J1="with-scroll-bars-hidden",Z1="--removed-body-scroll-bar-size";function oc(e,t){return typeof e=="function"?e(t):e&&(e.current=t),e}function X1(e,t){var r=m.useState(function(){return{value:e,callback:t,facade:{get current(){return r.value},set current(o){var n=r.value;n!==o&&(r.value=o,r.callback(o,n))}}}})[0];return r.callback=t,r.facade}var e0=typeof window<"u"?m.useLayoutEffect:m.useEffect,om=new WeakMap;function t0(e,t){var r=X1(null,function(o){return e.forEach(function(n){return oc(n,o)})});return e0(function(){var o=om.get(r);if(o){var n=new Set(o),s=new Set(e),i=r.current;n.forEach(function(a){s.has(a)||oc(a,null)}),s.forEach(function(a){n.has(a)||oc(a,i)})}om.set(r,e)},[e]),r}function r0(e){return e}function o0(e,t){t===void 0&&(t=r0);var r=[],o=!1,n={read:function(){if(o)throw new Error("Sidecar: could not `read` from an `assigned` medium. `read` could be used only with `useMedium`.");return r.length?r[r.length-1]:e},useMedium:function(s){var i=t(s,o);return r.push(i),function(){r=r.filter(function(a){return a!==i})}},assignSyncMedium:function(s){for(o=!0;r.length;){var i=r;r=[],i.forEach(s)}r={push:function(a){return s(a)},filter:function(){return r}}},assignMedium:function(s){o=!0;var i=[];if(r.length){var a=r;r=[],a.forEach(s),i=r}var d=function(){var p=i;i=[],p.forEach(s)},c=function(){return Promise.resolve().then(d)};c(),r={push:function(p){i.push(p),c()},filter:function(p){return i=i.filter(p),r}}}};return n}function n0(e){e===void 0&&(e={});var t=o0(null);return t.options=Ut({async:!0,ssr:!1},e),t}var Ny=function(e){var t=e.sideCar,r=Ey(e,["sideCar"]);if(!t)throw new Error("Sidecar: please provide `sideCar` property to import the right car");var o=t.read();if(!o)throw new Error("Sidecar medium not found");return m.createElement(o,Ut({},r))};Ny.isSideCarExport=!0;function s0(e,t){return e.useMedium(t),Ny}var qy=n0(),nc=function(){},ol=m.forwardRef(function(e,t){var r=m.useRef(null),o=m.useState({onScrollCapture:nc,onWheelCapture:nc,onTouchMoveCapture:nc}),n=o[0],s=o[1],i=e.forwardProps,a=e.children,d=e.className,c=e.removeScrollBar,p=e.enabled,u=e.shards,f=e.sideCar,g=e.noRelative,k=e.noIsolation,y=e.inert,b=e.allowPinchZoom,v=e.as,h=v===void 0?"div":v,w=e.gapMode,A=Ey(e,["forwardProps","children","className","removeScrollBar","enabled","shards","sideCar","noRelative","noIsolation","inert","allowPinchZoom","as","gapMode"]),S=f,I=t0([r,t]),P=Ut(Ut({},A),n);return m.createElement(m.Fragment,null,p&&m.createElement(S,{sideCar:qy,removeScrollBar:c,shards:u,noRelative:g,noIsolation:k,inert:y,setCallbacks:s,allowPinchZoom:!!b,lockRef:r,gapMode:w}),i?m.cloneElement(m.Children.only(a),Ut(Ut({},P),{ref:I})):m.createElement(h,Ut({},P,{className:d,ref:I}),a))});ol.defaultProps={enabled:!0,removeScrollBar:!0,inert:!1};ol.classNames={fullWidth:_i,zeroRight:Oi};var i0=function(){if(typeof __webpack_nonce__<"u")return __webpack_nonce__};function a0(){if(!document)return null;var e=document.createElement("style");e.type="text/css";var t=i0();return t&&e.setAttribute("nonce",t),e}function l0(e,t){e.styleSheet?e.styleSheet.cssText=t:e.appendChild(document.createTextNode(t))}function c0(e){var t=document.head||document.getElementsByTagName("head")[0];t.appendChild(e)}var d0=function(){var e=0,t=null;return{add:function(r){e==0&&(t=a0())&&(l0(t,r),c0(t)),e++},remove:function(){e--,!e&&t&&(t.parentNode&&t.parentNode.removeChild(t),t=null)}}},u0=function(){var e=d0();return function(t,r){m.useEffect(function(){return e.add(t),function(){e.remove()}},[t&&r])}},Ly=function(){var e=u0(),t=function(r){var o=r.styles,n=r.dynamic;return e(o,n),null};return t},p0={left:0,top:0,right:0,gap:0},sc=function(e){return parseInt(e||"",10)||0},g0=function(e){var t=window.getComputedStyle(document.body),r=t[e==="padding"?"paddingLeft":"marginLeft"],o=t[e==="padding"?"paddingTop":"marginTop"],n=t[e==="padding"?"paddingRight":"marginRight"];return[sc(r),sc(o),sc(n)]},m0=function(e){if(e===void 0&&(e="margin"),typeof window>"u")return p0;var t=g0(e),r=document.documentElement.clientWidth,o=window.innerWidth;return{left:t[0],top:t[1],right:t[2],gap:Math.max(0,o-r+t[2]-t[0])}},h0=Ly(),cn="data-scroll-locked",f0=function(e,t,r,o){var n=e.left,s=e.top,i=e.right,a=e.gap;return r===void 0&&(r="margin"),`
  .`.concat(J1,` {
   overflow: hidden `).concat(o,`;
   padding-right: `).concat(a,"px ").concat(o,`;
  }
  body[`).concat(cn,`] {
    overflow: hidden `).concat(o,`;
    overscroll-behavior: contain;
    `).concat([t&&"position: relative ".concat(o,";"),r==="margin"&&`
    padding-left: `.concat(n,`px;
    padding-top: `).concat(s,`px;
    padding-right: `).concat(i,`px;
    margin-left:0;
    margin-top:0;
    margin-right: `).concat(a,"px ").concat(o,`;
    `),r==="padding"&&"padding-right: ".concat(a,"px ").concat(o,";")].filter(Boolean).join(""),`
  }
  
  .`).concat(Oi,` {
    right: `).concat(a,"px ").concat(o,`;
  }
  
  .`).concat(_i,` {
    margin-right: `).concat(a,"px ").concat(o,`;
  }
  
  .`).concat(Oi," .").concat(Oi,` {
    right: 0 `).concat(o,`;
  }
  
  .`).concat(_i," .").concat(_i,` {
    margin-right: 0 `).concat(o,`;
  }
  
  body[`).concat(cn,`] {
    `).concat(Z1,": ").concat(a,`px;
  }
`)},nm=function(){var e=parseInt(document.body.getAttribute(cn)||"0",10);return isFinite(e)?e:0},v0=function(){m.useEffect(function(){return document.body.setAttribute(cn,(nm()+1).toString()),function(){var e=nm()-1;e<=0?document.body.removeAttribute(cn):document.body.setAttribute(cn,e.toString())}},[])},y0=function(e){var t=e.noRelative,r=e.noImportant,o=e.gapMode,n=o===void 0?"margin":o;v0();var s=m.useMemo(function(){return m0(n)},[n]);return m.createElement(h0,{styles:f0(s,!t,n,r?"":"!important")})},fd=!1;if(typeof window<"u")try{var wi=Object.defineProperty({},"passive",{get:function(){return fd=!0,!0}});window.addEventListener("test",wi,wi),window.removeEventListener("test",wi,wi)}catch{fd=!1}var Oo=fd?{passive:!1}:!1,w0=function(e){return e.tagName==="TEXTAREA"},Oy=function(e,t){if(!(e instanceof Element))return!1;var r=window.getComputedStyle(e);return r[t]!=="hidden"&&!(r.overflowY===r.overflowX&&!w0(e)&&r[t]==="visible")},b0=function(e){return Oy(e,"overflowY")},k0=function(e){return Oy(e,"overflowX")},sm=function(e,t){var r=t.ownerDocument,o=t;do{typeof ShadowRoot<"u"&&o instanceof ShadowRoot&&(o=o.host);var n=_y(e,o);if(n){var s=Vy(e,o),i=s[1],a=s[2];if(i>a)return!0}o=o.parentNode}while(o&&o!==r.body);return!1},A0=function(e){var t=e.scrollTop,r=e.scrollHeight,o=e.clientHeight;return[t,r,o]},S0=function(e){var t=e.scrollLeft,r=e.scrollWidth,o=e.clientWidth;return[t,r,o]},_y=function(e,t){return e==="v"?b0(t):k0(t)},Vy=function(e,t){return e==="v"?A0(t):S0(t)},I0=function(e,t){return e==="h"&&t==="rtl"?-1:1},P0=function(e,t,r,o,n){var s=I0(e,window.getComputedStyle(t).direction),i=s*o,a=r.target,d=t.contains(a),c=!1,p=i>0,u=0,f=0;do{if(!a)break;var g=Vy(e,a),k=g[0],y=g[1],b=g[2],v=y-b-s*k;(k||v)&&_y(e,a)&&(u+=v,f+=k);var h=a.parentNode;a=h&&h.nodeType===Node.DOCUMENT_FRAGMENT_NODE?h.host:h}while(!d&&a!==document.body||d&&(t.contains(a)||t===a));return(p&&(Math.abs(u)<1||!n)||!p&&(Math.abs(f)<1||!n))&&(c=!0),c},bi=function(e){return"changedTouches"in e?[e.changedTouches[0].clientX,e.changedTouches[0].clientY]:[0,0]},im=function(e){return[e.deltaX,e.deltaY]},am=function(e){return e&&"current"in e?e.current:e},C0=function(e,t){return e[0]===t[0]&&e[1]===t[1]},x0=function(e){return`
  .block-interactivity-`.concat(e,` {pointer-events: none;}
  .allow-interactivity-`).concat(e,` {pointer-events: all;}
`)},D0=0,_o=[];function T0(e){var t=m.useRef([]),r=m.useRef([0,0]),o=m.useRef(),n=m.useState(D0++)[0],s=m.useState(Ly)[0],i=m.useRef(e);m.useEffect(function(){i.current=e},[e]),m.useEffect(function(){if(e.inert){document.body.classList.add("block-interactivity-".concat(n));var y=Y1([e.lockRef.current],(e.shards||[]).map(am),!0).filter(Boolean);return y.forEach(function(b){return b.classList.add("allow-interactivity-".concat(n))}),function(){document.body.classList.remove("block-interactivity-".concat(n)),y.forEach(function(b){return b.classList.remove("allow-interactivity-".concat(n))})}}},[e.inert,e.lockRef.current,e.shards]);var a=m.useCallback(function(y,b){if("touches"in y&&y.touches.length===2||y.type==="wheel"&&y.ctrlKey)return!i.current.allowPinchZoom;var v=bi(y),h=r.current,w="deltaX"in y?y.deltaX:h[0]-v[0],A="deltaY"in y?y.deltaY:h[1]-v[1],S,I=y.target,P=Math.abs(w)>Math.abs(A)?"h":"v";if("touches"in y&&P==="h"&&I.type==="range")return!1;var D=sm(P,I);if(!D)return!0;if(D?S=P:(S=P==="v"?"h":"v",D=sm(P,I)),!D)return!1;if(!o.current&&"changedTouches"in y&&(w||A)&&(o.current=S),!S)return!0;var M=o.current||S;return P0(M,b,y,M==="h"?w:A,!0)},[]),d=m.useCallback(function(y){var b=y;if(!(!_o.length||_o[_o.length-1]!==s)){var v="deltaY"in b?im(b):bi(b),h=t.current.filter(function(S){return S.name===b.type&&(S.target===b.target||b.target===S.shadowParent)&&C0(S.delta,v)})[0];if(h&&h.should){b.cancelable&&b.preventDefault();return}if(!h){var w=(i.current.shards||[]).map(am).filter(Boolean).filter(function(S){return S.contains(b.target)}),A=w.length>0?a(b,w[0]):!i.current.noIsolation;A&&b.cancelable&&b.preventDefault()}}},[]),c=m.useCallback(function(y,b,v,h){var w={name:y,delta:b,target:v,should:h,shadowParent:z0(v)};t.current.push(w),setTimeout(function(){t.current=t.current.filter(function(A){return A!==w})},1)},[]),p=m.useCallback(function(y){r.current=bi(y),o.current=void 0},[]),u=m.useCallback(function(y){c(y.type,im(y),y.target,a(y,e.lockRef.current))},[]),f=m.useCallback(function(y){c(y.type,bi(y),y.target,a(y,e.lockRef.current))},[]);m.useEffect(function(){return _o.push(s),e.setCallbacks({onScrollCapture:u,onWheelCapture:u,onTouchMoveCapture:f}),document.addEventListener("wheel",d,Oo),document.addEventListener("touchmove",d,Oo),document.addEventListener("touchstart",p,Oo),function(){_o=_o.filter(function(y){return y!==s}),document.removeEventListener("wheel",d,Oo),document.removeEventListener("touchmove",d,Oo),document.removeEventListener("touchstart",p,Oo)}},[]);var g=e.removeScrollBar,k=e.inert;return m.createElement(m.Fragment,null,k?m.createElement(s,{styles:x0(n)}):null,g?m.createElement(y0,{noRelative:e.noRelative,gapMode:e.gapMode}):null)}function z0(e){for(var t=null;e!==null;)e instanceof ShadowRoot&&(t=e.host,e=e.host),e=e.parentNode;return t}const R0=s0(qy,T0);var By=m.forwardRef(function(e,t){return m.createElement(ol,Ut({},e,{ref:t,sideCar:R0}))});By.classNames=ol.classNames;var H0=function(e){if(typeof document>"u")return null;var t=Array.isArray(e)?e[0]:e;return t.ownerDocument.body},Vo=new WeakMap,ki=new WeakMap,Ai={},ic=0,Wy=function(e){return e&&(e.host||Wy(e.parentNode))},M0=function(e,t){return t.map(function(r){if(e.contains(r))return r;var o=Wy(r);return o&&e.contains(o)?o:(console.error("aria-hidden",r,"in not contained inside",e,". Doing nothing"),null)}).filter(function(r){return!!r})},E0=function(e,t,r,o){var n=M0(t,Array.isArray(e)?e:[e]);Ai[r]||(Ai[r]=new WeakMap);var s=Ai[r],i=[],a=new Set,d=new Set(n),c=function(u){!u||a.has(u)||(a.add(u),c(u.parentNode))};n.forEach(c);var p=function(u){!u||d.has(u)||Array.prototype.forEach.call(u.children,function(f){if(a.has(f))p(f);else try{var g=f.getAttribute(o),k=g!==null&&g!=="false",y=(Vo.get(f)||0)+1,b=(s.get(f)||0)+1;Vo.set(f,y),s.set(f,b),i.push(f),y===1&&k&&ki.set(f,!0),b===1&&f.setAttribute(r,"true"),k||f.setAttribute(o,"true")}catch(v){console.error("aria-hidden: cannot operate on ",f,v)}})};return p(t),a.clear(),ic++,function(){i.forEach(function(u){var f=Vo.get(u)-1,g=s.get(u)-1;Vo.set(u,f),s.set(u,g),f||(ki.has(u)||u.removeAttribute(o),ki.delete(u)),g||u.removeAttribute(r)}),ic--,ic||(Vo=new WeakMap,Vo=new WeakMap,ki=new WeakMap,Ai={})}},N0=function(e,t,r){r===void 0&&(r="data-aria-hidden");var o=Array.from(Array.isArray(e)?e:[e]),n=H0(e);return n?(o.push.apply(o,Array.from(n.querySelectorAll("[aria-live]"))),E0(o,n,r,"aria-hidden")):function(){return null}},nl="Dialog",[jy,hD]=xo(nl),[q0,Mt]=jy(nl),Uy=e=>{const{__scopeDialog:t,children:r,open:o,defaultOpen:n,onOpenChange:s,modal:i=!0}=e,a=m.useRef(null),d=m.useRef(null),[c,p]=Hn({prop:o,defaultProp:n??!1,onChange:s,caller:nl});return l.jsx(q0,{scope:t,triggerRef:a,contentRef:d,contentId:an(),titleId:an(),descriptionId:an(),open:c,onOpenChange:p,onOpenToggle:m.useCallback(()=>p(u=>!u),[p]),modal:i,children:r})};Uy.displayName=nl;var Gy="DialogTrigger",Fy=m.forwardRef((e,t)=>{const{__scopeDialog:r,...o}=e,n=Mt(Gy,r),s=Te(t,n.triggerRef);return l.jsx(ie.button,{type:"button","aria-haspopup":"dialog","aria-expanded":n.open,"aria-controls":n.contentId,"data-state":Vu(n.open),...o,ref:s,onClick:ne(e.onClick,n.onOpenToggle)})});Fy.displayName=Gy;var Ou="DialogPortal",[L0,Ky]=jy(Ou,{forceMount:void 0}),$y=e=>{const{__scopeDialog:t,forceMount:r,children:o,container:n}=e,s=Mt(Ou,t);return l.jsx(L0,{scope:t,forceMount:r,children:m.Children.map(o,i=>l.jsx(Do,{present:r||s.open,children:l.jsx(gu,{asChild:!0,container:n,children:i})}))})};$y.displayName=Ou;var ka="DialogOverlay",Qy=m.forwardRef((e,t)=>{const r=Ky(ka,e.__scopeDialog),{forceMount:o=r.forceMount,...n}=e,s=Mt(ka,e.__scopeDialog);return s.modal?l.jsx(Do,{present:o||s.open,children:l.jsx(_0,{...n,ref:t})}):null});Qy.displayName=ka;var O0=Cs("DialogOverlay.RemoveScroll"),_0=m.forwardRef((e,t)=>{const{__scopeDialog:r,...o}=e,n=Mt(ka,r);return l.jsx(By,{as:O0,allowPinchZoom:!0,shards:[n.contentRef],children:l.jsx(ie.div,{"data-state":Vu(n.open),...o,ref:t,style:{pointerEvents:"auto",...o.style}})})}),Io="DialogContent",Yy=m.forwardRef((e,t)=>{const r=Ky(Io,e.__scopeDialog),{forceMount:o=r.forceMount,...n}=e,s=Mt(Io,e.__scopeDialog);return l.jsx(Do,{present:o||s.open,children:s.modal?l.jsx(V0,{...n,ref:t}):l.jsx(B0,{...n,ref:t})})});Yy.displayName=Io;var V0=m.forwardRef((e,t)=>{const r=Mt(Io,e.__scopeDialog),o=m.useRef(null),n=Te(t,r.contentRef,o);return m.useEffect(()=>{const s=o.current;if(s)return N0(s)},[]),l.jsx(Jy,{...e,ref:n,trapFocus:r.open,disableOutsidePointerEvents:!0,onCloseAutoFocus:ne(e.onCloseAutoFocus,s=>{var i;s.preventDefault(),(i=r.triggerRef.current)==null||i.focus()}),onPointerDownOutside:ne(e.onPointerDownOutside,s=>{const i=s.detail.originalEvent,a=i.button===0&&i.ctrlKey===!0;(i.button===2||a)&&s.preventDefault()}),onFocusOutside:ne(e.onFocusOutside,s=>s.preventDefault())})}),B0=m.forwardRef((e,t)=>{const r=Mt(Io,e.__scopeDialog),o=m.useRef(!1),n=m.useRef(!1);return l.jsx(Jy,{...e,ref:t,trapFocus:!1,disableOutsidePointerEvents:!1,onCloseAutoFocus:s=>{var i,a;(i=e.onCloseAutoFocus)==null||i.call(e,s),s.defaultPrevented||(o.current||(a=r.triggerRef.current)==null||a.focus(),s.preventDefault()),o.current=!1,n.current=!1},onInteractOutside:s=>{var d,c;(d=e.onInteractOutside)==null||d.call(e,s),s.defaultPrevented||(o.current=!0,s.detail.originalEvent.type==="pointerdown"&&(n.current=!0));const i=s.target;((c=r.triggerRef.current)==null?void 0:c.contains(i))&&s.preventDefault(),s.detail.originalEvent.type==="focusin"&&n.current&&s.preventDefault()}})}),Jy=m.forwardRef((e,t)=>{const{__scopeDialog:r,trapFocus:o,onOpenAutoFocus:n,onCloseAutoFocus:s,...i}=e,a=Mt(Io,r),d=m.useRef(null),c=Te(t,d);return Q1(),l.jsxs(l.Fragment,{children:[l.jsx(Hy,{asChild:!0,loop:!0,trapped:o,onMountAutoFocus:n,onUnmountAutoFocus:s,children:l.jsx(_a,{role:"dialog",id:a.contentId,"aria-describedby":a.descriptionId,"aria-labelledby":a.titleId,"data-state":Vu(a.open),...i,ref:c,onDismiss:()=>a.onOpenChange(!1)})}),l.jsxs(l.Fragment,{children:[l.jsx(W0,{titleId:a.titleId}),l.jsx(U0,{contentRef:d,descriptionId:a.descriptionId})]})]})}),_u="DialogTitle",Zy=m.forwardRef((e,t)=>{const{__scopeDialog:r,...o}=e,n=Mt(_u,r);return l.jsx(ie.h2,{id:n.titleId,...o,ref:t})});Zy.displayName=_u;var Xy="DialogDescription",ew=m.forwardRef((e,t)=>{const{__scopeDialog:r,...o}=e,n=Mt(Xy,r);return l.jsx(ie.p,{id:n.descriptionId,...o,ref:t})});ew.displayName=Xy;var tw="DialogClose",rw=m.forwardRef((e,t)=>{const{__scopeDialog:r,...o}=e,n=Mt(tw,r);return l.jsx(ie.button,{type:"button",...o,ref:t,onClick:ne(e.onClick,()=>n.onOpenChange(!1))})});rw.displayName=tw;function Vu(e){return e?"open":"closed"}var ow="DialogTitleWarning",[fD,nw]=UA(ow,{contentName:Io,titleName:_u,docsSlug:"dialog"}),W0=({titleId:e})=>{const t=nw(ow),r=`\`${t.contentName}\` requires a \`${t.titleName}\` for the component to be accessible for screen reader users.

If you want to hide the \`${t.titleName}\`, you can wrap it with our VisuallyHidden component.

For more information, see https://radix-ui.com/primitives/docs/components/${t.docsSlug}`;return m.useEffect(()=>{e&&(document.getElementById(e)||console.error(r))},[r,e]),null},j0="DialogDescriptionWarning",U0=({contentRef:e,descriptionId:t})=>{const o=`Warning: Missing \`Description\` or \`aria-describedby={undefined}\` for {${nw(j0).contentName}}.`;return m.useEffect(()=>{var s;const n=(s=e.current)==null?void 0:s.getAttribute("aria-describedby");t&&n&&(document.getElementById(t)||console.warn(o))},[o,e,t]),null},sw=Uy,G0=Fy,iw=$y,sl=Qy,il=Yy,al=Zy,ll=ew,aw=rw;const F0=sw,K0=iw,lw=m.forwardRef(({className:e,...t},r)=>l.jsx(sl,{className:O("fixed inset-0 z-50 bg-black/80 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",e),...t,ref:r}));lw.displayName=sl.displayName;const $0=Wa("fixed z-50 gap-4 bg-background p-6 shadow-lg transition ease-in-out data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:duration-300 data-[state=open]:duration-500",{variants:{side:{top:"inset-x-0 top-0 border-b data-[state=closed]:slide-out-to-top data-[state=open]:slide-in-from-top",bottom:"inset-x-0 bottom-0 border-t data-[state=closed]:slide-out-to-bottom data-[state=open]:slide-in-from-bottom",left:"inset-y-0 left-0 h-full w-3/4 border-r data-[state=closed]:slide-out-to-left data-[state=open]:slide-in-from-left sm:max-w-sm",right:"inset-y-0 right-0 h-full w-3/4  border-l data-[state=closed]:slide-out-to-right data-[state=open]:slide-in-from-right sm:max-w-sm"}},defaultVariants:{side:"right"}}),cw=m.forwardRef(({side:e="right",className:t,children:r,...o},n)=>l.jsxs(K0,{children:[l.jsx(lw,{}),l.jsxs(il,{ref:n,className:O($0({side:e}),t),...o,children:[r,l.jsxs(aw,{className:"absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity data-[state=open]:bg-secondary hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none",children:[l.jsx(bu,{className:"h-4 w-4"}),l.jsx("span",{className:"sr-only",children:"Close"})]})]})]}));cw.displayName=il.displayName;const Q0=m.forwardRef(({className:e,...t},r)=>l.jsx(al,{ref:r,className:O("text-lg font-semibold text-foreground",e),...t}));Q0.displayName=al.displayName;const Y0=m.forwardRef(({className:e,...t},r)=>l.jsx(ll,{ref:r,className:O("text-sm text-muted-foreground",e),...t}));Y0.displayName=ll.displayName;function lm({className:e,...t}){return l.jsx("div",{className:O("animate-pulse rounded-md bg-muted",e),...t})}const J0="sidebar:state",Z0=60*60*24*7,X0="16rem",e3="18rem",t3="3rem",r3="b",dw=m.createContext(null);function Fs(){const e=m.useContext(dw);if(!e)throw new Error("useSidebar must be used within a SidebarProvider.");return e}const uw=m.forwardRef(({defaultOpen:e=!0,open:t,onOpenChange:r,className:o,style:n,children:s,...i},a)=>{const d=L1(),[c,p]=m.useState(!1),[u,f]=m.useState(e),g=t??u,k=m.useCallback(h=>{const w=typeof h=="function"?h(g):h;r?r(w):f(w),document.cookie=`${J0}=${w}; path=/; max-age=${Z0}`},[r,g]),y=m.useCallback(()=>d?p(h=>!h):k(h=>!h),[d,k,p]);m.useEffect(()=>{const h=w=>{w.key===r3&&(w.metaKey||w.ctrlKey)&&(w.preventDefault(),y())};return window.addEventListener("keydown",h),()=>window.removeEventListener("keydown",h)},[y]);const b=g?"expanded":"collapsed",v=m.useMemo(()=>({state:b,open:g,setOpen:k,isMobile:d,openMobile:c,setOpenMobile:p,toggleSidebar:y}),[b,g,k,d,c,p,y]);return l.jsx(dw.Provider,{value:v,children:l.jsx(cy,{delayDuration:0,children:l.jsx("div",{style:{"--sidebar-width":X0,"--sidebar-width-icon":t3,...n},className:O("group/sidebar-wrapper flex min-h-svh w-full has-[[data-variant=inset]]:bg-sidebar",o),ref:a,...i,children:s})})})});uw.displayName="SidebarProvider";const pw=m.forwardRef(({side:e="left",variant:t="sidebar",collapsible:r="offcanvas",className:o,children:n,...s},i)=>{const{isMobile:a,state:d,openMobile:c,setOpenMobile:p}=Fs();return r==="none"?l.jsx("div",{className:O("flex h-full w-[--sidebar-width] flex-col bg-sidebar text-sidebar-foreground",o),ref:i,...s,children:n}):a?l.jsx(F0,{open:c,onOpenChange:p,...s,children:l.jsx(cw,{"data-sidebar":"sidebar","data-mobile":"true",className:"w-[--sidebar-width] bg-sidebar p-0 text-sidebar-foreground [&>button]:hidden",style:{"--sidebar-width":e3},side:e,children:l.jsx("div",{className:"flex h-full w-full flex-col",children:n})})}):l.jsxs("div",{ref:i,className:"group peer hidden text-sidebar-foreground md:block","data-state":d,"data-collapsible":d==="collapsed"?r:"","data-variant":t,"data-side":e,children:[l.jsx("div",{className:O("relative h-svh w-[--sidebar-width] bg-transparent transition-[width] duration-200 ease-linear","group-data-[collapsible=offcanvas]:w-0","group-data-[side=right]:rotate-180",t==="floating"||t==="inset"?"group-data-[collapsible=icon]:w-[calc(var(--sidebar-width-icon)_+_theme(spacing.4))]":"group-data-[collapsible=icon]:w-[--sidebar-width-icon]")}),l.jsx("div",{className:O("fixed inset-y-0 z-10 hidden h-svh w-[--sidebar-width] transition-[left,right,width] duration-200 ease-linear md:flex",e==="left"?"left-0 group-data-[collapsible=offcanvas]:left-[calc(var(--sidebar-width)*-1)]":"right-0 group-data-[collapsible=offcanvas]:right-[calc(var(--sidebar-width)*-1)]",t==="floating"||t==="inset"?"p-2 group-data-[collapsible=icon]:w-[calc(var(--sidebar-width-icon)_+_theme(spacing.4)_+2px)]":"group-data-[collapsible=icon]:w-[--sidebar-width-icon] group-data-[side=left]:border-r group-data-[side=right]:border-l",o),...s,children:l.jsx("div",{"data-sidebar":"sidebar",className:"flex h-full w-full flex-col bg-sidebar group-data-[variant=floating]:rounded-lg group-data-[variant=floating]:border group-data-[variant=floating]:border-sidebar-border group-data-[variant=floating]:shadow",children:n})})]})});pw.displayName="Sidebar";const gw=m.forwardRef(({className:e,onClick:t,...r},o)=>{const{toggleSidebar:n}=Fs();return l.jsxs(Pe,{ref:o,"data-sidebar":"trigger",variant:"ghost",size:"icon",className:O("h-7 w-7",e),onClick:s=>{t==null||t(s),n()},...r,children:[l.jsx(YS,{}),l.jsx("span",{className:"sr-only",children:"Toggle Sidebar"})]})});gw.displayName="SidebarTrigger";const o3=m.forwardRef(({className:e,...t},r)=>{const{toggleSidebar:o}=Fs();return l.jsx("button",{ref:r,"data-sidebar":"rail","aria-label":"Toggle Sidebar",tabIndex:-1,onClick:o,title:"Toggle Sidebar",className:O("absolute inset-y-0 z-20 hidden w-4 -translate-x-1/2 transition-all ease-linear after:absolute after:inset-y-0 after:left-1/2 after:w-[2px] group-data-[side=left]:-right-4 group-data-[side=right]:left-0 hover:after:bg-sidebar-border sm:flex","[[data-side=left]_&]:cursor-w-resize [[data-side=right]_&]:cursor-e-resize","[[data-side=left][data-state=collapsed]_&]:cursor-e-resize [[data-side=right][data-state=collapsed]_&]:cursor-w-resize","group-data-[collapsible=offcanvas]:translate-x-0 group-data-[collapsible=offcanvas]:after:left-full group-data-[collapsible=offcanvas]:hover:bg-sidebar","[[data-side=left][data-collapsible=offcanvas]_&]:-right-2","[[data-side=right][data-collapsible=offcanvas]_&]:-left-2",e),...t})});o3.displayName="SidebarRail";const n3=m.forwardRef(({className:e,...t},r)=>l.jsx("main",{ref:r,className:O("relative flex min-h-svh flex-1 flex-col bg-background","peer-data-[variant=inset]:min-h-[calc(100svh-theme(spacing.4))] md:peer-data-[variant=inset]:m-2 md:peer-data-[state=collapsed]:peer-data-[variant=inset]:ml-2 md:peer-data-[variant=inset]:ml-0 md:peer-data-[variant=inset]:rounded-xl md:peer-data-[variant=inset]:shadow",e),...t}));n3.displayName="SidebarInset";const s3=m.forwardRef(({className:e,...t},r)=>l.jsx(Lu,{ref:r,"data-sidebar":"input",className:O("h-8 w-full bg-background shadow-none focus-visible:ring-2 focus-visible:ring-sidebar-ring",e),...t}));s3.displayName="SidebarInput";const i3=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,"data-sidebar":"header",className:O("flex flex-col gap-2 p-2",e),...t}));i3.displayName="SidebarHeader";const a3=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,"data-sidebar":"footer",className:O("flex flex-col gap-2 p-2",e),...t}));a3.displayName="SidebarFooter";const l3=m.forwardRef(({className:e,...t},r)=>l.jsx(Ry,{ref:r,"data-sidebar":"separator",className:O("mx-2 w-auto bg-sidebar-border",e),...t}));l3.displayName="SidebarSeparator";const mw=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,"data-sidebar":"content",className:O("flex min-h-0 flex-1 flex-col gap-2 overflow-auto group-data-[collapsible=icon]:overflow-hidden",e),...t}));mw.displayName="SidebarContent";const hw=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,"data-sidebar":"group",className:O("relative flex w-full min-w-0 flex-col p-2",e),...t}));hw.displayName="SidebarGroup";const fw=m.forwardRef(({className:e,asChild:t=!1,...r},o)=>{const n=t?Rn:"div";return l.jsx(n,{ref:o,"data-sidebar":"group-label",className:O("flex h-8 shrink-0 items-center rounded-md px-2 text-xs font-medium text-sidebar-foreground/70 outline-none ring-sidebar-ring transition-[margin,opa] duration-200 ease-linear focus-visible:ring-2 [&>svg]:size-4 [&>svg]:shrink-0","group-data-[collapsible=icon]:-mt-8 group-data-[collapsible=icon]:opacity-0",e),...r})});fw.displayName="SidebarGroupLabel";const c3=m.forwardRef(({className:e,asChild:t=!1,...r},o)=>{const n=t?Rn:"button";return l.jsx(n,{ref:o,"data-sidebar":"group-action",className:O("absolute right-3 top-3.5 flex aspect-square w-5 items-center justify-center rounded-md p-0 text-sidebar-foreground outline-none ring-sidebar-ring transition-transform hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 [&>svg]:size-4 [&>svg]:shrink-0","after:absolute after:-inset-2 after:md:hidden","group-data-[collapsible=icon]:hidden",e),...r})});c3.displayName="SidebarGroupAction";const vw=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,"data-sidebar":"group-content",className:O("w-full text-sm",e),...t}));vw.displayName="SidebarGroupContent";const yw=m.forwardRef(({className:e,...t},r)=>l.jsx("ul",{ref:r,"data-sidebar":"menu",className:O("flex w-full min-w-0 flex-col gap-1",e),...t}));yw.displayName="SidebarMenu";const ww=m.forwardRef(({className:e,...t},r)=>l.jsx("li",{ref:r,"data-sidebar":"menu-item",className:O("group/menu-item relative",e),...t}));ww.displayName="SidebarMenuItem";const d3=Wa("peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left text-sm outline-none ring-sidebar-ring transition-[width,height,padding] hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&>span:last-child]:truncate [&>svg]:size-4 [&>svg]:shrink-0",{variants:{variant:{default:"hover:bg-sidebar-accent hover:text-sidebar-accent-foreground",outline:"bg-background shadow-[0_0_0_1px_hsl(var(--sidebar-border))] hover:bg-sidebar-accent hover:text-sidebar-accent-foreground hover:shadow-[0_0_0_1px_hsl(var(--sidebar-accent))]"},size:{default:"h-8 text-sm",sm:"h-7 text-xs",lg:"h-12 text-sm group-data-[collapsible=icon]:!p-0"}},defaultVariants:{variant:"default",size:"default"}}),u3=m.forwardRef(({asChild:e=!1,isActive:t=!1,variant:r="default",size:o="default",tooltip:n,className:s,...i},a)=>{const d=e?Rn:"button",{isMobile:c,state:p}=Fs(),u=l.jsx(d,{ref:a,"data-sidebar":"menu-button","data-size":o,"data-active":t,className:O(d3({variant:r,size:o}),s),...i});return n?(typeof n=="string"&&(n={children:n}),l.jsxs(sx,{children:[l.jsx(ix,{asChild:!0,children:u}),l.jsx(dy,{side:"right",align:"center",hidden:p!=="collapsed"||c,...n})]})):u});u3.displayName="SidebarMenuButton";const p3=m.forwardRef(({className:e,asChild:t=!1,showOnHover:r=!1,...o},n)=>{const s=t?Rn:"button";return l.jsx(s,{ref:n,"data-sidebar":"menu-action",className:O("absolute right-1 top-1.5 flex aspect-square w-5 items-center justify-center rounded-md p-0 text-sidebar-foreground outline-none ring-sidebar-ring transition-transform peer-hover/menu-button:text-sidebar-accent-foreground hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 [&>svg]:size-4 [&>svg]:shrink-0","after:absolute after:-inset-2 after:md:hidden","peer-data-[size=sm]/menu-button:top-1","peer-data-[size=default]/menu-button:top-1.5","peer-data-[size=lg]/menu-button:top-2.5","group-data-[collapsible=icon]:hidden",r&&"group-focus-within/menu-item:opacity-100 group-hover/menu-item:opacity-100 data-[state=open]:opacity-100 peer-data-[active=true]/menu-button:text-sidebar-accent-foreground md:opacity-0",e),...o})});p3.displayName="SidebarMenuAction";const g3=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,"data-sidebar":"menu-badge",className:O("pointer-events-none absolute right-1 flex h-5 min-w-5 select-none items-center justify-center rounded-md px-1 text-xs font-medium tabular-nums text-sidebar-foreground","peer-hover/menu-button:text-sidebar-accent-foreground peer-data-[active=true]/menu-button:text-sidebar-accent-foreground","peer-data-[size=sm]/menu-button:top-1","peer-data-[size=default]/menu-button:top-1.5","peer-data-[size=lg]/menu-button:top-2.5","group-data-[collapsible=icon]:hidden",e),...t}));g3.displayName="SidebarMenuBadge";const m3=m.forwardRef(({className:e,showIcon:t=!1,...r},o)=>{const n=m.useMemo(()=>`${Math.floor(Math.random()*40)+50}%`,[]);return l.jsxs("div",{ref:o,"data-sidebar":"menu-skeleton",className:O("flex h-8 items-center gap-2 rounded-md px-2",e),...r,children:[t&&l.jsx(lm,{className:"size-4 rounded-md","data-sidebar":"menu-skeleton-icon"}),l.jsx(lm,{className:"h-4 max-w-[--skeleton-width] flex-1","data-sidebar":"menu-skeleton-text",style:{"--skeleton-width":n}})]})});m3.displayName="SidebarMenuSkeleton";const h3=m.forwardRef(({className:e,...t},r)=>l.jsx("ul",{ref:r,"data-sidebar":"menu-sub",className:O("mx-3.5 flex min-w-0 translate-x-px flex-col gap-1 border-l border-sidebar-border px-2.5 py-0.5","group-data-[collapsible=icon]:hidden",e),...t}));h3.displayName="SidebarMenuSub";const f3=m.forwardRef(({...e},t)=>l.jsx("li",{ref:t,...e}));f3.displayName="SidebarMenuSubItem";const v3=m.forwardRef(({asChild:e=!1,size:t="md",isActive:r,className:o,...n},s)=>{const i=e?Rn:"a";return l.jsx(i,{ref:s,"data-sidebar":"menu-sub-button","data-size":t,"data-active":r,className:O("flex h-7 min-w-0 -translate-x-px items-center gap-2 overflow-hidden rounded-md px-2 text-sidebar-foreground outline-none ring-sidebar-ring aria-disabled:pointer-events-none aria-disabled:opacity-50 hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 [&>span:last-child]:truncate [&>svg]:size-4 [&>svg]:shrink-0 [&>svg]:text-sidebar-accent-foreground","data-[active=true]:bg-sidebar-accent data-[active=true]:text-sidebar-accent-foreground",t==="sm"&&"text-xs",t==="md"&&"text-sm","group-data-[collapsible=icon]:hidden",o),...n})});v3.displayName="SidebarMenuSubButton";const y3=[{title:"Home",url:"/",icon:FS},{title:"Terraform",url:"/terraform",icon:Us},{title:"Azure Core",url:"/azure",icon:ja},{title:"Azure DevOps",url:"/azure-devops",icon:Ga},{title:"Docker",url:"/docker",icon:Ua},{title:"Kubernetes",url:"/kubernetes",icon:xs},{title:"Git",url:"/git",icon:wu},{title:"Azure Monitor",url:"/azure-monitor",icon:vu},{title:"Linux",url:"/linux",icon:da},{title:"Behavioral/HR",url:"/behavioral",icon:Av},{title:"Miscellaneous",url:"/miscellaneous",icon:kv},{title:"FinOps",url:"/finops",icon:yu}];function w3(){const{open:e}=Fs();return l.jsx(pw,{collapsible:"icon",children:l.jsx(mw,{children:l.jsxs(hw,{children:[l.jsxs(fw,{className:"text-sm font-semibold",children:[e&&l.jsx(OS,{className:"mr-2 h-4 w-4 inline"}),"Documentation"]}),l.jsx(vw,{children:l.jsx(yw,{children:y3.map(t=>l.jsx(ww,{children:l.jsxs(z1,{to:t.url,end:!0,className:({isActive:r})=>`flex items-center gap-3 rounded-lg px-3 py-2 transition-smooth ${r?"bg-primary text-primary-foreground font-medium":"hover:bg-accent"}`,children:[l.jsx(t.icon,{className:"h-4 w-4"}),e&&l.jsx("span",{children:t.title})]})},t.title))})})]})})})}function b3(){const{theme:e,setTheme:t}=q1();return l.jsxs(Pe,{variant:"ghost",size:"icon",onClick:()=>t(e==="light"?"dark":"light"),className:"transition-smooth hover:bg-accent",children:[l.jsx(XS,{className:"h-5 w-5 rotate-0 scale-100 transition-all dark:-rotate-90 dark:scale-0"}),l.jsx($S,{className:"absolute h-5 w-5 rotate-90 scale-0 transition-all dark:rotate-0 dark:scale-100"}),l.jsx("span",{className:"sr-only",children:"Toggle theme"})]})}const k3=sw,A3=G0,S3=iw,bw=m.forwardRef(({className:e,...t},r)=>l.jsx(sl,{ref:r,className:O("fixed inset-0 z-50 bg-black/80 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",e),...t}));bw.displayName=sl.displayName;const kw=m.forwardRef(({className:e,children:t,...r},o)=>l.jsxs(S3,{children:[l.jsx(bw,{}),l.jsxs(il,{ref:o,className:O("fixed left-[50%] top-[50%] z-50 grid w-full max-w-lg translate-x-[-50%] translate-y-[-50%] gap-4 border bg-background p-6 shadow-lg duration-200 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%] data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%] sm:rounded-lg",e),...r,children:[t,l.jsxs(aw,{className:"absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity data-[state=open]:bg-accent data-[state=open]:text-muted-foreground hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none",children:[l.jsx(bu,{className:"h-4 w-4"}),l.jsx("span",{className:"sr-only",children:"Close"})]})]})]}));kw.displayName=il.displayName;const Aw=({className:e,...t})=>l.jsx("div",{className:O("flex flex-col space-y-1.5 text-center sm:text-left",e),...t});Aw.displayName="DialogHeader";const Sw=m.forwardRef(({className:e,...t},r)=>l.jsx(al,{ref:r,className:O("text-lg font-semibold leading-none tracking-tight",e),...t}));Sw.displayName=al.displayName;const I3=m.forwardRef(({className:e,...t},r)=>l.jsx(ll,{ref:r,className:O("text-sm text-muted-foreground",e),...t}));I3.displayName=ll.displayName;function P3(){const[e,t]=m.useState(!1),[r,o]=m.useState("");return l.jsxs(k3,{open:e,onOpenChange:t,children:[l.jsx(A3,{asChild:!0,children:l.jsxs(Pe,{variant:"outline",className:"w-full md:w-64 justify-start gap-2",children:[l.jsx(JS,{className:"h-4 w-4"}),l.jsx("span",{className:"text-muted-foreground",children:"Search documentation..."})]})}),l.jsxs(kw,{className:"max-w-2xl",children:[l.jsx(Aw,{children:l.jsx(Sw,{children:"Search Documentation"})}),l.jsxs("div",{className:"mt-4",children:[l.jsx(Lu,{placeholder:"Type to search...",value:r,onChange:n=>o(n.target.value),className:"w-full"}),l.jsx("div",{className:"mt-4 text-sm text-muted-foreground",children:r?l.jsxs("p",{children:['Searching for "',r,'"...']}):l.jsx("p",{children:"Start typing to search across all topics"})})]})]})]})}function C3({children:e}){return l.jsx(uw,{children:l.jsxs("div",{className:"min-h-screen flex w-full",children:[l.jsx(w3,{}),l.jsxs("div",{className:"flex-1 flex flex-col",children:[l.jsx("header",{className:"sticky top-0 z-50 w-full border-b bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60",children:l.jsxs("div",{className:"container flex h-16 items-center gap-4 px-4",children:[l.jsx(gw,{className:"hover:bg-accent transition-smooth"}),l.jsxs("div",{className:"flex-1 flex items-center justify-between gap-4",children:[l.jsx("div",{className:"hidden md:block",children:l.jsx("h1",{className:"text-lg font-bold bg-gradient-to-r from-primary via-accent to-secondary bg-clip-text text-transparent",children:"DevOps Interview Guide"})}),l.jsxs("div",{className:"flex items-center gap-2",children:[l.jsx(P3,{}),l.jsx(b3,{}),l.jsx(Pe,{variant:"ghost",size:"icon",asChild:!0,className:"hover:bg-accent transition-smooth",children:l.jsx("a",{href:"https://github.com/Riteshatri/",target:"_blank",rel:"noopener noreferrer","aria-label":"GitHub",children:l.jsx(WS,{className:"h-5 w-5"})})})]})]})]})}),l.jsx("main",{className:"flex-1",children:e}),l.jsx("footer",{className:"border-t bg-background py-6",children:l.jsx("div",{className:"container px-4",children:l.jsxs("div",{className:"flex flex-col md:flex-row items-center justify-between gap-4",children:[l.jsx("div",{className:"text-sm text-muted-foreground",children:" 2025 DevOps Interview Guide. All rights reserved."}),l.jsxs("div",{className:"flex gap-4",children:[l.jsx("a",{href:"https://github.com/Riteshatri/",className:"text-sm text-muted-foreground hover:text-primary transition-smooth",target:"_blank",rel:"noopener noreferrer",children:"GitHub"}),l.jsx("a",{href:"https://www.linkedin.com/in/riteshatri/",className:"text-sm text-muted-foreground hover:text-primary transition-smooth",target:"_blank",rel:"noopener noreferrer",children:"LinkedIn"})]})]})})})]})]})})}var cl="Collapsible",[x3,Iw]=xo(cl),[D3,Bu]=x3(cl),Pw=m.forwardRef((e,t)=>{const{__scopeCollapsible:r,open:o,defaultOpen:n,disabled:s,onOpenChange:i,...a}=e,[d,c]=Hn({prop:o,defaultProp:n??!1,onChange:i,caller:cl});return l.jsx(D3,{scope:r,disabled:s,contentId:an(),open:d,onOpenToggle:m.useCallback(()=>c(p=>!p),[c]),children:l.jsx(ie.div,{"data-state":ju(d),"data-disabled":s?"":void 0,...a,ref:t})})});Pw.displayName=cl;var Cw="CollapsibleTrigger",xw=m.forwardRef((e,t)=>{const{__scopeCollapsible:r,...o}=e,n=Bu(Cw,r);return l.jsx(ie.button,{type:"button","aria-controls":n.contentId,"aria-expanded":n.open||!1,"data-state":ju(n.open),"data-disabled":n.disabled?"":void 0,disabled:n.disabled,...o,ref:t,onClick:ne(e.onClick,n.onOpenToggle)})});xw.displayName=Cw;var Wu="CollapsibleContent",Dw=m.forwardRef((e,t)=>{const{forceMount:r,...o}=e,n=Bu(Wu,e.__scopeCollapsible);return l.jsx(Do,{present:r||n.open,children:({present:s})=>l.jsx(T3,{...o,ref:t,present:s})})});Dw.displayName=Wu;var T3=m.forwardRef((e,t)=>{const{__scopeCollapsible:r,present:o,children:n,...s}=e,i=Bu(Wu,r),[a,d]=m.useState(o),c=m.useRef(null),p=Te(t,c),u=m.useRef(0),f=u.current,g=m.useRef(0),k=g.current,y=i.open||a,b=m.useRef(y),v=m.useRef(void 0);return m.useEffect(()=>{const h=requestAnimationFrame(()=>b.current=!1);return()=>cancelAnimationFrame(h)},[]),Yt(()=>{const h=c.current;if(h){v.current=v.current||{transitionDuration:h.style.transitionDuration,animationName:h.style.animationName},h.style.transitionDuration="0s",h.style.animationName="none";const w=h.getBoundingClientRect();u.current=w.height,g.current=w.width,b.current||(h.style.transitionDuration=v.current.transitionDuration,h.style.animationName=v.current.animationName),d(o)}},[i.open,o]),l.jsx(ie.div,{"data-state":ju(i.open),"data-disabled":i.disabled?"":void 0,id:i.contentId,hidden:!y,...s,ref:p,style:{"--radix-collapsible-content-height":f?`${f}px`:void 0,"--radix-collapsible-content-width":k?`${k}px`:void 0,...e.style},children:y&&n})});function ju(e){return e?"open":"closed"}var z3=Pw,R3=xw,H3=Dw,M3=m.createContext(void 0);function E3(e){const t=m.useContext(M3);return e||t||"ltr"}var Et="Accordion",N3=["Home","End","ArrowDown","ArrowUp","ArrowLeft","ArrowRight"],[Uu,q3,L3]=$f(Et),[dl,vD]=xo(Et,[L3,Iw]),Gu=Iw(),Tw=z.forwardRef((e,t)=>{const{type:r,...o}=e,n=o,s=o;return l.jsx(Uu.Provider,{scope:e.__scopeAccordion,children:r==="multiple"?l.jsx(B3,{...s,ref:t}):l.jsx(V3,{...n,ref:t})})});Tw.displayName=Et;var[zw,O3]=dl(Et),[Rw,_3]=dl(Et,{collapsible:!1}),V3=z.forwardRef((e,t)=>{const{value:r,defaultValue:o,onValueChange:n=()=>{},collapsible:s=!1,...i}=e,[a,d]=Hn({prop:r,defaultProp:o??"",onChange:n,caller:Et});return l.jsx(zw,{scope:e.__scopeAccordion,value:z.useMemo(()=>a?[a]:[],[a]),onItemOpen:d,onItemClose:z.useCallback(()=>s&&d(""),[s,d]),children:l.jsx(Rw,{scope:e.__scopeAccordion,collapsible:s,children:l.jsx(Hw,{...i,ref:t})})})}),B3=z.forwardRef((e,t)=>{const{value:r,defaultValue:o,onValueChange:n=()=>{},...s}=e,[i,a]=Hn({prop:r,defaultProp:o??[],onChange:n,caller:Et}),d=z.useCallback(p=>a((u=[])=>[...u,p]),[a]),c=z.useCallback(p=>a((u=[])=>u.filter(f=>f!==p)),[a]);return l.jsx(zw,{scope:e.__scopeAccordion,value:i,onItemOpen:d,onItemClose:c,children:l.jsx(Rw,{scope:e.__scopeAccordion,collapsible:!0,children:l.jsx(Hw,{...s,ref:t})})})}),[W3,ul]=dl(Et),Hw=z.forwardRef((e,t)=>{const{__scopeAccordion:r,disabled:o,dir:n,orientation:s="vertical",...i}=e,a=z.useRef(null),d=Te(a,t),c=q3(r),u=E3(n)==="ltr",f=ne(e.onKeyDown,g=>{var D;if(!N3.includes(g.key))return;const k=g.target,y=c().filter(M=>{var H;return!((H=M.ref.current)!=null&&H.disabled)}),b=y.findIndex(M=>M.ref.current===k),v=y.length;if(b===-1)return;g.preventDefault();let h=b;const w=0,A=v-1,S=()=>{h=b+1,h>A&&(h=w)},I=()=>{h=b-1,h<w&&(h=A)};switch(g.key){case"Home":h=w;break;case"End":h=A;break;case"ArrowRight":s==="horizontal"&&(u?S():I());break;case"ArrowDown":s==="vertical"&&S();break;case"ArrowLeft":s==="horizontal"&&(u?I():S());break;case"ArrowUp":s==="vertical"&&I();break}const P=h%v;(D=y[P].ref.current)==null||D.focus()});return l.jsx(W3,{scope:r,disabled:o,direction:n,orientation:s,children:l.jsx(Uu.Slot,{scope:r,children:l.jsx(ie.div,{...i,"data-orientation":s,ref:d,onKeyDown:o?void 0:f})})})}),Aa="AccordionItem",[j3,Fu]=dl(Aa),Mw=z.forwardRef((e,t)=>{const{__scopeAccordion:r,value:o,...n}=e,s=ul(Aa,r),i=O3(Aa,r),a=Gu(r),d=an(),c=o&&i.value.includes(o)||!1,p=s.disabled||e.disabled;return l.jsx(j3,{scope:r,open:c,disabled:p,triggerId:d,children:l.jsx(z3,{"data-orientation":s.orientation,"data-state":_w(c),...a,...n,ref:t,disabled:p,open:c,onOpenChange:u=>{u?i.onItemOpen(o):i.onItemClose(o)}})})});Mw.displayName=Aa;var Ew="AccordionHeader",Nw=z.forwardRef((e,t)=>{const{__scopeAccordion:r,...o}=e,n=ul(Et,r),s=Fu(Ew,r);return l.jsx(ie.h3,{"data-orientation":n.orientation,"data-state":_w(s.open),"data-disabled":s.disabled?"":void 0,...o,ref:t})});Nw.displayName=Ew;var vd="AccordionTrigger",qw=z.forwardRef((e,t)=>{const{__scopeAccordion:r,...o}=e,n=ul(Et,r),s=Fu(vd,r),i=_3(vd,r),a=Gu(r);return l.jsx(Uu.ItemSlot,{scope:r,children:l.jsx(R3,{"aria-disabled":s.open&&!i.collapsible||void 0,"data-orientation":n.orientation,id:s.triggerId,...a,...o,ref:t})})});qw.displayName=vd;var Lw="AccordionContent",Ow=z.forwardRef((e,t)=>{const{__scopeAccordion:r,...o}=e,n=ul(Et,r),s=Fu(Lw,r),i=Gu(r);return l.jsx(H3,{role:"region","aria-labelledby":s.triggerId,"data-orientation":n.orientation,...i,...o,ref:t,style:{"--radix-accordion-content-height":"var(--radix-collapsible-content-height)","--radix-accordion-content-width":"var(--radix-collapsible-content-width)",...e.style}})});Ow.displayName=Lw;function _w(e){return e?"open":"closed"}var U3=Tw,G3=Mw,F3=Nw,Vw=qw,Bw=Ow;const Nt=U3,mt=m.forwardRef(({className:e,...t},r)=>l.jsx(G3,{ref:r,className:O("border-b",e),...t}));mt.displayName="AccordionItem";const ht=m.forwardRef(({className:e,children:t,...r},o)=>l.jsx(F3,{className:"flex",children:l.jsxs(Vw,{ref:o,className:O("flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&[data-state=open]>svg]:rotate-180",e),...r,children:[t,l.jsx(gt,{className:"h-4 w-4 shrink-0 transition-transform duration-200"})]})}));ht.displayName=Vw.displayName;const ft=m.forwardRef(({className:e,children:t,...r},o)=>l.jsx(Bw,{ref:o,className:"overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down",...r,children:l.jsx("div",{className:O("pb-4 pt-0",e),children:t})}));ft.displayName=Bw.displayName;const cm=[{title:"1. Azure Fundamentals  Resource Groups, ARM & Governance",questions:[{question:"What is a Resource Group in Azure and why do we use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Resource Group (RG)</strong></h3>
        <p>
          A <strong>Resource Group</strong> is a logical container that holds related Azure resources (VMs, networks, storage, etc.) for unified management.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Simplifies management  deploy, monitor, and delete as one unit.</li>
          <li> Enables consistent <strong>RBAC, policies, and tagging</strong> across all resources inside.</li>
          <li> Supports lifecycle grouping  e.g., each environment (DEV/UAT/PROD) in its own RG.</li>
        </ul>
        <p><strong>In Practice:</strong> I group resources per environment or per application to apply role-based permissions and cost tracking easily.</p>
      </div>`},{question:"What is Azure Resource Manager (ARM)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Resource Manager (ARM)</strong></h3>
        <p>
          ARM is the <strong>deployment and management layer</strong> in Azure that provides a consistent way to create, update, and manage resources using templates or API calls.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Uses <strong>ARM templates (JSON)</strong> for declarative IaC.</li>
          <li> Supports <strong>RBAC, locks, and tags</strong> through the same control plane.</li>
          <li> Enables <strong>idempotent deployments</strong>  the same template ensures consistent infra every time.</li>
        </ul>
        <p><strong>In Practice:</strong> Terraform or Bicep both use the ARM API behind the scenes to provision Azure resources consistently.</p>
      </div>`},{question:"How do you control access to Azure resources (RBAC)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Role-Based Access Control (RBAC)</strong></h3>
        <p>
          RBAC manages who can access what at which scope (subscription, RG, or resource level).
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Assign roles like <code>Owner</code>, <code>Contributor</code>, <code>Reader</code> or <code>Custom</code>.</li>
          <li> Scope hierarchy: <strong>Management Group  Subscription  Resource Group  Resource</strong>.</li>
          <li> Supports <strong>Azure AD groups</strong> for enterprise-wide control.</li>
        </ul>
        <p><strong>Best Practice:</strong> Apply least-privilege at RG level; use Azure AD PIM for temporary elevated access.</p>
      </div>`},{question:"What are Azure Blueprints and why are they useful?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Blueprints</strong></h3>
        <p>
          Blueprints define a repeatable set of <strong>governance and configuration artifacts</strong>  like policies, RBAC roles, ARM templates, and resource groups.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Used to deploy <strong>standardized environments</strong> across subscriptions.</li>
          <li> Ensures compliance by automatically applying policies and access controls.</li>
          <li> Great for creating Landing Zones aligned with enterprise standards.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Blueprints to deploy baseline configurations (naming, tagging, policies) for new environments automatically.</p>
      </div>`},{question:"What is Azure Policy and how do you enforce it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Policy</strong></h3>
        <p>
          Azure Policy enforces organizational standards and compliance at scale.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Defines allowed or denied configurations  e.g., Only East US region allowed.</li>
          <li> Can <strong>audit, deny, or remediate</strong> non-compliant resources automatically.</li>
          <li> Evaluates compliance continuously via <strong>Policy Insights</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I assign policies at subscription or RG level  e.g., enforce tags, restrict VM SKUs, or require diagnostic logs.</p>
      </div>`},{question:"What tools are used for Azure governance and policy management?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Governance Tools</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Management Groups</strong>  Organize subscriptions for hierarchical policy/RBAC control.</li>
          <li> <strong>Azure Policy</strong>  Enforce compliance at resource or subscription level.</li>
          <li> <strong>Blueprints</strong>  Combine policies, roles, and templates for environment deployment.</li>
          <li> <strong>Cost Management + Budgets</strong>  Track and control cloud spend.</li>
          <li> <strong>Tags</strong>  Standardize resource identification for chargeback/showback.</li>
        </ul>
        <p><strong>In Practice:</strong> Combine Policy + Management Groups to ensure consistent compliance across 100+ subscriptions.</p>
      </div>`},{question:"What are the Azure Well-Architected Framework pillars?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Well-Architected Framework (WAF) Pillars</strong></h3>
        <p>
          The framework helps design reliable, secure, and cost-effective cloud solutions.  
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Security</strong>  Protect data, identity, and network.</li>
          <li> <strong>Operational Excellence</strong>  Automate, monitor, and improve processes.</li>
          <li> <strong>Reliability</strong>  Ensure resilience and disaster recovery.</li>
          <li> <strong>Cost Optimization</strong>  Maximize efficiency, eliminate waste.</li>
          <li> <strong>Performance Efficiency</strong>  Optimize scale and resource use.</li>
        </ul>
        <p><strong>In Practice:</strong> I use these pillars to review Azure workloads quarterly for performance, cost, and resilience improvements.</p>
      </div>`},{question:"What are the key design principles for an Azure Landing Zone?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Landing Zone Design Principles</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Use <strong>management groups</strong> for scalable governance.</li>
          <li> Implement <strong>RBAC, policies, and tagging</strong> at every level.</li>
          <li> Define <strong>Hub-and-Spoke</strong> networking for shared services (firewall, DNS, VPN).</li>
          <li> Enable <strong>centralized logging & monitoring</strong> via Log Analytics.</li>
          <li> Plan <strong>subscription separation</strong> by environment or workload.</li>
        </ul>
        <p><strong>In Practice:</strong> A good Landing Zone enforces security, scalability, and governance before workloads are deployed.</p>
      </div>`},{question:"Setting up an Azure Landing Zone with Hub-and-Spoke Architecture  explain the best-practice design.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Hub-and-Spoke Landing Zone  Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Hub:</strong> Shared services  Azure Firewall, Bastion, DNS, ExpressRoute, and monitoring agents.</li>
          <li><strong>Spokes:</strong> Individual workloads (Dev, QA, Prod) isolated via peering to the hub.</li>
          <li> Use <strong>NSGs and Azure Firewall</strong> for layered security.</li>
          <li> Centralize identity with <strong>Azure AD + RBAC</strong>.</li>
          <li> Use <strong>Azure Policy</strong> and <strong>Blueprints</strong> to enforce configuration standards.</li>
          <li> Connect all spokes to a <strong>central Log Analytics Workspace</strong> for unified monitoring.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy Landing Zones using Terraform with separate state files for each spoke, ensuring consistent hub integration and controlled access boundaries.</p>
      </div>`}]},{title:"2. Virtual Networks  VNet, Subnetting, Peering & Name Resolution",questions:[{question:"What is a Virtual Network (VNet) in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Virtual Network (VNet)</strong></h3>
        <p>
          A <strong>VNet</strong> is a logically isolated private network in Azure that connects resources securely  similar to a LAN in on-prem environments.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Enables secure communication between Azure resources like VMs, AKS, and App Services.</li>
          <li> Supports inbound/outbound traffic control through NSGs, UDRs, and Firewalls.</li>
          <li> Allows <strong>hybrid connectivity</strong> with on-prem via VPN or ExpressRoute.</li>
        </ul>
        <p><strong>In Practice:</strong> I use VNets to segment workloads  for example, one per environment (Dev/UAT/Prod) with controlled peering between them.</p>
      </div>`},{question:"What is a subnet and how is it used inside a VNet?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Subnet</strong></h3>
        <p>
          A <strong>subnet</strong> is a logical partition of a VNets IP range. It helps organize and secure workloads by function or role.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Subnets allow resource isolation  e.g., frontend, backend, and database subnets.</li>
          <li> NSGs and Route Tables are assigned at subnet level for traffic control.</li>
          <li> Subnets also reserve IPs for Azure services like Private Endpoints or Gateways.</li>
        </ul>
        <p><strong>In Practice:</strong> I keep separate subnets for AKS nodes, jumpboxes, and databases  improves security and network performance.</p>
      </div>`},{question:"Given CIDR 192.168.1.0/24, how would you create two subnets (address planning)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Subnetting Example: 192.168.1.0/24</strong></h3>
        <p>
          CIDR <code>/24</code> gives <strong>256 IPs</strong> (0255). To split into 2 equal subnets:
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>Subnet A: 192.168.1.0/25  128 IPs (0127)
Subnet B: 192.168.1.128/25  128 IPs (128255)</code></pre>
        <ul style="margin-left:1.2rem;">
          <li> Azure reserves 5 IPs per subnet (network, broadcast, and internal use).</li>
          <li> Plan address space to leave room for growth  avoid over-allocating small CIDRs.</li>
        </ul>
        <p><strong>In Practice:</strong> I use /25 for frontend and /25 for backend  keeping room for scaling later without overlap.</p>
      </div>`},{question:"What is VNet peering and how is it configured?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VNet Peering</strong></h3>
        <p>
          VNet Peering connects two VNets for private communication using the Azure backbone  no public internet involved.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Steps to configure:
            <ul>
              <li>1 Go to VNet  Peerings  Add Peering.</li>
              <li>2 Select remote VNet and configure traffic settings.</li>
              <li>3 Enable <strong>Allow forwarded traffic</strong> and <strong>Use remote gateway</strong> if needed for hybrid routing.</li>
            </ul>
          </li>
          <li> Traffic between peered VNets stays within Microsofts private network.</li>
        </ul>
        <p><strong>In Practice:</strong> I peer Dev  Shared Services VNet for monitoring and identity access, but block Prod  Dev for isolation.</p>
      </div>`},{question:"What are network peering limitations?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VNet Peering Limitations</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Overlapping IP ranges are <strong>not supported</strong>.</li>
          <li> Peering is <strong>non-transitive</strong>  VNet A  B and B  C doesnt mean A  C.</li>
          <li> Cannot use Azure Firewall across unpeered VNets without routing setup.</li>
          <li> Bandwidth is limited by VM NIC capacity, not by peering link.</li>
        </ul>
        <p><strong>Best Practice:</strong> Use Hub-and-Spoke model with a shared hub for central routing and inspection.</p>
      </div>`},{question:"What is Azure DNS? How do you configure DNS zones (Public vs Private)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure DNS Overview</strong></h3>
        <p>
          Azure DNS is a cloud-based DNS hosting service that provides name resolution for your domains.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Public DNS Zone:</strong> Resolves names over the Internet (e.g., <code>myapp.com</code>).</li>
          <li><strong>Private DNS Zone:</strong> Resolves names only inside your VNets (e.g., <code>db.internal.cloud</code>).</li>
          <li> You can link private DNS zones to one or multiple VNets for internal name resolution.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure private zones for service discovery (AKS, databases) and public zones for app domains.</p>
      </div>`},{question:"I have a Public DNS Zone and a Private DNS Zone  whats the difference?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Public vs Private DNS Zones</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th style="text-align:left;">Aspect</th><th>Public DNS Zone</th><th>Private DNS Zone</th></tr>
          <tr><td>Scope</td><td>Accessible over Internet</td><td>Accessible only inside linked VNets</td></tr>
          <tr><td>Use Case</td><td>Websites, APIs, external endpoints</td><td>Internal services (AKS, DB, VMs)</td></tr>
          <tr><td>Security</td><td>Public exposure (needs HTTPS/WAF)</td><td>Internal and isolated</td></tr>
        </table>
        <p><strong>In Practice:</strong> Public zone: <code>app.company.com</code>  WAF IP; Private zone: <code>db.internal.company.local</code>  internal IP.</p>
      </div>`},{question:"User sees DNS resolution errors and slow response for an app hosted on AKS  how would you troubleshoot and fix?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DNS Resolution & Latency Troubleshooting (AKS)</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Check Pod DNS config:
            <ul>
              <li>Run <code>kubectl exec &lt;pod&gt; -- cat /etc/resolv.conf</code> to verify DNS server (usually kube-dns or CoreDNS).</li>
              <li>Ensure CoreDNS pods are running and healthy in <code>kube-system</code> namespace.</li>
            </ul>
          </li>
          <li> Validate name resolution:
            <ul>
              <li><code>nslookup myservice.default.svc.cluster.local</code> inside pod.</li>
              <li>Check for CoreDNS ConfigMap misconfiguration or forwarding loops.</li>
            </ul>
          </li>
          <li> Optimize:
            <ul>
              <li>Increase CoreDNS replica count and enable caching.</li>
              <li>If using Private DNS Zone  ensure VNet link exists and firewall not blocking UDP/53.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> Most DNS slowness in AKS comes from CoreDNS overload or missing private DNS link  fix by scaling CoreDNS and validating zone linkage.</p>
      </div>`},{question:"How do you troubleshoot connectivity between VNets?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VNet Connectivity Troubleshooting</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 <strong>Check IP Addressing:</strong> Ensure no overlapping CIDRs.</li>
          <li>2 <strong>Verify Peering:</strong> Confirm Allow traffic both ways and Use remote gateway enabled if needed.</li>
          <li>3 <strong>NSGs:</strong> Check inbound/outbound rules allow required ports between subnets/VNets.</li>
          <li>4 <strong>Route Tables (UDRs):</strong> Verify routes arent forcing traffic away (e.g., via VPN gateway incorrectly).</li>
          <li>5 <strong>Firewall/NSG Flow Logs:</strong> Review logs for drops (use Network Watcher  Connection Troubleshoot).</li>
        </ul>
        <p><strong>In Practice:</strong> I use <code>Network Watcher Connection Troubleshoot</code> between private IPs  it pinpoints if packet drops occur at NSG, route, or peering layer.</p>
      </div>`}]},{title:"3. Secure Connectivity  VPN, ExpressRoute, Site-to-Site",questions:[{question:"What information is required to establish a site-to-site VPN connection?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Site-to-Site VPN  Prerequisites</strong></h3>
        <p>
          A Site-to-Site (S2S) VPN securely connects your on-prem network with Azure VNet via IPsec tunnel over the Internet.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>On-prem details:</strong> Public IP of the on-prem VPN device (no NAT, static IP).</li>
          <li> <strong>Address space:</strong> On-prem subnet range (e.g., 10.0.0.0/16).</li>
          <li> <strong>Azure details:</strong> Azure VNet address space (e.g., 10.1.0.0/16).</li>
          <li> <strong>Shared key (Pre-Shared Key  PSK)</strong> for IPsec authentication.</li>
          <li> <strong>IKE/IPsec parameters:</strong> Encryption algorithms, lifetime, and protocol (IKEv2).</li>
          <li> <strong>Azure VPN Gateway:</strong> Created in a dedicated Gateway Subnet (<code>10.1.255.0/27</code> typical).</li>
        </ul>
        <p><strong>In Practice:</strong> I verify both sides IPsec parameters match  mismatched PSK or lifetime often causes tunnel failures.</p>
      </div>`},{question:"What is an ExpressRoute circuit and how is it different from VPN Gateway?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ExpressRoute vs VPN Gateway</strong></h3>
        <p>
          <strong>ExpressRoute</strong> provides a private, dedicated connection between on-prem datacenter and Azure via a service provider  not over the Internet.
        </p>
        <table style="width:100%;border-collapse:collapse;margin-top:8px;">
          <tr><th style="text-align:left;">Feature</th><th>ExpressRoute</th><th>VPN Gateway</th></tr>
          <tr><td>Connectivity</td><td>Private (through MPLS)</td><td>Public Internet (IPsec)</td></tr>
          <tr><td>Performance</td><td>High bandwidth (up to 100 Gbps)</td><td>Moderate (up to 1.25 Gbps)</td></tr>
          <tr><td>Latency</td><td>Low and consistent</td><td>Variable</td></tr>
          <tr><td>Security</td><td>Private, not encrypted</td><td>Encrypted via IPsec</td></tr>
          <tr><td>Use Case</td><td>Enterprise hybrid setup</td><td>SMB or temporary connectivity</td></tr>
        </table>
        <p><strong>In Practice:</strong> I recommend ExpressRoute for production-grade hybrid setups where latency and SLA matter  VPN for smaller, flexible connections.</p>
      </div>`},{question:"How do you connect on-prem resources to Azure via VPN Gateway?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Connecting On-Prem to Azure via VPN Gateway</strong></h3>
        <p>
          VPN Gateway establishes an IPsec tunnel between Azure VNet and on-prem router/firewall.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 Create a <strong>VNet</strong> and a dedicated <code>GatewaySubnet</code> (e.g., 10.1.255.0/27).</li>
          <li>2 Deploy an <strong>Azure VPN Gateway</strong> in that subnet.</li>
          <li>3 Create a <strong>Local Network Gateway (LNG)</strong> with your on-prem public IP and local address range.</li>
          <li>4 Create a <strong>Connection</strong> between the VPN Gateway and LNG using a shared PSK.</li>
          <li>5 Configure matching tunnel parameters on the on-prem VPN device (Cisco, FortiGate, etc.).</li>
        </ul>
        <p><strong>Verification:</strong> Use <code>Get-AzVirtualNetworkGatewayConnection</code> or Azure Portal  Connection status: Connected.</p>
        <p><strong>In Practice:</strong> I always test with <code>Test-AzNetworkWatcherConnectivity</code> to verify reachability before production rollout.</p>
      </div>`},{question:"Define Hub & Spoke Architecture  brief the design and when to use it.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Hub & Spoke Network Architecture</strong></h3>
        <p>
          A <strong>Hub-and-Spoke</strong> model centralizes shared services in one VNet (Hub) and connects workload VNets (Spokes) to it via peering.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Hub:</strong> Contains shared resources like Firewall, VPN Gateway, DNS, Bastion, Log Analytics.</li>
          <li> <strong>Spokes:</strong> Separate VNets per workload or environment (Dev, QA, Prod).</li>
          <li> Communication:
            <ul>
              <li>Spokes  Hub  On-prem via VPN/ExpressRoute.</li>
              <li>Inter-spoke traffic flows via hub (transit peering).</li>
            </ul>
          </li>
          <li> Enhances security by isolating workloads and enforcing centralized traffic inspection.</li>
        </ul>
        <p><strong>When to Use:</strong> Multi-environment or multi-team setups needing centralized connectivity and governance.</p>
        <p><strong>In Practice:</strong> I use Hub-Spoke in enterprise projects  Hub has the VPN/Firewall, each Spoke connects via peering for clean separation.</p>
      </div>`},{question:"In hubspoke peering, which options do you select on the hub to peer with the remote/spoke VNet?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Peering Configuration  Hub to Spoke</strong></h3>
        <p>
          When setting up peering from Hub  Spoke, choose options carefully to allow routing and inspection:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Allow Virtual Network Access</strong>  Enables traffic flow between VNets.</li>
          <li> <strong>Allow Forwarded Traffic</strong>  Needed if Hub firewall routes traffic between spokes.</li>
          <li> <strong>Allow Gateway Transit</strong>  So Hubs VPN Gateway can be used by spokes.</li>
          <li> On spoke side  select <strong>Use Remote Gateway</strong> instead of creating its own.</li>
        </ul>
        <p><strong>In Practice:</strong> I always enable Allow Forwarded Traffic + Allow Gateway Transit on Hub; Use Remote Gateway on Spokes  ensures central routing works.</p>
      </div>`}]},{title:"4. Load Balancing & Traffic  LB, App Gateway, WAF, Front Door, Traffic Manager, CDN",questions:[{question:"What are the types of Load Balancers in Azure (internal/external)? Use cases?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Load Balancer Types</strong></h3>
        <p>
          Azure Load Balancer operates at <strong>Layer 4 (TCP/UDP)</strong> and comes in two types:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Public Load Balancer (External):</strong>
            <ul>
              <li>Routes traffic from Internet  Azure VMs via public IP.</li>
              <li>Use Case: Internet-facing apps (e.g., web frontend VMs).</li>
            </ul>
          </li>
          <li> <strong>Internal Load Balancer (Private):</strong>
            <ul>
              <li>Balances traffic inside VNets (no public IP).</li>
              <li>Use Case: Internal services (DB tiers, backend APIs, AKS pods).</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I use internal LBs behind Application Gateway for microservice load balancing within private subnets.</p>
      </div>`},{question:"Difference between Load Balancer (L4) and Application Gateway (L7)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Load Balancer vs Application Gateway</strong></h3>
        <table style="width:100%;border-collapse:collapse;margin-top:8px;">
          <tr><th>Feature</th><th>Azure Load Balancer (L4)</th><th>Application Gateway (L7)</th></tr>
          <tr><td>OSI Layer</td><td>Layer 4 (TCP/UDP)</td><td>Layer 7 (HTTP/HTTPS)</td></tr>
          <tr><td>Routing</td><td>Based on IP/Port</td><td>Based on URL, Host, or Header</td></tr>
          <tr><td>SSL Termination</td><td> No</td><td> Yes (TLS offloading)</td></tr>
          <tr><td>WAF Support</td><td> No</td><td> Yes</td></tr>
          <tr><td>Health Probes</td><td>Port/Protocol level</td><td>Path-based (HTTP/HTTPS)</td></tr>
        </table>
        <p><strong>In Practice:</strong> Use Load Balancer for backend/internal tiers and Application Gateway for web layer traffic inspection + SSL termination.</p>
      </div>`},{question:"What is Application Gateway, and what are its core components?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Application Gateway (L7 Load Balancer)</strong></h3>
        <p>
          Application Gateway distributes HTTP/HTTPS traffic using Layer 7 intelligence, providing WAF protection, SSL offload, and URL-based routing.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Frontend IP Config</strong>  Public or private endpoint for client access.</li>
          <li> <strong>Listeners</strong>  Detect incoming traffic (port, protocol, hostname).</li>
          <li> <strong>Backend Pools</strong>  VM, VMSS, App Service, or IPs.</li>
          <li> <strong>HTTP Settings</strong>  Defines backend port, protocol, and cookie-based affinity.</li>
          <li> <strong>Routing Rules</strong>  Map listener  backend pool.</li>
          <li> <strong>WAF Policy</strong>  Protects against OWASP top 10 vulnerabilities.</li>
        </ul>
        <p><strong>In Practice:</strong> I usually host multiple apps behind a single gateway using path-based routing and custom WAF policies per app.</p>
      </div>`},{question:"How do you configure routing rules in Application Gateway?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Configuring Routing Rules</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 Create <strong>Listener</strong>  Define frontend port (80/443), hostname, and SSL cert.</li>
          <li>2 Configure <strong>HTTP Settings</strong>  Backend port, protocol, and timeout.</li>
          <li>3 Define <strong>Backend Pool</strong>  Add backend VMs, IPs, or App Services.</li>
          <li>4 Create <strong>Routing Rule</strong>  Link listener  HTTP setting  backend pool.</li>
          <li>5 For advanced setups  Use <strong>Path-based routing</strong> or <strong>Multi-site listeners</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure routing like <code>/api/*</code>  backend API pool and <code>/ui/*</code>  frontend pool using separate listeners.</p>
      </div>`},{question:"What is WAF (Web Application Firewall)? Difference between Detection and Prevention modes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure WAF (Web Application Firewall)</strong></h3>
        <p>
          WAF protects web apps from OWASP Top 10 attacks like SQL injection, XSS, and request smuggling.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Detection Mode:</strong> Logs and monitors suspicious traffic but doesnt block.</li>
          <li><strong>Prevention Mode:</strong> Actively blocks requests that match WAF rules.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy in Detection Mode initially to analyze false positives, then switch to Prevention once stable.</p>
      </div>`},{question:"How do you troubleshoot 403 WAF block errors?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Troubleshooting WAF 403 Block Errors</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 Check <strong>WAF Logs</strong> in Log Analytics (Category: ApplicationGatewayFirewallLog).</li>
          <li>2 Identify <strong>RuleId</strong> causing block  often false positive.</li>
          <li>3 Verify request path and headers  rule might trigger on JSON payload or query string.</li>
          <li>4 <strong>Mitigation:</strong>
            <ul>
              <li>Exclude affected parameter or request body via WAF exclusion.</li>
              <li>Switch rule to <code>Log</code> mode if required temporarily.</li>
              <li>Avoid disabling full rule sets globally.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I use KQL to find blocked client IPs quickly and adjust rules at location or parameter level for precision tuning.</p>
      </div>`},{question:"Whats the difference between Azure Front Door and Traffic Manager?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Front Door vs Traffic Manager</strong></h3>
        <table style="width:100%;border-collapse:collapse;margin-top:8px;">
          <tr><th>Feature</th><th>Azure Front Door</th><th>Traffic Manager</th></tr>
          <tr><td>Layer</td><td>L7 (HTTP/HTTPS)</td><td>DNS-based routing</td></tr>
          <tr><td>Routing Type</td><td>Real-time, anycast-based</td><td>DNS name resolution based</td></tr>
          <tr><td>Failover Speed</td><td>Instant (<1 sec)</td><td>Depends on DNS TTL</td></tr>
          <tr><td>Features</td><td>CDN, WAF, SSL, Caching</td><td>Geographic, Weighted, Priority routing</td></tr>
          <tr><td>Best For</td><td>Global app acceleration + security</td><td>Simple geo or performance routing</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use Front Door for global web app delivery (with WAF) and Traffic Manager for backend failover scenarios.</p>
      </div>`},{question:"What routing methods does Traffic Manager provide?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Traffic Manager Routing Methods</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Priority Routing</strong>  Failover to secondary endpoint if primary fails.</li>
          <li> <strong>Geographic Routing</strong>  Route users based on their region.</li>
          <li> <strong>Performance Routing</strong>  Send to endpoint with lowest latency.</li>
          <li> <strong>Weighted Routing</strong>  Distribute traffic based on weights.</li>
          <li> <strong>Multivalue</strong>  Return multiple healthy endpoints for resilience.</li>
        </ul>
        <p><strong>In Practice:</strong> I commonly use <em>Priority</em> for DR failover and <em>Performance</em> for user latency optimization.</p>
      </div>`},{question:"What is a CDN? Why and where would you create Azure CDN?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure CDN (Content Delivery Network)</strong></h3>
        <p>
          CDN caches static content (images, JS, CSS, videos) closer to users to reduce latency and offload backend servers.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Delivers content from edge POPs worldwide.</li>
          <li> Integrated with Azure Storage, Web Apps, or Front Door.</li>
          <li> Reduces bandwidth, improves load time, and adds DDoS resilience.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Azure CDN for global static assets in SPAs and integrate it with Front Door for unified routing + caching.</p>
      </div>`},{question:"In Kubernetes, when you need an L7 load balancer (App Gateway Ingress Controller), what must you configure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS with Application Gateway Ingress Controller (AGIC)</strong></h3>
        <p>
          To use Application Gateway as L7 load balancer for AKS:
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 Deploy AKS in same VNet or peered VNet as the Application Gateway.</li>
          <li>2 Assign <strong>Managed Identity</strong> to AKS for App Gateway access.</li>
          <li>3 Install AGIC via Helm or add-on:
            <pre style="background:#111;padding:.4rem;border-radius:.4rem;"><code>az aks enable-addons --addons ingress-appgw --appgw-id &lt;appgw-id&gt;</code></pre>
          </li>
          <li>4 Create <code>Ingress</code> resources in Kubernetes  AGIC automatically updates routing rules.</li>
          <li>5 Ensure proper DNS mapping for external ingress endpoint.</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer AGIC for enterprise AKS  it gives full WAF + SSL termination + path-based routing natively.</p>
      </div>`}]},{title:"5. Private Access & Perimeter Security  Private Endpoints, Bastion, NSG, Firewall",questions:[{question:"What is a Private Endpoint and when do you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Private Endpoint  Secure Private Access</strong></h3>
        <p>
          A <strong>Private Endpoint (PE)</strong> provides a private IP in your VNet for an Azure PaaS service (Storage, Key Vault, SQL, etc.), removing public exposure.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Enables access over Azure backbone instead of Internet.</li>
          <li> Prevents data exfiltration risks from public endpoints.</li>
          <li> DNS resolves service FQDN to private IP through a <strong>Private DNS Zone</strong>.</li>
        </ul>
        <p><strong>Use Case:</strong> Secure communication between AKS/VMs and services like Storage or SQL without public IPs.</p>
        <p><strong>In Practice:</strong> I always use Private Endpoints in production for compliance  specially for finance/healthcare workloads.</p>
      </div>`},{question:"Have you created a Private Endpoint for a Storage Account? Steps and validation?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Creating Private Endpoint for Azure Storage</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 Go to Storage Account  Networking  Private Endpoint Connections  Create.</li>
          <li>2 Choose <strong>Target Sub-resource</strong> (Blob, File, Queue, or Table).</li>
          <li>3 Select <strong>VNet + Subnet</strong> where endpoint IP will reside.</li>
          <li>4 Link with <strong>Private DNS Zone</strong> (e.g., <code>privatelink.blob.core.windows.net</code>).</li>
          <li>5 Once deployed, Storage FQDN resolves to private IP.</li>
        </ul>
        <h4> Validation:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Run <code>nslookup mystorageaccount.blob.core.windows.net</code>  should return private IP.</li>
          <li>Check <code>az network private-endpoint-connection show</code> for Approved state.</li>
          <li>Ensure NSG allows outbound port 443 to Storage private IP.</li>
        </ul>
        <p><strong>In Practice:</strong> I always validate both DNS and route  most failures happen when private DNS zone is not linked properly.</p>
      </div>`},{question:"VM, Storage Account, SQL DB, App Service are in the same subnet and Storage has a Private Endpoint  how will the VM resolve the Storage FQDN privately?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Private DNS Resolution for Private Endpoint</strong></h3>
        <p>
          When a Private Endpoint is created, Azure automatically associates a <strong>Private DNS Zone</strong> (like <code>privatelink.blob.core.windows.net</code>) and links it with the VNet.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> The Storage Account FQDN resolves to the private IP through the linked DNS zone.</li>
          <li> If VM is in same VNet  DNS resolution happens automatically.</li>
          <li> If in peered VNet  you must link the private DNS zone manually.</li>
          <li> If not linked  VM will resolve public IP and fail due to firewall restrictions.</li>
        </ul>
        <p><strong>In Practice:</strong> I always verify this via <code>nslookup &lt;storage&gt;.blob.core.windows.net</code> to confirm private IP resolution.</p>
      </div>`},{question:"What is Azure Bastion and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Bastion  Secure VM Access Without Public IP</strong></h3>
        <p>
          Azure Bastion provides <strong>RDP/SSH access</strong> to VMs directly through the Azure Portal using private IPs  no public exposure required.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Removes need for opening RDP (3389) or SSH (22) ports to Internet.</li>
          <li> Access happens via HTML5 browser session inside Azure Portal.</li>
          <li> Deployed in <strong>Bastion subnet (AzureBastionSubnet)</strong> inside VNet.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Bastion for jumpbox-less environments  especially in production where public RDP/SSH is not allowed.</p>
      </div>`},{question:"Explain NSG (Network Security Group)  priorities and rule design.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Network Security Group (NSG)</strong></h3>
        <p>
          NSG filters inbound/outbound traffic at subnet or NIC level using 5-tuple rules (Source, Destination, Port, Protocol, Action).
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Rule Priority: <strong>1004096</strong> (lower number = higher priority).</li>
          <li> Default Inbound: Deny all (except Azure platform rules).</li>
          <li> Default Outbound: Allow all.</li>
          <li> Each rule defines: Source, Destination, Protocol, Port Range, Action, and Priority.</li>
        </ul>
        <h4> Example:</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AllowHTTP:
  Priority: 100
  Source: Internet
  Destination: Any
  Port: 80
  Action: Allow</code></pre>
        <p><strong>In Practice:</strong> I design NSGs per subnet, applying least privilege  only allow app ports and deny others explicitly.</p>
      </div>`},{question:"Application Security Group (ASG) vs NSG  explain and compare.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ASG vs NSG</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Feature</th><th>NSG</th><th>ASG</th></tr>
          <tr><td>Purpose</td><td>Traffic filtering (rules)</td><td>Grouping of VMs logically</td></tr>
          <tr><td>Scope</td><td>Subnet or NIC level</td><td>Attached to VMs within NSG rule</td></tr>
          <tr><td>Example</td><td>Allow port 443 from Internet</td><td>Allow traffic from ASG WebServers to DBServers</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use ASGs to simplify NSG management  e.g., instead of managing 50 IPs, I define ASG groups for web, app, and DB tiers.</p>
      </div>`},{question:"What is the difference between NSG and Azure Firewall? When do you use each?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>NSG vs Azure Firewall</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Feature</th><th>NSG</th><th>Azure Firewall</th></tr>
          <tr><td>Layer</td><td>Layer 3 & 4</td><td>Layer 37 (stateful inspection)</td></tr>
          <tr><td>Function</td><td>Basic allow/deny filtering</td><td>Advanced filtering, logging, FQDN filtering, NAT</td></tr>
          <tr><td>Use Case</td><td>Micro-segmentation inside VNets</td><td>Centralized perimeter control (north-south traffic)</td></tr>
          <tr><td>Integration</td><td>Applied on subnets or NICs</td><td>Deployed in Hub VNet</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use NSGs for internal isolation and Azure Firewall at perimeter to control outbound/inbound Internet access.</p>
      </div>`},{question:"How do you configure Firewall rules in Azure (threat-intel, DNAT/SNAT, FQDN tags)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Firewall Rules Configuration</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Application Rules:</strong> Allow outbound HTTP/S to specific FQDNs (e.g., <code>*.microsoft.com</code>).</li>
          <li> <strong>Network Rules:</strong> Allow/deny IP, port-based traffic (TCP/UDP).</li>
          <li> <strong>DNAT Rules:</strong> Map public IP  internal VM for inbound traffic.</li>
          <li> <strong>SNAT:</strong> Automatically translates private IP  Firewall public IP for outbound access.</li>
          <li> <strong>Threat Intelligence:</strong> Blocks known malicious IPs and domains (Alert/Deny modes).</li>
          <li> <strong>FQDN Tags:</strong> Predefined trusted domains like <code>WindowsUpdate</code> or <code>AzureMonitor</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> I always log traffic in Log Analytics and integrate Firewall alerts with Sentinel for threat detection.</p>
      </div>`},{question:"End-to-end: how would you manage Firewall and DNS setup for a secure application?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>End-to-End Secure Network Setup</strong></h3>
        <p>
          To design a secure perimeter for Azure applications:
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 Hub-Spoke model  Azure Firewall in Hub, apps in Spokes.</li>
          <li>2 NSGs for internal subnet control (least privilege).</li>
          <li>3 Private Endpoints for all PaaS services (Storage, Key Vault, DB).</li>
          <li>4 Private DNS Zones linked to Spokes for resolution.</li>
          <li>5 Azure Firewall forced tunneling for outbound traffic.</li>
          <li>6 Logs  Log Analytics  Sentinel for SIEM monitoring.</li>
        </ul>
        <p><strong>In Practice:</strong> I follow Deny by default, allow explicitly  combining NSG + Firewall + Private DNS gives full east-west and north-south isolation.</p>
      </div>`}]},{title:"6. Identities, Keys & Secrets  Managed Identity, Key Vault, RBAC",questions:[{question:"What are Managed Identities in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managed Identities  Passwordless Authentication for Azure Resources</strong></h3>
        <p>
          Managed Identity is an automatically managed service principal provided by Azure AD, allowing Azure resources (like VMs, Functions, AKS, Pipelines) to authenticate securely to other Azure services  <strong>without storing credentials</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>System-Assigned MI:</strong> Tied to a specific resource; lifecycle matches the resource (auto-deleted when resource is deleted).</li>
          <li> <strong>User-Assigned MI:</strong> Standalone identity that can be shared across multiple resources.</li>
          <li> Used to access Azure services like Key Vault, Storage, SQL, or REST APIs securely.</li>
        </ul>
        <h4> Example:</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>az vm identity assign --name myVM --resource-group rg-demo</code></pre>
        <p>Then assign Key Vault <code>get/list</code> permissions to that MI via RBAC.</p>
        <p><strong>In Practice:</strong> I always use Managed Identity instead of service principals to eliminate credential rotation and reduce attack surface.</p>
      </div>`},{question:"What is Azure Key Vault and how do you integrate it with applications and pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Key Vault  Central Secrets Store</strong></h3>
        <p>
          Azure Key Vault securely stores and manages <strong>secrets, keys, and certificates</strong> used by apps, infrastructure, and CI/CD pipelines.
        </p>
        <h4> Integration Approaches:</h4>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Applications:</strong> Use Managed Identity + SDK (or REST API) to fetch secrets securely.</li>
          <li> <strong>Azure Pipelines:</strong> 
            <ul>
              <li>Use Key Vault task in YAML pipeline.</li>
              <li>Or connect via Service Connection using RBAC + Managed Identity.</li>
            </ul>
          </li>
          <li> <strong>Terraform:</strong> Fetch secrets dynamically using <code>azurerm_key_vault_secret</code> data block.</li>
        </ul>
        <h4> Example (YAML Pipeline):</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>variables:
  - group: KeyVaultSecrets

steps:
- task: AzureKeyVault@2
  inputs:
    azureSubscription: 'myServiceConnection'
    KeyVaultName: 'my-keyvault'
    SecretsFilter: '*'
    RunAsPreJob: true</code></pre>
        <p><strong>In Practice:</strong> I configure all pipeline credentials (like SP passwords, storage keys) in Key Vault for centralized control and rotation tracking.</p>
      </div>`},{question:"Purpose of Key Vault  typical use cases (keys, secrets, certificates).",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Key Vault  Core Use Cases</strong></h3>
        <p>
          Key Vault is a cloud-based vault for securely storing cryptographic keys, app secrets, and SSL certificates.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Secrets:</strong> Store passwords, API tokens, connection strings (for pipelines and apps).</li>
          <li> <strong>Keys:</strong> Manage encryption keys for SQL TDE, disk encryption, and Azure Storage SSE.</li>
          <li> <strong>Certificates:</strong> Store and auto-renew TLS/SSL certificates with Azure App Service or CDN.</li>
        </ul>
        <h4> Real-World Use Case:</h4>
        <ul style="margin-left:1.2rem;">
          <li>VM Disk Encryption (uses Key Vault to hold KEK/BEK).</li>
          <li>AKS Pod Secrets fetched from Key Vault via CSI driver.</li>
          <li>Central credential store for CI/CD pipelines across environments.</li>
        </ul>
        <p><strong>In Practice:</strong> I always restrict Key Vault access via private endpoints + RBAC and disable public network access for compliance.</p>
      </div>`},{question:"How do you implement role-based access control (RBAC) for least-privilege?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>RBAC  Role-Based Access Control in Azure</strong></h3>
        <p>
          RBAC allows fine-grained access management using predefined or custom roles applied to users, groups, or Managed Identities.
        </p>
        <h4> Best Practices for Least Privilege:</h4>
        <ul style="margin-left:1.2rem;">
          <li> Assign roles at the lowest necessary scope (Resource  RG  Subscription).</li>
          <li> Use built-in roles like:
            <ul>
              <li><code>Reader</code>  view only</li>
              <li><code>Contributor</code>  modify but no access control</li>
              <li><code>Key Vault Secrets User</code>  for fetching secrets only</li>
            </ul>
          </li>
          <li> Avoid using Owner unless required.</li>
          <li> Combine RBAC with PIM (Privileged Identity Management) for just-in-time elevation.</li>
        </ul>
        <p><strong>In Practice:</strong> I assign Managed Identities <code>Key Vault Reader</code> role instead of full access  keeps pipeline secure and auditable.</p>
      </div>`},{question:"How would you integrate Azure Key Vault with AKS for pod secrets (CSI driver/Secret Store)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Key Vault with AKS (CSI Driver)</strong></h3>
        <p>
          The <strong>Secrets Store CSI Driver</strong> mounts Key Vault secrets as volumes or Kubernetes secrets inside AKS pods  removing hardcoded secrets.
        </p>
        <h4> Setup Steps:</h4>
        <ul style="margin-left:1.2rem;">
          <li>1 Enable Managed Identity for AKS.</li>
          <li>2 Grant that MI access to Key Vault (<code>get/list</code> permissions).</li>
          <li>3 Install CSI Driver:
            <pre style="background:#111;padding:.4rem;border-radius:.4rem;"><code>az aks enable-addons --addons azure-keyvault-secrets-provider --resource-group rg --name aks-cluster</code></pre>
          </li>
          <li>4 Create a SecretProviderClass manifest:
            <pre style="background:#111;padding:.4rem;border-radius:.4rem;"><code>apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: kv-secrets
spec:
  provider: azure
  parameters:
    keyvaultName: my-keyvault
    objects: |
      array:
        - objectName: db-password
          objectType: secret
    tenantId: &lt;tenant-guid&gt;</code></pre>
          </li>
          <li>5 Reference it in a pod spec:
            <pre style="background:#111;padding:.4rem;border-radius:.4rem;"><code>volumeMounts:
- name: secrets-store
  mountPath: /mnt/secrets
volumes:
- name: secrets-store
  csi:
    driver: secrets-store.csi.k8s.io
    readOnly: true
    volumeAttributes:
      secretProviderClass: kv-secrets</code></pre>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I use CSI for all AKS workloads  no credentials inside YAML, rotation auto-updates pods seamlessly.</p>
      </div>`}]},{title:"7. Storage  Accounts, Security, Tiers & Access",questions:[{question:"How do you secure Storage Accounts (public access disable, TLS, private endpoints, SAS policies)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Azure Storage Accounts</strong></h3>
        <p>
          Azure Storage security revolves around <strong>network isolation, encryption, and controlled access</strong>.  
          I always apply these hardening steps in production:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Disable Public Access:</strong> Turn off Allow blob public access at account level.</li>
          <li> <strong>Use Private Endpoints:</strong> Connect via private IP  traffic stays within Azure backbone.</li>
          <li> <strong>Restrict Network Access:</strong> Selected networks mode + Firewall rules for VNet/subnet.</li>
          <li> <strong>Use SAS (Shared Access Signatures):</strong> Granular, time-bound access instead of keys.</li>
          <li> <strong>Enforce TLS 1.2:</strong> Disable older protocols under Minimum TLS version.</li>
          <li> <strong>Use Managed Identity:</strong> Replace key-based access with RBAC-based identity access.</li>
        </ul>
        <h4> CLI Example:</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>az storage account update   --name mystorage   --resource-group rg-demo   --https-only true   --allow-blob-public-access false</code></pre>
        <p><strong>In Practice:</strong> I always combine private endpoints + SAS + RBAC for a complete zero-trust storage design.</p>
      </div>`},{question:"What are the network access settings for a Storage Account? Explain options.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Storage Account Network Access Settings</strong></h3>
        <p>
          You can control who can reach your storage account using the <strong>Networking</strong> blade. Three modes exist:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>All Networks:</strong> Fully open  accessible from Internet (not recommended).</li>
          <li> <strong>Selected Networks:</strong> Only specific VNets, subnets, or IP ranges allowed.</li>
          <li> <strong>Deny All:</strong> Access only via Private Endpoint (no public route).</li>
        </ul>
        <h4> Tip:</h4>
        <p>Always pair Selected Networks with <strong>Private Endpoints</strong> to enforce private-only access.</p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>az storage account network-rule add   --resource-group rg-demo   --account-name mystorage   --vnet-name myvnet   --subnet mysubnet</code></pre>
        <p><strong>In Practice:</strong> I apply Deny All + Private Endpoint in PROD, and Selected Networks in DEV/UAT for controlled testing.</p>
      </div>`},{question:"How do you configure firewall rules for Storage Accounts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Configuring Storage Firewall Rules</strong></h3>
        <p>
          Firewall rules let you allow or block specific public IP ranges for your Storage Account.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 Go to <strong>Storage Account  Networking  Firewalls and virtual networks</strong>.</li>
          <li>2 Choose Selected Networks.</li>
          <li>3 Add <strong>Client IPs</strong> or <strong>Virtual Networks</strong> that should have access.</li>
          <li>4 Optionally, allow trusted Microsoft services like Azure DevOps or Backup.</li>
        </ul>
        <h4> CLI Example:</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>az storage account network-rule add   --account-name mystorage   --ip-address 103.21.22.5</code></pre>
        <p><strong>In Practice:</strong> I restrict firewall access to jump servers or private endpoints only  never open to All Networks.</p>
      </div>`},{question:"What is Blob Storage and what are its access tiers (Hot/Cool/Archive)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Blob Storage  Object Store with Access Tiers</strong></h3>
        <p>
          Blob Storage stores unstructured data (images, logs, backups) and offers <strong>cost-optimized access tiers</strong> based on data usage patterns.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Hot Tier:</strong> For frequently accessed data  higher storage cost, lower access latency.</li>
          <li> <strong>Cool Tier:</strong> For infrequently accessed data  lower storage cost, slightly higher access cost.</li>
          <li> <strong>Archive Tier:</strong> For long-term retention  cheapest storage, but retrieval takes hours.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Hot for app logs, Cool for monthly backups, and Archive for compliance or audit data.</p>
      </div>`},{question:"How can you restrict public access to your Storage Account?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Restricting Public Access</strong></h3>
        <p>
          You can disable anonymous access to prevent anyone from accessing blobs without authentication.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 Go to <strong>Configuration</strong>  Set Allow Blob Public Access = <code>Disabled</code>.</li>
          <li>2 Set <strong>Network Access</strong> to Selected Networks or Deny All.</li>
          <li>3 Use <strong>Azure RBAC</strong> or SAS tokens for controlled access.</li>
          <li>4 Disable Shared Key authorization if using Managed Identity or AAD.</li>
        </ul>
        <h4> Example:</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>az storage account update   --name mystorage   --resource-group rg-demo   --allow-blob-public-access false</code></pre>
        <p><strong>In Practice:</strong> I make this setting part of policy  so even new storage accounts inherit public access disabled by default.</p>
      </div>`}]},{title:"8. Compute & App Platform  VM, VMSS, App Service, AKS (Access & Scale)",questions:[{question:"What resources do you create when provisioning a VM (NIC, NSG, Disk, VNet/Subnet, PIP)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resources Created When Provisioning a VM</strong></h3>
        <p>
          Deploying a Virtual Machine in Azure automatically provisions several dependent resources:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>NIC (Network Interface):</strong> Connects VM to a VNet/Subnet.</li>
          <li> <strong>VNet & Subnet:</strong> Defines private IP range and logical isolation.</li>
          <li> <strong>NSG:</strong> Filters inbound/outbound traffic (acts like a firewall).</li>
          <li> <strong>OS Disk:</strong> Boot disk attached to the VM (managed disk).</li>
          <li> <strong>Data Disks:</strong> Additional storage for app/data.</li>
          <li> <strong>Public IP (optional):</strong> For Internet access or RDP/SSH.</li>
          <li> <strong>Availability Options:</strong> Zone/Set for redundancy.</li>
        </ul>
        <p><strong>In Practice:</strong> I often deploy VMs using Terraform  all these components are declared as resources in the same module.</p>
      </div>`},{question:"What is an Availability Set vs Availability Zone?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Availability Set vs Availability Zone</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Availability Set</th><th>Availability Zone</th></tr>
          <tr><td>Level</td><td>Logical grouping within a data center</td><td>Physically separate data centers</td></tr>
          <tr><td>Fault Tolerance</td><td>Protects from rack/power failures</td><td>Protects from entire zone failure</td></tr>
          <tr><td>Placement</td><td>2+ Fault + Update domains</td><td>Zone 1, Zone 2, Zone 3 (physical separation)</td></tr>
          <tr><td>SLA</td><td>99.95%</td><td>99.99%</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use <strong>Availability Zones</strong> for production workloads requiring true regional resiliency.</p>
      </div>`},{question:"What is VMSS and when would you prefer it over individual VMs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Virtual Machine Scale Sets (VMSS)</strong></h3>
        <p>
          VMSS lets you deploy and manage identical VMs as a single group  ideal for scalable and load-balanced workloads.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Auto-scales up/down based on CPU, memory, or custom metrics.</li>
          <li> Integrated with Load Balancer or Application Gateway.</li>
          <li> Supports rolling upgrades and OS image updates.</li>
          <li> Uses a single VM model (SKU, OS, extensions) across instances.</li>
        </ul>
        <p><strong>When to prefer:</strong> High-traffic, stateless web tiers or API servers that need scale and automation.</p>
        <p><strong>In Practice:</strong> I use VMSS with custom images + autoscale rules linked to Azure Monitor metrics.</p>
      </div>`},{question:"If you need two VMs and two databases  when would you choose VMSS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>When to Choose VMSS Over Individual VMs</strong></h3>
        <p>
          VMSS is used only when multiple identical instances are required.  
          For example:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Web/App servers  can use VMSS (stateless, scalable).</li>
          <li> Databases  avoid VMSS (stateful workloads, data consistency issues).</li>
        </ul>
        <p><strong>In Practice:</strong> I use 2 DB VMs (manual HA) and deploy app layer via VMSS for scaling and auto-healing.</p>
      </div>`},{question:"How do you patch your VMs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VM Patching in Azure</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Use <strong>Azure Update Management</strong> (Automation Account + Log Analytics) for scheduling patches.</li>
          <li> Windows: Apply updates via <code>az vm run-command invoke</code> or ADO pipeline PowerShell tasks.</li>
          <li> Linux: Use <code>apt</code> or <code>yum</code> commands in patch jobs, or cron-based automation.</li>
          <li> Combine with <strong>Maintenance Configurations</strong> for reboot control.</li>
        </ul>
        <p><strong>In Practice:</strong> I schedule patching via Automation Account hybrid worker group  sends compliance report post patch window.</p>
      </div>`},{question:"What is swap space in a Linux VM?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Linux VM Swap Space</strong></h3>
        <p>
          Swap space acts as overflow memory when physical RAM is full. It allows Linux to temporarily move inactive pages from RAM to disk.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Can be a <strong>swap file</strong> or <strong>swap partition</strong>.</li>
          <li> Avoids OOM (Out of Memory) errors under heavy load.</li>
          <li> Azure automatically configures it for some distributions using ephemeral disks.</li>
        </ul>
        <p><strong>In Practice:</strong> I manually tune swap size for memory-heavy apps  e.g., double RAM size for smaller VMs.</p>
      </div>`},{question:"How do you connect to a VM with only a private IP (Bastion/Jumpbox/PE)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Accessing Private VMs Securely</strong></h3>
        <p>
          To connect to a VM without public IP:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Bastion:</strong> Connect directly via Azure Portal using RDP/SSH over private IP.</li>
          <li> <strong>Jumpbox VM:</strong> Connect to a single hardened VM that has access to private subnet.</li>
          <li> <strong>Private Endpoint/VPN:</strong> Access through private IP using VPN or ExpressRoute.</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer Bastion for corporate access (no key exposure) and jumpbox for limited DevOps SSH use.</p>
      </div>`},{question:"How will you horizontally scale your App Services?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Scaling Azure App Services</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Manual Scaling:</strong> Increase instance count from App Service Plan  Scale Out.</li>
          <li> <strong>Autoscaling:</strong> Configure rules based on CPU %, memory, HTTP queue length, or custom metrics.</li>
          <li> <strong>Scale Up:</strong> Move to higher plan (B1  P1V3) for more CPU/RAM.</li>
          <li> <strong>Scale Out:</strong> Add more instances horizontally behind Azure Load Balancer.</li>
        </ul>
        <h4> Example Rule:</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>If CPU > 70% for 10 min  add 1 instance
If CPU < 40% for 15 min  remove 1 instance</code></pre>
        <p><strong>In Practice:</strong> I use autoscale profiles + Application Insights metrics for predictive scaling.</p>
      </div>`},{question:"How will you make AKS private (private cluster, private endpoints, UDRs, DNS)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Private AKS Cluster Design</strong></h3>
        <p>
          To make AKS private, you isolate both API server and node traffic:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Private Cluster:</strong> Deploy with <code>--enable-private-cluster</code>  no public API endpoint.</li>
          <li> <strong>Private Endpoint:</strong> Control-plane exposed via private IP inside your VNet.</li>
          <li> <strong>User Defined Routes (UDRs):</strong> Force egress traffic through Firewall or NVA.</li>
          <li> <strong>Private DNS Zone:</strong> AKS registers control plane DNS here.</li>
          <li> <strong>RBAC & Managed Identity:</strong> Secure access to ACR, Key Vault, etc., using MI-based tokens.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy private AKS with UDR + Firewall + Private DNS  no Internet traffic allowed directly from pods.</p>
      </div>`},{question:"What compute services do you commonly use in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Azure Compute Services I Use</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Virtual Machines (VMs):</strong> Core compute for OS-level control.</li>
          <li> <strong>VM Scale Sets (VMSS):</strong> Auto-scaling web/API workloads.</li>
          <li> <strong>App Services:</strong> PaaS hosting for web apps & APIs.</li>
          <li> <strong>AKS (Azure Kubernetes Service):</strong> Containerized app orchestration.</li>
          <li> <strong>Azure Functions:</strong> Serverless event-driven compute.</li>
          <li> <strong>Azure Container Apps:</strong> Lightweight microservice platform for small workloads.</li>
        </ul>
        <p><strong>In Practice:</strong> I mix AKS + App Services for scalability and Functions for automation or event-driven triggers.</p>
      </div>`}]},{title:"9. Availability, BCDR & Backup",questions:[{question:"What is Azure Site Recovery (ASR)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Site Recovery (ASR)  Disaster Recovery as a Service</strong></h3>
        <p>
          Azure Site Recovery (ASR) replicates workloads (VMs, servers) from a primary site to a secondary Azure region for <strong>business continuity during outages</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Continuous replication of VM disks to target region.</li>
          <li> Supports both <strong>on-prem  Azure</strong> and <strong>Azure  Azure</strong> replication.</li>
          <li> Enables failover, failback, and test failover (DR drills).</li>
          <li> Policy-driven: replication frequency, retention, app-consistency checkpoints.</li>
        </ul>
        <h4> CLI Example:</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>az backup vault create --name DRVault --resource-group rg-prod
az backup policy create --vault-name DRVault --name DRPolicy --policy '{"schedule":{"frequency":"Daily"}}'</code></pre>
        <p><strong>In Practice:</strong> I configure ASR for all PROD VMs using paired region and quarterly failover drills to validate readiness.</p>
      </div>`},{question:"Define RPO and RTO in DR design.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>RPO vs RTO  Key DR Metrics</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>RPO (Recovery Point Objective):</strong> 
            Maximum acceptable data loss (how old the last available backup can be). Example: RPO = 15 min means last 15 min of data may be lost.</li>
          <li> <strong>RTO (Recovery Time Objective):</strong>
            Maximum acceptable downtime (how long it takes to restore service after failure).</li>
        </ul>
        <p><strong>In Practice:</strong> For production workloads I target <strong>RPO  15 min</strong> and <strong>RTO  1 hour</strong> using ASR + automation runbooks.</p>
      </div>`},{question:"What is BCDR in Azure? Difference between BCDR and DR?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>BCDR  Business Continuity & Disaster Recovery</strong></h3>
        <p>
          <strong>BCDR</strong> is a comprehensive strategy ensuring business operations continue during disasters.  
          It combines two key components:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Business Continuity (BC):</strong> Process planning  how business keeps running (alternate sites, manual operations).</li>
          <li> <strong>Disaster Recovery (DR):</strong> Technical recovery  restoring systems, VMs, and data after an outage.</li>
        </ul>
        <p><strong>In Practice:</strong> I define DR using ASR + Backup Vault and align BC via runbooks and alert escalation in Azure Monitor.</p>
      </div>`},{question:"How do you perform DR drills in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DR Drills (Test Failover)</strong></h3>
        <p>
          Azure Site Recovery supports <strong>non-disruptive test failovers</strong> to verify DR readiness.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 Go to <strong>Recovery Vault  Replicated Items  Test Failover</strong>.</li>
          <li>2 Choose target VNet (isolated test environment).</li>
          <li>3 Validate app functionality and connectivity.</li>
          <li>4 Cleanup test environment post validation.</li>
        </ul>
        <p><strong>In Practice:</strong> We schedule DR drills quarterly and share validation reports for audit compliance.</p>
      </div>`},{question:"How have you configured VM backups  what policies do you apply?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure VM Backup Configuration</strong></h3>
        <p>
          VM backups are configured via <strong>Recovery Services Vault</strong> with policy-based retention.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Daily backup with 30-day retention (standard for PROD).</li>
          <li> Weekly full backup for long-term retention (90 days).</li>
          <li> Backup triggered during non-peak hours.</li>
          <li> Stored in geo-redundant storage (GRS) for regional resiliency.</li>
        </ul>
        <p><strong>In Practice:</strong> I tag critical VMs and assign auto-backup policy via Azure Policy for compliance enforcement.</p>
      </div>`},{question:"If a VM has crashed and is inaccessible, how will you take/restore a backup?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VM Restore After Crash</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 Open Recovery Vault  Backed-up Items.</li>
          <li>2 Select VM  Restore VM or Restore Disks.</li>
          <li>3 Choose restore point  Create new VM or attach disk to a healthy VM for data recovery.</li>
          <li>4 Validate boot and connectivity post-restore.</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer Restore Disks method  faster and safer for forensic analysis or partial recovery.</p>
      </div>`},{question:"MABS vs MARS agent  when to use which?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>MABS vs MARS Agent</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>MABS (Azure Backup Server)</th><th>MARS (Azure Recovery Services Agent)</th></tr>
          <tr><td>Use Case</td><td>Centralized on-prem backup (VMs, SQL, Hyper-V)</td><td>Single server/file-level backup</td></tr>
          <tr><td>Dependency</td><td>Needs System Center DPM or local storage</td><td>No dependency  direct to Vault</td></tr>
          <tr><td>Best For</td><td>Enterprise workloads</td><td>Standalone servers or clients</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use MABS for data center-level protection and MARS for dev/test or file-server backups.</p>
      </div>`},{question:"Where are your DR servers located (region pair considerations)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Region Pair Considerations for DR</strong></h3>
        <p>
          Azure uses <strong>paired regions</strong> to ensure cross-region data durability and DR readiness.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Each Azure region is paired with another region in the same geography (e.g., East US  West US).</li>
          <li> Replication (GRS/RA-GRS) automatically targets the paired region.</li>
          <li> Azure never updates both regions simultaneously  ensures availability during maintenance.</li>
          <li> DR workloads (ASR, backups) are always hosted in the paired region for compliance.</li>
        </ul>
        <p><strong>In Practice:</strong> For Central India workloads, I configure DR in South India  both part of the same region pair.</p>
      </div>`}]},{title:"10. Monitoring, Logs & Security Alerts",questions:[{question:"What is Azure Monitor and how does it help track performance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Monitor  Centralized Monitoring Platform</strong></h3>
        <p>
          Azure Monitor collects, analyzes, and acts on telemetry from Azure and on-prem environments.  
          It helps detect, visualize, and respond to performance or availability issues across infrastructure and applications.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Metrics:</strong> Real-time numeric performance data (CPU %, memory, latency).</li>
          <li> <strong>Logs:</strong> Detailed event data (activity logs, diagnostics, security events).</li>
          <li> <strong>Visualizations:</strong> Dashboards and Workbooks for custom KPIs.</li>
          <li> <strong>Alerts:</strong> Triggered on metric thresholds or log query results.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate Azure Monitor with Log Analytics and Application Insights for full-stack visibility of AKS, App Services, and VMs.</p>
      </div>`},{question:"What are metrics and logs in Azure Monitor?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Metrics vs Logs  Key Difference</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Metrics:</strong> Numeric data collected at fixed intervals (e.g., CPU %, memory, disk IOPS).  
          Stored for 93 days and used for real-time dashboards or alerts.</li>
          <li> <strong>Logs:</strong> Detailed event data like audit trails, request traces, errors, and activity events.  
          Stored in <strong>Log Analytics workspace</strong> and queried using <strong>KQL (Kusto Query Language)</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> Metrics show <em>what</em> is wrong, Logs explain <em>why</em>.</p>
      </div>`},{question:"What are Diagnostic Settings and common log categories?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Diagnostic Settings  Enabling Deep Visibility</strong></h3>
        <p>
          Diagnostic settings define where Azure resource logs and metrics are sent (Log Analytics, Event Hub, Storage).
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Categories:</strong> 
            <ul>
              <li>Administrative (control-plane actions)</li>
              <li>Security (auth, key access, failed attempts)</li>
              <li>Service Health (availability, SLA)</li>
              <li>Audit Logs (policy/compliance actions)</li>
              <li>Performance Logs (metrics like latency, throughput)</li>
            </ul>
          </li>
          <li> Sent to: Log Analytics  for KQL query and alerts.</li>
        </ul>
        <p><strong>In Practice:</strong> I always enable Diagnostic Settings via Terraform for key services like App Gateway, Key Vault, and Storage.</p>
      </div>`},{question:"How do you analyze Activity Logs and Security Alerts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Analyzing Activity Logs & Security Alerts</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Activity Logs:</strong> Show who did what  e.g., resource creation, deletion, or RBAC change.</li>
          <li> <strong>Security Alerts:</strong> Generated by Defender for Cloud or Azure Monitor when suspicious actions occur.</li>
        </ul>
        <p>Use <strong>Log Analytics  Activity Log</strong> and filter by <code>OperationName</code> or <code>Caller</code> to find root cause.</p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureActivity
| where OperationName == "Delete Virtual Machine"
| project Caller, ResourceGroup, Resource, ActivityStatus, TimeGenerated</code></pre>
        <p><strong>In Practice:</strong> I integrate these alerts with Teams and ServiceNow for immediate triage by the security team.</p>
      </div>`},{question:"How do you monitor Application Gateway logs in Log Analytics (WAF logs, access logs)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring Application Gateway Logs</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 Enable <strong>Diagnostic Settings</strong> on the App Gateway  send to Log Analytics.</li>
          <li>2 Key logs:
            <ul>
              <li>Access Logs  client request data</li>
              <li>Performance Logs  backend response times</li>
              <li>Firewall Logs  WAF rule matches and blocked requests</li>
            </ul>
          </li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureDiagnostics
| where ResourceType == "APPLICATIONGATEWAYS"
| where Category == "ApplicationGatewayFirewallLog"
| summarize count() by clientIP_s, ruleId_s, action_s</code></pre>
        <p><strong>In Practice:</strong> I visualize top WAF blocks in Workbooks to spot frequent attacker IPs or misconfigured rules.</p>
      </div>`},{question:"What is Application Insights and when do you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Application Insights  APM for Azure</strong></h3>
        <p>
          Application Insights monitors the performance and availability of web apps, APIs, and microservices.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Tracks request rate, response time, failures, dependencies.</li>
          <li> Detects anomalies using AI-based smart detection.</li>
          <li> Integrates with Azure Monitor and Log Analytics.</li>
          <li> Connected via SDK or App Service instrumentation key.</li>
        </ul>
        <p><strong>In Practice:</strong> I use it with App Services and AKS APIs to track latency, response codes, and dependency bottlenecks.</p>
      </div>`},{question:"What are options to monitor network traffic (NSG flow logs, NPM/Workbooks)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring Network Traffic in Azure</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>NSG Flow Logs:</strong> Capture inbound/outbound traffic metadata, stored in Storage Account or Log Analytics.</li>
          <li> <strong>Traffic Analytics (via NPM):</strong> Visualizes flow logs to show top talkers, ports, and regions.</li>
          <li> <strong>Network Watcher:</strong> Run packet capture, connection monitor, and topology views.</li>
          <li> <strong>Workbooks:</strong> Custom dashboards for NSG/Firewall visualization.</li>
        </ul>
        <p><strong>In Practice:</strong> I enable NSG flow logs v2 + Traffic Analytics in every production subscription for deep visibility.</p>
      </div>`},{question:"There are certain security alerts in Azure  how would you triage and remediate them?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Triage & Remediation of Security Alerts</strong></h3>
        <p>
          When a security alert is generated by Defender for Cloud or Sentinel:
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 <strong>Review Alert:</strong> Identify severity and impacted resource.</li>
          <li>2 <strong>Investigate Root Cause:</strong> Use Log Analytics or Activity Logs.</li>
          <li>3 <strong>Containment:</strong> Disable public IP, revoke credentials, or apply NSG rules.</li>
          <li>4 <strong>Remediation:</strong> Patch or rotate secrets.</li>
          <li>5 <strong>Prevent Recurrence:</strong> Implement policy or automation (Logic App or Sentinel playbook).</li>
        </ul>
        <p><strong>In Practice:</strong> I use Sentinel playbooks to auto-disable VMs showing brute-force SSH attempts.</p>
      </div>`},{question:"Suppose Microsoft announces new Azure services  where do you get official updates?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Staying Updated on Azure Announcements</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <a href="https://azure.microsoft.com/updates" target="_blank"><strong>Azure Updates Portal</strong></a>  Official list of all new releases & previews.</li>
          <li> <strong>Azure Blog & Tech Community</strong>  Deep-dive posts and architecture updates.</li>
          <li> <strong>Microsoft Learn & Channel 9</strong>  Feature walk-throughs and demos.</li>
          <li> <strong>Azure Advisor & Service Health</strong>  Personalized impact and new service alerts inside portal.</li>
        </ul>
        <p><strong>In Practice:</strong> I subscribe to Azure Updates RSS feed and follow Microsofts Cloud Advocate teams on LinkedIn for insider previews.</p>
      </div>`}]},{title:"11. Cost, Architecture & Landing Zone Readiness",questions:[{question:"What is cost optimization and which Azure tools help (Cost Management, Advisor, Reservations, Spot, Autoscale)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cost Optimization in Azure</strong></h3>
        <p>Cost optimization means reducing unnecessary spend while maintaining performance and reliability.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Cost Management + Budgets:</strong> Tracks real-time costs and sends alerts when thresholds are reached.</li>
          <li> <strong>Azure Advisor:</strong> Recommends VM right-sizing, idle resource cleanup, and reserved instance opportunities.</li>
          <li> <strong>Reservations:</strong> Commit 1/3-year term for predictable workloads  save up to 70%.</li>
          <li> <strong>Autoscaling:</strong> Scale VMs/App Services dynamically based on CPU/memory to prevent over-provisioning.</li>
          <li> <strong>Spot VMs:</strong> Run non-critical workloads at huge discounts (up to 90%).</li>
        </ul>
        <p><strong>In Practice:</strong> I use a mix of <em>Autoscale</em> + <em>Budget Alerts</em> + <em>Azure Policy</em> to optimize cost across dev/test environments.</p>
      </div>`},{question:"What are Availability Zones and their benefits?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Availability Zones  Fault Isolation Units</strong></h3>
        <p>Availability Zones (AZs) are physically separate datacenters within an Azure region, each with independent power, cooling, and networking.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>High Availability:</strong> Protects apps from datacenter-level failures.</li>
          <li> <strong>Zone Redundancy:</strong> Replicates resources across zones for SLA up to 99.99%.</li>
          <li> <strong>Example Services:</strong> VMs, Disks, App Gateway, AKS, SQL Managed Instance.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy production workloads (App GW + AKS + SQL MI) across Zone 1/2/3 for true regional resiliency.</p>
      </div>`},{question:"What services of Azure have you used (VMs, VNets, NSGs, LB, Storage, App GW, AKS, App Service)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Core Azure Services I Use Regularly</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Compute:</strong> VMs, VMSS, App Service, AKS (container orchestration).</li>
          <li> <strong>Networking:</strong> VNets, Subnets, NSGs, UDRs, Load Balancer, Application Gateway (with WAF).</li>
          <li> <strong>Storage:</strong> Blob, File Share, Managed Disks, SAS policies, private endpoints.</li>
          <li> <strong>Security:</strong> Azure Firewall, Key Vault, Defender for Cloud.</li>
          <li> <strong>Monitoring:</strong> Log Analytics, Application Insights, Azure Monitor, Alerts.</li>
        </ul>
        <p><strong>In Practice:</strong> I build end-to-end landing zones with AKS + ACR + App GW + Key Vault + Azure Monitor integration.</p>
      </div>`},{question:"What are the pillars of the Well-Architected Framework?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Well-Architected Framework  5 Pillars</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Operational Excellence:</strong> Monitoring, CI/CD automation, Infrastructure as Code.</li>
          <li> <strong>Security:</strong> Identity protection, network isolation, secret management.</li>
          <li> <strong>Reliability:</strong> Backup, redundancy, failover, auto-healing systems.</li>
          <li> <strong>Cost Optimization:</strong> Autoscaling, right-sizing, reserved instances, budgets.</li>
          <li> <strong>Performance Efficiency:</strong> Autoscaling, caching, CDN, optimized queries.</li>
        </ul>
        <p><strong>In Practice:</strong> I evaluate my deployments against these pillars during architecture reviews and cost audits.</p>
      </div>`},{question:"What are the key design principles for an enterprise Landing Zone (policy, guardrails, identity, networking, management)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Enterprise Landing Zone  Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Identity:</strong> Centralized Azure AD, RBAC, and Conditional Access policies.</li>
          <li> <strong>Networking:</strong> Hub-Spoke model with private DNS, Firewall, and UDR-based routing.</li>
          <li> <strong>Security Guardrails:</strong> Azure Policy, Defender for Cloud, and Log Analytics integration.</li>
          <li> <strong>Governance:</strong> Management Groups and Blueprints for policy enforcement.</li>
          <li> <strong>Management:</strong> Centralized Log Analytics, Update Management, and Monitor Workbooks.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy landing zones via Terraform modules + Azure Policy Assignments to ensure compliance by design.</p>
      </div>`},{question:"How will you secure your frontend deployment (WAF, Front Door, Private Link, Managed Identity, secret scanning)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Frontend Deployment</strong></h3>
        <p>Frontend security focuses on protecting web apps from external threats and data leakage.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Front Door + WAF:</strong> Provides global load balancing with layer-7 threat protection.</li>
          <li> <strong>Private Link:</strong> Restrict frontend-to-backend traffic over private network.</li>
          <li> <strong>Managed Identity:</strong> Replace credentials with identity-based access (no secrets in code).</li>
          <li> <strong>Secret Scanning:</strong> Enable GitHub Advanced Security / ADO Secret Scanning to prevent key leaks.</li>
          <li> <strong>HTTPS Only:</strong> Enforce TLS 1.2+, use managed certificates in App Gateway.</li>
        </ul>
        <p><strong>In Practice:</strong> I host frontends behind Front Door + WAF with Private Endpoint connectivity to backend APIs.</p>
      </div>`},{question:"How will you work across multi-environment setups in Azure (dev/test/prod isolation, subscriptions, policies)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Multi-Environment Setups</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Environment Isolation:</strong> Separate subscriptions or resource groups for Dev, QA, UAT, and Prod.</li>
          <li> <strong>Azure Policy:</strong> Enforce naming conventions, location restrictions, and tagging.</li>
          <li> <strong>RBAC:</strong> Restrict access  DevOps team: Dev/Test; Ops team: Prod-only permissions.</li>
          <li> <strong>CI/CD Variables:</strong> Parameterize YAML pipelines to deploy per environment.</li>
          <li> <strong>Terraform Workspaces:</strong> Use environment-specific state files and variable sets.</li>
        </ul>
        <p><strong>In Practice:</strong> I follow a strict environment segregation model using Management Groups + YAML pipeline parameters for smooth multi-stage deployments.</p>
      </div>`}]},{title:"12. Migration & Platform Operations",questions:[{question:"What are the steps to migrate on-prem servers/VMs to Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>On-Prem to Azure Migration (Lift & Shift Approach)</strong></h3>
        <p>Azure Migrate is the primary tool used for assessing, replicating, and migrating on-prem servers to Azure.</p>
        <h4> Step-by-Step:</h4>
        <ul style="margin-left:1.2rem;">
          <li>1 <strong>Discovery & Assessment:</strong> Install <em>Azure Migrate Appliance</em> on-prem to collect VM inventory, performance, and readiness.</li>
          <li>2 <strong>Sizing & Cost Estimation:</strong> Use Azure Migrate to generate right-sized VM recommendations and pricing estimates.</li>
          <li>3 <strong>Replication Setup:</strong> Enable replication via <em>Azure Migrate: Server Migration</em> (uses ASR in background).</li>
          <li>4 <strong>Testing:</strong> Run test migrations to validate boot and app connectivity in Azure.</li>
          <li>5 <strong>Final Migration:</strong> Perform planned failover, validate workloads, and decommission on-prem servers.</li>
        </ul>
        <p><strong>In Practice:</strong> I perform discovery  replicate  test  migrate using <code>Azure Migrate + ASR</code> with DNS update and firewall rule validation post migration.</p>
      </div>`},{question:"What are the steps to migrate on-prem VMs to Azure with minimal downtime?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Near-Zero Downtime Migration</strong></h3>
        <p>
          For minimal downtime migrations, we use <strong>continuous replication + planned failover</strong> via Azure Site Recovery.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Step 1  Prepare:</strong> Install ASR Mobility agent on source VMs and connect to Recovery Vault.</li>
          <li> <strong>Step 2  Continuous Replication:</strong> ASR replicates disks to target Azure region in near real-time.</li>
          <li> <strong>Step 3  Test Failover:</strong> Validate functionality in Azure sandbox (non-disruptive).</li>
          <li> <strong>Step 4  Planned Failover:</strong> During cutover window, sync delta changes and start Azure VM.</li>
          <li> <strong>Step 5  DNS Switch:</strong> Redirect user traffic to new Azure endpoint.</li>
        </ul>
        <p><strong>In Practice:</strong> I usually plan failover during low-traffic hours and keep rollback option ready via reverse replication.</p>
      </div>`},{question:"How do you handle platform updates in Azure Infra (patches, reboots, host OS updates)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Platform Update & Patch Management</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Updates:</strong> Host OS updates and patches are managed automatically by Microsoft for PaaS services (AKS, App Service, Functions).</li>
          <li> <strong>VM Patch Management:</strong> Use <strong>Azure Update Management</strong> (Automation Account + Log Analytics) for scheduling patches.</li>
          <li> <strong>Maintenance Configurations:</strong> Define patch windows for grouped resources to control reboots.</li>
          <li> <strong>Notification:</strong> Service Health Alerts for planned Azure maintenance events.</li>
          <li> <strong>Automation:</strong> Use Runbooks to patch VMs automatically and send compliance reports.</li>
        </ul>
        <p><strong>In Practice:</strong> I patch PROD via <em>maintenance configurations</em> and use <em>custom scripts</em> to validate app health post-patch.</p>
      </div>`},{question:"Whats the process to move workloads between regions or subscriptions?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Workload Movement Between Regions or Subscriptions</strong></h3>
        <p>
          There are multiple ways to move resources depending on type and criticality.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>1. Move Between Subscriptions:</strong>  
            Use <em>Resource Move</em> (portal or CLI)  Resources must support move operation and remain in same region.
            <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>az resource move --destination-subscription-id "xxxx" --destination-group "rg-new" --ids /subscriptions/.../resources/...</code></pre>
          </li>
          <li> <strong>2. Move Between Regions:</strong>  
            - Use <strong>Azure Resource Mover</strong> for supported resources.  
            - Or use <strong>ARM template export + redeploy</strong> (Infra as Code approach).  
            - For VMs: use snapshot  copy to new region  recreate VM from disk.</li>
          <li> <strong>3. Networking & Dependencies:</strong>  
            Recreate VNet, NSG, Private Endpoints, and Service Connections manually.</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer <strong>Resource Mover</strong> for region migrations and <strong>Terraform automation</strong> for subscription transitions to ensure identical infra rebuilds.</p>
      </div>`}]},{title:"13. Storage, Backup & Region Placement  Practical Checks",questions:[{question:"How do you configure Azure Backup and retention policies?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Backup Configuration & Retention Strategy</strong></h3>
        <p>Azure Backup provides reliable, policy-based protection for VMs, files, and workloads using <strong>Recovery Services Vault</strong>.</p>

        <h4> Step-by-Step Configuration:</h4>
        <ul style="margin-left:1.2rem;">
          <li>1 Create a <strong>Recovery Services Vault</strong> in the same region as the VMs.</li>
          <li>2 Enable <strong>Azure Backup</strong> on the selected VMs.</li>
          <li>3 Define a <strong>Backup Policy</strong>  includes schedule and retention settings.</li>
          <li>4 Configure storage replication type:
            <ul>
              <li> <strong>GRS (Geo-Redundant Storage):</strong> For production workloads requiring cross-region durability.</li>
              <li> <strong>LRS (Locally Redundant Storage):</strong> For non-critical dev/test workloads.</li>
            </ul>
          </li>
        </ul>

        <h4> Typical Retention Policy:</h4>
        <ul style="margin-left:1.2rem;">
          <li> Daily backups retained for 30 days</li>
          <li> Weekly full backups retained for 12 weeks</li>
          <li> Monthly backups retained for 12 months</li>
          <li> Yearly retention for long-term archival</li>
        </ul>

        <p><strong>In Practice:</strong> I tag critical VMs with <em>backup:enabled</em> and auto-assign retention via <em>Azure Policy</em> so compliance reports are consistent across environments.</p>
      </div>`},{question:"Where are your VMs located  which region and why?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Region Selection Strategy for VMs</strong></h3>
        <p>Choosing the right Azure region depends on performance, cost, data residency, and redundancy needs.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Primary Region:</strong> Chosen closest to end-users to minimize latency (e.g., Central India for Indian clients, East US for global workloads).</li>
          <li> <strong>Compliance Requirement:</strong> Data residency laws (e.g., healthcare or finance) may mandate specific region usage.</li>
          <li> <strong>Cost Optimization:</strong> Evaluate region pricing  some zones (like North Europe) are 1015% cheaper than others.</li>
          <li> <strong>Availability Zones:</strong> Used for high availability and fault tolerance.</li>
        </ul>
        <p><strong>In Practice:</strong> I host most production workloads in <em>Central India</em> with Availability Zones enabled, ensuring minimal latency and maximum uptime.</p>
      </div>`},{question:"Where are your DR servers located  region pair and compliance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DR Server Placement & Region Pairing</strong></h3>
        <p>Azure provides <strong>paired regions</strong> to ensure high availability and disaster recovery capability across geographically separated data centers.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Region Pair Concept:</strong> Each Azure region is paired with another (e.g., Central India  South India) to ensure asynchronous replication.</li>
          <li> <strong>Backup & ASR Replication:</strong> All recovery vaults use the paired region automatically for geo-redundant data storage.</li>
          <li> <strong>Failover Strategy:</strong> Azure Site Recovery replicates VMs from primary region to its pair for DR readiness.</li>
          <li> <strong>Compliance Consideration:</strong> Data remains within the same geographic boundary  essential for industries like BFSI and healthcare.</li>
        </ul>
        <p><strong>In Practice:</strong> For workloads in <em>Central India</em>, I configure ASR failover to <em>South India</em> and perform quarterly DR drills to validate readiness and RTO/RPO targets.</p>
      </div>`}]},{title:"14. Quickfire: Clarifications & Choose the Right Service",questions:[{question:"Difference between Availability Zone and Availability Set?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Availability Zone vs Availability Set</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th style="text-align:left;">Feature</th><th>Availability Zone</th><th>Availability Set</th></tr>
          <tr><td>Scope</td><td>Physically separate datacenters in same region</td><td>Logical grouping within same datacenter</td></tr>
          <tr><td>Fault Tolerance</td><td>Protects from full datacenter outage</td><td>Protects from host/server failure only</td></tr>
          <tr><td>Uptime SLA</td><td>99.99%</td><td>99.95%</td></tr>
          <tr><td>Use Case</td><td>Critical production workloads</td><td>Basic redundancy for non-critical workloads</td></tr>
        </table>
        <p><strong>In Practice:</strong> I always choose <em>Availability Zones</em> for production workloads  they provide true datacenter-level isolation.</p>
      </div>`},{question:"Difference between Azure DNS and Azure Front Door?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure DNS vs Azure Front Door</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure DNS:</strong> Provides domain name resolution (maps FQDN  IP). Its a DNS hosting service only.</li>
          <li><strong>Azure Front Door:</strong> Acts as a global layer-7 load balancer with WAF, caching, SSL offloading, and routing.</li>
        </ul>
        <p><strong>Example:</strong> DNS just tells users where your site is; Front Door ensures its fast, secure, and globally available.</p>
      </div>`},{question:"Azure Front Door vs Traffic Manager  when to use which?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Front Door vs Traffic Manager</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Feature</th><th>Front Door</th><th>Traffic Manager</th></tr>
          <tr><td>Layer</td><td>Layer 7 (HTTP/HTTPS)</td><td>Layer 4 (DNS-based)</td></tr>
          <tr><td>Routing Method</td><td>Application-level routing</td><td>DNS-level redirection</td></tr>
          <tr><td>Performance</td><td>Faster (edge caching, SSL termination)</td><td>DNS TTL-based, slower updates</td></tr>
          <tr><td>Use Case</td><td>Global web applications</td><td>Global failover and endpoint health checks</td></tr>
        </table>
        <p><strong>Rule of Thumb:</strong> Use <em>Front Door</em> for HTTP(S) traffic optimization, <em>Traffic Manager</em> for global failover or non-HTTP apps.</p>
      </div>`},{question:"CDN vs Traffic Manager  main differences and placement in architecture?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CDN vs Traffic Manager</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>CDN (Content Delivery Network):</strong> Caches static content (images, videos, JS) at edge POPs for faster delivery.</li>
          <li><strong>Traffic Manager:</strong> Routes users to the nearest or healthiest endpoint using DNS-based redirection.</li>
        </ul>
        <p><strong>Placement:</strong> CDN sits in front of app/web layer  improves latency.  
        Traffic Manager sits above multiple app endpoints  controls routing.</p>
        <p><strong>In Practice:</strong> I often use CDN + Front Door together  CDN for static content, Front Door for dynamic routing & WAF.</p>
      </div>`},{question:"Types of Load Balancer  explain internal vs external scenarios.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Internal vs External Load Balancer</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>External Load Balancer:</strong> Has public IP, routes traffic from internet to backend pool (used for web apps, public endpoints).</li>
          <li> <strong>Internal Load Balancer:</strong> Has private IP, routes only internal (intranet) traffic between VNets/subnets (used for DB/app tiers).</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy a public-facing App GW and an internal Load Balancer for backend microservices within the same VNet.</p>
      </div>`},{question:"Application Gateway (Standard vs WAF SKU)  differences and selection.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Application Gateway SKUs</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Feature</th><th>Standard_v2</th><th>WAF_v2</th></tr>
          <tr><td>Purpose</td><td>Load balancing (L7)</td><td>Load balancing + Web Application Firewall</td></tr>
          <tr><td>Security</td><td>No WAF protection</td><td>OWASP protection, SQLi/XSS defense</td></tr>
          <tr><td>Mode</td><td>Detection only</td><td>Detection or Prevention mode</td></tr>
          <tr><td>Cost</td><td>Lower</td><td>Higher (due to WAF engine)</td></tr>
        </table>
        <p><strong>In Practice:</strong> I always choose <em>WAF_v2</em> for internet-facing applications to meet compliance and OWASP security standards.</p>
      </div>`},{question:"Private Endpoint vs Service Endpoint  differences and selection.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Private Endpoint vs Service Endpoint</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Feature</th><th>Private Endpoint</th><th>Service Endpoint</th></tr>
          <tr><td>Connection Type</td><td>Private IP within VNet (via NIC)</td><td>Public IP secured at network level</td></tr>
          <tr><td>Access Path</td><td>Traffic stays within Azure backbone</td><td>Uses public IP but restricted via VNet ACL</td></tr>
          <tr><td>Security</td><td>Highest (private IP)</td><td>Moderate (depends on NSG rules)</td></tr>
          <tr><td>Use Case</td><td>When you need full private access to PaaS (e.g., Key Vault, Storage)</td><td>When you just want secure public access from specific VNets</td></tr>
        </table>
        <p><strong>Rule of Thumb:</strong> Always use <em>Private Endpoint</em> for sensitive workloads  eliminates internet exposure completely.</p>
      </div>`}]},{title:"15. Real-World Scenarios",questions:[{question:"We have VM, Storage Account, SQL DB, App Service in one subnet, and a Private Endpoint on Storage  how will the VM resolve the Storage FQDN privately?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Private DNS Resolution with Private Endpoint</strong></h3>
        <p>
          When you create a <strong>Private Endpoint</strong> for a Storage Account, Azure automatically creates a Private DNS Zone (e.g., <code>privatelink.blob.core.windows.net</code>)
          and links it to the VNet. This ensures that the VM in the same VNet resolves the Storage FQDN privately.
        </p>
        <h4> Step-by-Step:</h4>
        <ul style="margin-left:1.2rem;">
          <li>1 Create Private Endpoint for the Storage Account (blob/file/table as needed).</li>
          <li>2 Azure auto-creates DNS zone: <code>privatelink.blob.core.windows.net</code>.</li>
          <li>3 Link that DNS zone to the VNet where the VM resides.</li>
          <li>4 VM DNS lookup for <code>storageaccount.blob.core.windows.net</code>  resolves to Private IP of the PE.</li>
        </ul>
        <p><strong>In Practice:</strong> I always verify using <code>nslookup</code> or <code>Resolve-DnsName</code> from the VM  it must show a private IP (10.x or 172.x).</p>
      </div>`},{question:"AKS app is slow and has DNS resolution errors  walk through end-to-end troubleshooting (CoreDNS, Private DNS Zone links, NSGs/UDRs, health probes).",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS DNS & Performance Troubleshooting</strong></h3>
        <h4> Step-by-Step Debug Flow:</h4>
        <ul style="margin-left:1.2rem;">
          <li>1 <strong>Check CoreDNS:</strong> Run <code>kubectl logs -n kube-system -l k8s-app=kube-dns</code>  look for timeouts or failures.</li>
          <li>2 <strong>Validate DNS Zone Link:</strong> Ensure AKS VNet is linked to the Private DNS Zone if workloads resolve internal endpoints (e.g., Storage PE).</li>
          <li>3 <strong>NSG & UDR Check:</strong> Confirm no NSG rules or UDRs are blocking port 53 (DNS) or outbound internet access if needed.</li>
          <li>4 <strong>Node Health:</strong> Run <code>kubectl get nodes -o wide</code> and verify Ready status and outbound connectivity.</li>
          <li>5 <strong>App Gateway / Load Balancer:</strong> Check backend health probes  unhealthy backends often show DNS resolution delay.</li>
          <li>6 <strong>Resource Saturation:</strong> If CoreDNS pods are throttled (CPU), increase replica count.</li>
        </ul>
        <p><strong>Root Cause Examples:</strong> Missing private DNS link or throttled CoreDNS pods.  
        <strong>Fix:</strong> Re-link private DNS zone and scale CoreDNS deployment via <code>kubectl scale</code>.</p>
      </div>`},{question:"How have you used Azure Load Balancers (internal/external) and Application Gateway? Give examples.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Load Balancer & App Gateway Use Cases</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>External Load Balancer:</strong> Used for exposing app endpoints (e.g., NGINX ingress controller in AKS) to internet.</li>
          <li> <strong>Internal Load Balancer:</strong> Used for backend microservices or DB tier  accessible only within the VNet.</li>
          <li> <strong>Application Gateway (WAF_v2):</strong> Used as a layer-7 reverse proxy with WAF  TLS termination, routing, and security.</li>
        </ul>
        <p><strong>Example:</strong> App Gateway (public)  AKS ingress (internal LB)  microservices.  
        This gives complete control + isolation with zero public exposure to pods.</p>
      </div>`},{question:"How do you secure Storage Account access? (Private endpoints, deny public, SAS, scoped tokens, firewall, RBAC)",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Storage Account Security Model</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 <strong>Disable Public Access:</strong> Set <code>AllowPublicAccess = false</code> for all containers.</li>
          <li>2 <strong>Private Endpoint:</strong> Access via private IP inside VNet (no internet exposure).</li>
          <li>3 <strong>RBAC:</strong> Grant least-privilege roles like <em>Storage Blob Data Reader</em>.</li>
          <li>4 <strong>Firewall:</strong> Restrict to specific VNets or IP ranges.</li>
          <li>5 <strong>SAS Tokens:</strong> Issue time-bound and permission-scoped access links.</li>
          <li>6 <strong>Managed Identity:</strong> Use for automation and apps instead of keys.</li>
        </ul>
        <p><strong>In Practice:</strong> My standard policy: no storage without PE + firewall + RBAC.  
        Every pipeline fetches secrets using Managed Identity, not shared keys.</p>
      </div>`},{question:"Tell me where you used NSG and Virtual Network Gateway in your project.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>NSG & Virtual Network Gateway  Real Project Example</strong></h3>
        <p>Both play key roles in Azure hybrid connectivity and traffic control.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>NSG (Network Security Group):</strong>
            <ul>
              <li>Applied on subnets and NICs to control inbound/outbound rules.</li>
              <li>Used for restricting app-tier to DB-tier (only 1433 allowed).</li>
              <li>Blocked all outbound internet traffic except updates.</li>
            </ul>
          </li>
          <li> <strong>Virtual Network Gateway:</strong>
            <ul>
              <li>Configured for site-to-site VPN to connect on-prem datacenter.</li>
              <li>Used with ExpressRoute for private connectivity.</li>
              <li>In project: Hybrid healthcare setup  Azure VNet  On-prem EMR servers via VPN Gateway.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> NSG handles east-west isolation, and VPN Gateway handles north-south connectivity to on-prem.</p>
      </div>`}]},{title:"16. Cross-Cloud & Edge Case (Bonus)",questions:[{question:"Your EC2 instance in a private subnet must download packages but no NAT Gateway exists  what alternatives (VPC endpoints, Instance Connect, proxy, S3/Dynamo endpoints) apply?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Accessing Internet from Private EC2 without NAT Gateway</strong></h3>
        <p>
          Normally, private subnets use a <strong>NAT Gateway</strong> to reach the internet for OS/package updates.
          But when NAT isnt available, we can use the following alternatives depending on the requirement:
        </p>

        <h4> <strong>1 VPC Interface Endpoints (AWS PrivateLink)</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Use AWS <strong>VPC Endpoints</strong> for essential services like <em>S3, ECR, DynamoDB, Systems Manager</em>.</li>
          <li>These allow private access via AWS backbone  no internet routing needed.</li>
        </ul>

        <h4> <strong>2 SSM Agent (AWS Systems Manager)</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Install <strong>SSM Agent</strong> on EC2 and use <em>Session Manager</em> for secure remote access without SSH/public IP.</li>
          <li>It can also push patches or pull packages via AWS internal APIs.</li>
        </ul>

        <h4> <strong>3 HTTP/HTTPS Proxy</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Configure a <strong>proxy server</strong> (like Squid or corporate proxy) in a public subnet.</li>
          <li>Route private EC2 outbound traffic through that proxy for package downloads.</li>
        </ul>

        <h4> <strong>4 S3 & DynamoDB Gateway Endpoints</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Gateway Endpoints</strong> for S3 or DynamoDB access  fully private and free of data transfer cost.</li>
        </ul>

        <p><strong>In Practice:</strong> I avoid NAT in non-production to save cost  instead use VPC endpoints + SSM Session Manager for controlled patching and connectivity.</p>
      </div>`},{question:"What is the difference between PaaS, IaaS, and SaaS in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>PaaS vs IaaS vs SaaS in Azure</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr>
            <th>Category</th>
            <th>Responsibility</th>
            <th>Examples</th>
            <th>Use Case</th>
          </tr>
          <tr>
            <td><strong>IaaS</strong></td>
            <td>User manages OS, runtime, security, and scaling.</td>
            <td>Azure VM, VNet, Load Balancer, Storage Account.</td>
            <td>Full control  ideal for legacy apps or custom infra.</td>
          </tr>
          <tr>
            <td><strong>PaaS</strong></td>
            <td>Azure manages platform, user manages app code/config.</td>
            <td>App Service, AKS, SQL Database, Functions.</td>
            <td>Focus on app development  no server management.</td>
          </tr>
          <tr>
            <td><strong>SaaS</strong></td>
            <td>Everything managed by provider  user just consumes service.</td>
            <td>Microsoft 365, Power BI, Azure DevOps, Salesforce.</td>
            <td>End-user ready services  no infra or patching required.</td>
          </tr>
        </table>
        <p><strong>In Practice:</strong> I use a mix  IaaS for custom tools, PaaS for app hosting, and SaaS for collaboration (ADO, M365). The balance depends on control vs convenience needs.</p>
      </div>`}]},{title:"17. Misc / Screening (Non-Tech)",questions:[{question:"Are you open to working from office 3 days a week and relocation?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Work Mode & Relocation Preference</strong></h3>
        <p>
          Yes, Im open to a hybrid work model  working from the office a few days a week is perfectly fine for collaboration and project alignment.  
          I understand that DevOps often requires close coordination with development and operations teams, so being on-site part-time is productive.
        </p>
        <p>
          Regarding relocation  Im flexible based on project needs and company policy.  
          My main focus is on contributing effectively to the project and ensuring smooth CI/CD operations,  
          so if relocation adds value to that, Im absolutely open to it.
        </p>
        <p><strong>In Practice:</strong> Ive already worked in both remote and hybrid setups  adapting to either isnt a problem for me as long as communication and delivery stay efficient.</p>
      </div>`}]},{title:"18. Azure Infrastructure & Networking",questions:[{question:"How can we connect to VPN securely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Connecting to VPN Securely in Azure</strong></h3>
        <p>
          In Azure, a <strong>VPN Gateway</strong> allows secure, encrypted communication between on-premises networks and Azure VNets using IPsec/IKE protocols.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Point-to-Site (P2S):</strong> Used by individual clients to connect securely to Azure using certificates or Azure AD authentication.</li>
          <li> <strong>Site-to-Site (S2S):</strong> Establishes a tunnel between on-premises firewall/VPN device and Azure VPN Gateway.</li>
          <li> <strong>ExpressRoute + VPN:</strong> For hybrid setups, adds an encrypted overlay on top of private connections.</li>
          <li> <strong>Security:</strong> IPsec encryption (AES256/SHA256), certificate-based authentication, and enforced MFA for client VPNs.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure Site-to-Site VPN for hybrid setups using BGP dynamic routing and certificate-based P2S VPN for remote developer access with conditional MFA.</p>
      </div>`},{question:"What is VNet Peering?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VNet Peering  Network Connectivity Between VNets</strong></h3>
        <p>
          <strong>VNet Peering</strong> allows direct, low-latency private communication between two Azure VNets using Microsofts backbone network (no public internet).
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Enables resource communication across VNets as if they were in the same network.</li>
          <li> Supports peering across regions (Global VNet Peering).</li>
          <li> No bandwidth bottleneck  uses Azure backbone, not VPN.</li>
          <li> Can enable/disable traffic forwarding, gateway transit, and remote access.</li>
        </ul>
        <pre><code># Example - Create VNet peering
az network vnet peering create \\
  --name hub-to-spoke \\
  --resource-group rg-network \\
  --vnet-name hub-vnet \\
  --remote-vnet spoke-vnet \\
  --allow-vnet-access</code></pre>
        <p><strong>In Practice:</strong> I use hub-spoke architecture with VNet peering  hub hosts shared services (firewall, jumpbox), spokes contain isolated app environments.</p>
      </div>`},{question:"Explain Basic Load Balancer (Layer 4).",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Load Balancer (Layer 4 - Transport Layer)</strong></h3>
        <p>
          The <strong>Azure Load Balancer</strong> operates at Layer 4 (TCP/UDP) and distributes traffic among backend resources like VMs or VMSS based on IP and port.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Two types: <strong>Public</strong> (internet-facing) and <strong>Internal</strong> (private network only).</li>
          <li> Uses <strong>health probes</strong> to detect unhealthy instances.</li>
          <li> Supports inbound NAT rules for RDP/SSH access.</li>
          <li> Provides ultra-low latency and high throughput.</li>
        </ul>
        <pre><code># Example: Create public Load Balancer
az network lb create --resource-group rg-prod --name myLB --sku Basic --frontend-ip-name myFront --backend-pool-name myBackPool</code></pre>
        <p><strong>In Practice:</strong> I use Basic Load Balancer for internal non-critical workloads, while Standard SKU with zone redundancy for production-grade deployments.</p>
      </div>`},{question:"Explain Application Gateway (Layer 7).",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Application Gateway  Layer 7 Load Balancer</strong></h3>
        <p>
          <strong>Application Gateway</strong> works at the <strong>Application Layer (HTTP/HTTPS)</strong> and routes traffic based on URL path or hostname.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Supports SSL termination, cookie-based affinity, and session persistence.</li>
          <li> Components include Frontend IP, Listener, Routing Rules, Backend Pools, and Health Probes.</li>
          <li> Can be deployed with <strong>WAF (Web Application Firewall)</strong> for OWASP threat protection.</li>
          <li> Integrates natively with AKS as an <strong>Ingress Controller</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Application Gateway WAF_v2 for web applications to handle HTTPS routing, enforce SSL policies, and protect from SQLi/XSS attacks.</p>
      </div>`},{question:"What is an Application Gateway and what are its components?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Application Gateway  Key Components</strong></h3>
        <p>
          The <strong>Application Gateway</strong> is a Layer 7 load balancer with intelligent routing capabilities. Its main components are:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Frontend IP Configuration:</strong> Public or private endpoint for client access.</li>
          <li> <strong>Listener:</strong> Listens on a port/protocol (e.g., HTTPS:443) and maps requests to routing rules.</li>
          <li> <strong>Routing Rules:</strong> Define how traffic is routed to backend pools.</li>
          <li> <strong>Backend Pool:</strong> Set of targets (VMs, App Services, or AKS ingress pods).</li>
          <li> <strong>Health Probes:</strong> Periodically check backend health to ensure routing only to healthy instances.</li>
          <li> <strong>WAF Policy (optional):</strong> Protects against Layer 7 attacks.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure separate listeners for each app domain (e.g., API, frontend) and route them via URL path-based rules to backend microservices running in AKS.</p>
      </div>`},{question:"How do you apply NSG (Network Security Group) in your network?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Applying Network Security Groups (NSG) in Azure</strong></h3>
        <p>
          <strong>NSGs</strong> control inbound and outbound traffic to Azure resources at subnet or NIC level based on defined security rules.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Rules include source/destination IP, port, protocol, and priority (1004096).</li>
          <li> Applied at <strong>Subnet level</strong> for group control or <strong>NIC level</strong> for fine-grained access.</li>
          <li> Can integrate with Azure Firewall for multi-layer protection.</li>
        </ul>
        <pre><code># Example: Apply NSG to Subnet
az network vnet subnet update --vnet-name myVnet --name backend-subnet --network-security-group myNSG</code></pre>
        <p><strong>In Practice:</strong> I create NSG per subnet (web, app, db) and apply rules following least-privilege principle  e.g., only app subnet can talk to DB subnet on 1433.</p>
      </div>`},{question:"Did you implement firewall with NSG?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Combining Azure Firewall with NSGs</strong></h3>
        <p>
          Yes  I use both <strong>Azure Firewall</strong> and <strong>NSGs</strong> together for layered network security.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>NSG:</strong> Controls basic traffic filtering at subnet/NIC level (Layer 3 & 4).</li>
          <li> <strong>Azure Firewall:</strong> Provides centralized Layer 7 protection, application rules, FQDN filtering, and logging.</li>
          <li> Combined via <strong>Hub-Spoke</strong> model  spokes route outbound traffic to firewall using <strong>UDRs (User Defined Routes)</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy Azure Firewall in the hub VNet and associate NSGs to spoke subnets, ensuring both perimeter and subnet-level isolation.</p>
      </div>`},{question:"What resources have you used to enhance network security? (Subnets, NSG, Firewall)",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Network Security Design in Azure</strong></h3>
        <p>
          To enhance network security, I implement a layered defense approach combining Azure networking resources:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Subnets:</strong> Segmented by tiers  Web, App, DB  for traffic isolation.</li>
          <li> <strong>NSGs:</strong> Enforce inbound/outbound rules per subnet (e.g., only App  DB on 1433).</li>
          <li> <strong>Azure Firewall:</strong> Centralized control for FQDN filtering, logging, and threat intelligence.</li>
          <li> <strong>Private Endpoints:</strong> Secure access to PaaS services without public exposure.</li>
          <li> <strong>UDRs:</strong> Direct outbound traffic through Firewall for inspection.</li>
          <li> <strong>Azure DDoS Protection:</strong> Shields VNets from volumetric and protocol attacks.</li>
        </ul>
        <p><strong>In Practice:</strong> I architect networks using a <strong>Hub-Spoke model</strong>  hub hosts shared security controls (Firewall, Bastion), while spokes host application subnets protected by NSGs and private endpoints.</p>
      </div>`},{question:"Difference between Service Endpoints and Private Endpoints?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Service Endpoint vs Private Endpoint  Key Differences</strong></h3>
        <p>Both enhance security by keeping Azure PaaS traffic within the Azure backbone network, but their implementation differs fundamentally:</p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Service Endpoint</th><th>Private Endpoint</th></tr>
          <tr><td>Network Access</td><td>Extends VNet identity to Azure services</td><td>Creates a private IP inside the VNet for the service</td></tr>
          <tr><td>Public Exposure</td><td>Service still has a public IP</td><td>No public exposure  accessed only via private IP</td></tr>
          <tr><td>Isolation Level</td><td>Network-level isolation</td><td>Full data-plane isolation (private connectivity)</td></tr>
          <tr><td>DNS Handling</td><td>Uses public DNS</td><td>Uses private DNS zone for name resolution</td></tr>
          <tr><td>Best For</td><td>Low-risk, internal PaaS traffic</td><td>High-security workloads needing strict isolation</td></tr>
        </table>
        <p><strong>In Practice:</strong> I prefer <strong>Private Endpoints</strong> for databases and Key Vaults in production, while <strong>Service Endpoints</strong> are suitable for internal, non-critical services.</p>
      </div>`},{question:"What is the role of Azure Bastion in network security?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Bastion  Secure Remote Access</strong></h3>
        <p>
          <strong>Azure Bastion</strong> provides <strong>secure RDP/SSH access</strong> to Azure VMs directly from the Azure portal, without exposing any public IPs.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Eliminates the need for inbound port 22/3389 on VMs.</li>
          <li> Connections occur over SSL via Azure portal (no local client needed).</li>
          <li> Deployed in a dedicated subnet named <code>AzureBastionSubnet</code>.</li>
          <li> Integrates seamlessly with RBAC and NSG rules for granular access control.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Bastion for admin access to production VMs  it ensures zero public IP exposure and enforces corporate access policies (MFA + RBAC).</p>
      </div>`},{question:"Can you elaborate on Availability Zones in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Availability Zones  High Availability Design</strong></h3>
        <p>
          <strong>Availability Zones (AZs)</strong> are physically separate data centers within a single Azure region  each with independent power, cooling, and networking.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Protects workloads from datacenter-level failures.</li>
          <li> Each zone is identified as Zone 1, Zone 2, Zone 3.</li>
          <li> Provides high availability for critical apps via zone-redundant deployments (e.g., ZRS storage, zone-aware VMSS).</li>
          <li> Supports cross-zone load balancing and replication.</li>
        </ul>
        <p><strong>In Practice:</strong> I distribute VMs and AKS node pools across multiple zones to ensure resiliency and maintain 99.99% SLA for production environments.</p>
      </div>`},{question:"What is an Availability Set in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Availability Set  Fault & Update Domain Protection</strong></h3>
        <p>
          An <strong>Availability Set</strong> is a logical grouping of VMs that ensures theyre distributed across multiple <strong>fault domains</strong> (hardware racks) and <strong>update domains</strong> (software maintenance groups).
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Fault Domains  Protect from hardware/power failures.</li>
          <li> Update Domains  Prevent downtime during system updates.</li>
          <li> Recommended for single-region HA setups.</li>
        </ul>
        <pre><code># Create VM in an Availability Set
az vm create --resource-group rg-prod --name appVM1 --availability-set as-prod</code></pre>
        <p><strong>In Practice:</strong> I use Availability Sets for critical workloads in single-zone regions, while preferring Zones for higher fault isolation in multi-zone regions.</p>
      </div>`},{question:"What arguments are required when creating a Resource Group?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Required Parameters for Resource Group Creation</strong></h3>
        <p>
          A Resource Group (RG) acts as a logical container for related Azure resources. Only two key arguments are mandatory during creation:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Name:</strong> Unique identifier within the subscription (e.g., <code>rg-prod</code>).</li>
          <li> <strong>Location:</strong> Azure region where metadata will be stored (e.g., <code>eastus</code>).</li>
        </ul>
        <pre><code>az group create --name rg-prod --location eastus</code></pre>
        <p><strong>In Practice:</strong> I follow naming conventions like <code>rg-&lt;env&gt;-&lt;app&gt;</code> and apply tags (Owner, CostCenter, Env) to maintain governance and cost visibility.</p>
      </div>`},{question:"What resources do you use when creating a VM in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure VM  Dependent Resources</strong></h3>
        <p>
          Creating a VM in Azure automatically provisions several dependent network and storage resources:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>VNet & Subnet:</strong> Provides network isolation and IP addressing.</li>
          <li> <strong>NIC (Network Interface):</strong> Connects VM to subnet.</li>
          <li> <strong>NSG (Network Security Group):</strong> Manages inbound/outbound rules.</li>
          <li> <strong>Public IP (optional):</strong> For external access.</li>
          <li> <strong>OS Disk + Data Disks:</strong> Storage for system and application data.</li>
          <li> <strong>Availability Options:</strong> Availability Set or Zone (for HA).</li>
        </ul>
        <p><strong>In Practice:</strong> I define these resources in Terraform or ARM templates to ensure consistent VM provisioning with tagging and security configurations baked in.</p>
      </div>`},{question:"Difference between VM and VMSS (Virtual Machine Scale Sets).",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VM vs VMSS  Comparison</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>VM (Virtual Machine)</th><th>VMSS (Virtual Machine Scale Set)</th></tr>
          <tr><td>Type</td><td>Single compute instance</td><td>Group of identical, auto-scalable VMs</td></tr>
          <tr><td>Scaling</td><td>Manual scaling only</td><td>Automatic scaling based on metrics (CPU, memory, etc.)</td></tr>
          <tr><td>Load Balancing</td><td>Configured manually</td><td>Integrated with Azure Load Balancer or App Gateway</td></tr>
          <tr><td>Management</td><td>Individual</td><td>Centralized (template-based)</td></tr>
          <tr><td>Use Case</td><td>Custom workloads, stateful apps</td><td>Web servers, API services, stateless workloads</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use VMSS for web tiers that need horizontal scaling and traditional VMs for stateful backend systems like DB servers.</p>
      </div>`},{question:"If you need to create two VMs and two databases, which approach will you follow: VM or VMSS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Choosing Between VM and VMSS for Mixed Workloads</strong></h3>
        <p>
          The choice depends on workload characteristics  stateless vs stateful:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>VMSS:</strong> For the two application VMs (web/app servers). Enables auto-scaling, LB integration, and uniform configuration.</li>
          <li> <strong>Standalone VMs:</strong> For the two databases. Databases are stateful, require persistent data and cannot be auto-scaled.</li>
        </ul>
        <p><strong>Architecture Example:</strong>  
          <ul style="margin-left:1.2rem;">
            <li>App Tier  VMSS behind Load Balancer</li>
            <li>DB Tier  VMs in Availability Set with Managed Disks</li>
          </ul>
        </p>
        <p><strong>In Practice:</strong> I follow a layered approach  VMSS for front-end scaling, dedicated HA VMs for databases, and secure both tiers with NSGs and private endpoints.</p>
      </div>`},{question:"How do you manage multiple Azure subscriptions in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Multiple Azure Subscriptions in Azure DevOps</strong></h3>
        <p>
          When managing multiple Azure subscriptions, we use <strong>Service Connections</strong> and <strong>Service Principals</strong> in Azure DevOps to securely authenticate pipelines.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Create one <strong>Service Connection</strong> per subscription, linked via a Service Principal (SPN).</li>
          <li> Assign least-privilege RBAC roles (Contributor / Reader / Custom) per subscription.</li>
          <li> Use variable groups or pipeline parameters to dynamically target subscriptions.</li>
          <li> Optionally use a <strong>Management Group</strong> for policy and billing hierarchy.</li>
        </ul>
        <pre><code># Example: Use different service connections in YAML
- task: AzureCLI@2
  inputs:
    azureSubscription: 'Prod-Sub-Connection'
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      az account show
      az group list</code></pre>
        <p><strong>In Practice:</strong> I use service connection naming standards (e.g., <code>sc-sub-prod</code>, <code>sc-sub-dev</code>) and store subscription IDs in variable groups to easily switch contexts between subscriptions during pipeline runs.</p>
      </div>`},{question:"How would you manage 1500 subscriptions from Azure DevOps level?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing 1500 Subscriptions  Scalable Approach</strong></h3>
        <p>
          Handling 1500 subscriptions manually is not scalable. I use a combination of <strong>Azure Management Groups</strong>, <strong>Service Principals with broad scope</strong>, and automation scripts.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Organize subscriptions under <strong>Management Groups</strong>  by environment (Prod/Dev/Test).</li>
          <li> Use a single <strong>Service Principal</strong> with delegated access to multiple management groups.</li>
          <li> Use <strong>Terraform or ARM templates</strong> for bulk operations.</li>
          <li> Automate onboarding using Azure REST APIs via pipelines (create, assign, audit subscriptions).</li>
          <li> Centralize policies, RBAC roles, and tagging via Azure Policy or Blueprints.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive used a single ADO pipeline with parameterized loops to deploy diagnostic settings, NSGs, and policies across hundreds of subscriptions in parallel using a single SPN token.</p>
      </div>`},{question:"What is a Service Principal and Managed Identity?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Service Principal vs Managed Identity  Authentication in Azure</strong></h3>
        <p>
          Both are used for non-interactive authentication between Azure resources and services, without using user credentials.
        </p>
        <h4> <strong>Service Principal:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Created in Azure AD to represent apps or automation tools.</li>
          <li>Requires <strong>App ID, Tenant ID, and Client Secret</strong> or certificate.</li>
          <li>Used by Terraform, pipelines, or CLI to deploy/manage Azure resources.</li>
        </ul>
        <h4> <strong>Managed Identity (MI):</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Automatically created and managed by Azure for resources like VMs, AKS, Functions.</li>
          <li>No secret rotation  Azure handles authentication internally.</li>
          <li>Ideal for secure resource-to-resource access (e.g., VM to Key Vault).</li>
        </ul>
        <p><strong>In Practice:</strong> I use Service Principals for DevOps pipelines and Managed Identities for runtime authentication from AKS or Function Apps to Key Vaults or Storage Accounts.</p>
      </div>`},{question:"What is Least Privilege Access?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Principle of Least Privilege Access</strong></h3>
        <p>
          The <strong>Least Privilege Principle</strong> means giving users, apps, or services the minimal level of permissions required to perform their job  nothing more.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Enforced via Azure RBAC and custom roles.</li>
          <li> Reduces the blast radius in case of a security breach.</li>
          <li> Often combined with <strong>Azure AD Privileged Identity Management (PIM)</strong> for just-in-time access.</li>
          <li> Prevents overprivileged Service Principals and users.</li>
        </ul>
        <p><strong>In Practice:</strong> I assign contributor access at resource group level only where required, create custom roles for pipelines, and review permissions monthly using Access Reviews.</p>
      </div>`},{question:"What is the difference between public and private IPs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Public IP vs Private IP  Azure Networking</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Public IP</th><th>Private IP</th></tr>
          <tr><td>Scope</td><td>Global  accessible from the Internet</td><td>Local  only within VNets or on-prem</td></tr>
          <tr><td>Usage</td><td>Used for internet-facing resources</td><td>Used for internal communication</td></tr>
          <tr><td>Example</td><td>Load Balancer frontend, Bastion, Application Gateway</td><td>VMs, Databases, Private Endpoints</td></tr>
          <tr><td>Security</td><td>Needs NSG/firewall restrictions</td><td>Inherently more secure (non-routable over internet)</td></tr>
        </table>
        <p><strong>In Practice:</strong> I assign public IPs only via front-end load balancers and keep all backend VMs and databases strictly on private IPs for zero external exposure.</p>
      </div>`},{question:"What is a Service Endpoint and Private Link?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Service Endpoint vs Private Link</strong></h3>
        <p>
          Both secure access to Azure PaaS services, but the isolation level differs:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Service Endpoint:</strong> Extends your VNets identity to Azure services, ensuring traffic stays within Microsoft backbone, but service still uses a public IP.</li>
          <li> <strong>Private Link (Private Endpoint):</strong> Maps the PaaS resource directly to a private IP within your VNet  full data isolation with no public internet access.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Private Links for SQL, Key Vault, and Storage Accounts in production; Service Endpoints for low-risk services like Event Hub or Service Bus in dev/test.</p>
      </div>`},{question:"What is Bastion Host and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Bastion Host  Secure VM Access</strong></h3>
        <p>
          <strong>Bastion Host</strong> is a managed service that allows secure RDP/SSH access to Azure VMs directly through the Azure portal, without requiring public IPs.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Eliminates inbound port exposure (22/3389).</li>
          <li> Connections occur over HTTPS (port 443) through Azure portal.</li>
          <li> Deployed in dedicated <code>AzureBastionSubnet</code> within the VNet.</li>
          <li> Integrates with RBAC and NSG for access control.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy Bastion in the hub VNet of a hub-spoke architecture  it gives secure remote access to private VMs in spoke VNets via peering, without exposing any public IPs.</p>
      </div>`},{question:"What is Azure Function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Function  Event-driven Serverless Compute</strong></h3>
        <p>
          <strong>Azure Functions</strong> allow you to run small pieces of code ("functions") in response to events without provisioning or managing servers.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Triggers: HTTP requests, Timer schedules, Blob/Queue events, or Service Bus messages.</li>
          <li> Auto-scales based on event volume.</li>
          <li> Supports multiple languages (C#, Python, PowerShell, Node.js).</li>
          <li> Integrates seamlessly with Azure services like Key Vault, Event Grid, and Logic Apps.</li>
        </ul>
        <p><strong>Use Cases:</strong> Automation, scheduled cleanup jobs, event-based notifications, and CI/CD integration.</p>
        <p><strong>In Practice:</strong> I use Azure Functions to trigger resource cleanup after deployments, rotate secrets automatically, and notify teams post-deployment via Teams webhook.</p>
      </div>`},{question:"How do you horizontally scale your app services?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Horizontal Scaling of Azure App Services</strong></h3>
        <p>
          Horizontal scaling (Scale-Out) in Azure App Services means adding more instances to handle increased traffic and load. It improves availability and performance without changing app configuration.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Manual Scaling:</strong> Manually increase instance count from the Azure portal or CLI.</li>
          <li> <strong>Autoscaling:</strong> Configure rules based on CPU %, memory, HTTP queue length, or custom metrics (from Application Insights).</li>
          <li> <strong>Scale-Up (Vertical):</strong> Change App Service Plan SKU (e.g., B1  P2V3) to get more CPU/RAM.</li>
          <li> <strong>Integration:</strong> Use App Service Environments (ASE) for enterprise-grade isolation and scaling.</li>
        </ul>
        <h4> Example Autoscale Rule:</h4>
        <pre><code>If CPU > 70% for 10 minutes  Add 1 instance
If CPU < 40% for 15 minutes  Remove 1 instance</code></pre>
        <p><strong>In Practice:</strong> I configure autoscale profiles for working hours (higher limits) and off-hours (lower instance count) to optimize cost and performance using Azure Monitor metrics.</p>
      </div>`},{question:"What are the different replication types in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Storage Replication Types</strong></h3>
        <p>
          Azure provides multiple replication options for durability and high availability of storage data. Each option offers different fault tolerance levels:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>LRS (Locally Redundant Storage):</strong> 3 copies of data within a single datacenter.</li>
          <li> <strong>ZRS (Zone-Redundant Storage):</strong> Data replicated across 3 availability zones in a region.</li>
          <li> <strong>GRS (Geo-Redundant Storage):</strong> Copies data to a paired region (6 total copies).</li>
          <li> <strong>RA-GRS (Read-Access Geo-Redundant Storage):</strong> Same as GRS but with read access from secondary region.</li>
          <li> <strong>GZRS / RA-GZRS:</strong> Zone-redundant + geo-redundant combo for mission-critical workloads.</li>
        </ul>
        <p><strong>In Practice:</strong> I use <strong>ZRS</strong> for AKS logs/state and <strong>GZRS</strong> for production backups ensuring zero data loss even during regional failures.</p>
      </div>`},{question:"What is Azure Key Vault and how do you secure secrets?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Key Vault  Secrets Management</strong></h3>
        <p>
          <strong>Azure Key Vault (AKV)</strong> securely stores and manages secrets, encryption keys, and certificates for applications and pipelines.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Secrets:</strong> API keys, passwords, connection strings.</li>
          <li> <strong>Keys:</strong> Encryption/decryption keys for apps and databases.</li>
          <li> <strong>Certificates:</strong> SSL/TLS management with auto-renewal.</li>
        </ul>
        <h4> <strong>Securing Secrets:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> Enable <strong>Soft Delete</strong> and <strong>Purge Protection</strong> for accidental deletion prevention.</li>
          <li> Access control via <strong>RBAC</strong> or <strong>Access Policies</strong>.</li>
          <li> Integrate with <strong>Managed Identity</strong>  no need to expose credentials.</li>
          <li> Audit all access using <strong>Azure Monitor Logs</strong> and <strong>Diagnostic Settings</strong>.</li>
        </ul>
        <pre><code># Example: Get a secret securely
az keyvault secret show --vault-name myVault --name sqlPassword</code></pre>
        <p><strong>In Practice:</strong> I integrate AKV with DevOps pipelines and AKS using Managed Identity  ensuring no plaintext secrets are stored in code or variables.</p>
      </div>`},{question:"What is Azure Storage Account and how to secure it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Storage Account  Secure Cloud Storage</strong></h3>
        <p>
          An <strong>Azure Storage Account</strong> provides durable, scalable storage for data objects like Blobs, Files, Queues, and Tables.
        </p>
        <h4> <strong>Storage Types:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> Blob Storage  for unstructured data (images, logs, backups).</li>
          <li> File Shares  for SMB/NFS file storage.</li>
          <li> Queue Storage  for async messaging between components.</li>
          <li> Table Storage  for NoSQL structured data.</li>
        </ul>
        <h4> <strong>Securing a Storage Account:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> Disable public network access  use <strong>Private Endpoints</strong>.</li>
          <li> Enforce <strong>Azure AD-based authentication</strong> (avoid SAS tokens where possible).</li>
          <li> Restrict traffic via <strong>VNet service endpoints</strong> or <strong>firewall rules</strong>.</li>
          <li> Enable <strong>Soft Delete</strong> for blob recovery.</li>
          <li> Use <strong>Encryption at Rest (AES-256)</strong> and customer-managed keys from Key Vault.</li>
          <li> Enable <strong>Diagnostic Logging</strong> and access logs for audit.</li>
        </ul>
        <pre><code># Example: Disable public access
az storage account update --name mystorage --resource-group rg-prod --public-network-access Disabled</code></pre>
        <p><strong>In Practice:</strong> I always integrate storage accounts with private endpoints and Key Vault CMK encryption  ensuring full compliance with security and governance standards.</p>
      </div>`}]},{title:"19. Azure Services & Serverless",questions:[{question:"Have you used Azure Function App? What is its role?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Function App  Event-driven Serverless Compute</strong></h3>
        <p>
          Yes  Ive used <strong>Azure Function Apps</strong> for automation and event-based tasks in CI/CD and infrastructure automation.  
          Function Apps run code without managing infrastructure  scaling automatically based on event triggers.
        </p>
        <h4> <strong>Core Triggers:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> HTTP Trigger  for webhooks, REST APIs.</li>
          <li> Blob Trigger  runs when a file is uploaded to Azure Blob.</li>
          <li> Timer Trigger  scheduled jobs (like CRON).</li>
          <li> Service Bus / Event Hub Trigger  for message processing.</li>
        </ul>
        <h4> <strong>Common Use Cases:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> Automating resource cleanup or tagging.</li>
          <li> Running small micro-jobs post-deployment.</li>
          <li> Integrating with pipelines or alerting via webhooks.</li>
          <li> Auto-scaling or triggering workflows (via Logic Apps).</li>
        </ul>
        <p><strong>In Practice:</strong> I created a Function App to rotate Key Vault secrets automatically and send notifications to Teams via a webhook whenever a new deployment completed.</p>
      </div>`},{question:"Which Azure resource is used to distribute static and dynamic content?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure CDN (Content Delivery Network)  Global Content Distribution</strong></h3>
        <p>
          <strong>Azure CDN</strong> is used to distribute static (HTML, CSS, JS, images) and dynamic content globally with low latency and high performance by caching content at edge locations.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Works with Storage, Web Apps, and Front Door.</li>
          <li> Reduces latency by serving requests from nearest edge node.</li>
          <li> Supports dynamic site acceleration (DSA) for APIs.</li>
          <li> Integrated with WAF for content-level protection.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate CDN with App Service for static content caching and route traffic through Azure Front Door for global load balancing with SSL offloading.</p>
      </div>`},{question:"Have you used Azure App Service?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure App Service  Managed PaaS for Web Applications</strong></h3>
        <p>
          Yes  I use <strong>Azure App Service</strong> to host APIs, web apps, and backend microservices in a fully managed environment.  
          It abstracts the OS and runtime management while providing continuous deployment and scaling.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Supports multiple stacks  .NET, Node.js, Python, Java, PHP.</li>
          <li> Integrated CI/CD with Azure DevOps and GitHub.</li>
          <li> Built-in SSL, authentication (AAD, OAuth), and VNet integration.</li>
          <li> Scalable  vertical and horizontal scaling supported.</li>
        </ul>
        <p><strong>In Practice:</strong> I deployed backend APIs via App Service integrated with ACR for image-based deployment, and configured staging slots for zero-downtime release promotion.</p>
      </div>`},{question:"What is difference between App Service and VM Service?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>App Service vs Virtual Machine  Comparison</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>App Service</th><th>Virtual Machine (VM)</th></tr>
          <tr><td>Type</td><td>PaaS (Managed platform)</td><td>IaaS (Full OS control)</td></tr>
          <tr><td>Management</td><td>Azure handles OS, scaling, patching</td><td>User responsible for configuration, updates, patching</td></tr>
          <tr><td>Scaling</td><td>Auto-scale (horizontal/vertical)</td><td>Manual or script-based scaling</td></tr>
          <tr><td>Use Case</td><td>Web apps, APIs, microservices</td><td>Custom software, databases, legacy apps</td></tr>
          <tr><td>Cost</td><td>Pay per plan tier</td><td>Pay per VM resource allocation</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use App Service for stateless web workloads and VMs for backend or middleware requiring OS-level configurations and persistent state.</p>
      </div>`},{question:"What is Azure Front Door?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Front Door  Global Layer 7 Load Balancer</strong></h3>
        <p>
          <strong>Azure Front Door</strong> provides global HTTP/HTTPS load balancing, intelligent routing, caching, and security features for web applications.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Uses Microsofts global edge network for high availability and low latency.</li>
          <li> Offers <strong>Web Application Firewall (WAF)</strong> and SSL offloading.</li>
          <li> Routes based on latency, geo-location, or URL path.</li>
          <li> Supports custom domains and CDN-style acceleration.</li>
        </ul>
        <h4> Example Use Case:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Front Door routes <code>api.app.com</code>  AKS service in East US.</li>
          <li>Routes <code>ui.app.com</code>  App Service in West Europe.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Azure Front Door for global failover  routing traffic to backup region automatically if primary backend fails, ensuring 99.99% uptime.</p>
      </div>`},{question:"What is Azure Dashboard?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Dashboard  Unified Visibility</strong></h3>
        <p>
          <strong>Azure Dashboard</strong> is a customizable, interactive surface in the Azure Portal to monitor and manage your environment visually.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Combine tiles from Monitor, Application Insights, and Metrics.</li>
          <li> Useful for viewing CPU usage, cost analysis, and alerts in one place.</li>
          <li> Share dashboards with team members via RBAC access.</li>
          <li> Supports JSON export/import for IaC versioning.</li>
        </ul>
        <pre><code># Export dashboard JSON for automation
az portal dashboard show --name ProdDashboard</code></pre>
        <p><strong>In Practice:</strong> I create dashboards combining AKS node metrics, App Service performance, and budget utilization  all in a single operational view for DevOps and management teams.</p>
      </div>`},{question:"How do you configure auto-scaling for AKS using Azure native tools?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS Auto-Scaling  Native Azure Tools</strong></h3>
        <p>
          Azure Kubernetes Service (AKS) supports auto-scaling at both <strong>pod level</strong> and <strong>node level</strong> using native Azure and Kubernetes mechanisms.
        </p>
        <h4> <strong>Pod Level  Horizontal Pod Autoscaler (HPA):</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Scales pods based on CPU/memory utilization or custom metrics.</li>
          <li>Defined in YAML (minReplicas, maxReplicas, targetCPUUtilization).</li>
        </ul>
        <pre><code>kubectl autoscale deployment webapp --cpu-percent=70 --min=2 --max=10</code></pre>
        <h4> <strong>Node Level  Cluster Autoscaler:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Automatically adjusts node count in a node pool based on pending pods.</li>
          <li>Enabled via Azure CLI or ARM during cluster creation.</li>
        </ul>
        <pre><code>az aks update --resource-group rg-aks --name prod-aks --enable-cluster-autoscaler --min-count 3 --max-count 10</code></pre>
        <p><strong>In Practice:</strong> I use both HPA and Cluster Autoscaler together  HPA scales pods dynamically, and Cluster Autoscaler ensures adequate nodes for workload demand.  
        Monitoring and scaling thresholds are tracked using Azure Monitor with alert-based actions for proactive scaling.</p>
      </div>`}]}];function qt(e){const[t,r]=m.useState(new Set),[o,n]=m.useState(new Set);m.useEffect(()=>{const c=localStorage.getItem(`progress-${e}`);if(c)try{const p=JSON.parse(c);r(new Set(p.viewed||[])),n(new Set(p.bookmarked||[]))}catch(p){console.error("Failed to parse progress data",p)}},[e]),m.useEffect(()=>{const c={viewed:Array.from(t),bookmarked:Array.from(o)};localStorage.setItem(`progress-${e}`,JSON.stringify(c))},[t,o,e]);const s=c=>{r(p=>new Set([...p,c]))},i=c=>{n(p=>{const u=new Set(p);return u.has(c)?u.delete(c):u.add(c),u})},a=c=>t.has(c),d=c=>o.has(c);return{viewedCount:t.size,bookmarkedCount:o.size,markAsViewed:s,toggleBookmark:i,isViewed:a,isBookmarked:d}}const Ku=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,className:O("rounded-lg border bg-card text-card-foreground shadow-sm",e),...t}));Ku.displayName="Card";const Ww=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,className:O("flex flex-col space-y-1.5 p-6",e),...t}));Ww.displayName="CardHeader";const jw=m.forwardRef(({className:e,...t},r)=>l.jsx("h3",{ref:r,className:O("text-2xl font-semibold leading-none tracking-tight",e),...t}));jw.displayName="CardTitle";const Uw=m.forwardRef(({className:e,...t},r)=>l.jsx("p",{ref:r,className:O("text-sm text-muted-foreground",e),...t}));Uw.displayName="CardDescription";const $u=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,className:O("p-6 pt-0",e),...t}));$u.displayName="CardContent";const K3=m.forwardRef(({className:e,...t},r)=>l.jsx("div",{ref:r,className:O("flex items-center p-6 pt-0",e),...t}));K3.displayName="CardFooter";function Lt({totalQuestions:e,viewedCount:t,bookmarkedCount:r}){const o=Math.round(t/e*100),n=Math.round(r/e*100);return l.jsx(Ku,{className:"mb-8 border-2 shadow-card",children:l.jsx($u,{className:"pt-6",children:l.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-3 gap-6",children:[l.jsxs("div",{className:"text-center",children:[l.jsx("div",{className:"text-3xl font-bold text-primary mb-2",children:e}),l.jsx("div",{className:"text-sm text-muted-foreground",children:"Total Questions"})]}),l.jsxs("div",{className:"text-center",children:[l.jsxs("div",{className:"flex items-center justify-center gap-2 mb-2",children:[l.jsx(VS,{className:"h-5 w-5 text-accent"}),l.jsx("div",{className:"text-3xl font-bold text-accent",children:t})]}),l.jsxs("div",{className:"text-sm text-muted-foreground mb-2",children:["Viewed (",o,"%)"]}),l.jsx("div",{className:"w-full bg-muted rounded-full h-2",children:l.jsx("div",{className:"bg-accent h-2 rounded-full transition-all duration-500",style:{width:`${o}%`}})})]}),l.jsxs("div",{className:"text-center",children:[l.jsxs("div",{className:"flex items-center justify-center gap-2 mb-2",children:[l.jsx(pt,{className:"h-5 w-5 text-secondary"}),l.jsx("div",{className:"text-3xl font-bold text-secondary",children:r})]}),l.jsxs("div",{className:"text-sm text-muted-foreground mb-2",children:["Bookmarked (",n,"%)"]}),l.jsx("div",{className:"w-full bg-muted rounded-full h-2",children:l.jsx("div",{className:"bg-secondary h-2 rounded-full transition-all duration-500",style:{width:`${n}%`}})})]})]})})})}function Gw(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("azure"),a=cm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-blue-500 to-cyan-500 shadow-glow",children:l.jsx(ja,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Azure Core"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Microsoft Azure Cloud Platform Interview Questions"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Comprehensive Azure interview questions covering fundamentals, compute, networking, storage, security, and identity management."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:cm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:(u==null?void 0:u.question)||"",b=typeof u=="object"&&u&&"answer"in u&&!!u.answer,v=typeof u=="object"&&u&&"answerHtml"in u&&!!u.answerHtml,h=b?u.answer:null,w=b||v;return l.jsx("div",{className:"border-l-2 border-secondary/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-secondary font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),w&&l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:u.answerHtml}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:h})})]})]})]})},f)})})})]},p))})]})]})}const $3=Object.freeze(Object.defineProperty({__proto__:null,default:Gw},Symbol.toStringTag,{value:"Module"})),dm=[{title:"1. Azure DevOps Fundamentals",questions:[{question:"What is Azure DevOps and what are its main components?",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> <strong>What is Azure DevOps?</strong></h3>
          <p style="margin:0 0 .6rem; font-weight:600;">
            Azure DevOps is a Microsoft platform that helps teams plan, build, test and deliver software reliably  all in one place. It's used to implement CI/CD, track work, store code and manage packages.
          </p>

          <strong> Main Components (quick):</strong>
          <ul style="margin:.4rem 0 0 1.2rem; line-height:1.6;">
            <li> <strong>Azure Repos</strong>  Git-based source control (branches, PRs).</li>
            <li> <strong>Pipelines</strong>  Build & release automation (YAML or classic).</li>
            <li> <strong>Boards</strong>  Work tracking (stories, tasks, bugs).</li>
            <li> <strong>Artifacts</strong>  Package feed (npm, NuGet, Maven).</li>
            <li> <strong>Test Plans</strong>  Manual & automated test management.</li>
          </ul>

          <div style="margin:.6rem 0;">
            <strong> Simple use:</strong>
            <p style="margin:.25rem 0 0;">Use Repos for code, Pipelines for CI/CD, Boards for planning and Artifacts to share builds. This creates a repeatable, auditable delivery flow.</p>
          </div>

          <div style="margin:.6rem 0; padding:.5rem; background:rgba(255,255,255,0.02); border-radius:.4rem;">
            <strong>In practice:</strong>
            <p style="margin:.25rem 0 0;">I glue these together  code in Repos  YAML pipeline builds Docker image  publish to ACR  Helm/Terraform deploy to AKS  Boards link deployment to work item.</p>
          </div>
        </div>`},{question:"What are Repos, Pipelines, Boards, Artifacts, and Test Plans in ADO?",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> Short mapping  what does each service do?</h3>
          <ul style="margin:.4rem 0 0 1.2rem; line-height:1.6;">
            <li> <strong>Repos</strong>  store code, manage branches, create PRs and enforce policies.</li>
            <li> <strong>Pipelines</strong>  CI/CD: build, test, and deploy automatically.</li>
            <li> <strong>Boards</strong>  track stories, sprints, tasks and link them to commits or builds.</li>
            <li> <strong>Artifacts</strong>  host and version internal packages and build outputs.</li>
            <li> <strong>Test Plans</strong>  write and run manual/automated tests and log results.</li>
          </ul>

          <div style="margin:.5rem 0;">
            <strong>Practical tip:</strong>
            <p style="margin:.25rem 0 0;">Always link PRs to Board work items and pipeline runs to get traceability: commit  build  release mapped to a ticket.</p>
          </div>
        </div>`},{question:"What services of the Azure DevOps portal have you used?",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> Hands-on services I use</h3>
          <ul style="margin:.4rem 0 0 1.2rem; line-height:1.6;">
            <li> <strong>Repos</strong>  branch policies, PR templates, protected branches.</li>
            <li> <strong>Pipelines</strong>  YAML pipelines, templates, self-hosted agents, service connections.</li>
            <li> <strong>Boards</strong>  sprint planning and work item linking for releases.</li>
            <li> <strong>Artifacts</strong>  internal npm/nuget feeds for CI reuse.</li>
          </ul>

          <div style="margin:.5rem 0;">
            <strong>Example:</strong>
            <p style="margin:.25rem 0 0;">Built CI pipelines that run unit tests, SonarQube scan, build Docker images and push to ACR; CD pipelines deploy via Helm to AKS with environment approvals.</p>
          </div>
        </div>`},{question:"Tell me about the complete CI/CD process followed in your company",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> End-to-end CI/CD  simple flow</h3>
          <p style="margin:0 0 .5rem;">We follow a gated CI/CD pipeline from code commit to production with checks and approvals at each stage.</p>

          <strong>Flow (short):</strong>
          <ol style="margin:.4rem 0 0 1.2rem; line-height:1.6;">
            <li>Developer creates feature branch  opens PR.</li>
            <li>PR triggers CI: compile, unit tests, lint, SonarQube, build Docker image, push to ACR.</li>
            <li>Artifact triggers CD: deploy to staging via Helm/Terraform  run smoke tests.</li>
            <li>After manual approval, promote to production with monitoring checks.</li>
          </ol>

          <div style="margin:.6rem 0; padding:.5rem; background:rgba(255,255,255,0.02); border-radius:.4rem;">
            <strong>Why this helps:</strong>
            <p style="margin:.25rem 0 0;">Automated checks reduce human error, artifacts are versioned so rollbacks are easy, and approvals/gates protect production.</p>
          </div>
        </div>`},{question:"What are pipelines and how are they structured?",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> Pipeline structure  stages, jobs, steps</h3>
          <p style="margin:0 0 .5rem;">A pipeline is automation defined as code (YAML). It has:</p>
          <ul style="margin:.4rem 0 0 1.2rem; line-height:1.6;">
            <li> <strong>Stages</strong>  logical phases (Build / Test / Deploy).</li>
            <li> <strong>Jobs</strong>  units that run on agents (Windows/Linux/self-hosted).</li>
            <li> <strong>Steps</strong>  individual tasks/scripts (dotnet build, docker build, terraform apply).</li>
          </ul>

          <div style="margin:.5rem 0;">
            <strong>Tip:</strong>
            <p style="margin:.25rem 0 0;">Keep stages small and use templates for repeated tasks (build-template.yml, deploy-template.yml). Use variable groups for env-specific values.</p>
          </div>
        </div>`},{question:"Explain the difference between Classic and YAML pipelines",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> Classic vs YAML  quick compare</h3>
          <table style="width:100%; border-collapse:collapse; margin-bottom:.5rem;">
            <tr><th style="text-align:left; padding:.25rem .5rem;">Feature</th><th style="text-align:left; padding:.25rem .5rem;">Classic</th><th style="text-align:left; padding:.25rem .5rem;">YAML</th></tr>
            <tr><td style="padding:.25rem .5rem;">Definition</td><td style="padding:.25rem .5rem;">UI-based tasks</td><td style="padding:.25rem .5rem;">Pipeline-as-code in repo</td></tr>
            <tr><td style="padding:.25rem .5rem;">Versioning</td><td style="padding:.25rem .5rem;">Not in repo</td><td style="padding:.25rem .5rem;">Stored in Git (PR reviews)</td></tr>
            <tr><td style="padding:.25rem .5rem;">Reusability</td><td style="padding:.25rem .5rem;">Limited</td><td style="padding:.25rem .5rem;">Supports templates/parameters</td></tr>
            <tr><td style="padding:.25rem .5rem;">Best for</td><td style="padding:.25rem .5rem;">POC or simple flows</td><td style="padding:.25rem .5rem;">Production, GitOps, multi-repo reuse</td></tr>
          </table>

          <div style="margin:.5rem 0;">
            <strong>Practical note:</strong>
            <p style="margin:.25rem 0 0;">Prefer YAML for production because changes are auditable via Git and go through PR review.</p>
          </div>
        </div>`},{question:"What are the advantages of YAML pipelines?",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> Why use YAML pipelines?</h3>
          <ul style="margin:.4rem 0 0 1.2rem; line-height:1.6;">
            <li> <strong>Version-controlled</strong>  pipeline changes live in repo history.</li>
            <li> <strong>Reusable templates</strong>  share across projects (DRY).</li>
            <li> <strong>Better for GitOps</strong>  triggers on commits and branch changes.</li>
            <li> <strong>Matrix & conditional builds</strong>  parallel test runs and platform support.</li>
          </ul>

          <div style="margin:.5rem 0;">
            <strong>Tip:</strong>
            <p style="margin:.25rem 0 0;">Create central templates for common tasks (build, test, release) to standardize CI across microservices.</p>
          </div>
        </div>`},{question:"What is GitOps and how is it implemented in ADO?",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> GitOps  make Git the source of truth</h3>
          <p style="margin:0 0 .5rem;">GitOps means your desired state (app manifests, infra code) lives in Git. Changes to Git trigger automated reconciliation of the real environment.</p>

          <div style="margin:.4rem 0;">
            <strong>How to implement in ADO:</strong>
            <ul style="margin:.4rem 0 0 1.2rem; line-height:1.6;">
              <li>Store Helm charts / Terraform modules in a repo.</li>
              <li>YAML pipeline triggers on push  validate, plan, apply.</li>
              <li>Rollback by reverting commits (Git is single source of truth).</li>
            </ul>
          </div>

          <div style="margin:.6rem 0; padding:.5rem; background:rgba(255,255,255,0.02); border-radius:.4rem;">
            <strong>Example:</strong>
            <p style="margin:.25rem 0 0;">I keep Helm charts in repo; any change triggers a pipeline that lints the chart, packages it and deploys to AKS  Git shows the history and rollback is a revert + redeploy.</p>
          </div>
        </div>`},{question:"What is the difference between GitOps and DevOps?",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> DevOps vs GitOps  simple</h3>
          <p style="margin:0 0 .5rem;">DevOps is a broad culture & set of practices for collaboration and delivery. GitOps is one way to implement those practices using Git as the single source of truth for both app and infra.</p>

          <ul style="margin:.4rem 0 0 1.2rem; line-height:1.6;">
            <li>DevOps = people + process + tools (CI/CD, monitoring, collaboration).</li>
            <li>GitOps = declarative, Git-driven deployments and infra management.</li>
          </ul>

          <div style="margin:.5rem 0;">
            <strong>Quick analogy:</strong>
            <p style="margin:.25rem 0 0;">If DevOps is the recipe book, GitOps is using a single labelled shelf (Git) to store every recipe and ingredient so the kitchen can replicate the dish exactly.</p>
          </div>
        </div>`},{question:"What is Trunk-Based Branching Strategy?",answerHtml:`
        <div class="answer-rich">
          <h3 style="margin:0 0 .5rem;"> What is Trunk-Based Branching Strategy?</h3>
          <p style="margin:0 0 .6rem; font-weight:600;">
             <strong>Trunk-based development</strong> is a lightweight branching approach where teams integrate small, frequent changes into the <strong>main</strong> branch so the codebase stays deployable and conflicts are minimized.
          </p>

          <p style="margin:.25rem 0 .5rem;"><strong> Too long; didnt read </strong> small commits, short-lived branches, PR validation, and fast CI feedback  keep <code>main</code> always releasable.</p>

          <div style="margin:.5rem 0;">
            <strong style="display:block; margin-bottom:.35rem;"> Key Highlights:</strong>
            <ul style="margin:0 0 .5rem 1.2rem; line-height:1.6;">
              <li> <strong>Short-lived feature branches</strong> (hours  a few days)  merge frequently to avoid drift.</li>
              <li> <strong>PRs + build validation & lint checks</strong> are mandatory before merge.</li>
              <li> Avoid long-lived branches  reduces complex merges and integration churn.</li>
              <li> Ensures <strong>fast CI feedback</strong>, <strong>stable releases</strong>, and clean automation pipelines.</li>
              <li> Great fit for microservices & small agile teams delivering independent features.</li>
            </ul>
          </div>

          <div style="margin:.5rem 0;">
            <strong style="display:block; margin-bottom:.35rem;"> Practical example:</strong>
            <p style="margin:0 0 .5rem;">A dev creates <code>feature/login-button</code>, pushes small commits, opens a PR. CI runs unit tests + lint; two reviewers approve; the PR merges within hours to <code>main</code>. CI then builds and deploys a canary to staging  quick feedback loop.</p>
          </div>

          <div style="margin:.5rem 0;">
            <strong style="display:block; margin-bottom:.35rem;"> Pro tip:</strong>
            <ul style="margin:0 0 .5rem 1.2rem; line-height:1.6;">
              <li>Keep PRs small  one focused change per PR makes reviews fast and rollbacks easy.</li>
              <li>Automate checks: tests, lint, security scan (SonarQube/Checkov) before allowing merges.</li>
            </ul>
          </div>

          <div style="margin:.5rem 0; padding:.6rem; border-radius:.5rem; background:rgba(255,255,255,0.02);">
            <strong>In Azure DevOps:</strong>
            <p style="margin:.25rem 0 0;">Enforce branch policies  require successful pipeline run, set mandatory reviewers, and enable a merge strategy (squash/merge). This keeps <code>main</code> reliably deployable and auditable.</p>
          </div>

          <div style="margin-top:.6rem; color:#9aa0a6; font-size:.9rem;">
            <em>When NOT to use:</em> If you have very large, cross-cutting changes that cannot be split, trunk-based might be hard  use feature flags or a short-lived integration branch while refactoring.
          </div>
        </div>`}]},{title:"2. Repos & Source Control",questions:[{question:"What are repositories in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Repositories in Azure DevOps</strong></h3>
        <p>
          A <strong>repository (repo)</strong> in Azure DevOps is a Git-based storage space where source code, configurations, and documentation are managed and versioned.  
          It allows teams to collaborate using branching, pull requests, and commit tracking.
        </p>

        <strong>Key Points:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Supports both Git and TFVC (Team Foundation Version Control), but Git is preferred.</li>
          <li>Each repo can host multiple branches  main, develop, feature, release, and hotfix.</li>
          <li>Integrated with pipelines for automatic build/test triggers on commits or PRs.</li>
          <li>Supports branch policies, code reviews, and PR validations to maintain quality.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        We maintain separate repos for each microservice  linked to its own CI/CD pipeline.  
        Branch policies ensure code is reviewed and validated before merging to <code>main</code>.</p>
      </div>`},{question:"How do you checkout code from multiple repositories in one pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Checkout multiple repositories in a single pipeline</strong></h3>
        <p>
          In Azure DevOps YAML pipelines, you can fetch code from multiple repositories using the <strong>resources.repositories</strong> keyword.  
          This is useful when your build or deployment depends on shared libraries or infrastructure code.
        </p>

        <strong>Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>resources:
  repositories:
    - repository: app_repo
      type: git
      name: Project/AppRepo
    - repository: lib_repo
      type: git
      name: Project/CommonLibrary

steps:
  - checkout: self
  - checkout: lib_repo</code></pre>

        <p><strong>Result:</strong> Both repositories (<code>self</code> and <code>lib_repo</code>) are checked out into the pipeline agent workspace.</p>

        <p><strong>Best Practice:</strong>  
        Always use repository aliases and specific branch references to maintain consistency in multi-repo pipelines.</p>
      </div>`},{question:"You have multiple repositories A and B and need to fetch both in one deployment  how do you configure that?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deploying using multiple repositories (A & B)</strong></h3>
        <p>
          When a deployment requires code from multiple repositories, configure both repos in the YAML pipeline using <strong>resources.repositories</strong> and checkout them separately.
        </p>

        <strong>Configuration Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>resources:
  repositories:
    - repository: repoA
      type: git
      name: Project/RepoA
      ref: main
    - repository: repoB
      type: git
      name: Project/RepoB
      ref: develop

steps:
  - checkout: repoA
  - checkout: repoB
  - script: |
      echo "Building from RepoA and RepoB"
</code></pre>

        <p>
          This structure ensures that both repositories are cloned in the agent environment.  
          The pipeline can then access code from <code>$(Build.SourcesDirectory)/repoA</code> and <code>repoB</code> paths.
        </p>

        <p><strong>In Practice:</strong>  
        I used this approach when deploying an API (RepoA) and shared utilities (RepoB) together to the same AKS namespace.</p>
      </div>`},{question:"What is GitOps and how do you manage Git branches for automation?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>GitOps & Branch Automation in Azure DevOps</strong></h3>
        <p>
          <strong>GitOps</strong> is an operational model where all infrastructure and application configurations are stored in Git and automatically deployed through pipelines.
        </p>

        <strong>Branch Management Approach:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>main</strong>  always deployable, represents the production environment.</li>
          <li> <strong>develop</strong>  integration branch where team merges tested features.</li>
          <li> <strong>feature/*</strong>  short-lived branches for new features, merged via PRs.</li>
          <li> <strong>release/*</strong>  created before deployment for staging/pre-prod testing.</li>
          <li> <strong>hotfix/*</strong>  for urgent fixes directly from <code>main</code>.</li>
        </ul>

        <p>
          Every merge to <code>main</code> triggers automated deployments using YAML pipelines  ensuring infrastructure and apps are always in sync with Git state.
        </p>

        <p><strong>In Practice:</strong>  
        I manage GitOps flow by linking Terraform & Helm code in Repos; any commit to <code>main</code> triggers infra updates via ADO pipelines.</p>
      </div>`},{question:"Explain the branch strategy followed in your project (feature/develop/release/hotfix).",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Branching Strategy (Git Flow Model)</strong></h3>
        <p>
          Our project follows a customized <strong>Git Flow strategy</strong> to manage features, releases, and hotfixes efficiently.
        </p>

        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Feature Branches:</strong> Created from <code>develop</code> for new functionality. Merged via PR after review.</li>
          <li> <strong>Develop Branch:</strong> Integration branch where all features are merged and validated.</li>
          <li> <strong>Release Branch:</strong> Cut from <code>develop</code> before production. Used for final QA and staging deployments.</li>
          <li> <strong>Hotfix Branch:</strong> Created directly from <code>main</code> to fix production issues quickly.</li>
          <li> <strong>Main Branch:</strong> Always stable, production-ready branch with strict PR validations.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        Each branch merge triggers automated pipelines  ensuring builds and deployments align with environment promotion flow (Dev  QA  Prod).</p>
      </div>`},{question:"What is a hotfix in pipelines? How do you deploy hotfixes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Hotfix Deployment in Azure Pipelines</strong></h3>
        <p>
          A <strong>hotfix</strong> is an urgent fix deployed directly to production to resolve critical issues.  
          In Azure DevOps, its handled via a separate branch and dedicated pipeline path.
        </p>

        <strong>Process:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Create branch <code>hotfix/issue-id</code> from <code>main</code>.</li>
          <li>Commit fix  trigger CI build automatically.</li>
          <li>Run unit tests, validations, and manual approvals if needed.</li>
          <li>Merge back to <code>main</code> and <code>develop</code> to sync future releases.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I use a separate YAML condition to identify hotfix branches and directly deploy artifacts to the production slot post-approval.</p>

        <p><strong>Benefit:</strong>  
        Reduces downtime while keeping mainline development unaffected.</p>
      </div>`},{question:"How do you rollback or revert a commit if build fails?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rollback or Revert a Commit</strong></h3>
        <p>
          Rollback is required when a recent commit causes a failed build or faulty deployment.  
          Azure DevOps supports both Git-level revert and pipeline-based redeployment rollback.
        </p>

        <strong>Two Ways to Rollback:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Git Revert:</strong> Use <code>git revert &lt;commit-id&gt;</code> to undo the specific change and push it.  
          This triggers the pipeline to redeploy the last known stable version.</li>
          <li> <strong>Pipeline Rollback:</strong> Redeploy the last successful build artifact from Azure DevOps Releases or Runs section.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        If a deployment fails after a Terraform apply, I revert the commit, re-run the pipeline, and the previous state is restored via backend state file (S3/Storage Account).</p>
      </div>`}]},{title:"3. Variables, Secrets & Configuration",questions:[{question:"What are Variables in a pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Variables in an Azure Pipeline</strong></h3>
        <p>
          <strong>Variables</strong> in a pipeline store reusable values such as environment names, build numbers, or connection strings.
          They make your pipeline flexible, maintainable, and easier to manage across stages.
        </p>

        <strong>Common Uses:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Define environment-specific values (e.g., dev, test, prod).</li>
          <li> Reuse credentials or paths across multiple jobs.</li>
          <li> Control conditional logic  e.g., run deploy only if <code>env == 'prod'</code>.</li>
        </ul>

        <p><strong>In YAML:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>variables:
  environment: "dev"
  appName: "myApp"

steps:
  - script: echo "Deploying to $(environment)"</code></pre>

        <p><strong>In Practice:</strong>  
        I use variables for image tags, artifact names, and resource group names  making the pipeline reusable across environments.</p>
      </div>`},{question:"Difference between pipeline variables and environment variables.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline Variables vs Environment Variables</strong></h3>
        <p>
          Both are used to store dynamic data in Azure Pipelines, but they differ in scope and visibility.
        </p>

        <strong>Key Differences:</strong>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th style="text-align:left;">Aspect</th><th style="text-align:left;">Pipeline Variable</th><th style="text-align:left;">Environment Variable</th></tr>
          <tr><td>Scope</td><td>Pipeline-level (defined in YAML or UI)</td><td>System or agent-level (in OS environment)</td></tr>
          <tr><td>Definition</td><td>Defined under <code>variables:</code> block</td><td>Defined using <code>env:</code> keyword or OS</td></tr>
          <tr><td>Access</td><td>$(variableName)</td><td>$VARIABLE_NAME or %VARIABLE_NAME%</td></tr>
          <tr><td>Persistence</td><td>Only for that pipeline run</td><td>Agent-specific or global</td></tr>
        </table>

        <p><strong>Example:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>variables:
  app: "myApp"
steps:
  - script: echo "Pipeline var $(app)"
    env:
      REGION: "eastus"</code></pre>

        <p><strong>In Practice:</strong>  
        Pipeline vars define logic and flow, while environment vars configure runtime settings inside containers or scripts.</p>
      </div>`},{question:"How do you create and manage variable groups in ADO?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Variable Groups in Azure DevOps</strong></h3>
        <p>
          <strong>Variable Groups</strong> are shared collections of variables that can be linked to multiple pipelines  used to manage common configurations like connection strings, environment names, or subscription IDs.
        </p>

        <strong>Creation Steps:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Go to <strong>Pipelines  Library  Variable Groups</strong>.</li>
          <li>Click <strong>+ Variable Group</strong>, give it a name.</li>
          <li>Add key-value pairs or mark sensitive data as secret .</li>
          <li>Link this variable group inside your YAML pipeline.</li>
        </ol>

        <strong>YAML Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>variables:
- group: 'Shared-Config-Group'</code></pre>

        <p><strong>In Practice:</strong>  
        I maintain variable groups like DEV-CONFIG and PROD-CONFIG  so multiple pipelines use the same consistent environment setup.</p>
      </div>`},{question:"What is the sequence or order of variable precedence?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Variable Precedence Order in Azure Pipelines</strong></h3>
        <p>
          When the same variable name exists in multiple sources, Azure DevOps follows a defined order to determine which value takes precedence.
        </p>

        <strong>Precedence (Highest  Lowest):</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>1 Secret variables (Key Vault or Library marked as secret)</li>
          <li>2 Runtime variables passed via pipeline UI or CLI</li>
          <li>3 Variables set within YAML (<code>variables:</code> block)</li>
          <li>4 Variable Groups linked in the YAML</li>
          <li>5 Pipeline defaults and system variables</li>
        </ol>

        <p><strong>Example:</strong>  
        If <code>env=prod</code> is defined both in YAML and a Library group  the Library value wins unless overridden at runtime.</p>

        <p><strong>In Practice:</strong>  
        I always keep secrets in Key Vault and environment toggles in YAML for clear separation of security vs configuration.</p>
      </div>`},{question:"How do you secure secrets in pipelines? (Library, Key Vault, Environment Variables)",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Secrets in Azure Pipelines</strong></h3>
        <p>
          Secrets like passwords, API keys, or tokens must be encrypted and securely managed  not stored in plain text or code.
        </p>

        <strong>Secure Methods:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Library Variable Groups:</strong> Mark variables as <em>secret</em> in Library settings.</li>
          <li> <strong>Azure Key Vault Integration:</strong> Link pipeline to Key Vault and auto-fetch secrets at runtime.</li>
          <li> <strong>Environment Variables:</strong> Pass temporary credentials only at runtime to avoid permanent storage.</li>
        </ul>

        <p><strong>Best Practice:</strong>  
        Never hardcode secrets in YAML. Instead, store them in Key Vault or the Library and reference as <code>$(mySecret)</code> during execution.</p>

        <p><strong>In Practice:</strong>  
        I use Azure Key Vault-backed variable groups to auto-sync secrets  especially for SPN credentials, DB passwords, and webhook tokens.</p>
      </div>`},{question:"How do you link Azure Key Vault with ADO to pull secrets?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Azure Key Vault with Azure DevOps</strong></h3>
        <p>
          Azure DevOps can automatically fetch secrets from <strong>Azure Key Vault</strong> into pipelines by linking the vault to a variable group.
        </p>

        <strong>Steps to Link:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Go to <strong>Pipelines  Library  Variable Groups</strong>.</li>
          <li>Create a new group  click <strong>Link secrets from Azure Key Vault</strong>.</li>
          <li>Provide subscription and Key Vault details.</li>
          <li>Select secrets to include  Save.</li>
        </ol>

        <p><strong>YAML Example:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>variables:
- group: 'KeyVault-Secrets'</code></pre>

        <p><strong>In Practice:</strong>  
        My pipeline uses SPN credentials and DB connection strings fetched from Key Vault at runtime  no manual secret management required.</p>
      </div>`},{question:"How can you store configuration secrets to authorize pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Storing Configuration Secrets for Pipeline Authorization</strong></h3>
        <p>
          To authorize pipelines securely, configuration secrets like <strong>service principal credentials, PAT tokens,</strong> or <strong>API keys</strong> should be stored using secure methods in Azure DevOps.
        </p>

        <strong>Options:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Azure Key Vault:</strong> Safest option  secrets never appear in logs.</li>
          <li> <strong>Library Variable Groups:</strong> Mark as secret to encrypt values.</li>
          <li> <strong>Service Connections:</strong> Used for Azure, Docker, GitHub auth  securely stores SPNs and tokens.</li>
        </ul>

        <p><strong>In YAML:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- script: az login --service-principal -u $(clientId) -p $(clientSecret) --tenant $(tenantId)</code></pre>

        <p><strong>In Practice:</strong>  
        I store Azure SPN credentials in Key Vault + link it to Library. The pipeline fetches these at runtime for Terraform or ACR authentication  no manual exposure needed.</p>
      </div>`}]},{title:"4. Agents, Pools & Runners",questions:[{question:"What is an agent pool and how does it work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Agent Pool in Azure DevOps</strong></h3>
        <p>
          An <strong>agent pool</strong> is a collection of agents that run your pipeline jobs.  
          Each pipeline picks an agent from the pool to execute its tasks (build, test, deploy).
        </p>

        <strong>How it Works:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Each pipeline job requests an agent from a specific pool.</li>
          <li>Azure assigns a free agent to execute the jobs tasks sequentially.</li>
          <li>Once completed, the agent is released back to the pool.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I maintain separate pools for <em>Build</em>, <em>Deploy</em>, and <em>Infra</em> pipelines  ensuring controlled capacity and better resource utilization.</p>
      </div>`},{question:"What is a self-hosted agent and when would you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Self-Hosted Agent</strong></h3>
        <p>
          A <strong>self-hosted agent</strong> is a machine (VM or physical server) that you manage and configure to run pipeline jobs.  
          It connects securely to Azure DevOps via an authentication token.
        </p>

        <strong>When to Use:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Custom tools or software are required (e.g., Terraform, Ansible, or specific SDKs).</li>
          <li>Private network access  like deploying inside your VNet or internal servers.</li>
          <li>When you need faster execution and control over the build environment.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I use self-hosted Linux agents for Terraform deployments and Windows agents for .NET app builds.</p>
      </div>`},{question:"What is the advantage of a self-hosted agent?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Advantages of Self-Hosted Agents</strong></h3>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Speed:</strong> Pre-installed tools mean faster builds (no setup time).</li>
          <li> <strong>Security:</strong> Stays within private network boundaries.</li>
          <li> <strong>Cost-Efficient:</strong> No charge per pipeline minute (unlike Microsoft-hosted).</li>
          <li> <strong>Customization:</strong> Install custom dependencies as per project needs.</li>
        </ul>

        <p><strong>Example:</strong>  
        I configured a self-hosted Ubuntu agent with Terraform, kubectl, Helm, and Azure CLI  optimized for IaC deployments.</p>
      </div>`},{question:"Which agents do you use  Microsoft-hosted or self-hosted?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Agents Used in My Environment</strong></h3>
        <p>
          I use a <strong>hybrid approach</strong>  both Microsoft-hosted and self-hosted agents based on workload type.
        </p>

        <strong>Usage:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Microsoft-hosted:</strong> For lightweight build/test pipelines like Node.js or React apps.</li>
          <li> <strong>Self-hosted:</strong> For infrastructure pipelines using Terraform, Docker, or restricted network deployments.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        Most of my IaC and deployment pipelines run on self-hosted agents; build/test pipelines use Microsoft-hosted ones for scalability.</p>
      </div>`},{question:"How many agents are configured in your organization?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Agent Configuration Count</strong></h3>
        <p>
          We have around <strong>1012 agents</strong> configured  distributed across pools:
        </p>

        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>6 Linux agents (for Terraform, Docker, and Kubernetes workloads)</li>
          <li>4 Windows agents (for .NET and PowerShell-based tasks)</li>
          <li>2 spare agents used as fallback during maintenance or load peaks</li>
        </ul>

        <p><strong>Note:</strong> Agents are auto-scaled via Azure VM Scale Sets based on pipeline concurrency.</p>
      </div>`},{question:"If 12 developers trigger pipelines at once and only 6 agents are available  how do you handle concurrency?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Concurrency in Pipelines</strong></h3>
        <p>
          When multiple pipelines trigger simultaneously, Azure DevOps queues jobs if agents are unavailable.
        </p>

        <strong>How I Handle It:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Auto-scaling:</strong> Configure VM Scale Sets to spin up new agents automatically.</li>
          <li> <strong>Parallel Jobs:</strong> Purchase additional parallel job licenses or add more agents to the pool.</li>
          <li> <strong>Scheduling:</strong> Use <code>dependsOn</code> and pipeline triggers to optimize non-critical jobs at off-peak hours.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        We added two additional agents dynamically during heavy load  managed via VMSS scaling policies and pipeline concurrency control.</p>
      </div>`},{question:"How do you monitor and maintain self-hosted agents?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring & Maintenance of Self-Hosted Agents</strong></h3>
        <p>
          Self-hosted agents require proactive monitoring to ensure reliability and availability.
        </p>

        <strong>Monitoring Steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Track agent health and job success in <strong>Project Settings  Agent Pools</strong>.</li>
          <li> Use custom scripts or Azure Monitor to check CPU, disk, and memory utilization.</li>
          <li> Rotate tokens periodically and update agent version regularly.</li>
          <li> Clean up workspace cache after builds using <code>--clean</code> option.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I use a cron-based shell script that checks agent service status and restarts it if inactive  reducing downtime.</p>
      </div>`},{question:"What is a deployment group and how is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deployment Group in Azure DevOps</strong></h3>
        <p>
          A <strong>deployment group</strong> is a collection of target machines (VMs/servers) where you deploy applications via release pipelines.
        </p>

        <strong>Usage:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Each target machine runs an agent configured to listen for deployment jobs.</li>
          <li>Deployments can target specific tags (e.g., Web, DB, API servers).</li>
          <li>Used in classic release pipelines for VM-based deployments.</li>
        </ul>

        <p><strong>Example:</strong>  
        In one project, I configured a WebAppDeploymentGroup for IIS servers  pipeline deployed build artifacts to those servers directly.</p>
      </div>`},{question:"When will you use which type of agent? (Linux/Windows, self-hosted/cloud)",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Choosing the Right Agent Type</strong></h3>
        <p>
          Selection depends on project requirements, workload type, and deployment environment.
        </p>

        <strong>Typical Use Cases:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Linux Agent:</strong> Best for Docker, Terraform, Kubernetes, Python, and Node.js workloads.</li>
          <li> <strong>Windows Agent:</strong> Required for .NET, PowerShell, IIS, and MSBuild tasks.</li>
          <li> <strong>Microsoft-Hosted:</strong> Quick setup, best for short-lived builds.</li>
          <li> <strong>Self-Hosted:</strong> Use when you need private network access or custom tools installed.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I use Linux self-hosted agents for Terraform and container builds, while Windows hosted agents for application packaging.</p>
      </div>`},{question:"What is the difference between run-once and multi-stage jobs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Run-Once vs Multi-Stage Jobs</strong></h3>
        <p>
          Azure Pipelines can execute jobs either as a single run or across multiple structured stages.
        </p>

        <strong>Difference:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Run-Once Job:</strong> Executes a single job with all steps  simple for builds or tests.</li>
          <li> <strong>Multi-Stage Job:</strong> Divides process into stages like Build  Test  Deploy  provides visibility, approvals, and better control.</li>
        </ul>

        <p><strong>Example:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>stages:
- stage: Build
  jobs:
    - job: buildApp
- stage: Deploy
  dependsOn: Build
  jobs:
    - job: deployToDev</code></pre>

        <p><strong>In Practice:</strong>  
        I use multi-stage YAML pipelines for production releases with gated approvals and rollback options.</p>
      </div>`}]},{title:"5. Service Connections & Integrations",questions:[{question:"What is a service connection and how do you configure it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>What is a Service Connection?</strong></h3>
        <p>
          A <strong>Service Connection</strong> is a secure authentication link that allows Azure DevOps pipelines to connect with external systems  
          like Azure, Docker Hub, GitHub, or Kubernetes  without exposing credentials in YAML.
        </p>

        <strong>Configuration Steps:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Go to <strong>Project Settings  Service Connections</strong>.</li>
          <li>Click <strong>New service connection</strong>  choose provider (Azure, Docker, GitHub, etc.).</li>
          <li>Authenticate using a <strong>Service Principal</strong> or <strong>PAT (Personal Access Token)</strong>.</li>
          <li>Grant permissions to pipelines to use this connection.</li>
        </ol>

        <p><strong>In Practice:</strong>  
        I use Azure Resource Manager (ARM) service connections to authenticate Terraform and deployment stages to Azure subscriptions securely.</p>
      </div>`},{question:"How do you authorize pipelines to access Azure subscriptions?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Authorizing Pipelines to Access Azure</strong></h3>
        <p>
          Azure DevOps uses <strong>Service Principals</strong> under ARM connections to authorize pipelines to deploy or manage Azure resources.
        </p>

        <strong>Steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Create an <strong>App Registration</strong> in Azure AD.</li>
          <li>Assign it required role (e.g., Contributor) at the subscription or resource group level.</li>
          <li>Use its <code>clientId</code>, <code>clientSecret</code>, and <code>tenantId</code> to configure an ARM service connection.</li>
          <li>Link the connection inside your pipeline under <code>serviceConnection:</code>.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My Terraform pipelines authenticate to Azure using SPNs stored in an ARM service connection.  
        This allows automatic provisioning without manual Azure CLI login.</p>
      </div>`},{question:"If you see the error 'Pipeline doesn't have authorization to run,' what are possible causes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline Authorization Error  Causes & Fix</strong></h3>
        <p>
          This error occurs when the pipeline lacks permission to access a resource or service connection.
        </p>

        <strong>Common Causes:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Service connection not authorized for the pipeline.</li>
          <li> Incorrect role assignment on the Azure subscription (SPN lacks Contributor access).</li>
          <li> Access restrictions in Project Settings  Permissions.</li>
          <li> Connection expired or credentials revoked.</li>
        </ul>

        <p><strong>Fix:</strong></p>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Go to the Service Connection  Click <strong>Authorize for use in all pipelines</strong>.</li>
          <li>Revalidate SPN permissions in Azure portal.</li>
          <li>Regenerate PAT or credentials if expired.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I faced this during AKS deployment  resolved it by reauthorizing the pipeline for the ARM service connection.</p>
      </div>`},{question:"How do you integrate ADO pipelines with GitHub repositories?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Azure DevOps Pipelines with GitHub</strong></h3>
        <p>
          You can connect Azure DevOps pipelines with GitHub repos for source control, CI/CD triggers, and automated builds.
        </p>

        <strong>Integration Methods:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Service Connection:</strong> Create a GitHub connection using a PAT.</li>
          <li> <strong>Triggers:</strong> Configure YAML to trigger on GitHub pushes or PRs.</li>
          <li> <strong>Artifacts:</strong> Build outputs can be pushed back to GitHub releases or packages.</li>
        </ul>

        <p><strong>YAML Example:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>trigger:
  branches:
    include:
      - main

resources:
  repositories:
    - repository: self
      type: github
      name: myorg/myrepo
      endpoint: GitHub-ServiceConnection</code></pre>

        <p><strong>In Practice:</strong>  
        I use this for microservices hosted in GitHub  pipelines run tests and push Docker images to ACR upon every commit to <code>main</code>.</p>
      </div>`},{question:"What is PAT (Personal Access Token) and where do you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Personal Access Token (PAT)</strong></h3>
        <p>
          A <strong>PAT</strong> is an authentication token generated by a user to securely access Azure DevOps REST APIs, Git repos, or integrations like GitHub and Docker.
        </p>

        <strong>Use Cases:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Integrating ADO with GitHub, Jenkins, or external tools.</li>
          <li>Authenticating Git operations in pipelines.</li>
          <li>Setting up service connections for source access.</li>
        </ul>

        <p><strong>Best Practices:</strong></p>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Generate with <strong>minimum required scope</strong> (e.g., Code (Read & Write)).</li>
          <li>Store in Azure Key Vault or pipeline library (marked as secret).</li>
          <li>Rotate regularly to avoid expired credentials.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I used PAT for GitHub integration with ADO pipelines where OAuth was restricted due to org policies.</p>
      </div>`},{question:"How do you authenticate with Azure Container Registry (ACR) from pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Authenticating with Azure Container Registry (ACR)</strong></h3>
        <p>
          Azure Pipelines can push or pull Docker images from ACR using an authenticated service connection.
        </p>

        <strong>Options to Authenticate:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Service Connection:</strong> Create a Docker Registry or ARM connection linked to ACR.</li>
          <li> <strong>Azure CLI Login:</strong> Use SPN credentials for manual authentication.</li>
        </ul>

        <strong>Example (YAML):</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: Docker@2
  inputs:
    containerRegistry: 'ACR-Service-Connection'
    repository: 'myapp'
    command: 'push'
    tags: |
      latest
      $(Build.BuildId)</code></pre>

        <p><strong>In Practice:</strong>  
        My build stage pushes Docker images to ACR via service connection, and deploy stage pulls the same images to AKS automatically.</p>
      </div>`},{question:"What is ACR and how do you push images from pipeline to ACR?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Container Registry (ACR)</strong></h3>
        <p>
          ACR is a private Docker registry in Azure used to store and manage container images securely.  
          Pipelines can build and push images automatically during CI/CD.
        </p>

        <strong>Steps to Push Image:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Authenticate pipeline to ACR via Docker or ARM service connection.</li>
          <li>Build Docker image in the pipeline.</li>
          <li>Push it using <code>Docker@2</code> task.</li>
        </ol>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: Docker@2
  inputs:
    containerRegistry: 'ACR-ServiceConnection'
    repository: 'app/backend'
    command: 'push'
    tags: |
      latest
      $(Build.BuildId)</code></pre>

        <p><strong>In Practice:</strong>  
        Our CI pipelines build microservice images, push them to ACR, and trigger deployment to AKS via Helm in CD stage.</p>
      </div>`},{question:"How do you link external feeds or publish artifacts to external registries?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Linking External Feeds & Registries</strong></h3>
        <p>
          Azure DevOps can connect to external feeds (npm, NuGet, Maven, PyPI, DockerHub) via service connections.
        </p>

        <strong>Steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Create a new service connection of type <strong>External Feed</strong>.</li>
          <li>Provide credentials (PAT, token, or feed URL).</li>
          <li>Use <code>npm authenticate</code> or <code>nuget restore</code> tasks in your YAML.</li>
        </ul>

        <p><strong>Example:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: NpmAuthenticate@0
  inputs:
    workingFile: .npmrc
- script: npm publish</code></pre>

        <p><strong>In Practice:</strong>  
        We publish shared npm packages from builds to Azure Artifacts and Docker images to both ACR and Docker Hub.</p>
      </div>`},{question:"What is required while creating a service connection?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Requirements for Creating a Service Connection</strong></h3>
        <p>
          A service connection requires credentials or tokens that authenticate Azure DevOps with the target system.
        </p>

        <strong>Common Requirements:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Client ID, Client Secret, and Tenant ID (for Azure ARM).</li>
          <li>PAT (for GitHub, DockerHub, or Azure DevOps REST APIs).</li>
          <li>Access permissions (Contributor or Owner) for the resource.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        While integrating with Azure, I always ensure SPN has least-privilege Contributor access and test via <code>az login</code> before using in pipeline.</p>
      </div>`},{question:"How do you create a service connection for GitHub in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create a GitHub Service Connection</strong></h3>
        <p>
          To connect your Azure DevOps pipeline with GitHub for source control or automation:
        </p>

        <strong>Steps:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Go to <strong>Project Settings  Service Connections  New Connection</strong>.</li>
          <li>Select <strong>GitHub</strong> as the connection type.</li>
          <li>Authenticate using either OAuth or Personal Access Token (PAT).</li>
          <li>Grant repository-level access.</li>
        </ol>

        <p><strong>In Practice:</strong>  
        I configured GitHub connections for pipelines that automatically build and deploy code pushed from GitHub main branches to Azure environments.</p>
      </div>`}]},{title:"6. CI/CD Pipeline Structure & YAML",questions:[{question:"What are the stages in a typical CI/CD pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Stages in a Typical CI/CD Pipeline</strong></h3>
        <p>
          A CI/CD pipeline is divided into logical <strong>stages</strong> that represent the software delivery flow  from code build to deployment.
        </p>

        <strong>Common Stages:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Build:</strong> Compile code, restore dependencies, and package artifacts.</li>
          <li> <strong>Test:</strong> Run unit, integration, and quality checks (e.g., SonarQube, Checkov).</li>
          <li> <strong>Publish:</strong> Push build outputs to Artifacts or container registries (ACR).</li>
          <li> <strong>Deploy:</strong> Deploy to environments (Dev  QA  UAT  Prod).</li>
          <li> <strong>Monitor:</strong> Track deployments using logs, Prometheus, or Application Insights.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I design pipelines with Build  Scan  Deploy  Notify flow, ensuring approvals and rollback points between environments.</p>
      </div>`},{question:"How do you define triggers, agents, and service connections in YAML?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Defining Triggers, Agents & Service Connections in YAML</strong></h3>
        <p>
          In Azure DevOps, YAML pipelines define <strong>triggers</strong> (what starts the pipeline), <strong>agents</strong> (where it runs), 
          and <strong>service connections</strong> (how it authenticates to external resources).
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>trigger:
  branches:
    include:
      - main
      - develop

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: 'AppSettings'

steps:
  - task: AzureCLI@2
    inputs:
      azureSubscription: 'ARM-ServiceConnection'
      scriptType: bash
      scriptLocation: inlineScript
      inlineScript: |
        az group list</code></pre>

        <p><strong>In Practice:</strong>  
        I set branch-based triggers and use separate service connections for each environment  avoiding cross-deployment risks.</p>
      </div>`},{question:"What are pipeline templates and their benefits?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline Templates in Azure DevOps</strong></h3>
        <p>
          <strong>Templates</strong> are reusable YAML files that store common logic (build, deploy, test) and can be imported into multiple pipelines.
        </p>

        <strong>Benefits:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Reusability  define once, reuse across projects.</li>
          <li> Consistency  same logic for all pipelines.</li>
          <li> Central governance  easier maintenance and version control.</li>
        </ul>

        <strong>Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># main-pipeline.yml
stages:
- stage: Build
  jobs:
    - template: templates/build.yml</code></pre>

        <p><strong>In Practice:</strong>  
        I use a shared repo for templates  one change updates all service pipelines instantly.</p>
      </div>`},{question:"How do you write a YAML pipeline for application deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Example  YAML Pipeline for App Deployment</strong></h3>
        <p>Below is a typical YAML pipeline structure used to build and deploy a web app to Azure App Service.</p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>trigger:
  branches:
    include:
      - main

pool:
  vmImage: 'ubuntu-latest'

variables:
  - group: 'AppSecrets'

stages:
- stage: Build
  jobs:
    - job: build
      steps:
        - task: NodeTool@0
          inputs:
            versionSpec: '18.x'
        - script: npm install && npm run build
        - task: PublishBuildArtifacts@1

- stage: Deploy
  dependsOn: Build
  jobs:
    - deployment: deployWeb
      environment: 'dev'
      strategy:
        runOnce:
          deploy:
            steps:
              - task: AzureWebApp@1
                inputs:
                  azureSubscription: 'App-ServiceConnection'
                  appName: 'dev-webapp'
                  package: '$(Pipeline.Workspace)/drop/*.zip'</code></pre>

        <p><strong>In Practice:</strong>  
        I use multi-stage pipelines like this for frontend and API apps  integrated with approvals and notifications post-deployment.</p>
      </div>`},{question:"Write a sample Azure pipeline YAML and explain each stage",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sample Azure Pipeline with Explanation</strong></h3>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>trigger:
  branches:
    include:
      - main

stages:
- stage: Build
  jobs:
    - job: buildApp
      steps:
        - script: echo "Building code"
        - task: PublishBuildArtifacts@1

- stage: Deploy
  dependsOn: Build
  jobs:
    - job: deployToDev
      steps:
        - script: echo "Deploying to Dev environment"</code></pre>

        <strong>Explanation:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Build:</strong> Code compilation and artifact publishing.</li>
          <li> <strong>Deploy:</strong> Fetches artifacts and deploys to target environment.</li>
          <li> <strong>dependsOn:</strong> Ensures Deploy runs only if Build succeeds.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        Each stage has environment-based approvals and conditional triggers  ensuring safe and controlled deployments.</p>
      </div>`},{question:"What is the difference between dependsOn and condition in YAML?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>dependsOn vs condition in Azure Pipelines</strong></h3>
        <p>
          Both control pipeline flow, but they serve different purposes:
        </p>

        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th style="text-align:left;">Aspect</th><th style="text-align:left;">dependsOn</th><th style="text-align:left;">condition</th></tr>
          <tr><td>Purpose</td><td>Defines stage/job dependency order.</td><td>Controls whether a stage/job should run based on logic.</td></tr>
          <tr><td>Default Behavior</td><td>Runs when dependent stage succeeds.</td><td>Can run on success, failure, or custom expression.</td></tr>
        </table>

        <p><strong>Example:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- stage: Deploy
  dependsOn: Build
  condition: succeeded()</code></pre>

        <p><strong>In Practice:</strong>  
        I use <code>dependsOn</code> to define order (Build  Deploy) and <code>condition</code> to skip optional test stages dynamically.</p>
      </div>`},{question:"What is the difference between deployment job and normal job?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deployment Job vs Normal Job</strong></h3>
        <p>
          A <strong>deployment job</strong> is used for environment-based deployments with tracking and approvals, while a <strong>normal job</strong> executes generic tasks.
        </p>

        <strong>Difference Table:</strong>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Normal Job</th><th>Deployment Job</th></tr>
          <tr><td>Purpose</td><td>Runs tasks or scripts</td><td>Deploys to environment</td></tr>
          <tr><td>Environment</td><td>No association</td><td>Linked with Dev/Test/Prod</td></tr>
          <tr><td>Approvals</td><td>Not supported</td><td>Supports pre/post approvals</td></tr>
        </table>

        <p><strong>In Practice:</strong>  
        I use <code>deployment</code> jobs for AKS, App Service, and VM deployments  since they allow rollback and approvals.</p>
      </div>`},{question:"What is the use of environment variables in YAML?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environment Variables in YAML</strong></h3>
        <p>
          <strong>Environment variables</strong> provide runtime context (like credentials, regions, or API keys) to scripts and tasks inside the pipeline.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>steps:
- script: echo "Running on $ENV_NAME"
  env:
    ENV_NAME: "Development"</code></pre>

        <p><strong>In Practice:</strong>  
        I pass sensitive values (like client secrets) from variable groups as env vars to avoid hardcoding in scripts.</p>
      </div>`},{question:"What is the purpose of environment approvals and gates in YAML pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environment Approvals & Gates</strong></h3>
        <p>
          Approvals and gates ensure controlled deployments by adding checks before or after environment stages.
        </p>

        <strong>Common Gates:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Manual approval from QA/Manager before production deploy.</li>
          <li> Query monitoring tools (e.g., ServiceNow, App Insights).</li>
          <li> Wait timer before automatic promotion.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My pipelines include environment-level approvals for UAT  Prod to ensure verified and auditable releases.</p>
      </div>`},{question:"What is the use of templates for reusable pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reusable Templates in Pipelines</strong></h3>
        <p>
          Templates make large pipelines modular and maintainable by separating repeated logic into shared YAML files.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># templates/deploy.yml
steps:
- task: AzureCLI@2
  inputs:
    azureSubscription: 'ARM-Conn'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: echo "Deploying app"</code></pre>

        <p><strong>In Practice:</strong>  
        I keep reusable build, scan, and deploy templates  helping standardize pipelines across 30+ microservices.</p>
      </div>`},{question:"What is a pipeline in Azure DevOps? Explain YAML pipeline flow",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline & YAML Flow in Azure DevOps</strong></h3>
        <p>
          A <strong>pipeline</strong> is a defined automation workflow that builds, tests, and deploys your code using YAML syntax.
        </p>

        <strong>YAML Flow:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Trigger activates the pipeline (push, PR, schedule).</li>
          <li>Stages execute sequentially (Build  Test  Deploy).</li>
          <li>Each stage runs one or more jobs.</li>
          <li>Jobs contain steps (tasks, scripts, or templates).</li>
          <li>Outputs or artifacts are passed between stages.</li>
        </ol>

        <p><strong>In Practice:</strong>  
        My pipelines follow a GitOps-driven flow  every commit triggers a build, publishes Docker image to ACR, and deploys via Helm to AKS using YAML pipelines.</p>
      </div>`}]},{title:"7. Pipeline Automation & Scheduling",questions:[{question:"How do you trigger pipelines automatically on Git commits?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Triggering Pipelines on Git Commits</strong></h3>
        <p>
          The most common way is to configure branch triggers in YAML so that pushes/merges to specific branches automatically start the pipeline.
        </p>

        <strong>YAML Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>trigger:
  branches:
    include:
      - main
      - develop</code></pre>

        <p><strong>PR Validation:</strong> Use <code>pr:</code> to run CI for pull requests before merge:</p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>pr:
  branches:
    include:
      - develop</code></pre>

        <p><strong>In Practice:</strong> I trigger CI on PRs for dev branches and only allow merge to <code>main</code> after successful CI + approvals  keeps main stable and ensures early feedback.</p>
      </div>`},{question:"How do you schedule pipelines to run at specific times?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Scheduling Pipelines (CRON)</strong></h3>
        <p>
          Use the <code>schedules</code> section in YAML or the pipeline scheduler in the ADO UI to run pipelines at defined CRON intervals.
        </p>

        <strong>YAML Scheduled Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>schedules:
- cron: '0 2 * * *'   # daily at 02:00 UTC
  displayName: 'Nightly build'
  branches:
    include:
      - main
  always: true</code></pre>

        <p><strong>UI Scheduler:</strong> You can also add schedules in Pipeline  Edit  Schedules for timezone-friendly scheduling and toggling without changing YAML.</p>

        <p><strong>In Practice:</strong> I schedule nightly builds for integration tests and security scans (Checkov/TFLint) so heavy scans run off-hours and artifacts are ready for the next day.</p>
      </div>`},{question:"How do you trigger pipelines conditionally (manual, scheduled, PR-based)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Conditional Pipeline Triggers</strong></h3>
        <p>
          Combine trigger types and use conditions/parameters to control when stages or jobs run.
        </p>

        <strong>Options:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>PR-based:</strong> <code>pr:</code> block in YAML runs CI for PRs.</li>
          <li> <strong>Scheduled:</strong> <code>schedules:</code> or UI scheduler for CRON runs.</li>
          <li> <strong>Manual:</strong> Use <code>trigger: none</code> and run via UI or REST API; or use environment approvals for manual gating.</li>
        </ul>

        <strong>Conditional Stage Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>trigger:
  branches:
    include: [main]

parameters:
- name: runE2E
  type: boolean
  default: false

stages:
- stage: Build
  jobs: ...

- stage: E2E
  condition: eq(parameters.runE2E, true)
  jobs: ...
</code></pre>

        <p><strong>In Practice:</strong> I use PR triggers for fast CI, scheduled triggers for nightly heavy tests, and parameters + manual runs for optional long-running stages (e.g., full integration tests).</p>
      </div>`},{question:"How do you handle multi-stage deployments (DEV  QA  UAT  PROD)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-stage Deployment Flow</strong></h3>
        <p>
          Implement stages for each environment and connect them with approvals, gates, and environment-specific variables. Use deployment jobs for environment-level features like approvals and deployment history.
        </p>

        <strong>Structure Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>stages:
- stage: Dev
  jobs: ...

- stage: QA
  dependsOn: Dev
  jobs: ...
  approvals: # via environment in ADO UI

- stage: UAT
  dependsOn: QA
  jobs: ...

- stage: Prod
  dependsOn: UAT
  jobs: ...
  # Prod has manual approver + monitoring gates</code></pre>

        <p><strong>Gates & Approvals:</strong> Use environment approvals (QA sign-off) and health checks (automated smoke tests) before promoting to the next stage.</p>

        <p><strong>In Practice:</strong> I configure auto-deploy to Dev/QA, QA sign-off for UAT, and a manual approval + canary deployment strategy for Prod to minimize risk.</p>
      </div>`},{question:"How do you deploy to multiple environments automatically?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated Multi-Environment Deployment</strong></h3>
        <p>
          Use the same pipeline with parameterized stages or templates that accept environment-specific values (variable groups or parameter files). This avoids duplicating pipelines per environment.
        </p>

        <strong>Template-driven Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># deploy-template.yml
parameters:
- name: envName
  type: string

jobs:
- deployment: deploy
  environment: \${{ parameters.envName }}
  strategy:
    runOnce:
      deploy:
        steps:
          - script: echo "Deploying to \${{ parameters.envName }}"

# main-pipeline.yml
stages:
- stage: DeployDev
  jobs:
    - template: deploy-template.yml
      parameters:
        envName: 'dev'

- stage: DeployProd
  jobs:
    - template: deploy-template.yml
      parameters:
        envName: 'prod'</code></pre>

        <p><strong>In Practice:</strong> I maintain variable groups per environment (DEV/QA/PROD) and pass them into templates  this keeps logic identical while changing config only.</p>
      </div>`},{question:"If we have one pipeline that must run for multiple environments  how do you parameterize it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Parameterizing a Single Pipeline</strong></h3>
        <p>
          Use top-level <code>parameters</code> in YAML for runtime values and environment selection. Combine with variable groups and templates for maintainability.
        </p>

        <strong>Parameter Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>parameters:
- name: targetEnv
  type: string
  default: 'dev'

variables:
- group: '$(targetEnv)-vars'   # resolved at runtime if allowed

stages:
- stage: Deploy
  jobs:
    - deployment: deployToEnv
      environment: \${{ parameters.targetEnv }}
      strategy:
        runOnce:
          deploy:
            steps:
              - script: echo "Deploying to \${{ parameters.targetEnv }}"</code></pre>

        <p><strong>How to run:</strong> Trigger with parameters from UI or via REST API to choose <code>targetEnv</code> (dev/qa/prod).</p>

        <p><strong>In Practice:</strong> I expose <code>targetEnv</code> as a runtime parameter in Release pipeline UI for manual runs and use CI triggers for automatic pushes to <code>dev</code> or <code>qa</code>.</p>
      </div>`}]},{title:"8. Approvals, Gates & Validations",questions:[{question:"How do you handle approvals and gates in pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Approvals and Gates in Pipelines</strong></h3>
        <p>
          Approvals and gates control when a deployment can proceed  ensuring human validation and automated checks before production release.
        </p>

        <strong>How I Handle It:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Approvals:</strong> Require manual sign-off before a critical stage (e.g., Prod).</li>
          <li> <strong>Gates:</strong> Add automated checks like service health or ticket validation.</li>
          <li> <strong>Environment-level:</strong> Use environments in Azure DevOps for built-in approval workflows.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My UAT  Prod stages use dual approvals (QA + Release Manager) and gates that check ServiceNow tickets and app health before promotion.</p>
      </div>`},{question:"How do you configure manual validations?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Manual Validation in Pipelines</strong></h3>
        <p>
          Manual validations pause the pipeline until a user manually confirms to continue or reject deployment.
        </p>

        <strong>YAML Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- stage: DeployProd
  jobs:
    - job: WaitForApproval
      pool: server
      steps:
        - task: ManualValidation@0
          inputs:
            notifyUsers: 'devops.manager@litmusit.com'
            instructions: 'Approve to deploy in production'</code></pre>

        <p><strong>In Practice:</strong>  
        I use <code>ManualValidation@0</code> in PROD pipelines to ensure manager approval before pushing to customer-facing systems.</p>
      </div>`},{question:"What is the difference between approvals in environments vs release pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Approvals: Environments vs Release Pipelines</strong></h3>
        <p>
          Both provide control before deployment, but differ in structure and scope.
        </p>

        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Environment Approvals</th><th>Release Pipeline Approvals</th></tr>
          <tr><td>Defined In</td><td>YAML Environments</td><td>Classic Release Pipeline UI</td></tr>
          <tr><td>Automation Level</td><td>Fully code-based, reusable</td><td>Manual UI setup</td></tr>
          <tr><td>Gates Support</td><td>Yes (YAML)</td><td>Yes (Classic)</td></tr>
          <tr><td>Use Case</td><td>Modern CI/CD with YAML</td><td>Legacy release flows</td></tr>
        </table>

        <p><strong>In Practice:</strong>  
        I prefer environment approvals in YAML pipelines since theyre version-controlled and portable across projects.</p>
      </div>`},{question:"How do you integrate approval flow before production deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Approval Flow Before Production Deployment</strong></h3>
        <p>
          Integrate approval gates at the production stage  so deployment only starts after authorized users approve.
        </p>

        <strong>Steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Create a <strong>Prod Environment</strong> in Azure DevOps.</li>
          <li>Add approvers under Environment  Approvals and Checks.</li>
          <li>Reference this environment in YAML deployment job.</li>
        </ul>

        <strong>Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- deployment: DeployProd
  environment: 'prod'
  strategy:
    runOnce:
      deploy:
        steps:
          - script: echo "Deploying to production"</code></pre>

        <p><strong>In Practice:</strong>  
        I add mandatory approvers (Lead + Manager) for PROD so no auto-deployment happens accidentally.</p>
      </div>`},{question:"What is an environment in ADO and how do you manage access controls?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environments in Azure DevOps</strong></h3>
        <p>
          An <strong>Environment</strong> represents a logical deployment target like Dev, QA, or Prod  with built-in approvals, security, and deployment tracking.
        </p>

        <strong>Features:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Track deployments per environment.</li>
          <li> Assign user/group permissions (who can deploy or approve).</li>
          <li> Define approvals & gates for controlled promotion.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My team defines ADO Environments for DEV, QA, UAT, PROD with RBAC  DevOps engineers deploy to lower envs, leads approve PROD.</p>
      </div>`},{question:"How do you handle validation and rollback for failed releases?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Validation & Rollback Handling</strong></h3>
        <p>
          Validation ensures a release is healthy; rollback recovers from failure using last known stable build or infrastructure state.
        </p>

        <strong>Approach:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Run post-deployment validation tests (smoke tests, health checks).</li>
          <li> Configure rollback tasks or re-deploy last successful artifact.</li>
          <li> For infra (Terraform), use remote state to roll back automatically.</li>
        </ul>

        <p><strong>YAML Tip:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>condition: failed()
steps:
  - script: echo "Reverting to previous version"
  - task: DownloadPipelineArtifact@2
    inputs:
      artifactName: 'previous-build'</code></pre>

        <p><strong>In Practice:</strong>  
        I automate rollback in Helm + Terraform deployments to restore last known stable version if health checks fail post-release.</p>
      </div>`},{question:"What are gates in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Gates in Azure DevOps</strong></h3>
        <p>
          <strong>Gates</strong> are automated checks that run before or after deployment to validate external conditions.
        </p>

        <strong>Common Gate Types:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Query monitoring tools like App Insights or Azure Monitor for health signals.</li>
          <li> Call REST APIs to check change requests (ServiceNow, Jira).</li>
          <li> Add delay timers for canary rollouts.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I configure App Insights gates to ensure no active incidents before Prod deployment and ServiceNow gates for approval ticket validation.</p>
      </div>`},{question:"What is a deployment gate in ADO?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deployment Gate</strong></h3>
        <p>
          A <strong>deployment gate</strong> is a pre-deployment or post-deployment condition that must pass before moving to the next stage.
        </p>

        <p><strong>Example Use:</strong></p>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Verify ServiceNow ticket is approved.</li>
          <li> Check App Insights error count &lt; threshold.</li>
          <li> Wait 30 mins for canary feedback before full rollout.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I configure post-deployment gates in PROD to validate monitoring metrics  deployment continues only if app remains stable for 10 mins.</p>
      </div>`},{question:"What is the purpose of approvals and gates?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Purpose of Approvals & Gates</strong></h3>
        <p>
          Approvals and gates together ensure <strong>safe, auditable, and controlled deployments</strong> in CI/CD pipelines.
        </p>

        <strong>Purpose:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Prevent unauthorized releases.</li>
          <li> Ensure pre-deployment validation & compliance checks.</li>
          <li> Enable traceability and accountability for production changes.</li>
          <li> Improve stability by catching issues before full rollout.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My PROD pipelines require dual manual approvals + automated ServiceNow gate + AppInsights gate  ensuring both governance and automation balance.</p>
      </div>`}]},{title:"9. Quality, Testing & Security",questions:[{question:"How do you integrate SonarQube for code quality analysis?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating SonarQube for Code Quality</strong></h3>
        <p>
          SonarQube ensures code quality, bug detection, and security analysis directly inside the CI pipeline.
        </p>

        <strong>Integration Steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Configure a <strong>SonarQube Service Connection</strong> in Azure DevOps.</li>
          <li> Install the SonarQube extension in your ADO organization.</li>
          <li> Add the <code>Prepare Analysis</code>, <code>Run Code Analysis</code>, and <code>Publish Quality Gate</code> tasks in your YAML pipeline.</li>
        </ul>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>steps:
- task: SonarQubePrepare@5
  inputs:
    SonarQube: 'SonarConnection'
    projectKey: 'app-key'
    projectName: 'WebApp'

- script: mvn clean install

- task: SonarQubeAnalyze@5

- task: SonarQubePublish@5
  inputs:
    pollingTimeoutSec: '300'</code></pre>

        <p><strong>In Practice:</strong>  
        Every PR triggers a build and SonarQube scan  failing builds if the Quality Gate doesnt meet standards (code coverage, bug threshold, vulnerability score).</p>
      </div>`},{question:"How do you handle unit and functional testing inside pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Unit & Functional Testing in Pipelines</strong></h3>
        <p>
          Testing ensures code reliability before deployment. I integrate both unit and functional test stages inside YAML pipelines.
        </p>

        <strong>Structure:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Unit Tests:</strong> Executed during the Build stage (e.g., Jest, JUnit, PyTest).</li>
          <li> <strong>Functional / Integration Tests:</strong> Run in separate test environments post-deploy.</li>
        </ul>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- stage: Test
  jobs:
    - job: RunTests
      steps:
        - script: npm run test:unit
        - script: npm run test:functional</code></pre>

        <p><strong>In Practice:</strong>  
        I use separate Test stages that fail early on critical test cases  preventing deployment if coverage or smoke test fails.</p>
      </div>`},{question:"How do you integrate Checkov, TFLint, and Terraform validation in your pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Checkov, TFLint & Terraform Validation</strong></h3>
        <p>
          Checkov and TFLint validate infrastructure code (Terraform) for compliance and security before applying.
        </p>

        <strong>Pipeline Steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Terraform Validate:</strong> Check syntax and structure.</li>
          <li> <strong>TFLint:</strong> Detect logical errors and naming violations.</li>
          <li> <strong>Checkov:</strong> Security & compliance scan for misconfigurations.</li>
        </ul>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>steps:
- script: terraform init
- script: terraform validate
- script: tflint --config .tflint.hcl
- script: checkov -d . --compact</code></pre>

        <p><strong>In Practice:</strong>  
        My Terraform pipelines fail automatically if Checkov finds high severity issues  ensuring IaC security before deployment.</p>
      </div>`},{question:"TFLint and Checkov make the pipeline slow  how can you optimize it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Optimizing Checkov & TFLint Performance</strong></h3>
        <p>
          These tools can be heavy, so optimization is key to keep CI times low.
        </p>

        <strong>My Optimization Steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Run them in parallel using separate jobs.</li>
          <li> Use <code>--directory</code> flags to limit scans only to changed Terraform modules.</li>
          <li> Cache <code>.terraform</code> and plugin directories to skip re-downloads.</li>
          <li> Run full scans nightly, but only delta scans (modified code) in PR pipelines.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I use a lightweight fast-check mode on PRs and a complete security scan on nightly builds  this keeps PR feedback fast while maintaining compliance.</p>
      </div>`},{question:"How do you ensure secure code scanning and compliance checks in pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Code Scanning & Compliance</strong></h3>
        <p>
          I enforce DevSecOps practices directly in the CI pipeline using static and dynamic analysis tools.
        </p>

        <strong>Typical Security Tools:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>SonarQube:</strong> For code smell, vulnerabilities, and coverage.</li>
          <li> <strong>Checkov / Trivy:</strong> For IaC and container image scans.</li>
          <li> <strong>Secret Scanning:</strong> Prevent accidental credential commits.</li>
        </ul>

        <p><strong>Example Integration:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- script: trivy image myapp:latest
- script: checkov -d ./infra</code></pre>

        <p><strong>In Practice:</strong>  
        I fail the build if vulnerability score > threshold (CVSS  7) and publish reports to ADO Summary Tab for visibility.</p>
      </div>`},{question:"How do you enable Splunk or monitoring integration for pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Splunk / Monitoring with Pipelines</strong></h3>
        <p>
          Integrating monitoring tools like Splunk or Application Insights helps track pipeline runs, errors, and deployment metrics in real-time.
        </p>

        <strong>Approach:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Send custom logs/events using REST API calls from YAML steps.</li>
          <li> Use <code>Invoke-RestMethod</code> or <code>curl</code> to post run metadata to Splunk HEC endpoint.</li>
          <li> Add Application Insights SDK in your deployed app for runtime telemetry.</li>
        </ul>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- script: |
    curl -k https://splunk.company.com:8088/services/collector       -H "Authorization: Splunk $SPLUNK_TOKEN"       -d '{"event":"Pipeline succeeded","project":"devops-app"}'</code></pre>

        <p><strong>In Practice:</strong>  
        I log every deployment start/finish event to Splunk  mapped with commit ID and user info, so ops can trace any issue easily.</p>
      </div>`},{question:"What are security practices used to secure CI/CD pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing the CI/CD Pipeline</strong></h3>
        <p>
          Security starts from source to deploy  the goal is to protect credentials, code, and build integrity.
        </p>

        <strong>Key Practices:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Use Azure Key Vault for all secrets, not plain variables.</li>
          <li> Restrict service connections via RBAC and least privilege.</li>
          <li> Enable branch protection and PR approvals.</li>
          <li> Implement signed artifact publishing.</li>
          <li> Add Checkov, SonarQube, and Trivy scans.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My organization uses managed identities for ADO pipelines  no stored passwords or service principals, everything resolved at runtime securely.</p>
      </div>`},{question:"What are security best practices in pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Security Best Practices in Pipelines</strong></h3>
        <p>
          Security is continuous. These are my personal DevOps golden rules:
        </p>

        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Use <strong>Key Vault</strong> + Variable Groups for secrets.</li>
          <li> Never store creds or tokens in YAML.</li>
          <li> Restrict agent permissions (read-only builds).</li>
          <li> Implement <strong>PR validations</strong> with SonarQube/Checkov scans.</li>
          <li> Keep dependencies updated and signed.</li>
          <li> Continuously monitor pipeline metrics and failures.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My ADO pipelines have built-in security gates  no deployment happens until both static code and IaC scans pass successfully.</p>
      </div>`}]},{title:"10. Terraform & Infrastructure Automation",questions:[{question:"How do you integrate Terraform execution in an ADO pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Terraform in Azure DevOps Pipelines</strong></h3>
        <p>
          Run Terraform as part of a YAML pipeline: initialize, plan (with output), review, then apply. Use service connections (ARM/SPN) for auth and secure secrets via Key Vault or variable groups.
        </p>

        <strong>Typical YAML steps:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>pool:
  vmImage: 'ubuntu-latest'

steps:
- task: UseTerraform@0   # or install terraform manually
  inputs:
    terraformVersion: '1.4.x'

- script: terraform init -backend-config="storage_account_name=$(tf_storage)"
- script: terraform plan -out=tfplan -input=false
- task: PublishPipelineArtifact@1
  inputs:
    targetPath: 'tfplan'
- script: terraform apply -input=false tfplan</code></pre>

        <p><strong>Key points:</strong> never run <code>apply</code> automatically on PRs  require manual approval for apply in protected branches.</p>
      </div>`},{question:"How will you create one resource in multiple subscriptions under the same pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Creating resources across multiple subscriptions</strong></h3>
        <p>
          Use multiple service connections (SPNs)  one per subscription  and parameterize the subscription / backend configuration per job/stage.
        </p>

        <strong>Approach:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Define each subscription as a service connection in ADO (e.g., subA-conn, subB-conn).</li>
          <li>Parameterize the pipeline with a list of targets and loop jobs/stages to deploy per-subscription.</li>
        </ul>

        <strong>YAML sketch:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>parameters:
- name: subscriptions
  type: object
  default:
    - { name: 'subA', connection: 'subA-conn', rg: 'rg-a' }
    - { name: 'subB', connection: 'subB-conn', rg: 'rg-b' }

stages:
- \${{ each sub in parameters.subscriptions }}:
  - stage: deploy_\${{ sub.name }}
    jobs:
    - job: terraform_\${{ sub.name }}
      steps:
        - script: |
            az login --service-principal -u $(clientId_\${{ sub.name }}) -p $(clientSecret_\${{ sub.name }}) --tenant $(tenant)
        - script: terraform apply -var "resource_group=\${{ sub.rg }}"</code></pre>

        <p><strong>In Practice:</strong> run terraform in isolated jobs per subscription to avoid cross-subscription state conflicts.</p>
      </div>`},{question:"How will you create a one-click pipeline that provisions full infrastructure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>One-click provisioning pipeline</strong></h3>
        <p>
          Build a multi-stage pipeline: Prep  Plan (staging)  Manual Approval  Apply (production). Use templates for each layer (network, infra, app). Publish plan artifacts and require approval before apply.
        </p>

        <strong>Design:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Stage 1: Validate & Plan  runs on feature branches and PRs.</li>
          <li>Stage 2: Plan for Target (staging/prod)  stores plan artifact.</li>
          <li>Stage 3: Manual approval gate.</li>
          <li>Stage 4: Apply  runs with SPN credentials and locked backend.</li>
        </ul>

        <strong>Why:</strong> prevents accidental destructive changes and provides auditable plan before apply.</p>
      </div>`},{question:"How do you run Terraform init/plan/apply securely in a pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Terraform Execution</strong></h3>
        <p>
          Use service principals, remote backend with locking, and secrets stored in Key Vault. Keep sensitive flags off logs and require approvals for apply.
        </p>

        <strong>Security checklist:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Store SPN credentials in Key Vault & fetch via variable group.</li>
          <li>Use remote backend (Azure Storage) with state locking (Blob lease) or use DynamoDB lock for AWS.</li>
          <li>Run <code>terraform plan -out=plan</code> and publish plan as artifact instead of printing sensitive values.</li>
          <li>Use <code>-input=false</code> and <code>-no-color</code> for CI runs.</li>
        </ul>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>terraform init -backend-config="key=env/terraform.tfstate"
terraform plan -out=tfplan -input=false
# publish tfplan, require approval
terraform apply -input=false tfplan</code></pre>

        <p><strong>In Practice:</strong> restrict SPN to least privilege and rotate secrets regularly; enable soft-delete & versioning on state storage.</p>
      </div>`},{question:"How do you handle Terraform state backend and secrets inside pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>State Backend & Secrets Management</strong></h3>
        <p>
          Use a remote backend for shared state and a secure vault for secrets. Ensure backend & secrets are protected with RBAC and access logging.
        </p>

        <strong>Backend options:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Azure: Storage Account + container + blob locking (use blob-level lease for locking).</li>
          <li>AWS: S3 + DynamoDB for state locking.</li>
        </ul>

        <strong>Secrets:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Store SPN/client secrets in Azure Key Vault and link to ADO variable group.</li>
          <li>Never echo secrets to logs; mask sensitive variables in pipeline settings.</li>
        </ul>

        <p><strong>In Practice:</strong> I use storage account with soft-delete and immutable policies, and Key Vault for credentials  granting only pipeline service principal 'Storage Blob Data Contributor' rights.</p>
      </div>`},{question:"How do you optimize long-running Terraform pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Optimizing Long-running Terraform Runs</strong></h3>
        <p>
          Split plan/apply, use parallelism wisely, cache providers, and run expensive checks off the PR path (nightly).
        </p>

        <Optimization tactics:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Plan-only on PRs:</strong> fast check for diffs; full apply only on approved merges.</li>
          <li> <strong>Modularize:</strong> split infra into smaller Terraform modules executed independently.</li>
          <li> <strong>Caching:</strong> cache terraform plugins & provider downloads across runs.</li>
          <li> <strong>Parallelism:</strong> control <code>-parallelism</code> to avoid API throttling.</li>
        </ul>

        <p><strong>In Practice:</strong> run heavy policy-as-code checks (OPA, Checkov full scan) in nightly runs while PRs run quick checks.</p>
      </div>`},{question:"How do you automate Terraform in CI/CD?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Terraform in CI/CD</strong></h3>
        <p>
          Automate plan and apply via pipelines, publish plans for approval, and use automation accounts or service principals to run applies with RBAC controls.
        </p>

        <Flow:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>PR triggers terraform validate & plan (fast checks).</li>
          <li>Merge to main triggers plan for target environment and publishes plan artifact.</li>
          <li>Manual approval gate  apply runs using pipeline service principal.</li>
        </ol>

        <p><strong>In Practice:</strong> I use pipeline artifacts and approvals so infrastructure changes are auditable  artifact contains exact plan that will be applied.</p>
      </div>`},{question:"How do you automate Terraform runs in CI/CD using Bash or PowerShell?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Terraform with Bash / PowerShell</strong></h3>
        <p>
          Use scripts to wrap terraform commands, handle environment variables, fetch secrets, and fail safely. Store these scripts in repo and call them from pipeline tasks.
        </p>

        <Bash example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>#!/bin/bash
set -euo pipefail

# fetch secrets (example: from Azure Key Vault via az cli)
az login --service-principal -u "$SPN_ID" -p "$SPN_SECRET" --tenant "$TENANT_ID"
az keyvault secret show --vault-name "$KV" -n tf-backend-key --query value -o tsv > backend-key

terraform init -backend-config="access_key=$(cat backend-key)"
terraform plan -out=tfplan -input=false
# Upload tfplan artifact for approval
</code></pre>

        <PowerShell example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>Set-StrictMode -Version Latest
az login --service-principal -u $env:SPN_ID -p $env:SPN_SECRET -t $env:TENANT_ID
$secret = az keyvault secret show --vault-name $env:KV -n tf-backend-key --query value -o tsv
terraform init -backend-config="access_key=$secret"
terraform plan -out=tfplan -input=false</code></pre>

        <p><strong>In Practice:</strong> wrap scripts with logging, error handling, and masking to avoid leaking secrets in logs.</p>
      </div>`}]},{title:"11. Pipeline Failures & Debugging",questions:[{question:"What are the common issues faced during pipeline runs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Issues in Pipeline Runs</strong></h3>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> Authentication errors  wrong service connection, expired token, or missing access.</li>
          <li> Missing dependencies or incorrect path references in build tasks.</li>
          <li> Network/firewall restrictions while accessing external registries or APIs.</li>
          <li> YAML indentation or variable resolution issues.</li>
          <li> Agent timeout or job queue delays due to limited pool capacity.</li>
        </ul>
        <p><strong>In Practice:</strong> I always start debugging from the error logs, check job summary, verify credentials, then re-run in debug mode using <code>system.debug=true</code>.</p>
      </div>`},{question:"If build fails though Dockerfile is correct  how do you troubleshoot?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Debugging Docker Build Failures</strong></h3>
        <p>Even a valid Dockerfile can fail in CI if environment or context is misconfigured.</p>

        <strong>Debug checklist:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Check if build context (path) is correct  ADO often runs from root, not src folder.</li>
          <li>Verify file references like <code>.env</code> or COPY paths.</li>
          <li>Ensure agent has Docker installed and correct permissions.</li>
          <li>Inspect logs with <code>--progress=plain</code> for actual failing layer.</li>
        </ul>

        <p><strong>In Practice:</strong> I reproduce locally using the same Dockerfile and tag as pipeline to identify whether issue is environment-specific or CI-related.</p>
      </div>`},{question:"How do you handle pipeline failures and send notifications?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Failures & Notifications</strong></h3>
        <p>
          Use post-job actions or ADO service hooks to trigger notifications on failure.
        </p>

        <strong>Example:</strong>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: PowerShell@2
  condition: failed()
  inputs:
    targetType: inline
    script: |
      Send-MailMessage -To 'team@litmusit.com' -Subject 'Pipeline Failed'       -Body 'Build $(Build.BuildNumber) failed. Please check logs.'</code></pre>

        <p><strong>In Practice:</strong> I use Teams & Slack webhooks via ADO service hooks for instant notifications when Prod or Infra pipelines fail.</p>
      </div>`},{question:"How do you rollback failed deployments automatically?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automatic Rollback on Failure</strong></h3>
        <p>Automate rollback using conditional jobs that trigger when deployment fails.</p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- stage: Rollback
  condition: failed()
  jobs:
    - job: revertToPrevious
      steps:
        - script: |
            az webapp deployment source config-zip --src last-successful.zip
        - script: echo "Rollback complete"</code></pre>

        <p><strong>In Practice:</strong> I maintain a last successful artifact and rollback environment to it automatically using <code>DownloadPipelineArtifact@2</code> task.</p>
      </div>`},{question:"How do you implement retry logic in YAML pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Retry Logic for Transient Failures</strong></h3>
        <p>Azure DevOps allows retry conditions using expressions and custom scripts.</p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>steps:
- task: Bash@3
  inputs:
    targetType: inline
    script: |
      for i in {1..3}; do
        echo "Attempt $i"
        curl https://api.company.com && break || sleep 10
      done</code></pre>

        <p><strong>In Practice:</strong> I use retries for API rate limits, network instability, or external service readiness in early deployment stages.</p>
      </div>`},{question:"What steps do you take if the pipeline is running too long?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline Performance Optimization</strong></h3>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Break monolithic jobs into parallel jobs.</li>
          <li>Cache dependencies (npm, Maven, Terraform providers).</li>
          <li>Skip redundant stages for unchanged code using <code>condition:</code>.</li>
          <li>Move heavy security scans to scheduled nightly pipelines.</li>
        </ul>
        <p><strong>In Practice:</strong> I use the <code>cache@2</code> task to store dependencies and reduced CI runtime from 22 min to 8 min.</p>
      </div>`},{question:"How do you debug authorization or permission-related issues?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Debugging Authorization Issues</strong></h3>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Verify pipelines service connection permissions on subscription/resource group.</li>
          <li>Check role assignment (Contributor vs Owner).</li>
          <li>Ensure pipeline agent identity has access to target resources.</li>
          <li>Run command with <code>--debug</code> flag to print REST call failures.</li>
        </ul>
        <p><strong>In Practice:</strong> I often test SPN manually using <code>az login</code> to confirm it can access required resources before running pipeline again.</p>
      </div>`},{question:"How do you manage concurrency and job parallelism to optimize execution time?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Concurrency & Job Parallelism</strong></h3>
        <p>Parallel jobs allow simultaneous stage execution while controlling system load.</p>

        <strong>Techniques:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Set <code>dependsOn: []</code> to allow independent stage runs.</li>
          <li>Use <code>jobs: [ job1, job2 ]</code> under one stage for parallel builds.</li>
          <li>Use <code>maxParallel:</code> in matrix jobs to cap concurrency.</li>
        </ul>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- stage: Build
  jobs:
  - job: api
  - job: ui</code></pre>

        <p><strong>In Practice:</strong> My microservices pipelines run 10 build jobs in parallel  reduces total time by ~60%.</p>
      </div>`},{question:"How do you troubleshoot a multi-stage pipeline issue?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Debugging Multi-stage Pipeline Issues</strong></h3>
        <p>Follow a systematic approach:</p>

        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Identify which stage failed from summary view.</li>
          <li>Check artifact passing or variable scope (often a cause).</li>
          <li>Run failed stage in isolation (<code>rerun failed jobs</code>).</li>
          <li>Enable <code>system.debug=true</code> for verbose logs.</li>
        </ol>

        <p><strong>In Practice:</strong> I log artifact paths and variables to confirm flow continuity between build  deploy stages.</p>
      </div>`},{question:"What happens if count=0 in pipeline execution?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Effect of count=0</strong></h3>
        <p>If <code>count=0</code> in resource creation logic (Terraform or ARM templates), that resource is skipped  no creation or deletion occurs.</p>

        <p><strong>In Practice:</strong> I use <code>count=0</code> to conditionally skip non-prod deployments or optional infra modules (e.g., bastion, VPN).</p>
      </div>`},{question:"How do you handle lock state if pipeline fails after apply?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Locked State after Failure</strong></h3>
        <p>Terraform backends like Azure Storage can leave state locked after a failed job.</p>

        <strong>Fix:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Use <code>terraform force-unlock &lt;lock-id&gt;</code> to manually release lock.</li>
          <li>Check for active lease in the blob storage and remove if job aborted.</li>
          <li>Ensure only one apply runs at a time per workspace.</li>
        </ul>

        <p><strong>In Practice:</strong> I always isolate plan/apply per workspace and add <code>lock=false</code> for read-only operations (validate, plan).</p>
      </div>`}]},{title:"12. Releases, Artifacts & Environments",questions:[{question:"What is the difference between build and release pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Build vs Release Pipelines</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Build Pipeline</th><th>Release Pipeline</th></tr>
          <tr><td>Purpose</td><td>Compiles and packages code</td><td>Deploys built artifacts to environments</td></tr>
          <tr><td>Trigger</td><td>Code push, PR merge</td><td>After successful build or manual approval</td></tr>
          <tr><td>Output</td><td>Artifact (zip, image, package)</td><td>Running application</td></tr>
          <tr><td>Stages</td><td>Build & Test</td><td>Deploy, Approve, Validate</td></tr>
        </table>

        <p><strong>In Practice:</strong>  
        My pipelines are YAML-based, so build and release are in a single multi-stage pipeline.  
        But for legacy projects, we still use separate classic release pipelines with environment approvals.</p>
      </div>`},{question:"How do you use artifacts between build and release pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using Artifacts Across Pipelines</strong></h3>
        <p>
          Artifacts are the link between build and deployment stages. Build pipelines publish them; release pipelines consume them.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># Build pipeline
steps:
- task: PublishBuildArtifacts@1
  inputs:
    PathtoPublish: '$(Build.ArtifactStagingDirectory)'
    ArtifactName: 'drop'

# Release pipeline (YAML)
resources:
  pipelines:
    - pipeline: buildPipeline
      source: 'my-app-build'
      trigger: true

stages:
- stage: Deploy
  jobs:
    - job: deploy
      steps:
        - download: buildPipeline
        - script: echo "Deploying $(Pipeline.Workspace)/drop"</code></pre>

        <p><strong>In Practice:</strong>  
        I always version artifacts using build numbers or git commit SHA  ensures reproducible deployments.</p>
      </div>`},{question:"What is the difference between build artifacts and external feeds?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Build Artifacts vs External Feeds</strong></h3>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li><strong>Build Artifacts:</strong> Outputs generated by a build (e.g., .zip, .jar, Docker image) stored temporarily in ADO or storage.</li>
          <li><strong>External Feeds:</strong> Permanent package repositories (e.g., ADO Artifacts, npm, NuGet, Maven) used for dependency sharing.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I publish compiled libs to ADO Artifact feeds so other teams can consume them, and use build artifacts only for deployment deliverables.</p>
      </div>`},{question:"How do you manage artifacts and publish them securely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Artifact Management</strong></h3>
        <p>Use Azure Artifacts service or private registries (ACR, Nexus) with RBAC and token-based authentication.</p>

        <strong>Best Practices:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Use personal access tokens (PATs) or service connections for publishing.</li>
          <li>Enable retention policies to auto-clean old builds.</li>
          <li>Restrict feed access to specific pipelines or users.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My org uses private ACR and ADO feeds for artifacts; only service principals with contributor role can push or pull images.</p>
      </div>`},{question:"Where do you deploy the artifacts (VMs, AKS, App Service)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Artifact Deployment Targets</strong></h3>
        <p>
          Deployment targets depend on the application architecture:
        </p>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li> <strong>Web Apps:</strong> Azure App Service, AKS, or Containers.</li>
          <li> <strong>Microservices:</strong> Azure Kubernetes Service (Helm charts, YAML manifests).</li>
          <li> <strong>Legacy Apps:</strong> Deployed on VMs using Deployment Groups or WinRM tasks.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I deploy .NET and Node.js apps to App Service; containerized workloads to AKS via Helm in YAML pipelines.</p>
      </div>`},{question:"How do you manage releases across DEV, UAT, and PROD?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Multi-Environment Releases</strong></h3>
        <p>
          Use multi-stage YAML pipelines or classic releases with defined environments (DEV  QA  UAT  PROD) and approvals.
        </p>

        <strong>Approach:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Deploy automatically to DEV and QA.</li>
          <li>Require manual approval for UAT and PROD.</li>
          <li>Use variable groups and Key Vaults per environment.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I use YAML-based environment deployments; QA sign-off is mandatory before promoting to PROD to avoid unverified changes.</p>
      </div>`},{question:"What are environment approvals and how are they configured?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environment Approvals</strong></h3>
        <p>
          Approvals ensure only authorized users can deploy to sensitive environments.
        </p>

        <strong>Configuration Steps:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Create environment in ADO (e.g., 'prod').</li>
          <li>Add approvers and checks under Environment settings  Approvals & Checks.</li>
          <li>Reference the environment in YAML via <code>environment:</code> keyword.</li>
        </ol>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- deployment: DeployProd
  environment: 'prod'
  strategy:
    runOnce:
      deploy:
        steps:
          - script: echo "Deploying to PROD"</code></pre>

        <p><strong>In Practice:</strong>  
        I configure multiple approvers for PROD (Manager + DevOps Lead) and track approvals in ADO audit logs.</p>
      </div>`},{question:"What is a release vs build vs artifact?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Release vs Build vs Artifact</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Term</th><th>Description</th></tr>
          <tr><td><strong>Build</strong></td><td>Process that compiles code and runs tests.</td></tr>
          <tr><td><strong>Artifact</strong></td><td>Output of build  deployable package or image.</td></tr>
          <tr><td><strong>Release</strong></td><td>Deployment of that artifact to an environment.</td></tr>
        </table>

        <p><strong>In Practice:</strong>  
        Build creates Docker image  pushed to ACR (artifact)  release pipeline deploys it to AKS (release).</p>
      </div>`},{question:"What is an artifact repository?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Artifact Repository</strong></h3>
        <p>
          A centralized storage for versioned build outputs  ensures consistency across teams and environments.
        </p>

        <strong>Examples:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Azure Artifacts</li>
          <li>JFrog Artifactory</li>
          <li>Nexus</li>
          <li>GitHub Packages</li>
        </ul>

        <p><strong>In Practice:</strong>  
        We use Azure Artifacts to host NuGet and npm packages  accessible by multiple teams with RBAC control.</p>
      </div>`},{question:"What is release pipeline in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Release Pipeline Overview</strong></h3>
        <p>
          A release pipeline automates deployment of build artifacts across environments with approvals, gates, and rollback control.
        </p>

        <strong>Features:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Multi-environment deployment stages.</li>
          <li>Manual approvals and automated gates.</li>
          <li>Artifact version tracking.</li>
          <li>Rollback and retention policies.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I use YAML-based release stages  each with its own environment and gate checks, integrated with ServiceNow approval for production releases.</p>
      </div>`}]},{title:"13. Boards & Agile",questions:[{question:"What is a work item?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Work Item in Azure Boards</strong></h3>
        <p>
          A <strong>Work Item</strong> represents a single task, bug, feature, or user story tracked in Azure Boards.  
          Each work item has fields like Title, Assigned To, State, and Links (commits, PRs, builds).
        </p>
        <p><strong>In Practice:</strong>  
          I link every commit and PR to a work item  it gives full traceability from code  build  release.
        </p>
      </div>`},{question:"What is a sprint?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sprint in Agile Methodology</strong></h3>
        <p>
          A <strong>Sprint</strong> is a fixed time-box (usually 23 weeks) in which a team plans, develops, tests, and delivers specific features or improvements.
        </p>
        <p><strong>In Practice:</strong>  
          Our team runs 2-week sprints  we use the Sprint Board in ADO to track progress via To Do  In Progress  Done swimlanes.
        </p>
      </div>`},{question:"What is a stakeholder in ADO?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Stakeholder Role in Azure DevOps</strong></h3>
        <p>
          A <strong>Stakeholder</strong> is a non-technical user (e.g., Product Owner, Manager, Client) who can view dashboards, track progress, and approve work  without needing a paid license.
        </p>
        <p><strong>In Practice:</strong>  
          Product Owners use Stakeholder access to review backlog items and approve sprint goals after planning meetings.
        </p>
      </div>`},{question:"What is the difference between sprint and work item?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Difference Between Sprint & Work Item</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Sprint</th><th>Work Item</th></tr>
          <tr><td>Definition</td><td>Time-boxed iteration</td><td>Task or unit of work inside sprint</td></tr>
          <tr><td>Purpose</td><td>Organize work for a period</td><td>Track specific deliverables or issues</td></tr>
          <tr><td>Ownership</td><td>Team level</td><td>Individual level</td></tr>
        </table>
        <p><strong>In Practice:</strong>  
          We create a sprint first, then assign multiple work items (tasks, bugs, stories) to that sprint.
        </p>
      </div>`},{question:"What methodology do you follow in Azure Boards?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Agile Methodology in Azure Boards</strong></h3>
        <p>
          We follow <strong>Scrum</strong> for iterative development  plan sprints, track user stories, and measure velocity using burndown charts.
        </p>
        <p><strong>Process Flow:</strong></p>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Create Epic  Feature  User Story  Task hierarchy.</li>
          <li>Each sprint includes prioritized stories with acceptance criteria.</li>
          <li>Daily stand-ups and sprint retrospectives to track progress.</li>
        </ul>
        <p><strong>In Practice:</strong>  
          We visualize tasks on Scrum Board and use queries to generate sprint velocity reports.</p>
      </div>`},{question:"How do you restrict an ADO user to only pipelines and repo access?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Restricting User Access in Azure DevOps</strong></h3>
        <p>
          Use ADOs <strong>Security & Permissions</strong> to control user-level access:
        </p>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Navigate to <code>Project Settings  Permissions</code>.</li>
          <li>Assign user to a custom group (e.g., DevOps-Contributor).</li>
          <li>Grant access to Repos & Pipelines only, deny Boards & Test Plans.</li>
        </ul>
        <p><strong>In Practice:</strong>  
          I maintain custom permission groups  developers can trigger pipelines but not modify board items or environment gates.</p>
      </div>`},{question:"Agile vs Kanban boards  which one have you used and why?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Agile vs Kanban  Practical Comparison</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Scrum (Agile)</th><th>Kanban</th></tr>
          <tr><td>Structure</td><td>Work in Sprints</td><td>Continuous Flow</td></tr>
          <tr><td>Planning</td><td>Fixed sprint goal</td><td>No fixed iterations</td></tr>
          <tr><td>Metrics</td><td>Velocity, Burndown</td><td>Lead Time, Cycle Time</td></tr>
          <tr><td>Use Case</td><td>Feature development</td><td>Maintenance, support tasks</td></tr>
        </table>
        <p><strong>In Practice:</strong>  
          Our product teams use Scrum for development and Kanban for Infra/DevOps teams  it suits ongoing operational work without sprint pressure.</p>
      </div>`}]},{title:"14. Real-World Scenarios",questions:[{question:"You have two Azure subscriptions  how do you give access to new resources?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Granting Access Across Subscriptions</strong></h3>
        <p>
          I use <strong>Azure RBAC</strong> and <strong>Service Connections</strong> in ADO to handle cross-subscription access.
        </p>

        <strong>Steps:</strong>
        <ol style="margin:0 0 .5rem 1.2rem;">
          <li>Create a separate Service Principal (SPN) for each subscription.</li>
          <li>Assign <code>Contributor</code> or <code>Reader</code> role on required Resource Group only (least privilege).</li>
          <li>Configure a Service Connection in ADO using that SPN.</li>
          <li>Use pipeline parameters to switch between subscriptions dynamically.</li>
        </ol>

        <p><strong>In Practice:</strong>  
        I maintain one pipeline that deploys infra to multiple subscriptions by switching service connection context per stage.</p>
      </div>`},{question:"How will you manage multi-environment pipelines with conditional triggers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Multi-Environment Pipelines</strong></h3>
        <p>
          I use multi-stage YAML pipelines with conditional triggers and variable groups for each environment.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>trigger:
  branches:
    include:
      - main
      - dev

stages:
- stage: Dev
  condition: eq(variables['Build.SourceBranchName'], 'dev')

- stage: Prod
  condition: eq(variables['Build.SourceBranchName'], 'main')</code></pre>

        <p><strong>In Practice:</strong>  
        DEV auto-deploys on commit; PROD waits for approval.  
        This ensures safe promotion flow without maintaining multiple pipelines.</p>
      </div>`},{question:"How would you rollback a production deployment if a bug is found later?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rolling Back Production Deployment</strong></h3>
        <p>
          I maintain versioned artifacts (Docker tags or zip files) and rollback by redeploying the last successful version.
        </p>

        <strong>Rollback steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Identify the last successful release (from ADO Releases tab or tags).</li>
          <li>Redeploy using the same artifact ID or image tag.</li>
          <li>Update monitoring (Grafana/Prometheus) to validate recovery.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I tag each successful deployment as <code>prod-stable-vX.Y</code> so rollback is one-click from the artifact history.</p>
      </div>`},{question:"You are asked to add one resource in multiple subscriptions  how will you implement that?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deploying to Multiple Subscriptions</strong></h3>
        <p>
          I parameterize the subscription ID and loop deployment stages or use Terraform workspaces.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>parameters:
- name: subs
  type: object
  default:
    - sub1
    - sub2

stages:
- \${{ each s in parameters.subs }}:
  - stage: Deploy_\${{ s }}
    jobs:
      - job: deploy
        steps:
          - script: az login --service-principal -u $(spnId) -p $(spnKey) -t $(tenant)
          - script: terraform apply -var "subscription=\${{ s }}"</code></pre>

        <p><strong>In Practice:</strong>  
        I maintain one reusable pipeline template; looping ensures consistent infra deployment across all subscriptions.</p>
      </div>`},{question:"You added Checkov and TFLint  pipeline became slow. How do you speed it up?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Optimizing Slow Terraform Security Scans</strong></h3>
        <p>
          I optimize by separating quick checks (syntax, validate) and heavy scans (Checkov/TFLint full) into different jobs.
        </p>

        <strong>Optimizations:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Run lint/validate in PR pipelines only.</li>
          <li>Run Checkov full scans in nightly scheduled pipelines.</li>
          <li>Use <code>--skip-check</code> for irrelevant policies.</li>
          <li>Enable caching for provider downloads.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        This reduced our Terraform pipeline from 26 min to 9 min while maintaining compliance quality.</p>
      </div>`},{question:"A developer ran a pipeline but authorization failed  what are the root causes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Debugging Authorization Failures</strong></h3>
        <p>
          Authorization errors usually mean permission or token scope mismatch.
        </p>

        <strong>Common causes:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Service connection not authorized for that pipeline.</li>
          <li>Expired PAT or SPN secret.</li>
          <li>Pipeline not granted access to the resource group/subscription.</li>
          <li>Allow access to all pipelines not enabled in service connection settings.</li>
        </ul>

        <p><strong>Fix:</strong>  
        Re-authorize connection, renew secrets, and ensure <code>Contributor</code> role on resource scope.</p>
      </div>`},{question:"A deployment failed midway  how do you recover?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recovering from Partial Deployment</strong></h3>
        <p>
          I use Terraforms <strong>state consistency</strong> or Azure deployment history to rollback incomplete changes.
        </p>

        <strong>Steps:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Check logs for last successful resource creation.</li>
          <li>Run <code>terraform plan</code> again  it auto-detects drift and brings state back in sync.</li>
          <li>Manually delete failed half-created resources if required.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I always store state remotely (Azure Blob) so even if a job fails, another rerun resumes safely from state.</p>
      </div>`},{question:"What branching strategy did you use for hotfixes and releases?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Branching for Hotfix & Release</strong></h3>
        <p>
          We follow <strong>GitFlow</strong> with short-lived feature branches and separate hotfix branches from production.
        </p>

        <strong>Flow:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li><code>main</code>  stable production branch.</li>
          <li><code>develop</code>  active sprint work.</li>
          <li><code>feature/*</code>  new feature branches.</li>
          <li><code>hotfix/*</code>  fixes applied directly to <code>main</code> then merged back to <code>develop</code>.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        Hotfix pipelines auto-deploy to PROD after approval; once verified, they are merged back to maintain sync.</p>
      </div>`},{question:"If two teams have access to the same repo, how do you auto-raise a PR?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Auto-Raising Pull Requests Across Teams</strong></h3>
        <p>
          I use automation scripts or GitHub Actions / ADO APIs to create PRs automatically when a branch updates.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: PowerShell@2
  inputs:
    targetType: inline
    script: |
      az repos pr create --repository myrepo         --source feature/teamA         --target main         --title "Auto PR from Team A"</code></pre>

        <p><strong>In Practice:</strong>  
        This ensures integration branches always stay up-to-date without manual PR creation.</p>
      </div>`},{question:"How to manage 1500 subscriptions using ADO?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Large-Scale Multi-Subscription Infra</strong></h3>
        <p>
          At scale, use <strong>Management Groups</strong>, <strong>Service Principals with delegated permissions</strong>, and parameterized pipelines.
        </p>

        <strong>Strategy:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Group subscriptions under Management Groups for RBAC inheritance.</li>
          <li>Use Terraform or ARM templates with <code>for_each</code> loops.</li>
          <li>Integrate a metadata CSV or key vault list to feed pipeline parameters dynamically.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        We manage 1000+ subs via one pipeline template  dynamically looping through IDs fetched from Key Vault secrets.</p>
      </div>`},{question:"What is parallelism in pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Parallelism in Pipelines</strong></h3>
        <p>
          Parallelism allows multiple jobs or stages to execute at the same time  improving speed and resource utilization.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>jobs:
- job: Build_UI
- job: Build_API
- job: Infra_Deploy
  dependsOn: []   # runs in parallel</code></pre>

        <p><strong>In Practice:</strong>  
        I run microservices builds in parallel  reduced total CI time from 25 min  7 min.  
        Use <code>maxParallel</code> to control resource limits safely.</p>
      </div>`}]},{title:"15. CI/CD Concepts",questions:[{question:"What is CI/CD and why do we use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CI/CD Overview</strong></h3>
        <p>
          <strong>CI/CD (Continuous Integration / Continuous Delivery or Deployment)</strong> automates code build, test, and deployment  ensuring faster and more reliable software delivery.
        </p>

        <strong>CI (Continuous Integration):</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Every commit triggers automated build & unit tests.</li>
          <li>Detects integration bugs early.</li>
        </ul>

        <strong>CD (Continuous Delivery / Deployment):</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Automates packaging and deployment to environments.</li>
          <li>Ensures consistent, error-free releases.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        My pipelines automatically build, test, and deploy Docker images to AKS  reducing release time from 2 hours  15 minutes.</p>
      </div>`},{question:"What is Continuous Delivery vs Continuous Deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Continuous Delivery vs Continuous Deployment</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Continuous Delivery</th><th>Continuous Deployment</th></tr>
          <tr><td>Definition</td><td>Code is ready for deployment; needs manual approval</td><td>Code auto-deploys after tests</td></tr>
          <tr><td>Control</td><td>Manual gate before production</td><td>Fully automated release</td></tr>
          <tr><td>Risk</td><td>Lower (manual check)</td><td>Higher (auto push)</td></tr>
        </table>

        <p><strong>In Practice:</strong>  
        We follow <strong>Continuous Delivery</strong>  PROD deploys require approval; lower envs (DEV/UAT) are fully automated.</p>
      </div>`},{question:"What is blue-green deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Blue-Green Deployment</strong></h3>
        <p>
          Two identical environments  <code>Blue</code> (current) and <code>Green</code> (new).  
          Traffic switches to Green only after validation  ensuring zero downtime.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>Current: Blue  serving live traffic  
Deploy: Green  validate  
Switch: Route traffic to Green  
Rollback: Switch back to Blue if issues</code></pre>

        <p><strong>In Practice:</strong>  
        I use Azure Traffic Manager to route 100% traffic to Green after smoke tests, then decommission Blue after 24 hours.</p>
      </div>`},{question:"What is canary deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Canary Deployment</strong></h3>
        <p>
          A progressive rollout strategy  deploy to a small percentage of users first, monitor metrics, then gradually increase traffic.
        </p>

        <strong>Example:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>10 % traffic  Canary v2</li>
          <li>Monitor errors & latency</li>
          <li>100 % rollout after validation</li>
        </ul>

        <p><strong>In Practice:</strong>  
        Implemented in AKS using <code>Ingress</code> rules or <strong>Argo Rollouts</strong> to control weighted traffic routing.</p>
      </div>`},{question:"What is rollback in CI/CD?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rollback Strategy</strong></h3>
        <p>
          Rollback means reverting to the last known stable release after a failed deployment.
        </p>

        <strong>Rollback Approaches:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Redeploy previous artifact version.</li>
          <li>Switch traffic in Blue-Green setup.</li>
          <li>Use Helm <code>rollback</code> command in Kubernetes.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I store build artifacts with version tags (v1.2.5)  rollback is simply re-deploying the older image tag.</p>
      </div>`},{question:"What are microservices?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Microservices Architecture</strong></h3>
        <p>
          Microservices break a large app into smaller independent services  each has its own codebase, CI/CD, and deployment lifecycle.
        </p>

        <strong>Benefits:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Independent scaling and deployment.</li>
          <li>Failure isolation  one service crash doesnt kill the system.</li>
          <li>Polyglot tech  each service can use best-fit stack.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        We deploy each microservice via its own YAML pipeline  Docker image  ACR  AKS namespace.</p>
      </div>`},{question:"What is DevSecOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DevSecOps Concept</strong></h3>
        <p>
          DevSecOps integrates <strong>security early</strong> into the CI/CD process  shift-left security.
        </p>

        <strong>Typical Integrations:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Static code scans (SonarQube, Snyk).</li>
          <li>IaC scans (Checkov, TFLint, OPA).</li>
          <li>Container image vulnerability checks (Trivy, Aqua).</li>
          <li>Secret detection via GitHub Advanced Security.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I integrated <code>Checkov</code> + <code>Trivy</code> into pipelines  if any high severity issue is found, build fails before deployment.</p>
      </div>`},{question:"What are SRE principles?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Site Reliability Engineering (SRE) Principles</strong></h3>
        <p>
          SRE ensures reliability and performance of services through automation, observability, and incident response.
        </p>

        <strong>Core Principles:</strong>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Define <strong>SLI/SLO/SLA</strong>  measure uptime and latency.</li>
          <li>Use <strong>Error Budgets</strong> to balance reliability vs innovation.</li>
          <li>Automate operations  self-healing, auto-rollback.</li>
          <li>Post-mortems for incidents.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I use Prometheus + Grafana for SLI metrics and automated rollback when error rate > SLO threshold.</p>
      </div>`},{question:"What are environment variables in a pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environment Variables</strong></h3>
        <p>
          Key-value pairs used to store dynamic values like paths, secrets, and environment-specific configurations in pipelines.
        </p>

        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>variables:
  env: "prod"
  connection: "$(azureServiceConn)"</code></pre>

        <p><strong>In Practice:</strong>  
        I use environment variables for region, storage name, and image tags so same YAML works for all environments.</p>
      </div>`},{question:"How do you ensure pipeline security?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline Security Best Practices</strong></h3>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Use Key Vault + variable groups for secrets.</li>
          <li>Enable approvals for PROD deployments.</li>
          <li>Restrict service connection access (least privilege).</li>
          <li>Scan dependencies & containers for vulnerabilities.</li>
          <li>Mask secrets in logs and disable system.debug for PROD runs.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        I integrate Checkov + Trivy + Microsoft Defender scanning before apply  any critical CVE blocks release.</p>
      </div>`},{question:"What are the benefits of automation in DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automation Benefits</strong></h3>
        <ul style="margin:0 0 .5rem 1.2rem;">
          <li>Removes human error and manual effort.</li>
          <li>Enables faster, repeatable deployments.</li>
          <li>Improves reliability, consistency, and traceability.</li>
          <li>Enhances developer productivity and feedback loops.</li>
        </ul>

        <p><strong>In Practice:</strong>  
        Automation reduced manual release steps from 30 to 3 clicks  everything from build  security scan  deploy  notify runs automatically.</p>
      </div>`}]},{title:"16. Terraform & Azure DevOps Integration",questions:[{question:"Have you worked with Azure DevOps? Explain end-to-end flow for deploying Terraform in pipeline.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>End-to-End Terraform Deployment Flow in Azure DevOps</strong></h3>
        <p>
          Yes  Ive implemented multiple Terraform deployments via Azure DevOps YAML pipelines for provisioning infrastructure in Azure using Service Principal authentication.
        </p>
        <h4> <strong>Typical End-to-End Flow:</strong></h4>
        <ol style="margin-left:1.2rem;">
          <li><strong>Code:</strong> Store Terraform code (.tf files) in Azure Repos or GitHub.</li>
          <li><strong>Service Connection:</strong> Create an Azure RM connection using a Service Principal (SPN) for authentication.</li>
          <li><strong>Pipeline Trigger:</strong> Trigger the pipeline on PR merge or commit to main branch.</li>
          <li><strong>Initialize:</strong> Run <code>terraform init</code> to download providers and configure backend (Azure Storage for remote state).</li>
          <li><strong>Validate & Plan:</strong> Run <code>terraform validate</code> and <code>terraform plan</code> to ensure code correctness and preview infra changes.</li>
          <li><strong>Approval:</strong> Manual approval gate for production before applying changes.</li>
          <li><strong>Apply:</strong> Run <code>terraform apply</code> to provision infrastructure.</li>
          <li><strong>Post-Deployment:</strong> Output values (e.g., VM IP, resource group name) are exported using <code>terraform output -json</code>.</li>
        </ol>
        <pre><code># Example YAML Snippet
- task: TerraformTaskV4@4
  inputs:
    provider: 'azurerm'
    command: 'init'
    backendServiceArm: 'sc-azure-prod'
    backendAzureRmResourceGroupName: 'riteshnew'
    backendAzureRmStorageAccountName: 'riteshsttg'
    backendAzureRmContainerName: 'riteshcontainer'
    backendAzureRmKey: 'main.tfstate'</code></pre>
        <p><strong>In Practice:</strong> I structure Terraform modules for reusability (network, compute, storage), store state remotely in Azure Storage, and enforce approval gates for critical deployments.</p>
      </div>`},{question:"What are the typical steps in a CI pipeline for Terraform deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Typical CI/CD Stages for Terraform Deployment</strong></h3>
        <p>
          A Terraform pipeline in Azure DevOps follows a clean modular structure with clearly defined stages:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Checkout Code:</strong> Pull Terraform code from Azure Repos or GitHub.</li>
          <li> <strong>Azure Login:</strong> Authenticate via Service Principal or Managed Identity.</li>
          <li> <strong>Init:</strong> Initialize the backend and provider configuration.</li>
          <li> <strong>Validate:</strong> Validate the syntax and dependencies.</li>
          <li> <strong>Plan:</strong> Generate an execution plan and store as artifact (<code>terraform plan -out=tfplan</code>).</li>
          <li> <strong>Manual Gate (Optional):</strong> Approval for production environments.</li>
          <li> <strong>Apply:</strong> Execute <code>terraform apply</code> with stored plan.</li>
          <li> <strong>Post-Deployment:</strong> Publish Terraform outputs as pipeline variables or store in Key Vault.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate linting (Checkov or tfsec) before the plan stage and use separate pipelines for PR validation vs. production apply to maintain governance.</p>
      </div>`},{question:"What are deployment groups and task groups in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deployment Groups vs Task Groups  Azure DevOps</strong></h3>
        <p>
          Both help in managing large-scale, repeatable deployments but serve different purposes.
        </p>
        <h4> <strong>Deployment Group:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Represents a set of target machines (on-prem or Azure VMs) for deployments.</li>
          <li>Used in <strong>Classic Release Pipelines</strong>  agents installed on each target machine.</li>
          <li>Ideal for hybrid environments or app deployments to multiple servers.</li>
        </ul>
        <h4> <strong>Task Group:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Reusable collection of tasks combined into a single unit.</li>
          <li>Promotes consistency across multiple pipelines.</li>
          <li>Example: Terraform Init + Plan group used across all infrastructure projects.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Task Groups to standardize Terraform deployment steps and Deployment Groups for legacy VM deployments where agent-based release orchestration is needed.</p>
      </div>`},{question:"What is a sprint and how does it differ from a work item in Azure Boards?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sprint vs Work Item  Agile in Azure Boards</strong></h3>
        <p>
          In Azure Boards, <strong>Sprint</strong> represents a time-boxed iteration of work, while <strong>Work Items</strong> are individual units of effort tracked within that sprint.
        </p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Sprint</th><th>Work Item</th></tr>
          <tr><td>Definition</td><td>Time-bound iteration (24 weeks)</td><td>Task, Bug, Feature, or User Story</td></tr>
          <tr><td>Purpose</td><td>Organize and plan deliverables</td><td>Track specific work progress</td></tr>
          <tr><td>Ownership</td><td>Team-level</td><td>Individual or team member</td></tr>
        </table>
        <p><strong>In Practice:</strong> During sprint planning, I map Terraform module enhancements or infra automation tickets (work items) into sprints to track delivery in Azure Boards.</p>
      </div>`},{question:"What is the process for a Pull Request (PR)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pull Request (PR) Process in Azure Repos</strong></h3>
        <p>
          A <strong>Pull Request</strong> is the standard mechanism for merging code changes while enforcing reviews and validations.
        </p>
        <ol style="margin-left:1.2rem;">
          <li>Developer commits Terraform or YAML changes in a feature branch.</li>
          <li>Creates a Pull Request targeting <code>main</code> or <code>develop</code> branch.</li>
          <li>Code reviewers validate changes, comments, and approve.</li>
          <li>Automated checks (lint, plan, unit tests) run as branch policies.</li>
          <li>Once approved, PR merges into main  triggering the deployment pipeline.</li>
        </ol>
        <p><strong>In Practice:</strong> I enforce branch policies with minimum 2 approvers and mandatory terraform plan validation pipeline before any PR is merged to main branch.</p>
      </div>`},{question:"What are security best practices in Azure DevOps pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Security Best Practices in Azure DevOps Pipelines</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Store secrets in <strong>Azure Key Vault</strong>  never in variables or YAML directly.</li>
          <li> Use <strong>Service Principals</strong> with least privilege RBAC (Contributor at RG level, not subscription).</li>
          <li> Enable <strong>Approvals & Checks</strong> before production deploys.</li>
          <li> Use <strong>Private Agents</strong> for sensitive workloads (not hosted agents).</li>
          <li> Implement <strong>Branch Policies</strong> and signed commits for code integrity.</li>
          <li> Enable <strong>Pipeline Permissions</strong> and limit variable group access.</li>
          <li> Regularly rotate credentials and secrets.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Key Vault-backed variable groups and service connections with Managed Identity to completely eliminate static credentials in Terraform pipelines.</p>
      </div>`},{question:"What are gates in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Gates  Automated Quality Checks Before Deployment</strong></h3>
        <p>
          <strong>Gates</strong> are pre-deployment quality or compliance checks that must pass before a release can proceed to the next stage.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Query REST APIs or Azure Monitor alerts before deploying.</li>
          <li> Validate cost compliance or environment health.</li>
          <li> Used in multi-stage release pipelines for production environments.</li>
        </ul>
        <p><strong>Example:</strong> Before Terraform apply stage, a gate checks if budget utilization < 90% or no active alerts exist on the target resource group.</p>
        <p><strong>In Practice:</strong> I configure gates in release pipelines to verify environment stability and approval before infra provisioning continues.</p>
      </div>`},{question:"Is macOS supported as an agent pool in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>macOS Agents in Azure DevOps</strong></h3>
        <p>
          Yes  Azure DevOps supports <strong>macOS agents</strong> as part of Microsoft-hosted agent pools.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Useful for building iOS, macOS, and cross-platform mobile apps.</li>
          <li> macOS agents include pre-installed tools (Xcode, Homebrew, .NET, Node, etc.).</li>
          <li> YAML Example:</li>
        </ul>
        <pre><code>pool:
  vmImage: 'macOS-latest'</code></pre>
        <p><strong>In Practice:</strong> Ive used macOS-latest agent for mobile pipeline builds while using Linux agents for Terraform and containerized workloads.</p>
      </div>`},{question:"Explain service connections and what is needed to create them.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure DevOps Service Connections  Secure Resource Authentication</strong></h3>
        <p>
          A <strong>Service Connection</strong> allows Azure DevOps pipelines to authenticate and interact securely with external systems such as Azure, GitHub, Docker Hub, or Artifactory.
        </p>
        <h4> <strong>Common Types:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Resource Manager (ARM):</strong> Used for Terraform, ARM templates, and Azure CLI tasks.</li>
          <li> <strong>GitHub / Bitbucket:</strong> For source control integration.</li>
          <li> <strong>Docker Registry / ACR:</strong> For container image builds and pushes.</li>
        </ul>
        <h4> <strong>Requirements to Create an Azure RM Service Connection:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Client ID (App ID of Service Principal)</li>
          <li>Client Secret</li>
          <li>Tenant ID</li>
          <li>Subscription ID</li>
          <li>RBAC Role (Contributor / Reader / Custom Role)</li>
        </ul>
        <pre><code># Example: Create SP for service connection
az ad sp create-for-rbac --name ado-terraform-sp --role Contributor --scopes /subscriptions/&lt;subId&gt;</code></pre>
        <p><strong>In Practice:</strong> I create one SP per environment (dev, test, prod) with least-privilege scope and link it to ADO service connections for Terraform pipelines.</p>
      </div>`},{question:"How do you secure Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Azure DevOps  Best Practices</strong></h3>
        <p>
          Security in Azure DevOps focuses on access control, secret management, and environment protection.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Store all secrets in <strong>Azure Key Vault</strong> and link via variable groups  never hardcode credentials.</li>
          <li> Implement <strong>RBAC & Least Privilege</strong> for project members and service principals.</li>
          <li> Enable <strong>Approvals & Checks</strong> on pipelines before deploying to production.</li>
          <li> Use <strong>Private Agent Pools</strong> for internal builds (not public agents).</li>
          <li> Enforce <strong>Branch Policies</strong>  code reviews, build validation before PR merges.</li>
          <li> Enable <strong>Auditing & Conditional Access Policies</strong> via Azure AD integration.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate ADO with Azure AD for centralized MFA, restrict pipelines with environment approvals, and monitor activity logs through Azure Monitor.</p>
      </div>`},{question:"What is the difference between push and commit in Git?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Push vs Commit  Git Fundamentals</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Commit</th><th>Push</th></tr>
          <tr><td>Definition</td><td>Records changes locally in your Git repo</td><td>Sends committed changes to the remote repository</td></tr>
          <tr><td>Scope</td><td>Local (developer machine)</td><td>Remote (GitHub/Azure Repos)</td></tr>
          <tr><td>Command</td><td><code>git commit -m "message"</code></td><td><code>git push origin branch-name</code></td></tr>
          <tr><td>Effect</td><td>Saves changes in local history</td><td>Updates central repo for others</td></tr>
        </table>
        <p><strong>In Practice:</strong> I commit frequently for logical checkpoints and push only after successful linting and testing to maintain clean Git history.</p>
      </div>`},{question:"Explain the process of renaming a Git branch.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Renaming a Git Branch  Step-by-Step</strong></h3>
        <p>
          You can rename a branch locally and reflect changes in remote:
        </p>
        <ol style="margin-left:1.2rem;">
          <li>Rename locally: <code>git branch -m old-name new-name</code></li>
          <li>Delete old remote branch: <code>git push origin --delete old-name</code></li>
          <li>Push new branch: <code>git push origin new-name</code></li>
          <li>Set upstream: <code>git push --set-upstream origin new-name</code></li>
        </ol>
        <p><strong>In Practice:</strong> I rename feature branches during review or reorganization to match naming conventions (e.g., <code>feature/aks-monitoring</code>).</p>
      </div>`},{question:"How do you configure a local Git repo to remote repo?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Configuring Local Git Repository with Remote</strong></h3>
        <p>
          You can connect your local repository to a remote (e.g., Azure Repos or GitHub) using the following commands:
        </p>
        <pre><code># Initialize local repo
git init

# Add files and commit
git add .
git commit -m "Initial commit"

# Add remote origin
git remote add origin https://github.com/user/repo.git

# Push local code to remote
git push -u origin main</code></pre>
        <p><strong>In Practice:</strong> I link local Terraform or YAML project repos to Azure Repos and push updates via VS Code terminal  ensuring each environment branch maps to its respective pipeline.</p>
      </div>`},{question:"How do you commit a file and update commit message?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Committing and Updating Git Commit Messages</strong></h3>
        <h4> <strong>Commit a File:</strong></h4>
        <pre><code>git add filename
git commit -m "Added AKS deployment configuration"</code></pre>
        <h4> <strong>Update Last Commit Message:</strong></h4>
        <pre><code>git commit --amend -m "Updated AKS pipeline config"</code></pre>
        <p>
          The <code>--amend</code> flag lets you modify the most recent commit message or include additional changes.
        </p>
        <p><strong>In Practice:</strong> I use commit amends before PR creation to clean up messages and ensure clear commit history for reviewers.</p>
      </div>`},{question:"What CI/CD tools have you used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CI/CD Tools Ive Worked With</strong></h3>
        <p>
          Ive worked with several CI/CD platforms for infrastructure automation and application deployments:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure DevOps:</strong> For pipelines, repos, artifacts, and environments.</li>
          <li> <strong>GitHub Actions:</strong> For event-driven workflows integrated with ACR/AKS.</li>
          <li> <strong>Jenkins:</strong> For legacy automation and scripted pipelines.</li>
          <li> <strong>GitLab CI:</strong> For container-based deployments.</li>
          <li> <strong>Terraform Cloud:</strong> For IaC pipeline automation with drift detection.</li>
        </ul>
        <p><strong>In Practice:</strong> I primarily use Azure DevOps for enterprise-grade infrastructure delivery pipelines integrated with Terraform and Key Vault for secure automation.</p>
      </div>`},{question:"How do you generate artifacts in CI pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Generating Artifacts in CI Pipelines</strong></h3>
        <p>
          Artifacts are output files or build packages generated from CI runs that are later consumed by release pipelines.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Use <strong>PublishPipelineArtifact</strong> or <strong>PublishBuildArtifacts</strong> task in YAML.</li>
          <li> Typical artifacts: Terraform plans, .zip build packages, Helm charts, or Docker images.</li>
        </ul>
        <pre><code># Example - Publish Terraform plan as artifact
- task: PublishPipelineArtifact@1
  inputs:
    targetPath: '&dollar;(System.DefaultWorkingDirectory)/tfplan'
    artifact: 'TerraformPlan'</code></pre>
        <p><strong>In Practice:</strong> I store Terraform plans or build outputs as artifacts and trigger downstream pipelines (apply, deploy) only after approval and artifact validation.</p>
      </div>`},{question:"How do you pass Terraform output block values into Azure pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Passing Terraform Outputs into Azure DevOps Pipelines</strong></h3>
        <p>
          Terraform output values can be exported and used in Azure DevOps pipelines dynamically  for example, to pass resource IDs, IPs, or connection strings.
        </p>
        <ol style="margin-left:1.2rem;">
          <li>Define outputs in Terraform:
          <pre><code>output "vm_public_ip" {
  value = azurerm_public_ip.myVM.ip_address
}</code></pre></li>
          <li>Run Terraform command and capture output as JSON:
          <pre><code>terraform output -json &gt; tf_output.json</code></pre></li>
          <li>Read and export value in pipeline:
          <pre><code>- script: |
    vm_ip=&dollar;(jq -r '.vm_public_ip.value' tf_output.json)
    echo "##vso[task.setvariable variable=VM_IP]&dollar;vm_ip"</code></pre></li>
          <li>Use variable in later stages:
          <pre><code>echo &dollar;(VM_IP)</code></pre></li>
        </ol>
        <p><strong>In Practice:</strong> I frequently export Terraform outputs (like resource IDs or URLs) into Azure pipeline variables to dynamically configure post-deployment validations or trigger Function Apps.</p>
      </div>`},{question:"Command to get VM result in JSON format after Terraform apply.",answerHtml:`<div class="answer-rich">
        <h3> <strong>Get VM details in JSON after apply</strong></h3>
        <p>Two practical approaches depending on what you want  a Terraform <em>output</em> you declared, or the raw resource data from the state.</p>

        <h4>1) Best / recommended  use <code>output</code> block &amp; export JSON</h4>
        <p>Add an output in Terraform (outputs.tf):</p>
        <pre><code>output "vm_public_ip" {
  value = azurerm_public_ip.my_vm.ip_address
  description = "Public IP of VM"
}</code></pre>
        <p>Then after <code>terraform apply</code> run:</p>
        <pre><code>terraform output -json vm_public_ip
# or for all outputs in machine-readable format
terraform output -json > tf_outputs.json</code></pre>

        <h4>2) If you need the entire resource/state in JSON</h4>
        <p>Use Terraform's <code>show -json</code> against the current state (or saved state file):</p>
        <pre><code>terraform show -json > state.json
# then filter using jq, e.g. get a resource by address
jq '.values.root_module.resources[] | select(.address=="azurerm_linux_virtual_machine.my_vm")' state.json</code></pre>

        <p><strong>Practical tip:</strong> I always expose only required values as outputs and consume them with <code>terraform output -json</code> in pipelines (then parse with <code>jq</code> or set pipeline variables).</p>
      </div>`},{question:"How to integrate Azure Key Vault into pipeline using scripting?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Key Vault in pipelines  script-based approach</strong></h3>
        <p>Two common patterns: (A) use built-in Key Vault variable groups (preferred), or (B) fetch secrets at runtime using az cli / MSI in script. I'll show the script/MSI method (works anywhere):</p>

        <h4>Prereqs</h4>
        <ul style="margin-left:1.2rem;">
          <li>AKV access for the SPN / Managed Identity used by pipeline (Key Vault access policy or RBAC).</li>
          <li>az cli available on the agent (or use AzureCLI task).</li>
        </ul>

        <h4>Example (Azure DevOps YAML)  fetch secret and set pipeline variable</h4>
        <pre><code>- task: AzureCLI@2
  displayName: 'Fetch secret from Key Vault'
  inputs:
    azureSubscription: 'sc-azure-prod'     # service connection
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: |
      # install jq if needed
      # fetch secret (using az cli)
      secret=&dollar;(az keyvault secret show --vault-name myVault --name MyDbPassword --query value -o tsv)
      echo "##vso[task.setvariable variable=DB_PASSWORD;issecret=true]&dollar;secret"
</code></pre>

        <p>Now subsequent tasks can use <code>&dollar;(DB_PASSWORD)</code> securely (pipeline masks secret in logs).</p>

        <h4>Alternative: Key Vault linked variable group</h4>
        <p>Create variable group in DevOps and link Key Vault  DevOps injects secrets as pipeline variables (no script needed).</p>

        <p><strong>Practical tip:</strong> Prefer Managed Identity or Service Principal with minimal Key Vault RBAC, and avoid printing secrets. Use secret-masking logging and rotate secrets regularly.</p>
      </div>`},{question:"How to trigger a pipeline when dev branch passes 90 % of validation?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Trigger pipeline conditionally on validation score (e.g., 90% coverage)</strong></h3>
        <p>Azure DevOps doesn't natively trigger a pipeline only when a metric passes a threshold  you implement this with a two-stage approach:</p>

        <h4>Pattern (recommended)</h4>
        <ol style="margin-left:1.2rem;">
          <li>Run <strong>validation pipeline</strong> on PR/commit for the <code>dev</code> branch which executes tests &amp; generates a coverage report.</li>
          <li>The validation job parses coverage and sets an output/flag (or publishes a short status file/artifact).</li>
          <li>If coverage >= 90%, the validation pipeline calls the downstream pipeline via REST API / Azure DevOps CLI to trigger the real pipeline.</li>
        </ol>

        <h4>Script example to trigger when coverage &gt;= 90%</h4>
        <pre><code># assume coverage calculated and saved as COVERAGE variable
if [ &dollar;(echo "&dollar;COVERAGE >= 90" | bc) -eq 1 ]; then
  # trigger downstream pipeline via Azure DevOps REST API
  curl -u :&dollar;{ADO_PAT} -X POST \\
    -H 'Content-Type: application/json' \\
    -d '{ "resources": { "repositories": {} } }' \\
    https://dev.azure.com/{org}/{project}/_apis/pipelines/{pipelineId}/runs?api-version=6.0-preview.1
else
  echo 'Coverage below threshold  stopping downstream trigger'
  exit 1
fi</code></pre>

        <p><strong>Alternatives:</strong> use YAML pipeline <code>resources: pipelines</code> to trigger downstream but with a gate script that fails if coverage below threshold; or use GitHub Actions with workflow_run conditions.</p>

        <p><strong>Practical tip:</strong> Keep validation and promotion decoupled. Use artifacts (coverage report) and an explicit, auditable trigger rather than implicit conditions inside the downstream pipeline for clearer ops and auditing.</p>
      </div>`},{question:"What is parallelism in pipeline and how is it optimized?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Parallelism in CI/CD  what &amp; how to optimize</strong></h3>
        <p><strong>Parallelism</strong> = multiple jobs/agents running simultaneously to reduce pipeline runtime. Azure DevOps supports parallel jobs (hosted/ self-hosted agent pools).</p>

        <h4>Ways to use/optimize parallelism</h4>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Matrix &amp; job parallelism:</strong> Use <code>strategy.matrix</code> or multiple jobs to run tests across platforms in parallel.</li>
          <li> <strong>Shard tests:</strong> Split your test suite into buckets (shards) and run each shard on separate agents.</li>
          <li> <strong>Cache dependencies:</strong> Use pipeline caching to avoid repeated long installs (node_modules, pip cache).</li>
          <li> <strong>Reusable jobs:</strong> Factor repeated work into templates or task groups to avoid duplicate setup time.</li>
          <li> <strong>Agent sizing:</strong> Use more powerful agents (bigger VM image) for CPU-heavy tasks but balance cost vs speed.</li>
          <li> <strong>Limit concurrency:</strong> Use <code>resource</code> or <code>semaphores</code> if tasks must not run concurrently (shared infra access).</li>
        </ul>

        <h4>Example: matrix in YAML</h4>
        <pre><code>strategy:
  matrix:
    linux: { imageName: 'ubuntu-latest' }
    windows: { imageName: 'windows-latest' }
    mac: { imageName: 'macOS-latest' }</code></pre>

        <p><strong>Practical tip:</strong> Parallelism reduces feedback time. Optimize by caching, splitting tests, and reusing warm-up steps (pre-baked images or self-hosted agents) to reduce total run-time and cost.</p>
      </div>`},{question:"What happens when Terraform pipeline fails during apply with lock state?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform lock behavior on failed apply</strong></h3>
        <p>Most backends (including Azure Storage backend with blob locking) create a lock during <code>terraform apply</code> to prevent concurrent state mutations. If the pipeline fails mid-apply:</p>
        <ul style="margin-left:1.2rem;">
          <li> The lock might remain (stale) preventing subsequent runs from acquiring it.</li>
          <li> Terraform usually releases the lock on normal exit; on abrupt failures you may need to force-unlock.</li>
        </ul>

        <h4>How to recover</h4>
        <pre><code># Identify lock error text from terraform output
# Force unlock (use the lock ID reported in error)
terraform force-unlock LOCK_ID
# For Azure blob backend you can also check/clear lease on the blob via az storage blob lease break</code></pre>

        <p><strong>Best practices to avoid stuck locks:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li> Use backend with proper locking (Azure blob, Terraform Cloud).</li>
          <li> Add retries and timeouts in pipeline and fail fast for non-recoverable errors.</li>
          <li> Use <code>terraform plan -out</code> and then apply the saved plan to minimize unexpected changes during apply.</li>
          <li> For long runs, ensure agent/timeouts are configured so the process can finish and release locks.</li>
        </ul>

        <p><strong>In Practice:</strong> I keep an "unlock runbook" with the <code>terraform force-unlock</code> command and the steps to inspect blob leases in Azure if a pipeline dies during apply; for production always require manual approval for re-tries to avoid accidental state corruption.</p>
      </div>`},{question:"How to release Terraform plan automatically in multi-stage pipeline?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Promote &amp; release Terraform plan in multi-stage pipelines</strong></h3>
        <p>Multi-stage pipelines provide a safe way to generate a plan once and apply it later (same plan file). Typical flow:</p>

        <h4>Stages:</h4>
        <ol style="margin-left:1.2rem;">
          <li><strong>Plan stage (CI):</strong> checkout  terraform init  terraform plan -out=tfplan  publish tfplan as pipeline artifact (<code>PublishPipelineArtifact</code>).</li>
          <li><strong>Approval / Gates (optional):</strong> manual approval for production, policy gates, or automatic checks.</li>
          <li><strong>Apply stage (CD):</strong> download tfplan artifact  terraform apply "tfplan" (the saved binary plan)  guarantees apply matches plan.</li>
        </ol>

        <h4>YAML snippet (concept):</h4>
        <pre><code># Stage: Plan
- stage: Plan
  jobs:
  - job: TerraformPlan
    steps:
    - script: terraform init
    - script: terraform plan -out=tfplan
    - task: PublishPipelineArtifact@1
      inputs:
        targetPath: 'tfplan'
        artifact: 'tfplan-artifact'

# Stage: Apply (runs after approval)
- stage: Apply
  dependsOn: Plan
  condition: succeeded()
  jobs:
  - job: TerraformApply
    steps:
    - task: DownloadPipelineArtifact@2
      inputs:
        artifact: 'tfplan-artifact'
        path: '&dollar;(System.DefaultWorkingDirectory)/tfplan'
    - script: terraform apply -auto-approve tfplan</code></pre>

        <h4>Automating release</h4>
        <p>If you want fully automatic promotion to Apply (e.g., non-prod), configure the pipeline to auto-run the Apply stage when Plan succeeds and gates are satisfied. For Prod, keep manual approvals.</p>

        <p><strong>Security note:</strong> Do not auto-apply plans in production without approvals. Ensure the service principal used for apply has scoped, least-privilege permissions and the plan artifact is stored securely.</p>

        <p><strong>In Practice:</strong> I publish the plan artifact and require at least one approver for production Apply; for dev/stage I auto-apply to enable fast feedback loops.</p>
      </div>`}]},{title:"17. Miscellaneous / General DevOps Concepts",questions:[{question:"Introduce yourself.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Professional Introduction</strong></h3>
        <p>
          Hi, Im <strong>Ritesh Sharma</strong>, a DevOps Engineer with over <strong>10 years of experience</strong> in automating infrastructure, CI/CD pipelines, and cloud operations across enterprise environments.  
          I specialize in <strong>Azure DevOps, Terraform, Docker, Kubernetes, and monitoring tools</strong> like Grafana and Prometheus.
        </p>
        <p>
          Currently, Im working with <strong>Litmus Information Systems LLP</strong>, focusing on building and managing Azure-based DevOps solutions  including provisioning infrastructure with Terraform, automating deployments via YAML pipelines, and ensuring high availability for healthcare systems.
        </p>
        <p><strong>In short:</strong> I enjoy designing secure, automated, and scalable DevOps workflows that accelerate software delivery while maintaining governance and cost efficiency.</p>
      </div>`},{question:"Tell me about your recent project.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recent Project  Centralized Patient Monitoring System (Healthcare Domain)</strong></h3>
        <p>
          I worked on a <strong>centralized patient monitoring platform</strong> for a European healthcare provider. The goal was to collect real-time vitals from IoT medical devices and display them on a unified dashboard.
        </p>
        <h4> <strong>Tech Stack:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> Azure Cloud  AKS, App Services, Key Vault, and Storage Accounts.</li>
          <li> Azure DevOps  CI/CD pipelines, Repos, Boards.</li>
          <li> Terraform  for provisioning all Azure resources using remote state in Azure Storage.</li>
          <li> Docker & Kubernetes  containerization and orchestration of microservices.</li>
          <li> Monitoring  Prometheus, Grafana, and Azure Monitor.</li>
        </ul>
        <p><strong>My Role:</strong> Designed IaC modules, built release pipelines (plan  apply), implemented Helm-based deployments, and configured auto-scaling & alerts in AKS for production stability.</p>
        <p><strong>Outcome:</strong> Reduced deployment time from 2 hours to under 15 minutes and achieved fully automated, auditable infrastructure provisioning.</p>
      </div>`},{question:"What are your roles and responsibilities?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Roles & Responsibilities as a DevOps Engineer</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Design and manage CI/CD pipelines using Azure DevOps YAML.</li>
          <li> Build Infrastructure as Code using Terraform with remote state and modular design.</li>
          <li> Containerize applications using Docker and deploy via Kubernetes (AKS).</li>
          <li> Manage secrets via Azure Key Vault and integrate securely into pipelines.</li>
          <li> Implement logging, alerting, and dashboards with Prometheus & Grafana.</li>
          <li> Perform system patching, monitoring, and performance tuning for Linux servers.</li>
          <li> Collaborate with developers for seamless CI/CD, security scanning, and cost optimization.</li>
        </ul>
        <p><strong>In Practice:</strong> I handle complete lifecycle  from environment setup and pipeline design to production deployments and post-release monitoring.</p>
      </div>`},{question:"What are the tools you have worked on in DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DevOps Tools Ive Worked With</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Cloud:</strong> Microsoft Azure, AWS (basic).</li>
          <li> <strong>IaC:</strong> Terraform, ARM Templates.</li>
          <li> <strong>CI/CD:</strong> Azure DevOps, Jenkins, GitHub Actions.</li>
          <li> <strong>Containers:</strong> Docker, Kubernetes (AKS).</li>
          <li> <strong>Security:</strong> Azure Key Vault, Checkov, SonarQube, TruffleHog.</li>
          <li> <strong>Monitoring:</strong> Prometheus, Grafana, Azure Monitor.</li>
          <li> <strong>Version Control:</strong> Git, GitHub, Azure Repos.</li>
          <li> <strong>Automation/Scripting:</strong> Bash, PowerShell, Azure CLI.</li>
        </ul>
        <p><strong>In Practice:</strong> My DevOps toolchain is centered around Azure + Terraform + Docker + AKS for infrastructure and application delivery automation.</p>
      </div>`},{question:"What is CI/CD and how does it help in automation?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CI/CD  Continuous Integration & Continuous Delivery</strong></h3>
        <p>
          CI/CD is the backbone of DevOps automation. It ensures reliable, repeatable, and fast software delivery through automated build, test, and deployment pipelines.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Continuous Integration (CI):</strong> Automates build & testing whenever developers push code  ensures integration issues are caught early.</li>
          <li><strong>Continuous Delivery (CD):</strong> Automates deployment to staging or production  ensures applications are always in a deployable state.</li>
        </ul>
        <p><strong>Benefits:</strong> Reduces manual errors, accelerates release cycles, improves feedback loops, and enforces quality through validation gates.</p>
        <p><strong>In Practice:</strong> I use Azure Pipelines to automate code checkout  build  Terraform plan/apply  deploy to AKS/App Service  notify via Teams.</p>
      </div>`},{question:"What is Continuous Delivery vs Continuous Deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Continuous Delivery vs Continuous Deployment</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Continuous Delivery</th><th>Continuous Deployment</th></tr>
          <tr><td>Automation</td><td>Build & Test fully automated; Deploy requires manual approval</td><td>Full pipeline automated including deployment</td></tr>
          <tr><td>Control</td><td>Manual trigger for production release</td><td>Automatic deployment on successful tests</td></tr>
          <tr><td>Risk</td><td>Lower (manual review before go-live)</td><td>Higher (depends on test reliability)</td></tr>
          <tr><td>Use Case</td><td>Enterprises with approval workflows</td><td>Agile startups or microservices with fast cycles</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use Continuous Delivery for production with manual approvals and Continuous Deployment for non-prod (QA/UAT) to enable faster testing.</p>
      </div>`},{question:"What are microservices?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Microservices Architecture</strong></h3>
        <p>
          Microservices is an architectural style where an application is divided into small, independent services  each handling a specific function and communicating via APIs.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Each service is independently deployable and scalable.</li>
          <li>Promotes agility  teams can develop, test, and deploy independently.</li>
          <li>Failure isolation  one service crash doesnt bring down the whole system.</li>
          <li>Usually containerized and orchestrated with Docker/Kubernetes.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy microservices in AKS clusters where each service runs as a separate deployment and is exposed via Ingress or API Gateway for external access.</p>
      </div>`},{question:"What is blue-green deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Blue-Green Deployment Strategy</strong></h3>
        <p>
          Blue-Green deployment reduces downtime and risk by maintaining two environments  one live (blue) and one idle (green).  
          The new version is deployed on green; after validation, traffic is switched from blue to green instantly.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Enables instant rollback by switching traffic back.</li>
          <li> Zero downtime releases.</li>
          <li> Used in AKS, App Services, or load balancer-based setups.</li>
        </ul>
        <pre><code># Example in Azure App Service
az webapp deployment slot swap --name myapp --slot staging --target-slot production</code></pre>
        <p><strong>In Practice:</strong> I use Blue-Green strategy in production App Services  new build goes to staging slot, tested, then swapped to production slot.</p>
      </div>`},{question:"What is canary deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Canary Deployment  Gradual Release Strategy</strong></h3>
        <p>
          Canary deployment releases new versions to a small subset of users before rolling out to all.  
          Its used to reduce risk by monitoring real user impact.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Rollout starts with 510% traffic  increase gradually if metrics are healthy.</li>
          <li> Used with load balancers, Ingress controllers, or feature flags.</li>
          <li> Easy rollback if errors or performance drops are detected.</li>
        </ul>
        <p><strong>In Practice:</strong> In AKS, I implement canary deployment using Helm and Kubernetes Service annotations  directing a percentage of traffic to the new pods before full rollout.</p>
      </div>`},{question:"What is rollback in CI/CD and why is it important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rollback in CI/CD  Ensuring Stability</strong></h3>
        <p>
          A <strong>rollback</strong> is reverting a system or deployment to a previously stable version when a new release causes issues (bugs, downtime, performance drops).
        </p>
        <h4> <strong>Why its important:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> Minimizes downtime and business impact from failed deployments.</li>
          <li> Ensures quick recovery without manual fixes.</li>
          <li> Maintains application stability during continuous delivery cycles.</li>
        </ul>
        <h4> <strong>Typical Rollback Methods:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Azure DevOps: Redeploy a previous release artifact or commit.</li>
          <li>Kubernetes: <code>kubectl rollout undo deployment/myapp</code>.</li>
          <li>App Services: Swap back to previous slot in Blue-Green deployment.</li>
        </ul>
        <p><strong>In Practice:</strong> I use versioned artifacts and maintain Terraform state snapshots  so I can rollback both infrastructure and app versions quickly in case of failures.</p>
      </div>`},{question:"What are different branching strategies you have used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Branching Strategies in Git</strong></h3>
        <p>
          Branching strategy defines how teams organize development and release workflows in Git.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Feature Branching:</strong> Each feature or bug fix in its own branch, merged via PR.</li>
          <li><strong>GitFlow:</strong> Main branches: <code>main/master</code> (prod), <code>develop</code> (staging), plus <code>feature/</code>, <code>release/</code>, <code>hotfix/</code>.</li>
          <li><strong>Trunk-Based:</strong> Developers commit frequently to <code>main</code> with short-lived feature branches.</li>
          <li><strong>Release Branching:</strong> Stable release branches maintained separately for version control.</li>
        </ul>
        <p><strong>In Practice:</strong> I follow <strong>GitFlow</strong> for enterprise projects and <strong>Trunk-Based</strong> for microservices where CI/CD is highly automated.</p>
      </div>`},{question:"What is an artifact and how do you publish it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Artifacts  Build Outputs in CI/CD</strong></h3>
        <p>
          An <strong>artifact</strong> is a compiled or packaged output (like a .zip, .jar, .tar.gz, or Terraform plan file) created after a build process, stored for deployment or auditing.
        </p>
        <pre><code># Example: Publish artifact in Azure Pipeline
- task: PublishBuildArtifacts@1
  inputs:
    pathToPublish: '&dollar;(System.DefaultWorkingDirectory)/drop'
    artifactName: 'build-output'</code></pre>
        <p><strong>In Practice:</strong> I publish Terraform plan files, Docker image manifests, and Helm chart packages as artifacts  ensuring each deployment uses immutable, traceable build outputs.</p>
      </div>`},{question:"What are Service Endpoints and Private Links in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Service Endpoints vs Private Links</strong></h3>
        <p>
          Both enhance network security for Azure resources, but differ in isolation and connectivity approach.
        </p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Service Endpoint</th><th>Private Link</th></tr>
          <tr><td>Connection Type</td><td>Extends VNet to Azure service via backbone</td><td>Private IP in VNet via Private Endpoint</td></tr>
          <tr><td>Data Path</td><td>Public service with VNet access control</td><td>Entirely private path; no public internet</td></tr>
          <tr><td>Security</td><td>Uses service tags + NSG rules</td><td>Full isolation via private IP + DNS mapping</td></tr>
          <tr><td>Use Case</td><td>When secure connection from VNet to Azure PaaS is needed</td><td>When complete private connectivity and compliance required</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use <strong>Private Links</strong> for production workloads (Key Vault, Storage, SQL DB) and <strong>Service Endpoints</strong> for internal tools or dev environments.</p>
      </div>`},{question:"What is a Load Balancer and how many types exist?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Load Balancer  Distribute Traffic Efficiently</strong></h3>
        <p>
          A Load Balancer distributes incoming traffic across multiple backend servers to ensure high availability and reliability.
        </p>
        <h4> Types of Load Balancers:</h4>
        <ul style="margin-left:1.2rem;">
          <li><strong>Basic Load Balancer (Layer 4):</strong> Works at transport layer; distributes traffic based on IP/port. Used for simple internal setups.</li>
          <li><strong>Standard Load Balancer (Layer 4):</strong> Secure, zonal, supports HA & cross-region balancing.</li>
          <li><strong>Application Gateway (Layer 7):</strong> Operates at HTTP/HTTPS layer; supports SSL offload, WAF, path-based routing.</li>
          <li><strong>Front Door (Global Layer 7):</strong> Global load balancer using edge network for low latency & failover.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Application Gateway for web workloads with SSL termination and Load Balancer for backend APIs or VMSS traffic distribution.</p>
      </div>`},{question:"What is DNS and why is it important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DNS  Domain Name System</strong></h3>
        <p>
          <strong>DNS</strong> translates human-readable domain names (like <code>app.company.com</code>) into IP addresses that machines understand.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Fundamental for service discovery in the internet and Kubernetes clusters.</li>
          <li> Prevents manual IP management, simplifies scalability.</li>
          <li> DNS Zones in Azure allow hosting custom domains (Public/Private).</li>
        </ul>
        <p><strong>In Practice:</strong> I configure Private DNS zones (e.g., <code>privatelink.blob.core.windows.net</code>) for private endpoints in AKS and App Service setups.</p>
      </div>`},{question:"What is NAT?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>NAT  Network Address Translation</strong></h3>
        <p>
          NAT allows multiple devices in a private network to share a single public IP for external communication.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Source NAT (SNAT):</strong> Used for outbound connections from private to public network.</li>
          <li> <strong>Destination NAT (DNAT):</strong> Used to expose internal resources to the public by mapping a public IP.</li>
        </ul>
        <p><strong>In Azure:</strong> Implemented via <strong>Azure NAT Gateway</strong> for outbound connectivity of subnets without exposing public IPs on VMs.</p>
        <p><strong>In Practice:</strong> I configure NAT Gateways for AKS clusters outbound traffic to keep IPs static for whitelisting at partner firewalls.</p>
      </div>`},{question:"What is Middleware and how do you install it on Linux?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Middleware  The Glue Between App & System</strong></h3>
        <p>
          Middleware is software that connects applications and services  enabling communication, data exchange, and management. Examples: Apache Tomcat, Nginx, WebLogic, Kafka.
        </p>
        <pre><code># Example: Install Apache Tomcat on Linux
sudo apt update
sudo apt install default-jdk -y
wget https://downloads.apache.org/tomcat/tomcat9.tar.gz
tar -xzf tomcat9.tar.gz -C /opt/
sudo systemctl enable tomcat</code></pre>
        <p><strong>In Practice:</strong> I deploy middleware in Linux VMs or containers via scripts or Terraform provisioners, ensuring consistent configuration and versioning.</p>
      </div>`},{question:"What is Sudo command and when do we use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>sudo  Execute Commands as Root</strong></h3>
        <p>
          <strong>sudo</strong> (Superuser Do) temporarily elevates privileges to execute commands as another user, typically root.
        </p>
        <pre><code>sudo apt update
sudo systemctl restart nginx</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Used for administrative tasks like package installation, service management, or system configuration.</li>
          <li>Safer than logging in as root permanently.</li>
          <li>Access controlled via <code>/etc/sudoers</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure limited sudo access for DevOps agents to perform patching and deployment without exposing full root permissions.</p>
      </div>`},{question:"What is top command in Linux?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>top  Real-time Linux Process Monitor</strong></h3>
        <p>
          The <strong>top</strong> command displays real-time system resource usage  including CPU, memory, and processes.
        </p>
        <pre><code>top
# Interactive shortcuts:
P - sort by CPU
M - sort by memory
k - kill process
q - quit</code></pre>
        <ul style="margin-left:1.2rem;">
          <li> Helps identify high CPU/memory processes.</li>
          <li> Used in troubleshooting performance issues on servers or containers.</li>
        </ul>
        <p><strong>In Practice:</strong> I often use <code>top</code> and <code>htop</code> to monitor application pods or VM performance during high-load testing or deployments.</p>
      </div>`},{question:"What is difference between App Service and Function App?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>App Service vs Function App  Platform Comparison</strong></h3>
        <p>
          Both are managed compute services in Azure, but they differ in use case and execution model.
        </p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>App Service</th><th>Function App</th></tr>
          <tr><td>Type</td><td>PaaS web hosting platform</td><td>Serverless compute for event-driven tasks</td></tr>
          <tr><td>Execution</td><td>Always running</td><td>Runs on trigger (HTTP, Timer, Blob, EventHub, etc.)</td></tr>
          <tr><td>Scaling</td><td>Manual or autoscale via plan</td><td>Automatic scale per event load</td></tr>
          <tr><td>Use Case</td><td>Web apps, APIs, microservices</td><td>Lightweight automation, scheduled tasks</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use App Service for hosting APIs or UIs, and Function Apps for automating CI/CD tasks like secret rotation, resource cleanup, and alert triggers.</p>
      </div>`},{question:"What is VPC and how does it differ from VNet?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VPC vs VNet  Cloud Network Equivalents</strong></h3>
        <p>
          Both <strong>VPC (Virtual Private Cloud)</strong> and <strong>VNet (Virtual Network)</strong> provide isolated, secure cloud network environments.  
          The difference lies in the provider terminology.
        </p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>VPC (AWS/GCP)</th><th>VNet (Azure)</th></tr>
          <tr><td>Provider</td><td>AWS / GCP</td><td>Microsoft Azure</td></tr>
          <tr><td>Networking Model</td><td>Region-scoped</td><td>Region-scoped, with global peering support</td></tr>
          <tr><td>Subnets</td><td>Private/Public subnets under VPC</td><td>Subnets inside VNet with NSG & UDRs</td></tr>
          <tr><td>Peering</td><td>VPC Peering</td><td>VNet Peering (even across regions)</td></tr>
        </table>
        <p><strong>In Practice:</strong> I design VNets with multiple subnets (App, DB, Bastion) and secure them using NSGs, Private Endpoints, and UDRs  similar to AWS VPC design principles.</p>
      </div>`},{question:"What is the difference between Terraform and ARM template?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform vs ARM Template  IaC Tools Comparison</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Terraform</th><th>ARM Template</th></tr>
          <tr><td>Language</td><td>HCL (HashiCorp Configuration Language)</td><td>JSON</td></tr>
          <tr><td>Scope</td><td>Multi-cloud (Azure, AWS, GCP, etc.)</td><td>Azure-only</td></tr>
          <tr><td>State Management</td><td>Maintains remote/local state files</td><td>No state tracking</td></tr>
          <tr><td>Modularity</td><td>Highly modular (reusable modules)</td><td>Less modular</td></tr>
          <tr><td>Execution</td><td>Declarative + procedural (plan/apply)</td><td>Declarative only</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use Terraform for end-to-end IaC provisioning across environments with remote state in Azure Storage  preferred over ARM for flexibility and readability.</p>
      </div>`},{question:"What is the difference between Continuous Integration and Continuous Deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CI vs CD  The DevOps Automation Chain</strong></h3>
        <p>
          CI/CD are complementary phases of software delivery automation.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Continuous Integration (CI):</strong> Merges developer code frequently, builds automatically, and runs tests to catch integration issues early.</li>
          <li><strong>Continuous Deployment (CD):</strong> Extends CI by automatically deploying every code change to production after passing validation.</li>
        </ul>
        <pre><code>CI:  Code  Build  Test  Package
CD:  Package  Deploy  Validate  Monitor</code></pre>
        <p><strong>In Practice:</strong> My Azure DevOps pipelines implement CI for Terraform linting & Docker builds, and CD for AKS deployments post approval gates.</p>
      </div>`},{question:"What is DevSecOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DevSecOps  Security Integrated into DevOps</strong></h3>
        <p>
          DevSecOps embeds security practices and tools within the CI/CD lifecycle  shifting security left to detect and fix vulnerabilities early.
        </p>
        <h4> <strong>Key Practices:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Static code analysis (SonarQube, Checkov, Trivy).</li>
          <li>Secret scanning (TruffleHog, GitGuardian).</li>
          <li>Dependency scanning (Snyk, DependencyCheck).</li>
          <li>Container image scanning before deployment.</li>
          <li>RBAC & Key Vault integration for credential management.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate Checkov and SonarQube into pipelines to enforce security policies and block builds on high-severity vulnerabilities.</p>
      </div>`},{question:"What is Site Reliability Engineering (SRE)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>SRE  Bridging Ops and Engineering</strong></h3>
        <p>
          <strong>Site Reliability Engineering (SRE)</strong> applies software engineering principles to IT operations  automating reliability, scaling, and performance.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Define <strong>SLOs, SLIs, SLAs</strong> to measure availability.</li>
          <li> Automate monitoring, scaling, and incident response.</li>
          <li> Minimize manual ops through self-healing systems.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Prometheus + Grafana to monitor uptime, error rates, latency, and integrate alert rules for proactive incident detection and SLA compliance.</p>
      </div>`},{question:"What are key metrics you monitor for production health?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Production Health  Key Metrics</strong></h3>
        <p>
          I focus on the <strong>Golden Signals</strong> and system reliability KPIs:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Latency:</strong> Response time of APIs or app endpoints.</li>
          <li> <strong>Traffic:</strong> Requests per second, throughput.</li>
          <li> <strong>Errors:</strong> 4xx/5xx rate, failed requests.</li>
          <li> <strong>Saturation:</strong> CPU, memory, disk, network utilization.</li>
          <li> <strong>Availability:</strong> Uptime % across regions.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure Azure Monitor, Application Insights, and Grafana dashboards to visualize and alert on these KPIs across production clusters.</p>
      </div>`},{question:"How do you ensure security and compliance in CI/CD pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Security & Compliance in CI/CD</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Store secrets in Azure Key Vault or pipeline secrets (never in code).</li>
          <li> Enforce code scanning with SonarQube, Checkov, Trivy, and dependency scanners.</li>
          <li> Restrict service connections and use Managed Identities.</li>
          <li> Enable approvals, RBAC, and auditing for production pipelines.</li>
          <li> Use signed artifacts and verify hashes before deploy.</li>
        </ul>
        <p><strong>In Practice:</strong> My pipelines integrate Key Vault, use PR validation with code-quality gates, and automatically fail if any security scan exceeds policy thresholds.</p>
      </div>`},{question:"What is Infrastructure Drift and how do you fix it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Infrastructure Drift  Detection & Remediation</strong></h3>
        <p>
          <strong>Infrastructure Drift</strong> occurs when actual cloud resources deviate from whats defined in IaC (Terraform, ARM).  
          This happens due to manual changes in the portal or ad-hoc scripts.
        </p>
        <h4> <strong>Detection:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Run <code>terraform plan</code> regularly  detects changes outside IaC.</li>
          <li>Integrate drift detection tools (Checkov, driftctl).</li>
        </ul>
        <h4> <strong>Fix:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Re-apply IaC: <code>terraform apply</code> to restore desired state.</li>
          <li>Or import manual resources into Terraform using <code>terraform import</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> I run nightly Terraform plan checks in pipelines  if drift is detected, it notifies the DevOps channel for review and reconciliation.</p>
      </div>`},{question:"What are availability sets and availability zones?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Availability Set vs Availability Zone</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Availability Set</th><th>Availability Zone</th></tr>
          <tr><td>Scope</td><td>Logical grouping within a data center</td><td>Physically separate data centers in a region</td></tr>
          <tr><td>Protection</td><td>From rack/power failures</td><td>From entire data center failure</td></tr>
          <tr><td>Components</td><td>Fault domains & update domains</td><td>Zone 1, Zone 2, Zone 3</td></tr>
          <tr><td>Redundancy</td><td>VMs spread across racks</td><td>VMs spread across zones</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use Availability Sets for legacy workloads and Availability Zones for production-grade workloads needing regional resiliency (99.99% uptime SLA).</p>
      </div>`},{question:"How do you migrate on-prem to Azure cloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>On-Prem to Azure Migration  Step-by-Step Approach</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li> <strong>Assessment:</strong> Use Azure Migrate to discover and assess on-prem VMs, databases, and dependencies.</li>
          <li> <strong>Plan:</strong> Choose migration strategy  Rehost (Lift & Shift), Refactor (Containerize), or Rebuild.</li>
          <li> <strong>Migrate:</strong> Use Azure Migrate tool or Site Recovery (ASR) for VM replication and cutover.</li>
          <li> <strong>Secure:</strong> Configure NSGs, Key Vault, and role-based access post migration.</li>
          <li> <strong>Optimize:</strong> Monitor costs, resize resources, and enable backups/monitoring.</li>
        </ol>
        <p><strong>In Practice:</strong> I use Azure Migrate for VM discovery, replicate via ASR, and re-platform databases to Azure SQL or PaaS equivalents  minimizing downtime during migration.</p>
      </div>`},{question:"What is Helm vs Kustomize?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Helm vs Kustomize  Kubernetes Configuration Management</strong></h3>
        <p>
          Both <strong>Helm</strong> and <strong>Kustomize</strong> manage Kubernetes manifests but differ in their approach.
        </p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Helm</th><th>Kustomize</th></tr>
          <tr><td>Type</td><td>Package manager for Kubernetes</td><td>Native manifest customization tool</td></tr>
          <tr><td>Template Language</td><td>Go templating (charts)</td><td>YAML overlays and patches (no templating)</td></tr>
          <tr><td>Use Case</td><td>Reusable, parameterized deployments (charts)</td><td>Simple manifest layering for environments</td></tr>
          <tr><td>Reusability</td><td>High  publish charts via repositories</td><td>Medium  maintain overlays manually</td></tr>
          <tr><td>Complexity</td><td>Suited for production, multi-service apps</td><td>Lightweight, easy for small projects</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use Helm for AKS production deployments (with values.yaml per environment) and Kustomize for quick overlays in non-prod clusters.</p>
      </div>`},{question:"What is Front Door in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Front Door  Global Layer 7 Load Balancer</strong></h3>
        <p>
          <strong>Azure Front Door</strong> is a global, scalable Layer 7 (HTTP/HTTPS) load balancer that accelerates traffic using Microsofts edge network and provides advanced routing.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Routes users to nearest backend (latency-based routing).</li>
          <li> Provides SSL offload and Web Application Firewall (WAF).</li>
          <li> Supports path-based routing and session affinity.</li>
          <li> Integrates with CDN for static content acceleration.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Azure Front Door to route traffic between global AKS clusters (EU, US) with health probes and failover policies  ensuring 99.99% uptime and low latency.</p>
      </div>`},{question:"What is Azure Monitor and how do you configure alerts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Monitor  Unified Observability Platform</strong></h3>
        <p>
          <strong>Azure Monitor</strong> collects, analyzes, and visualizes metrics and logs from applications and resources.  
          It helps track performance, detect anomalies, and trigger automated responses.
        </p>
        <h4> <strong>Steps to Configure Alerts:</strong></h4>
        <ol style="margin-left:1.2rem;">
          <li>Go to <strong>Azure Monitor  Alerts  Create Alert Rule</strong>.</li>
          <li>Select the target resource (VM, App Service, AKS, etc.).</li>
          <li>Define condition (e.g., CPU > 80%, HTTP 5xx count).</li>
          <li>Create or attach an <strong>Action Group</strong> (email, SMS, webhook, Logic App).</li>
          <li>Review and enable the rule.</li>
        </ol>
        <pre><code># Example (CLI)
az monitor metrics alert create --name HighCPU --resource-group rg-app --scopes /subscriptions/.../vm/myVM --condition "avg Percentage CPU > 80" --action-group opsAlertGroup</code></pre>
        <p><strong>In Practice:</strong> I use Action Groups to send alerts to Teams or trigger Logic Apps that scale out AKS pods automatically when CPU thresholds are breached.</p>
      </div>`},{question:"What is Recovery Services Vault and Backup Vault?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recovery Services Vault vs Backup Vault</strong></h3>
        <p>
          Both are Azure-managed vaults for data protection, but differ by use case and architecture generation.
        </p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Recovery Services Vault (RSV)</th><th>Backup Vault</th></tr>
          <tr><td>Purpose</td><td>VM backup, ASR (Disaster Recovery), classic workloads</td><td>Modern vault for newer backup workloads (Azure Files, Disks, SQL)</td></tr>
          <tr><td>Architecture</td><td>Legacy model</td><td>New architecture with RBAC & soft delete enhancements</td></tr>
          <tr><td>Supported Scenarios</td><td>VM, ASR, On-prem backups</td><td>Blob, Disk, Database backups</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use RSV for VM backups & ASR (failover testing), and Backup Vaults for granular, modern workloads with policy-based retention management.</p>
      </div>`},{question:"What is difference between local and remote backend in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Backend  Local vs Remote</strong></h3>
        <p>
          Backend defines how Terraform stores and locks its state files.
        </p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Local Backend</th><th>Remote Backend</th></tr>
          <tr><td>Storage</td><td>State stored locally on machine</td><td>State stored in remote location (e.g., Azure Blob, S3)</td></tr>
          <tr><td>Collaboration</td><td>Single user only</td><td>Supports multiple users with state locking</td></tr>
          <tr><td>Security</td><td>Risk of accidental deletion</td><td>Secured, versioned, encrypted remotely</td></tr>
          <tr><td>Example</td><td><code>backend "local" { path="terraform.tfstate" }</code></td><td><code>backend "azurerm" { storage_account_name="stgacct" }</code></td></tr>
        </table>
        <p><strong>In Practice:</strong> I always use remote backend (Azure Blob) with state locking and access restricted via Managed Identity for team-based IaC projects.</p>
      </div>`},{question:"What is the difference between Feature, Hotfix, and Release branches?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Feature vs Hotfix vs Release Branches</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Branch Type</th><th>Purpose</th><th>Source</th><th>Merge Target</th></tr>
          <tr><td>Feature</td><td>For new features or modules</td><td>develop / main</td><td>develop branch (after review)</td></tr>
          <tr><td>Hotfix</td><td>Critical production bug fix</td><td>main / release</td><td>main + develop</td></tr>
          <tr><td>Release</td><td>Stabilization before production</td><td>develop</td><td>main</td></tr>
        </table>
        <p><strong>In Practice:</strong> I follow GitFlow: feature branches merged into develop, release branches for UAT testing, and hotfix directly from production to minimize downtime.</p>
      </div>`},{question:"How do you optimize pipeline execution time?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline Optimization Strategies</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Enable <strong>parallel jobs</strong> for independent stages (e.g., build + lint + scan).</li>
          <li> Use <strong>caching</strong> for dependencies (npm, pip, Terraform plugins).</li>
          <li> Implement <strong>incremental builds</strong>  run only affected modules.</li>
          <li> Reuse <strong>templates and task groups</strong> to reduce redundancy.</li>
          <li> Store <strong>artifacts</strong> for reuse across stages (avoid rebuilds).</li>
          <li> Optimize YAML triggers to skip irrelevant pipeline runs (<code>paths</code> filter).</li>
        </ul>
        <p><strong>In Practice:</strong> I reduced build times by 40% by using caching, pre-built container agents, and splitting validation/test stages into parallel jobs in Azure DevOps.</p>
      </div>`},{question:"What is difference between Stateful and Stateless apps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Stateful vs Stateless Applications</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Stateless</th><th>Stateful</th></tr>
          <tr><td>Definition</td><td>No session/data stored between requests</td><td>Maintains session or user data across requests</td></tr>
          <tr><td>Scaling</td><td>Horizontally easy to scale</td><td>Scaling needs shared storage or replication</td></tr>
          <tr><td>Examples</td><td>Web APIs, REST services</td><td>Databases, message queues</td></tr>
          <tr><td>Failure Impact</td><td>Low  any instance can handle traffic</td><td>High  requires data consistency & recovery</td></tr>
        </table>
        <p><strong>In Practice:</strong> I keep microservices stateless for scalability and use persistent storage (Azure Disk, Blob, or Cosmos DB) for stateful components.</p>
      </div>`},{question:"What is bare metal in cloud context?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Bare Metal  Physical Servers Without Virtualization</strong></h3>
        <p>
          In the cloud context, <strong>bare metal</strong> refers to dedicated physical servers provisioned directly to customers  no hypervisor layer.  
          It provides full hardware access and performance isolation.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> No virtualization overhead  ideal for high-performance workloads.</li>
          <li> Complete isolation  compliance-sensitive or latency-critical systems.</li>
          <li> Offered by Azure (Dedicated Hosts), AWS (Bare Metal EC2), Oracle (Bare Metal Instances).</li>
        </ul>
        <p><strong>In Practice:</strong> Ive worked with dedicated Azure hosts (bare metal) for regulatory workloads requiring OS-level hardening and restricted shared tenancy.</p>
      </div>`},{question:"What is a monolithic vs microservice application?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monolithic vs Microservice Architecture</strong></h3>
        <p>
          Both define how applications are structured and deployed, but differ in modularity, scalability, and deployment approach.
        </p>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Monolithic</th><th>Microservice</th></tr>
          <tr><td>Structure</td><td>Single, large codebase</td><td>Collection of independent small services</td></tr>
          <tr><td>Deployment</td><td>Deployed as one unit</td><td>Each service deployed independently</td></tr>
          <tr><td>Scaling</td><td>Scale whole app together</td><td>Scale individual services as needed</td></tr>
          <tr><td>Technology Stack</td><td>Single tech/language</td><td>Polyglot (different tech per service)</td></tr>
          <tr><td>Fault Isolation</td><td>Failure can impact whole app</td><td>Failures isolated per service</td></tr>
        </table>
        <p><strong>In Practice:</strong> Ive migrated monolithic .NET APIs to containerized microservices on AKS  enabling independent scaling, CI/CD pipelines per service, and faster release cycles.</p>
      </div>`},{question:"How do you automate backups and retention?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Backups & Retention in Azure</strong></h3>
        <p>
          Backup automation ensures data protection and compliance without manual intervention.  
          I use <strong>Azure Backup</strong> (Recovery Services Vault / Backup Vault) with retention policies and automation scripts.
        </p>
        <h4> <strong>Steps:</strong></h4>
        <ol style="margin-left:1.2rem;">
          <li>Create a Recovery Services Vault (or Backup Vault).</li>
          <li>Define backup policy  daily, weekly, or long-term retention.</li>
          <li>Enable backup on VM or resource group using CLI or Terraform.</li>
          <li>Automate retention enforcement and expiry cleanup.</li>
        </ol>
        <pre><code># Example (Azure CLI)
az backup protection enable-for-vm --policy-name DailyBackup --vault-name DRVault --vm vm-prod-01
az backup policy set --vault-name DRVault --name DailyBackup --retention-weekly 4</code></pre>
        <p><strong>In Practice:</strong> I integrate backup configuration scripts in post-deployment pipelines  ensuring every production VM or DB is automatically backed up with 30-day retention.</p>
      </div>`},{question:"What are basic Linux commands used in DevOps daily?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Linux Commands for DevOps</strong></h3>
        <p>
          These are the day-to-day Linux commands I use for system monitoring, file management, and troubleshooting:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Navigation:</strong> <code>cd</code>, <code>pwd</code>, <code>ls -l</code></li>
          <li> <strong>File Management:</strong> <code>cat</code>, <code>tail -f</code>, <code>grep</code>, <code>find</code>, <code>cp</code>, <code>mv</code></li>
          <li> <strong>System Monitoring:</strong> <code>top</code>, <code>htop</code>, <code>df -h</code>, <code>free -m</code>, <code>du -sh *</code></li>
          <li> <strong>Permissions:</strong> <code>chmod</code>, <code>chown</code>, <code>sudo</code></li>
          <li> <strong>Network:</strong> <code>ping</code>, <code>curl</code>, <code>netstat -tulnp</code>, <code>ss -lntp</code></li>
          <li> <strong>Package Management:</strong> <code>apt</code> / <code>yum</code> / <code>dnf</code></li>
          <li> <strong>Logs:</strong> <code>journalctl -u servicename</code>, <code>dmesg</code></li>
        </ul>
        <p><strong>In Practice:</strong> I use these commands daily for debugging deployments, verifying services (systemctl status), and checking container logs in AKS or VM hosts.</p>
      </div>`},{question:"What are your steps for securing Terraform and Azure Pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Terraform & Azure Pipelines  Best Practices</strong></h3>
        <h4> <strong>Terraform Security:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Store state remotely (Azure Blob) with access control and encryption.</li>
          <li>Use Managed Identity or SPN with minimal RBAC permissions (Least Privilege).</li>
          <li>Never hardcode credentials  use Key Vault references or environment variables.</li>
          <li>Implement Checkov/TFLint scans to catch security misconfigurations.</li>
          <li>Enable version locking (<code>required_version</code>, <code>required_providers</code>) to prevent drift.</li>
        </ul>
        <h4> <strong>Azure Pipeline Security:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Service Connections</strong> with least privilege scope.</li>
          <li>Store secrets in <strong>Azure Key Vault</strong> or pipeline secrets (marked secure).</li>
          <li>Enable <strong>Approval Gates</strong> for production deployments.</li>
          <li>Use <strong>branch protection rules</strong>  only approved PRs trigger main pipeline.</li>
          <li>Restrict pipeline agent permissions (disable unauthorized script execution).</li>
        </ul>
        <p><strong>In Practice:</strong> I secure IaC pipelines by integrating Key Vault secrets dynamically and scanning Terraform with Checkov before every plan/apply  enforcing zero-secrets policy in code.</p>
      </div>`}]}];function Fw(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("azure-devops"),a=dm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-indigo-500 to-blue-500 shadow-glow",children:l.jsx(Ga,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Azure DevOps"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"CI/CD Pipelines & DevOps Practices Interview Questions"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Master Azure DevOps with questions on pipelines, repos, builds, releases, and integration with infrastructure as code tools."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:dm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:(u==null?void 0:u.question)||"",b=typeof u=="object"&&u&&"answer"in u&&!!u.answer,v=typeof u=="object"&&u&&"answerHtml"in u&&!!u.answerHtml,h=b?u.answer:null,w=b||v;return l.jsx("div",{className:"border-l-2 border-primary/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-primary font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),w&&l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:u.answerHtml}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:h})})]})]})]})},f)})})})]},p))})]})]})}const Q3=Object.freeze(Object.defineProperty({__proto__:null,default:Fw},Symbol.toStringTag,{value:"Module"})),um=[{title:"Azure Monitor & Observability",questions:[{question:"What is Azure Monitor and what does it do?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Monitor Overview</strong></h3>
        <p>
          <strong>Azure Monitor</strong> is a centralized service that collects, analyzes, and visualizes telemetry data from Azure resources, applications, and infrastructure.  
          It helps track performance, diagnose issues, and trigger alerts automatically.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Collects <strong>metrics</strong> (performance counters) and <strong>logs</strong> (activity + diagnostic data).</li>
          <li>Integrates with Log Analytics, Application Insights, and Azure Alerts.</li>
          <li>Visualizes data via Dashboards, Workbooks, and Grafana.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Azure Monitor to track AKS node health, VM CPU, and application latency with alerting integrated into Teams and ServiceNow.</p>
      </div>`},{question:"What are Metrics and Logs in Azure Monitor?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Metrics vs Logs</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Metrics</th><th>Logs</th></tr>
          <tr><td>Type</td><td>Numeric data over time</td><td>Detailed event or trace data</td></tr>
          <tr><td>Storage</td><td>Time-series database</td><td>Log Analytics Workspace</td></tr>
          <tr><td>Use Case</td><td>Quick performance monitoring</td><td>Deep diagnostics and auditing</td></tr>
        </table>
        <p><strong>Example:</strong> Metrics track VM CPU %; Logs capture system events, errors, and audit trails.</p>
      </div>`},{question:"What is a Log Analytics Workspace and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Log Analytics Workspace</strong></h3>
        <p>
          A <strong>Log Analytics Workspace</strong> is a centralized repository where Azure Monitor stores log data from resources, agents, and diagnostics.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Used to run <strong>Kusto Query Language (KQL)</strong> queries.</li>
          <li>Enables correlation of logs across multiple resources.</li>
          <li>Integrates with Sentinel, Application Insights, and custom dashboards.</li>
        </ul>
        <p><strong>In Practice:</strong> All our VMs, AKS, and Application Insights send logs to a single workspace for centralized query and alerting.</p>
      </div>`},{question:"What is Application Insights and how is it used for application monitoring?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Application Insights (App Insights)</strong></h3>
        <p>
          <strong>App Insights</strong> is an APM (Application Performance Monitoring) service under Azure Monitor that tracks request performance, dependencies, exceptions, and user behavior.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Collects telemetry via SDK or agent.</li>
          <li>Shows real-time response times, failure rates, and request traces.</li>
          <li>Supports distributed tracing for microservices.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate App Insights with AKS pods to trace API latency and backend dependency failures in real time.</p>
      </div>`},{question:"How do you configure Diagnostic Settings in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Configuring Diagnostic Settings</strong></h3>
        <p>
          Diagnostic settings define <strong>where and what</strong> monitoring data gets sent  Metrics, Logs, or both.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Navigate  Resource  Monitoring  Diagnostic Settings.</li>
          <li>Select categories (Activity Logs, Audit Logs, Performance).</li>
          <li>Send data to: Log Analytics Workspace / Event Hub / Storage Account.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure Diagnostic Settings on resource groups to automatically route all logs to a centralized workspace for compliance.</p>
      </div>`},{question:"What are retention policies in Log Analytics?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Retention Policy</strong></h3>
        <p>
          Retention policies control how long log data is stored in Log Analytics before being purged automatically.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Default retention: 30 days (free tier)  can be increased up to 2 years.</li>
          <li>Used to manage cost and comply with data policies.</li>
        </ul>
        <p><strong>In Practice:</strong> I keep 90 days retention for normal logs and 365 days for audit logs, using export rules for long-term archive to Blob Storage.</p>
      </div>`},{question:"What is the difference between metrics-based and log-based alerts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Metrics-based vs Log-based Alerts</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Metrics Alert</th><th>Log Alert</th></tr>
          <tr><td>Source</td><td>Real-time metrics</td><td>Queried log data (KQL)</td></tr>
          <tr><td>Latency</td><td>Near real-time</td><td>Depends on query schedule</td></tr>
          <tr><td>Example</td><td>CPU > 80%</td><td>Errors > 10 in last 5 mins</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use metrics alerts for VM health and log alerts for application exceptions or failed deployments.</p>
      </div>`},{question:"How do you monitor the health of AKS clusters and nodes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring AKS Clusters</strong></h3>
        <p>
          AKS integrates directly with Azure Monitor for Containers  collects performance metrics, logs, and container insights.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Monitor CPU/memory utilization of nodes & pods.</li>
          <li>Track pod restarts, failures, and image pull errors.</li>
          <li>Visualize with Container Insights dashboard.</li>
        </ul>
        <p><strong>In Practice:</strong> I enable Container Insights for AKS  it sends all container telemetry to Log Analytics for real-time analysis.</p>
      </div>`},{question:"What are the common use cases of Azure Monitor in production?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Production Use Cases</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Alerting on infrastructure health (CPU, memory, disk).</li>
          <li>Application telemetry via Application Insights.</li>
          <li>Security event correlation with Sentinel.</li>
          <li>Centralized logging for audit and compliance.</li>
          <li>Custom dashboards for business KPIs.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Monitor dashboards to show uptime %, deployment status, and error trends for management visibility.</p>
      </div>`},{question:"How do you monitor Azure resources like VMs (CPU, memory, disk)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VM Performance Monitoring</strong></h3>
        <p>
          Azure Monitor collects VM metrics and agent-based performance data.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Enable Azure Monitor Agent (AMA) or Diagnostic Extension.</li>
          <li>Collect CPU %, Memory %, Disk I/O, and network stats.</li>
          <li>Create metric alerts and visualize via Workbooks.</li>
        </ul>
        <p><strong>In Practice:</strong> I monitor VM CPU and memory with metric alerts (threshold >80%) and integrate those with Teams for incident alerts.</p>
      </div>`},{question:"What tools do you use for centralized monitoring and logging?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Centralized Monitoring Stack</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Azure Monitor + Log Analytics Workspace.</li>
          <li>Application Insights for APM data.</li>
          <li>Grafana for custom dashboards and visual correlation.</li>
          <li>Prometheus for Kubernetes metrics scraping.</li>
          <li>Splunk for organization-wide log ingestion.</li>
        </ul>
        <p><strong>In Practice:</strong> Logs go to Log Analytics  visualized in Grafana  alerts routed via Logic App to ServiceNow or Teams.</p>
      </div>`},{question:"How do you integrate Azure Monitor alerts with external systems like Slack or ServiceNow?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Alerts with Slack / ServiceNow</strong></h3>
        <p>
          Azure Monitor alerts can be routed to external tools using <strong>Action Groups</strong> and <strong>Logic Apps</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Create an <strong>Action Group</strong>  choose Logic App as action type.</li>
          <li>Design a Logic App with HTTP or built-in connectors (Slack, ServiceNow, Teams).</li>
          <li>Pass alert payload (JSON) to send incident notifications or create ServiceNow tickets automatically.</li>
        </ul>
        <p><strong>In Practice:</strong> I use a Logic App flow  whenever a critical alert triggers, it posts a message to our <code>#devops-alerts</code> Slack channel and simultaneously opens a ServiceNow incident.</p>
      </div>`},{question:"How do you configure automated notifications for Azure Monitor alerts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated Alert Notifications</strong></h3>
        <p>
          Use <strong>Action Groups</strong> in Azure Monitor to define how notifications are sent when an alert fires.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Go to <em>Monitor  Alerts  Action Groups  New</em>.</li>
          <li>Add Email, SMS, Push, or Webhook recipients.</li>
          <li>Attach this Action Group to any metric or log alert rule.</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain separate action groups for Infra (Teams), App (Slack), and Security (ServiceNow) to route alerts to the correct team automatically.</p>
      </div>`},{question:"How do you visualize and correlate monitoring data using Grafana or Workbooks?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Visualization & Correlation</strong></h3>
        <p>
          Azure Monitor data can be visualized in both <strong>Azure Workbooks</strong> and <strong>Grafana dashboards</strong> for cross-platform analysis.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure Workbooks:</strong> Interactive dashboards built directly in Azure portal using KQL queries and charts.</li>
          <li><strong>Grafana:</strong> Connect via the Azure Monitor Data Source plugin to visualize metrics, logs, and App Insights telemetry together.</li>
          <li>Combine metrics from multiple subscriptions or AKS clusters in a single Grafana dashboard.</li>
        </ul>
        <p><strong>In Practice:</strong> I created Grafana dashboards showing AKS node health, error rate from App Insights, and VM performance  one screen gives a full environment view.</p>
      </div>`}]},{title:"2. KQL (Kusto Query Language)",questions:[{question:"What is KQL (Kusto Query Language)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Introduction to KQL</strong></h3>
        <p>
          <strong>KQL (Kusto Query Language)</strong> is a read-only query language used to analyze large volumes of structured, semi-structured, and unstructured log data in Azure services like <strong>Log Analytics</strong>, <strong>Application Insights</strong>, and <strong>Azure Sentinel</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Optimized for fast log exploration and real-time analytics.</li>
          <li>Syntax is SQL-like but designed for big data and time-series logs.</li>
          <li>Used for filtering, aggregation, correlation, and visualization of telemetry data.</li>
        </ul>
        <p><strong>In Practice:</strong> I use KQL daily in Log Analytics to investigate failed deployments, WAF block patterns, and AKS pod crash trends.</p>
      </div>`},{question:"Where do we use KQL in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Where KQL is Used</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Log Analytics Workspace</strong>  for querying VM, AKS, and resource logs.</li>
          <li><strong>Application Insights</strong>  to analyze request telemetry, failures, and performance.</li>
          <li><strong>Azure Sentinel</strong>  for threat detection, SIEM correlation, and hunting queries.</li>
          <li><strong>Diagnostics and Activity Logs</strong>  to trace infrastructure changes and access logs.</li>
        </ul>
        <p><strong>In Practice:</strong> I use KQL in Sentinel to detect repeated failed logins and in App Insights to analyze API latency.</p>
      </div>`},{question:"How do you query WAF logs using KQL?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Querying WAF Logs with KQL</strong></h3>
        <p>
          WAF logs are stored in the Log Analytics workspace under the table <code>AzureDiagnostics</code>.  
          You can filter them using <strong>category</strong> and <strong>action_s</strong> fields.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureDiagnostics
| where Category == "ApplicationGatewayFirewallLog"
| where action_s == "Blocked"
| project TimeGenerated, clientIP_s, requestUri_s, ruleSetType_s, ruleId_s</code></pre>
        <p><strong>In Practice:</strong> I use this query to identify top IPs blocked by WAF and visualize them in a time chart.</p>
      </div>`},{question:"What is the difference between summarize, project, and extend in KQL?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>KQL Commands  Summarize vs Project vs Extend</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Command</th><th>Purpose</th><th>Example</th></tr>
          <tr><td><strong>summarize</strong></td><td>Aggregates data (count, avg, sum)</td><td><code>| summarize count() by clientIP_s</code></td></tr>
          <tr><td><strong>project</strong></td><td>Selects specific columns</td><td><code>| project TimeGenerated, clientIP_s</code></td></tr>
          <tr><td><strong>extend</strong></td><td>Adds new calculated columns</td><td><code>| extend region = tostring(split(requestUri_s, "/")[2])</code></td></tr>
        </table>
        <p><strong>In Practice:</strong> I use <code>summarize</code> to count blocked requests, <code>project</code> to limit output columns, and <code>extend</code> to enrich logs with derived fields.</p>
      </div>`},{question:"What are the best practices for writing KQL queries for performance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>KQL Performance Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Always <strong>filter early</strong> with <code>where</code> to reduce dataset size.</li>
          <li>Use <strong>project</strong> to limit unnecessary columns.</li>
          <li>Apply <strong>summarize</strong> after filtering to optimize aggregation.</li>
          <li>Use <strong>bin()</strong> for time grouping (e.g., 5m intervals).</li>
          <li>Avoid wildcards like <code>contains</code>  prefer <code>startswith</code> or <code>==</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> I tune long queries from 40s  8s using early filters and targeted projections.</p>
      </div>`},{question:"Can you give an example KQL query to fetch blocked WAF requests?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Example  Fetch Blocked WAF Requests</strong></h3>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureDiagnostics
| where Category == "ApplicationGatewayFirewallLog"
| where action_s == "Blocked"
| summarize BlockCount = count() by bin(TimeGenerated, 5m), clientIP_s
| sort by BlockCount desc</code></pre>
        <p><strong>In Practice:</strong> I export this result to a workbook to visualize IP-wise attack spikes over time.</p>
      </div>`},{question:"Have you used KQL in Log Analytics or Azure Sentinel?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>KQL in Log Analytics & Sentinel</strong></h3>
        <p>
          Yes, extensively  KQL is the backbone of both <strong>Log Analytics</strong> and <strong>Azure Sentinel</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Log Analytics:</strong> Investigate performance, deployment logs, or custom app telemetry.</li>
          <li><strong>Azure Sentinel:</strong> Write detection rules, hunting queries, and incident correlation logic.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive built Sentinel queries to detect brute-force login attempts and correlate them with WAF attack logs for unified incident tracking.</p>
      </div>`}]},{title:"3. Alerts & Automation",questions:[{question:"What is an Action Group in Azure Alerts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Action Group in Azure Alerts</strong></h3>
        <p>
          An <strong>Action Group</strong> defines <em>who gets notified</em> and <em>what actions are triggered</em> when an alert fires.  
          It is a reusable notification and automation container for all alert rules.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Includes actions like Email, SMS, Push, Voice, Logic App, Webhook, or Function.</li>
          <li>Can be attached to multiple alert rules across resources.</li>
          <li>Supports grouping by team  e.g., Infra, App, or Security teams.</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain three Action Groups  <code>Infra-Alerts</code> (Teams + SMS), <code>App-Alerts</code> (Slack + Email), and <code>SecOps</code> (ServiceNow via Logic App).</p>
      </div>`},{question:"How do you create an alert rule in Azure Monitor?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Creating an Alert Rule</strong></h3>
        <p>
          Alert rules define <strong>what condition triggers an alert</strong> and <strong>what action follows</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Navigate to <em>Azure Monitor  Alerts  Create  Alert Rule</em>.</li>
          <li>Select resource (VM, AKS, App Service, etc.).</li>
          <li>Define <strong>signal type</strong>  Metric, Log, or Activity.</li>
          <li>Set threshold (e.g., CPU > 80% for 10 minutes).</li>
          <li>Attach <strong>Action Group</strong> and set severity (Sev 04).</li>
        </ul>
        <p><strong>In Practice:</strong> I create alert rules using Terraform for consistent deployment  ensures same logic across DEV, QA, and PROD.</p>
      </div>`},{question:"How do you send Azure Monitor alerts to email, Teams, or Webhook?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sending Alerts to Email, Teams, or Webhook</strong></h3>
        <p>
          Alerts are delivered using <strong>Action Groups</strong>.  
          Each notification method is configured within an Action Group.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Email:</strong> Add user/group email under notification type.</li>
          <li><strong>Teams:</strong> Use <em>Logic App</em> or <em>Webhook</em> connector to post adaptive card messages.</li>
          <li><strong>Webhook:</strong> Send alert payload (JSON) to external systems like ServiceNow or Slack.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>{
  "alertRule": "HighCPU",
  "resource": "VM-Prod01",
  "severity": "Sev2",
  "message": "CPU usage above 85% for 10 minutes"
}</code></pre>
        <p><strong>In Practice:</strong> I send critical alerts to Teams via Logic App (with card summary + runbook link) and all Sev3 alerts via email.</p>
      </div>`},{question:"What are custom alert rules and when do you create them?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Custom Alert Rules</strong></h3>
        <p>
          Custom alert rules are user-defined queries or logic used to monitor non-default scenarios that metrics cant capture.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Created using <strong>KQL queries</strong> on log data.</li>
          <li>Used for application-level or security event detection.</li>
          <li>Can include complex filters, joins, and pattern detection.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureDiagnostics
| where Category == "ApplicationGatewayFirewallLog"
| where action_s == "Blocked"
| summarize count() by bin(TimeGenerated, 10m)
| where count_ > 20</code></pre>
        <p><strong>In Practice:</strong> I use custom log alerts to detect repeated WAF block events or high 5xx error spikes from API logs.</p>
      </div>`},{question:"How do you use alert severity levels (Critical, Warning, Informational)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Alert Severity Levels</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Severity</th><th>Meaning</th><th>Typical Action</th></tr>
          <tr><td>Sev 0</td><td>Critical  service down</td><td>Immediate incident, notify on-call</td></tr>
          <tr><td>Sev 1</td><td>High  major degradation</td><td>Urgent triage, partial outage</td></tr>
          <tr><td>Sev 2</td><td>Warning  performance issues</td><td>Monitor trend, alert team</td></tr>
          <tr><td>Sev 3</td><td>Informational</td><td>Log for reference, no action</td></tr>
        </table>
        <p><strong>In Practice:</strong> We map Sev01 alerts to PagerDuty on-call, Sev2 to Teams channels, and Sev3 to daily summary reports.</p>
      </div>`},{question:"How do you handle auto-remediation of triggered alerts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Auto-Remediation of Alerts</strong></h3>
        <p>
          Auto-remediation means automatically executing corrective actions when an alert fires  eliminating manual response time.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Attach <strong>Automation Runbooks</strong> or <strong>Azure Functions</strong> in Action Groups.</li>
          <li>Logic App can run workflows to restart VMs, scale AKS, or recycle App Services.</li>
          <li>Use tags or metadata to scope the automation only to relevant resources.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># Example Runbook Script
param([string]$VMName)
Write-Output "Restarting VM: $VMName"
Restart-AzVM -Name $VMName -ResourceGroup "ProdRG"</code></pre>
        <p><strong>In Practice:</strong> When a CPU > 95% alert triggers, my attached Runbook auto-scales the VM size and updates the incident with the action log.</p>
      </div>`}]},{title:"4. Prometheus & Grafana (Container Monitoring)",questions:[{question:"What is Prometheus and how does it work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Prometheus Overview</strong></h3>
        <p>
          <strong>Prometheus</strong> is an open-source monitoring system designed for time-series data collection and alerting.  
          It scrapes metrics from configured endpoints (usually on <code>/metrics</code> URL) at regular intervals.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Uses a pull-based model  Prometheus server pulls data from exporters.</li>
          <li>Stores data in its own time-series database (TSDB).</li>
          <li>Supports PromQL (Prometheus Query Language) for querying and alerting.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy Prometheus inside AKS using Helm; it scrapes node, pod, and application-level metrics for real-time insights.</p>
      </div>`},{question:"How do you integrate Prometheus with AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Prometheus Integration with AKS</strong></h3>
        <p>
          Integration is done using the <strong>Prometheus Operator</strong> or <strong>kube-prometheus-stack Helm chart</strong>.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack -n monitoring</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Scrapes metrics from <code>kubelet</code>, <code>cAdvisor</code>, and <code>kube-state-metrics</code>.</li>
          <li>Uses ServiceMonitors and PodMonitors to auto-discover metrics endpoints.</li>
          <li>Stores data locally in Prometheus TSDB or remote storage like Azure Blob.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy Prometheus using Helm + custom values to scrape both system and application exporters inside AKS.</p>
      </div>`},{question:"What kind of metrics can Prometheus collect from containers and pods?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Prometheus Metrics Types</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>CPU and memory usage per container and node.</li>
          <li>Pod restarts, uptime, and status (Pending, Running, CrashLoopBackOff).</li>
          <li>Network I/O, disk read/write, and storage utilization.</li>
          <li>Application metrics via exporters  e.g., HTTP requests, latency, queue depth.</li>
        </ul>
        <p><strong>In Practice:</strong> I use node-exporter for infrastructure metrics, cAdvisor for container stats, and custom app metrics exposed via <code>/metrics</code> endpoint.</p>
      </div>`},{question:"What is Grafana and how is it used for visualization?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Grafana Overview</strong></h3>
        <p>
          <strong>Grafana</strong> is an open-source analytics and visualization platform that connects to data sources like Prometheus, InfluxDB, or Azure Monitor.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Provides interactive dashboards and charts for metrics visualization.</li>
          <li>Allows alerting, annotations, and templating for dynamic dashboards.</li>
          <li>Integrates with Teams, Slack, and email for alert notifications.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Grafana dashboards to visualize AKS pod resource utilization, latency, and error rate trends.</p>
      </div>`},{question:"What are the main features of Grafana?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Grafana Key Features</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Multiple data sources (Prometheus, Loki, Elastic, Azure Monitor).</li>
          <li>Dashboard templating  dynamic filters by cluster, namespace, pod.</li>
          <li>Alerting system with thresholds and rules.</li>
          <li>User authentication and role-based access control (RBAC).</li>
          <li>Shareable dashboard links and snapshot exports.</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain Grafana dashboard folders by environment  DEV, QA, PROD  with variable filters for microservice-level visibility.</p>
      </div>`},{question:"How do you set up dashboards in Grafana?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Setting up Dashboards</strong></h3>
        <p>
          Grafana dashboards can be imported from prebuilt templates or built manually using panels and PromQL queries.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Go to <em>+  Dashboard  Add Panel</em>.</li>
          <li>Select data source  enter PromQL query.</li>
          <li>Choose visualization type (Graph, Gauge, Table).</li>
          <li>Save and tag dashboard for environment/team.</li>
        </ul>
        <p><strong>In Practice:</strong> I use imported dashboards for Kubernetes clusters (ID: 315 or 6417) and customize panels for app-specific metrics.</p>
      </div>`},{question:"How do you connect Grafana with Prometheus data source?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Connecting Grafana with Prometheus</strong></h3>
        <p>
          Grafana connects to Prometheus as a data source via HTTP API.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Navigate to <em>Configuration  Data Sources  Add data source</em>.</li>
          <li>Select <strong>Prometheus</strong> and set URL (e.g., <code>http://prometheus-server.monitoring.svc:9090</code>).</li>
          <li>Click <strong>Save & Test</strong> to validate connection.</li>
        </ul>
        <p><strong>In Practice:</strong> I connect multiple Prometheus instances (DEV/PROD) in one Grafana to compare performance trends across clusters.</p>
      </div>`},{question:"What are the benefits of using Prometheus and Grafana together?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Prometheus + Grafana = Perfect Monitoring Stack</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Prometheus handles metric collection; Grafana handles visualization.</li>
          <li>Open-source and cloud-agnostic  works on any Kubernetes cluster.</li>
          <li>Supports real-time alerting and dashboard sharing.</li>
          <li>High scalability  scrape thousands of metrics with low overhead.</li>
        </ul>
        <p><strong>In Practice:</strong> Prometheus feeds time-series data, Grafana turns it into actionable dashboards  ideal for microservice observability.</p>
      </div>`},{question:"How do you create custom dashboards in Grafana?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Creating Custom Dashboards</strong></h3>
        <p>
          Custom dashboards are built using <strong>PromQL queries</strong> and Grafana panels.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Use variables for dynamic filters (e.g., namespace, pod).</li>
          <li>Combine multiple visualizations (CPU, memory, latency) in one view.</li>
          <li>Apply thresholds, transformations, and alert rules per panel.</li>
        </ul>
        <p><strong>In Practice:</strong> I build dashboards showing microservice health, API latency, and error rate per namespace in AKS.</p>
      </div>`},{question:"How do you configure alert rules in Grafana?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Grafana Alerting</strong></h3>
        <p>
          Grafana allows alert configuration per panel or via the unified alerting system.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Navigate  <em>Alert  Create Rule</em> and select data source.</li>
          <li>Define expression using PromQL and threshold (e.g., CPU > 80%).</li>
          <li>Attach <strong>Contact Points</strong> (Email, Teams, Slack, Webhook).</li>
        </ul>
        <p><strong>In Practice:</strong> I route Grafana alerts to Teams via webhook and auto-create ServiceNow tickets for critical AKS node failures.</p>
      </div>`},{question:"Can you explain your Prometheus + Grafana monitoring setup for AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>My Prometheus + Grafana AKS Setup</strong></h3>
        <p>
          In my project, I implemented a full container monitoring stack for Azure Kubernetes Service (AKS) using <strong>Prometheus + Grafana + Alertmanager</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Deployed Prometheus and Grafana using <code>kube-prometheus-stack</code> Helm chart.</li>
          <li>Scrapes metrics from <code>kubelet</code>, <code>cAdvisor</code>, and custom app exporters.</li>
          <li>Alertmanager sends notifications to Teams and Slack.</li>
          <li>Grafana visualizes cluster resource usage, pod health, and app latency.</li>
        </ul>
        <p><strong>In Practice:</strong> I built custom Grafana dashboards per environment and configured auto-scaling alerts via Prometheus rules for proactive issue resolution.</p>
      </div>`}]},{title:"5. Azure Defender / Security Center / Sentinel",questions:[{question:"What is Microsoft Defender for Cloud (formerly Azure Security Center)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Microsoft Defender for Cloud Overview</strong></h3>
        <p>
          <strong>Microsoft Defender for Cloud</strong> (previously Azure Security Center) is a cloud-native security management platform that helps you:
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Continuously assess security posture across Azure, AWS, and GCP.</li>
          <li>Detect and remediate vulnerabilities in workloads.</li>
          <li>Protect servers, containers, and databases from active threats.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Defender to track recommendations like Enable MFA, Encrypt disks, and Restrict NSG ports  improving overall Secure Score for the subscription.</p>
      </div>`},{question:"How does it help identify and remediate security vulnerabilities?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Identifying & Remediating Vulnerabilities</strong></h3>
        <p>
          Defender for Cloud scans all Azure resources and correlates data from vulnerability assessment tools to detect misconfigurations and threats.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Integrates with <strong>Microsoft Defender for Endpoint</strong> to detect malware or anomalous activity.</li>
          <li>Uses <strong>recommendations</strong> and <strong>Secure Score</strong> to prioritize remediation.</li>
          <li>Supports <strong>Quick Fix</strong> actions directly from the portal or via automation scripts.</li>
        </ul>
        <p><strong>In Practice:</strong> When Unrestricted NSG rule alert appears, Defender auto-triggers a Logic App to disable the rule and notify the SecOps channel.</p>
      </div>`},{question:"How do you secure Log Analytics Workspace?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Log Analytics Workspace</strong></h3>
        <p>
          Log Analytics stores sensitive telemetry data, so securing it is essential for compliance and data protection.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Restrict access using <strong>RBAC roles</strong> (Reader, Contributor, Log Analytics Contributor).</li>
          <li>Enable <strong>Customer-Managed Keys (CMK)</strong> for data encryption at rest.</li>
          <li>Use <strong>Private Endpoints</strong> to block public network access.</li>
          <li>Configure <strong>Diagnostic Settings</strong> to send logs securely to Sentinel or Storage.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive implemented workspace isolation  only monitoring service accounts can query logs; developers get read-only dashboards via Grafana.</p>
      </div>`},{question:"What is Azure Sentinel, and how is it different from Azure Monitor?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Sentinel vs Azure Monitor</strong></h3>
        <p>
          <strong>Azure Sentinel</strong> is a <strong>SIEM + SOAR</strong> solution (Security Information & Event Management + Security Orchestration Automation Response).  
          It extends <strong>Azure Monitor</strong> to provide threat detection, investigation, and automated response.
        </p>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Feature</th><th>Azure Monitor</th><th>Azure Sentinel</th></tr>
          <tr><td>Purpose</td><td>Operational monitoring</td><td>Security analytics & automation</td></tr>
          <tr><td>Data Source</td><td>Performance & logs</td><td>Security events & threat intel</td></tr>
          <tr><td>Automation</td><td>Basic alert rules</td><td>Playbooks (Logic Apps)</td></tr>
          <tr><td>Scope</td><td>Health & metrics</td><td>Attack detection & correlation</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use Sentinel for incident correlation (WAF + Defender logs) and automate ticket creation when multiple threat indicators match.</p>
      </div>`},{question:"How do you configure Key Vault logging, and why is it important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Key Vault Logging Configuration</strong></h3>
        <p>
          Enabling Key Vault diagnostics ensures visibility into access and operation logs for auditing and threat detection.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Go to <em>Key Vault  Diagnostic Settings  Add Diagnostic Setting</em>.</li>
          <li>Send logs to <strong>Log Analytics</strong>, <strong>Storage Account</strong>, or <strong>Event Hub</strong>.</li>
          <li>Monitor operations like <code>GetSecret</code>, <code>ListKeys</code>, and failed authentication attempts.</li>
        </ul>
        <p><strong>Why its Important:</strong> Detects unauthorized access attempts or unusual key retrievals  critical for PCI and ISO compliance.</p>
        <p><strong>In Practice:</strong> I route Key Vault logs to Sentinel and use KQL queries to detect abnormal access from new IPs or service principals.</p>
      </div>`},{question:"What are security alerts in Defender for Cloud, and how do you handle them?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Security Alerts in Defender for Cloud</strong></h3>
        <p>
          Security alerts are threat signals generated when suspicious or malicious activity is detected in Azure resources.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Examples: Brute-force login attempts, malicious script execution, or privilege escalation.</li>
          <li>Alerts are classified by severity (High, Medium, Low).</li>
          <li>Integrated with Sentinel and Logic Apps for incident automation.</li>
        </ul>
        <p><strong>In Practice:</strong> When Defender detects Multiple failed logins from IP, Sentinel triggers a Logic App that blocks the IP and notifies the SOC via Teams.</p>
      </div>`},{question:"How do you investigate and remediate alerts automatically?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated Alert Investigation & Remediation</strong></h3>
        <p>
          Azure Sentinel supports <strong>Playbooks</strong> (Logic Apps) for automated investigation and response.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Triggered by Sentinel incidents or Defender alerts.</li>
          <li>Performs automated actions  isolate VM, disable user, revoke token.</li>
          <li>Enriches incident with threat intelligence or IP reputation info.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># Example: Sentinel Playbook Steps
1 Trigger: Alert Multiple Failed Logins
2 Run: Logic App  Check IP Reputation API
3 If malicious  Add IP to NSG Deny Rule
4 Notify: SOC channel with summary card</code></pre>
        <p><strong>In Practice:</strong> I implemented an auto-remediation flow that disables compromised accounts and logs remediation details in Sentinel.</p>
      </div>`},{question:"What are the key security controls you implement while designing Azure networks?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Key Azure Network Security Controls</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>NSGs</strong> and <strong>ASGs</strong> to restrict inbound/outbound traffic.</li>
          <li>Enable <strong>Azure Firewall</strong> and <strong>DDoS Protection Standard</strong>.</li>
          <li>Use <strong>Private Endpoints</strong> to access PaaS services securely.</li>
          <li>Apply <strong>Just-In-Time (JIT)</strong> VM access for RDP/SSH control.</li>
          <li>Enforce <strong>Network Policies</strong> on AKS for pod-level isolation.</li>
        </ul>
        <p><strong>In Practice:</strong> My production networks use a hub-spoke model with central firewall, NSG-based isolation, and Defender-integrated alerts for any misconfigurations.</p>
      </div>`}]},{title:"6. Backup, Recovery, and Disaster Recovery (DR)",questions:[{question:"What is a Backup Vault in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Backup Vault</strong></h3>
        <p>
          A <strong>Backup Vault</strong> in Azure is a storage entity that securely stores backup data such as VMs, disks, and file shares.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>It is a modern, ARM-based replacement for Recovery Services Vault (RSV).</li>
          <li>Supports <strong>Azure Backup Center</strong> for centralized backup management.</li>
          <li>Stores backup data encrypted at rest using Azure-managed or customer-managed keys (CMK).</li>
        </ul>
        <p><strong>In Practice:</strong> I use Backup Vaults for new workloads and RSV for legacy ones  both integrated with Azure Policy for auto-onboarding of new VMs.</p>
      </div>`},{question:"What is the difference between Recovery Services Vault and Backup Vault?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recovery Services Vault vs Backup Vault</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Feature</th><th>Recovery Services Vault (RSV)</th><th>Backup Vault</th></tr>
          <tr><td>Architecture</td><td>Classic</td><td>Modern (ARM-based)</td></tr>
          <tr><td>Supported Workloads</td><td>VMs, SQL, SAP HANA</td><td>VMs, Blob Backup, Azure Files</td></tr>
          <tr><td>Management</td><td>Per vault</td><td>Centralized via Backup Center</td></tr>
          <tr><td>Encryption</td><td>Azure-managed</td><td>Supports Customer-Managed Keys (CMK)</td></tr>
        </table>
        <p><strong>In Practice:</strong> For new Azure regions and workloads, I always prefer <strong>Backup Vault</strong> for its better integration with Azure Backup policies and automation APIs.</p>
      </div>`},{question:"How do you configure VM backups in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Configuring VM Backups</strong></h3>
        <p>VM backups are configured using Azure Backup policies and Vault association.</p>
        <ul style="margin-left:1.2rem;">
          <li>Create or select a <strong>Backup Vault / RSV</strong>.</li>
          <li>Define a <strong>Backup Policy</strong> (frequency, retention, schedule).</li>
          <li>Associate policy with VM  Azure installs the backup extension.</li>
          <li>Initial backup is a full snapshot; subsequent backups are incremental.</li>
        </ul>
        <p><strong>In Practice:</strong> I use daily backups with 30-day retention for production VMs and weekly backups with 15-day retention for non-prod workloads.</p>
      </div>`},{question:"How do you monitor and validate successful backups?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring Backup Jobs</strong></h3>
        <p>
          Azure provides built-in reports and alerts via Backup Center and Log Analytics.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Backup Center</strong>  Jobs view for backup status.</li>
          <li>Enable diagnostic settings  send logs to <strong>Log Analytics Workspace</strong>.</li>
          <li>Create alert rules for failed or missed backups.</li>
        </ul>
        <p><strong>In Practice:</strong> I monitor backup compliance using Azure Monitor dashboards and a weekly email summary of failed jobs.</p>
      </div>`},{question:"How do you perform VM restore or file-level recovery?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VM Restore & File-Level Recovery</strong></h3>
        <p>
          Azure Backup supports both full VM restore and file-level recovery directly from the vault.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Full VM Restore:</strong> Restore point  Select region  Create new VM or replace existing one.</li>
          <li><strong>File Recovery:</strong> Download script  Mount recovery volume  Copy files from snapshot.</li>
        </ul>
        <p><strong>In Practice:</strong> For production, I restore to a sandbox network for verification before switching over to the restored VM.</p>
      </div>`},{question:"What is Azure Site Recovery (ASR) and how is it used for DR?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Site Recovery (ASR)</strong></h3>
        <p>
          <strong>ASR</strong> is Azures native <strong>Disaster Recovery as a Service (DRaaS)</strong> that replicates workloads across regions for business continuity.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Continuously replicates VM disks to a secondary region.</li>
          <li>Enables one-click failover and failback.</li>
          <li>Supports DR for Azure VMs, on-prem VMs, and physical servers.</li>
        </ul>
        <p><strong>In Practice:</strong> I use ASR to replicate critical production VMs from Central India  South India with 15-min RPO and DR drills every quarter.</p>
      </div>`},{question:"What are RPO and RTO in Disaster Recovery planning?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>RPO vs RTO</strong></h3>
        <p>
          <strong>RPO (Recovery Point Objective)</strong>  Maximum acceptable data loss (time between last backup and failure).  
          <strong>RTO (Recovery Time Objective)</strong>  Maximum acceptable downtime to restore service.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>RPO Example:</strong> 15 minutes  ASR replication every 15 mins.</li>
          <li><strong>RTO Example:</strong> 1 hour  DR site should be up within 60 minutes.</li>
        </ul>
        <p><strong>In Practice:</strong> For mission-critical healthcare apps, we maintain RPO  15 min and RTO  45 min.</p>
      </div>`},{question:"How do you perform DR drills in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Performing DR Drills</strong></h3>
        <p>
          DR drills verify the readiness of your disaster recovery plan without impacting production.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>In ASR  Select replicated items  Test Failover.</li>
          <li>Use isolated virtual network to prevent production impact.</li>
          <li>Validate data, app connectivity, and DNS failover.</li>
          <li>After verification  Cleanup Test Failover.</li>
        </ul>
        <p><strong>In Practice:</strong> We run DR drills every 3 months and document results as part of business continuity compliance.</p>
      </div>`},{question:"How do you manage cross-region replication for business continuity?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cross-Region Replication</strong></h3>
        <p>
          Azure provides multiple replication mechanisms for data resiliency and continuity.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Enable <strong>Geo-Redundant Storage (GRS)</strong> or <strong>RA-GRS</strong> for backup data.</li>
          <li>Use <strong>ASR</strong> for VM-level replication between paired regions.</li>
          <li>Deploy critical services in <strong>paired regions</strong> (e.g., East US  West US).</li>
          <li>Leverage <strong>Traffic Manager</strong> or <strong>Front Door</strong> for regional failover.</li>
        </ul>
        <p><strong>In Practice:</strong> I replicate production workloads from Central India to South India and configure Traffic Manager for automatic DNS-based failover.</p>
      </div>`}]},{title:"7. Cost Optimization & Governance",questions:[{question:"What is cost optimization in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cost Optimization in Azure</strong></h3>
        <p>
          <strong>Cost optimization</strong> in Azure means ensuring you're paying only for what you need  by right-sizing resources, automating idle shutdowns, and using reserved or spot instances efficiently.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Monitor and analyze spending patterns regularly.</li>
          <li>Eliminate unused or idle resources (stopped VMs, unattached disks).</li>
          <li>Apply auto-scaling to handle dynamic workloads.</li>
          <li>Use reserved instances (RI) or Azure Hybrid Benefit for savings.</li>
        </ul>
        <p><strong>In Practice:</strong> I reduced ~30% monthly Azure cost by enforcing VM auto-shutdown policies and moving long-running workloads to reserved instances.</p>
      </div>`},{question:"What tools or strategies do you use for cost management (Cost Analysis, Budgets)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Cost Management Tools & Strategies</strong></h3>
        <p>
          Azure provides built-in cost management capabilities to analyze, plan, and control cloud spend.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Cost Analysis:</strong> Visualize spend per resource group, service, or tag.</li>
          <li><strong>Budgets:</strong> Set monthly or quarterly budget limits with alerts.</li>
          <li><strong>Advisor Recommendations:</strong> Get AI-driven insights for right-sizing and idle resources.</li>
          <li><strong>Tags:</strong> Allocate costs by department, project, or owner.</li>
        </ul>
        <p><strong>In Practice:</strong> I use <em>Azure Cost Analysis</em> to track project-level expenses and configure budget alerts to trigger Teams notifications when spending hits 80%.</p>
      </div>`},{question:"Have you used any third-party tools for cost monitoring?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Third-Party Cost Monitoring Tools</strong></h3>
        <p>
          Yes, for enterprise-scale cost visibility and automation beyond native Azure tools.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>CloudHealth</strong>  detailed multi-cloud reporting and automation.</li>
          <li><strong>Spot.io</strong>  automatic workload optimization and autoscaling for savings.</li>
          <li><strong>Kubecost</strong>  Kubernetes-level cost breakdown by namespace and pod.</li>
          <li><strong>Datadog Cloud Cost Monitor</strong>  unified monitoring + billing insights.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Kubecost to track per-namespace AKS costs and trigger scaling recommendations for idle microservices.</p>
      </div>`},{question:"What is the Azure Well-Architected Framework?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Well-Architected Framework (WAF)</strong></h3>
        <p>
          The <strong>Azure Well-Architected Framework</strong> provides a set of best practices and design principles to build secure, efficient, and resilient cloud solutions.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Helps review architecture across key pillars like cost, performance, and security.</li>
          <li>Guides design decisions aligned with Microsofts cloud reliability standards.</li>
        </ul>
        <p><strong>In Practice:</strong> We used the Well-Architected Review to align our healthcare app with best practices  improving performance and reducing cost by 25%.</p>
      </div>`},{question:"What are the five pillars of the Well-Architected Framework?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Five Pillars of Azure Well-Architected Framework</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Security</strong>  Protect data and systems using least privilege and defense in depth.</li>
          <li> <strong>Reliability</strong>  Design for high availability, failover, and DR readiness.</li>
          <li> <strong>Performance Efficiency</strong>  Optimize workloads and scaling strategies.</li>
          <li> <strong>Cost Optimization</strong>  Balance performance with operational expenses.</li>
          <li> <strong>Operational Excellence</strong>  Automate and continuously improve processes.</li>
        </ul>
        <p><strong>In Practice:</strong> I use these pillars as a checklist before every production release or architecture review.</p>
      </div>`},{question:"How do you use Azure Policy for governance and compliance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Policy for Governance & Compliance</strong></h3>
        <p>
          <strong>Azure Policy</strong> enforces organizational standards and compliance automatically across subscriptions.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Define rules  e.g., Allow only specific VM sizes or Require tagging.</li>
          <li>Assign policy to management group, subscription, or resource group.</li>
          <li>Monitor compliance status via Azure Policy dashboard.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive implemented policies that enforce mandatory cost tags, restrict public IPs, and ensure diagnostics are enabled on all VMs.</p>
      </div>`},{question:"How do you identify underutilized resources in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Identifying Underutilized Resources</strong></h3>
        <p>
          Underutilized resources increase unnecessary cost  Azure provides insights to identify and optimize them.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Advisor</strong> recommendations for idle VMs or unattached disks.</li>
          <li>Monitor low CPU/memory utilization using <strong>Azure Monitor Metrics</strong>.</li>
          <li>Leverage <strong>Cost Analysis + Tags</strong> to isolate non-critical resources.</li>
        </ul>
        <p><strong>In Practice:</strong> I use automation scripts to shut down non-prod VMs nightly and deallocate unused dev resources over weekends.</p>
      </div>`},{question:"How do you optimize storage, compute, and network costs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Optimization Across Storage, Compute, and Network</strong></h3>
        <p>
          Cost optimization focuses on reducing waste while maintaining performance and reliability.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Storage:</strong> Use lifecycle management  move old blobs to Cool/Archive tier, delete unattached disks.</li>
          <li> <strong>Compute:</strong> Right-size VMs, use spot instances for non-critical workloads, enable auto-shutdown.</li>
          <li> <strong>Network:</strong> Use Azure Front Door or CDN to reduce egress costs, and peer VNets efficiently.</li>
        </ul>
        <p><strong>In Practice:</strong> After analyzing cost reports, I applied blob lifecycle rules and switched 20% of workloads to spot instances  cutting monthly spend by 40%.</p>
      </div>`}]},{title:"8. SonarQube & Code Quality",questions:[{question:"What is SonarQube and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>SonarQube Overview</strong></h3>
        <p>
          <strong>SonarQube</strong> is a static code analysis tool used to continuously inspect code quality, detect bugs, code smells, and security vulnerabilities in your codebase.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Performs <strong>static code analysis</strong> for 25+ languages (Java, .NET, Python, etc.).</li>
          <li>Evaluates metrics like code coverage, duplications, complexity, and maintainability.</li>
          <li>Integrates seamlessly into CI/CD pipelines for automated scanning during builds.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate SonarQube in Azure DevOps pipelines to ensure every PR meets quality gates before merging.</p>
      </div>`},{question:"How do you integrate SonarQube with CI/CD pipelines (Azure DevOps, Jenkins)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating SonarQube with CI/CD</strong></h3>
        <p>
          Integration involves running SonarQube scanner as part of your build pipeline.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 Install <strong>SonarQube extension</strong> from Azure DevOps marketplace.</li>
          <li>2 Create <strong>Service Connection</strong> to your SonarQube server.</li>
          <li>3 Add pipeline tasks:
            <ul>
              <li> Prepare analysis configuration</li>
              <li> Run build/test tasks</li>
              <li> Publish analysis results to SonarQube</li>
            </ul>
          </li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>steps:
- task: SonarQubePrepare@5
  inputs:
    SonarQube: 'SonarServiceConn'
    scannerMode: 'MSBuild'
    projectKey: 'myproject'
- task: MSBuild@1
- task: SonarQubeAnalyze@5
- task: SonarQubePublish@5</code></pre>
        <p><strong>In Practice:</strong> I automate SonarQube scans in every pull request pipeline  code merges only if the analysis passes the quality gate.</p>
      </div>`},{question:"What are Quality Gates in SonarQube?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>SonarQube Quality Gates</strong></h3>
        <p>
          A <strong>Quality Gate</strong> is a set of conditions that define whether code meets your organization's quality standards.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Ensures new code is maintainable and free of critical issues.</li>
          <li>Default conditions include:
            <ul>
              <li>Code coverage > 80%</li>
              <li>No blocker or critical bugs</li>
              <li>Duplications < 3%</li>
              <li>Security vulnerabilities = 0</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I configured pipelines to fail automatically if the SonarQube quality gate fails  preventing low-quality code merges.</p>
      </div>`},{question:"How do you enforce code coverage thresholds in pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Enforcing Code Coverage</strong></h3>
        <p>
          Code coverage ensures sufficient test coverage for each build. SonarQube integrates with test tools like <code>JUnit</code>, <code>JaCoCo</code>, <code>DotCover</code>, or <code>Cobertura</code> to measure this.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Integrate your test framework output with SonarQube analysis.</li>
          <li>Define threshold (e.g., 80%) under Quality Gate.</li>
          <li>Fail build automatically when coverage drops below threshold.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>sonar.coverage.exclusions=**/tests/**
sonar.coverage.minimum=80</code></pre>
        <p><strong>In Practice:</strong> I maintain 85% coverage in microservices projects  pipeline fails if test coverage falls short.</p>
      </div>`},{question:"How do you manage static code analysis and security scanning?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Static Code & Security Scanning</strong></h3>
        <p>
          SonarQube performs static code analysis during build to detect vulnerabilities and non-compliant patterns.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Integrate scans in CI pipelines for early detection.</li>
          <li>Combine SonarQube with tools like <strong>Checkov</strong> or <strong>Trivy</strong> for IaC & container security.</li>
          <li>Tag critical issues and auto-assign to developers in SonarQube dashboard.</li>
        </ul>
        <p><strong>In Practice:</strong> Our CI pipeline runs SonarQube + Checkov scans together  blocking deployments with high or critical vulnerabilities.</p>
      </div>`},{question:"What is the difference between SonarQube and SonarCloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>SonarQube vs SonarCloud</strong></h3>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Feature</th><th>SonarQube</th><th>SonarCloud</th></tr>
          <tr><td>Hosting</td><td>Self-hosted (on-prem/cloud VM)</td><td>Fully managed SaaS</td></tr>
          <tr><td>Setup</td><td>Manual installation & maintenance</td><td>No setup  hosted by SonarSource</td></tr>
          <tr><td>Access</td><td>Private network</td><td>Public cloud (good for GitHub integration)</td></tr>
          <tr><td>Best For</td><td>Enterprise & on-prem environments</td><td>Cloud-native CI/CD pipelines</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use SonarCloud for GitHub-hosted projects  easy integration and auto-analysis without managing servers.</p>
      </div>`},{question:"How do you configure SonarQube extensions in YAML pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Configuring SonarQube in YAML Pipelines</strong></h3>
        <p>
          You can use the official <code>SonarQubePrepare</code>, <code>SonarQubeAnalyze</code>, and <code>SonarQubePublish</code> tasks directly in your YAML pipelines.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>trigger:
- main

pool:
  vmImage: 'ubuntu-latest'

steps:
- task: SonarQubePrepare@5
  inputs:
    SonarQube: 'SonarQubeServiceConn'
    projectKey: 'MyApp'
    projectName: 'MyApp'
- script: |
    dotnet build
    dotnet test --collect:"XPlat Code Coverage"
- task: SonarQubeAnalyze@5
- task: SonarQubePublish@5
  inputs:
    pollingTimeoutSec: '300'</code></pre>
        <p><strong>In Practice:</strong> I parameterize project key and token in pipeline variables, ensuring secure reuse across multiple projects.</p>
      </div>`}]},{title:"9. Logging & Diagnostics",questions:[{question:"What are diagnostic settings and why are they important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Diagnostic Settings</strong></h3>
        <p>
          <strong>Diagnostic settings</strong> in Azure define how and where platform and resource logs are collected, stored, and analyzed.  
          Theyre crucial for monitoring, compliance, and troubleshooting.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Capture platform metrics and resource logs (activity logs, audit, and performance data).</li>
          <li>Send data to <strong>Log Analytics</strong>, <strong>Storage Account</strong>, or <strong>Event Hub</strong>.</li>
          <li>Enable end-to-end visibility across infrastructure and applications.</li>
        </ul>
        <p><strong>In Practice:</strong> I enable diagnostic settings on every production resource  Application Gateway, Key Vault, and AKS  to centralize logs for analysis.</p>
      </div>`},{question:"How do you enable diagnostic logs for Application Gateway, Key Vault, or Storage Accounts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Enabling Diagnostic Logs</strong></h3>
        <p>
          You can enable diagnostic logs via the Azure Portal, CLI, or Terraform.  
          Diagnostic logs provide detailed operational and audit data for each resource.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># Example using Azure CLI
az monitor diagnostic-settings create \\
  --name "appgw-logs" \\
  --resource "/subscriptions/xxxx/resourceGroups/rg/providers/Microsoft.Network/applicationGateways/myAppGw" \\
  --workspace "/subscriptions/xxxx/resourcegroups/log-rg/providers/microsoft.operationalinsights/workspaces/log-ws" \\
  --logs '[{"category": "ApplicationGatewayAccessLog", "enabled": true}]'</code></pre>
        <p><strong>In Practice:</strong> I send Application Gateway, Key Vault, and Storage Account logs to a single Log Analytics workspace for centralized monitoring.</p>
      </div>`},{question:"How do you monitor WAF blocked requests in Log Analytics?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring WAF Blocked Requests</strong></h3>
        <p>
          You can query Application Gateway WAF logs in <strong>Log Analytics Workspace</strong> using <strong>KQL (Kusto Query Language)</strong>.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureDiagnostics
| where Category == "ApplicationGatewayFirewallLog"
| where action_s == "Blocked"
| summarize count() by bin(TimeGenerated, 1h), clientIp_s, ruleGroup_s, ruleId_s</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Filter blocked requests by IP, country, or rule ID.</li>
          <li>Identify false positives and tune custom WAF rules.</li>
        </ul>
        <p><strong>In Practice:</strong> I monitor WAF blocks daily  any unusual spike triggers a Teams alert for security investigation.</p>
      </div>`},{question:"What tools are used for centralized logging (e.g., Log Analytics, ELK, Splunk)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Centralized Logging Tools</strong></h3>
        <p>
          Centralized logging helps consolidate logs from multiple Azure services and environments.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Log Analytics</strong>  native tool for collecting, querying, and visualizing logs.</li>
          <li> <strong>ELK Stack (Elasticsearch, Logstash, Kibana)</strong>  open-source analytics stack for large-scale logging.</li>
          <li> <strong>Splunk</strong>  enterprise-grade SIEM for log correlation and threat detection.</li>
          <li> <strong>Grafana Loki</strong>  lightweight log aggregation for Kubernetes workloads.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Azure Log Analytics for platform logs and Splunk for enterprise-wide log correlation with security alerts.</p>
      </div>`},{question:"What is a log retention policy, and how do you manage it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Log Retention Policy</strong></h3>
        <p>
          A <strong>log retention policy</strong> defines how long logs are stored before automatic deletion  ensuring compliance and cost control.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Default retention: 30 days (can be increased up to 2 years in Log Analytics).</li>
          <li>Adjust per compliance (GDPR, HIPAA, PCI-DSS).</li>
          <li>Automate cleanup using <strong>Azure Policy</strong> or PowerShell.</li>
        </ul>
        <p><strong>In Practice:</strong> I set 90-day retention for app logs and 365 days for security logs  balancing audit compliance and storage cost.</p>
      </div>`},{question:"How do you troubleshoot backend health issues using logs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Troubleshooting Backend Health</strong></h3>
        <p>
          Application Gateway and App Service logs help identify backend health issues such as failed probes or timeouts.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Query <strong>ApplicationGatewayAccessLog</strong> or <strong>Performance Logs</strong> in Log Analytics.</li>
          <li>Filter on <code>BackendStatus_s != "Healthy"</code> or HTTP 5xx codes.</li>
          <li>Correlate results with VM or container health metrics.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureDiagnostics
| where Category == "ApplicationGatewayAccessLog"
| where BackendStatus_s != "Healthy"
| project TimeGenerated, BackendServer_s, ResponseStatus_s</code></pre>
        <p><strong>In Practice:</strong> I built alerts that auto-notify the team when backend health degrades beyond threshold.</p>
      </div>`},{question:"How do you analyze Activity Logs for security auditing?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Analyzing Activity Logs for Auditing</strong></h3>
        <p>
          Azure <strong>Activity Logs</strong> record all control-plane operations like resource creation, deletion, and access changes.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Query logs via <strong>Azure Portal  Monitor  Activity Log</strong>.</li>
          <li>Send Activity Logs to <strong>Log Analytics</strong> for KQL analysis.</li>
          <li>Filter by operation name, caller, or resource group.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureActivity
| where OperationNameValue == "Microsoft.Compute/virtualMachines/delete"
| project Caller, ResourceGroup, Resource, TimeGenerated</code></pre>
        <p><strong>In Practice:</strong> I use these logs for security audits  identifying unauthorized deletions or role changes in production.</p>
      </div>`},{question:"What are resource-specific logs, and where are they stored?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resource-Specific Logs</strong></h3>
        <p>
          Resource-specific logs are detailed logs generated by individual Azure services  different from activity logs which track control-plane actions.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Examples: Application Gateway access logs, Key Vault audit logs, SQL diagnostic logs.</li>
          <li>They provide operational visibility (traffic, errors, requests).</li>
          <li>Stored in <strong>Log Analytics Workspace</strong>, <strong>Storage Account</strong>, or forwarded to <strong>Event Hub</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I centralize all resource logs into one Log Analytics workspace and use KQL queries to correlate issues across App Gateway, AKS, and App Service layers.</p>
      </div>`}]},{title:"10. Best Practices & Real-World Scenarios",questions:[{question:"How do you ensure end-to-end observability for your environment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Ensuring End-to-End Observability</strong></h3>
        <p>
          Observability ensures full visibility into the health, performance, and reliability of your environment across apps, infra, and network.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Metrics:</strong> Use Azure Monitor and Prometheus for real-time metrics (CPU, memory, request latency).</li>
          <li> <strong>Logs:</strong> Send logs to Log Analytics, Splunk, or ELK for central analysis.</li>
          <li> <strong>Tracing:</strong> Enable Application Insights distributed tracing for microservices.</li>
          <li> <strong>Alerting:</strong> Create action groups that route alerts to Teams or ServiceNow.</li>
        </ul>
        <p><strong>In Practice:</strong> I combine Application Insights (APM) + Azure Monitor + Grafana dashboards to trace user requests from frontend to backend in real time.</p>
      </div>`},{question:"How do you maintain security posture for your resources?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Maintaining Security Posture</strong></h3>
        <p>
          Security posture is maintained through continuous assessments, alerts, and compliance enforcement.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Enable <strong>Microsoft Defender for Cloud</strong> for real-time vulnerability scanning.</li>
          <li>Apply <strong>Azure Policy</strong> for tagging, encryption, and region compliance.</li>
          <li>Integrate with <strong>Key Vault</strong> for secret management.</li>
          <li>Automate patching via Azure Update Management or Azure Automation.</li>
        </ul>
        <p><strong>In Practice:</strong> I review Defender Secure Score weekly and enforce baseline policies via Policy Assignments across subscriptions.</p>
      </div>`},{question:"How do you configure alerting for backend down or high latency?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Alerting for Backend Failures or Latency</strong></h3>
        <p>
          Backend health and latency alerts are configured using <strong>Application Insights</strong> and <strong>Azure Monitor Metrics</strong>.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>AzureMetrics
| where Resource == "my-appservice"
| where MetricName == "ServerResponseTime"
| summarize avg(Total) by bin(TimeGenerated, 5m)</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Create metric alert: Avg Response Time > 2 sec for 5 mins.</li>
          <li>Add action group  Notify via Teams and ServiceNow.</li>
        </ul>
        <p><strong>In Practice:</strong> Our rule detects 502/503 spikes  triggering a Teams alert and automatic restart via Logic App if failure persists.</p>
      </div>`},{question:"How do you automate backup validation and report compliance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Backup Validation & Compliance</strong></h3>
        <p>
          Backup validation ensures your backup data is recoverable and compliant with retention policies.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Automation</strong> runbooks to check backup status daily.</li>
          <li>Query Recovery Services Vault via PowerShell or Azure CLI.</li>
          <li>Send compliance report to Teams or email using Logic Apps.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>Get-AzRecoveryServicesBackupJob | 
Where-Object { $_.Status -ne "Completed" } |
Export-Csv backup-status.csv</code></pre>
        <p><strong>In Practice:</strong> I automated daily backup validation  if any job fails, a ServiceNow ticket auto-generates for investigation.</p>
      </div>`},{question:"How do you monitor and remediate high CPU utilization alerts automatically?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Auto-Remediation for High CPU Alerts</strong></h3>
        <p>
          Combine <strong>Azure Monitor Alerts</strong> with <strong>Logic Apps</strong> or <strong>Runbooks</strong> for auto-remediation.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Create metric alert  CPU > 85% for 10 mins.</li>
          <li>Trigger Logic App that restarts the VM or scales out the App Service Plan.</li>
          <li>Post remediation status to Teams or ServiceNow.</li>
        </ul>
        <p><strong>In Practice:</strong> I use an Azure Monitor alert linked with a Logic App  it scales AKS nodes automatically when CPU hits threshold.</p>
      </div>`},{question:"How do you link Azure Monitor to ServiceNow for ticketing?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Azure Monitor with ServiceNow</strong></h3>
        <p>
          Azure Monitor can trigger incident creation in ServiceNow using <strong>Action Groups</strong> and <strong>Logic Apps</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Create an <strong>Action Group</strong>  choose Logic App as action type.</li>
          <li>In Logic App, use ServiceNow connector  Create Incident.</li>
          <li>Pass alert payload (severity, resource, message) as input.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>{
  "short_description": "Azure VM High CPU",
  "urgency": "1",
  "impact": "2",
  "assignment_group": "CloudOps"
}</code></pre>
        <p><strong>In Practice:</strong> Every critical Azure alert auto-creates a ServiceNow ticket  ensuring SLA-driven response tracking.</p>
      </div>`},{question:"How do you ensure governance and security compliance across environments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Governance & Security Compliance</strong></h3>
        <p>
          Governance ensures consistency, compliance, and security across DEV  UAT  PROD environments.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Management Groups</strong> for environment-level control.</li>
          <li>Apply <strong>Azure Policies</strong> to restrict disallowed resources or public IPs.</li>
          <li>Assign <strong>RBAC roles</strong> to enforce least-privilege access.</li>
          <li>Continuously assess via <strong>Defender for Cloud Secure Score</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I applied policies to enforce tagging, restrict regions, and auto-deploy Defender for Cloud to all new subscriptions.</p>
      </div>`},{question:"What are your recommended best practices for monitoring in production?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Production Monitoring Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Combine metrics, logs, and traces  enable full-stack observability.</li>
          <li> Set actionable alerts with context (dont alert on every small event).</li>
          <li> Use automation (Logic Apps / Runbooks) for remediation.</li>
          <li> Integrate monitoring with ITSM (ServiceNow / Jira) for tracking incidents.</li>
          <li> Build Grafana dashboards for real-time visualization.</li>
          <li> Conduct weekly alert review meetings to reduce noise and improve signal quality.</li>
        </ul>
        <p><strong>In Practice:</strong> We follow a monitor-everything, alert-intelligently strategy  combining Azure Monitor + App Insights + Grafana for unified visibility.</p>
      </div>`}]},{title:"11. Terraform + Security Scanning (Checkov, TFLint, OPA, Sentinel Policies)",questions:[{question:"What is Checkov and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Checkov Overview</strong></h3>
        <p>
          <strong>Checkov</strong> is an open-source static analysis tool that scans Terraform, Kubernetes, and CloudFormation templates for security and compliance misconfigurations <em>before</em> deployment.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Detects 1000+ CIS, NIST, and Azure Security Benchmark violations.</li>
          <li>Runs in CI/CD pipelines to block unsafe infrastructure changes.</li>
          <li>Works offline and does not require cloud access.</li>
        </ul>
        <p><strong>In Practice:</strong> I run Checkov in Azure DevOps and GitHub Actions before <code>terraform plan</code>  to ensure no public IP or unencrypted storage gets deployed.</p>
      </div>`},{question:"How do you integrate Checkov in Azure DevOps pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Checkov in ADO Pipelines</strong></h3>
        <p>
          Integration is done by adding a Checkov scan task in the build stage of the Terraform pipeline.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>steps:
- task: Bash@3
  displayName: "Run Checkov Scan"
  inputs:
    targetType: 'inline'
    script: |
      pip install checkov
      checkov -d ./Terraform --soft-fail --framework terraform</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><code>--soft-fail</code>  warns but doesnt break build (for testing).</li>
          <li>Remove flag for enforcing strict compliance.</li>
        </ul>
        <p><strong>In Practice:</strong> I store scan reports in artifacts and send summary to Teams channel for visibility.</p>
      </div>`},{question:"What is TFLint and how is it different from Checkov?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>TFLint vs Checkov</strong></h3>
        <p>
          Both analyze Terraform code but serve different purposes:
        </p>
        <table style="width:100%;margin:.6rem 0;border-collapse:collapse;">
          <tr><th>Tool</th><th>Purpose</th><th>Focus Area</th></tr>
          <tr><td>TFLint</td><td>Linting & best-practice checks</td><td>Syntax, naming, unused vars, deprecated APIs</td></tr>
          <tr><td>Checkov</td><td>Security & compliance validation</td><td>CIS, NIST, policy violations</td></tr>
        </table>
        <p><strong>In Practice:</strong> I run TFLint first to catch code quality issues, then Checkov for deep security analysis before approval.</p>
      </div>`},{question:"What is Open Policy Agent (OPA) and how is it used with Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>OPA (Open Policy Agent)</strong></h3>
        <p>
          OPA is a policy-as-code engine that evaluates custom rules (Rego policies) to enforce compliance on Terraform plans and Kubernetes deployments.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Policies written in Rego language.</li>
          <li>Checks Terraform plan output before apply.</li>
          <li>Blocks non-compliant resources (e.g. unencrypted disks).</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>package terraform

deny[msg] {
  input.resource_type == "azurerm_storage_account"
  not input.values.enable_https_traffic_only
  msg = "Storage Account must have HTTPS only enabled"
}</code></pre>
        <p><strong>In Practice:</strong> I use OPA with Conftest in pipeline  it scans <code>terraform plan -out</code> JSON to enforce custom rules before deployment.</p>
      </div>`},{question:"What are Sentinel Policies and how are they used in Terraform Enterprise?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sentinel Policies (Terraform Enterprise)</strong></h3>
        <p>
          <strong>Sentinel</strong> is HashiCorps policy-as-code framework built into Terraform Cloud/Enterprise for governance and compliance.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Runs after <code>terraform plan</code> to evaluate compliance rules.</li>
          <li>Can enforce mandatory policy checks before approval.</li>
          <li>Used for governance like tagging, cost limits, region restrictions.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>import "tfplan/v2" as tfplan

main = rule {
  all tfplan.resources.azurerm_storage_account as _, r {
    r.applied.tags.Environment is "prod"
  }
}</code></pre>
        <p><strong>In Practice:</strong> We use Sentinel in Terraform Cloud to block any plan missing mandatory tags or deploying to non-approved regions.</p>
      </div>`},{question:"How do you combine Checkov, TFLint, and OPA in one Azure DevOps pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Combined IaC Security Pipeline</strong></h3>
        <p>
          You can chain these tools in a single CI/CD pipeline for multi-layered IaC validation:
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>stages:
- stage: Validate
  jobs:
  - job: StaticAnalysis
    pool:
      vmImage: 'ubuntu-latest'
    steps:
      - script: pip install checkov tflint conftest -q
        displayName: Install Tools
      - script: tflint --init && tflint
        displayName: Run TFLint
      - script: checkov -d . --quiet
        displayName: Run Checkov
      - script: conftest test plan.json
        displayName: Run OPA Policies</code></pre>
        <p><strong>In Practice:</strong> I execute all three tools before Terraform apply  only compliant infrastructure gets provisioned to Azure.</p>
      </div>`},{question:"What are your best practices for secure Terraform pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Pipeline Security Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Store state files in encrypted Azure Storage Account with RBAC.</li>
          <li> Never store secrets in code  use Azure Key Vault integration.</li>
          <li> Scan Terraform code with TFLint & Checkov in every PR.</li>
          <li> Enforce tagging and naming standards via Sentinel or OPA policies.</li>
          <li> Maintain version-locked Terraform providers to avoid breaking changes.</li>
        </ul>
        <p><strong>In Practice:</strong> Our DevOps pipeline uses TFLint + Checkov + OPA before <code>terraform plan</code>, then auto-applies only if all policies pass  ensuring zero drift and full compliance.</p>
      </div>`}]},{title:"12. Incident Management & Root Cause Analysis (RCA)",questions:[{question:"What is Incident Management in DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Incident Management Overview</strong></h3>
        <p>
          <strong>Incident Management</strong> is the process of identifying, analyzing, and resolving unexpected disruptions in production systems to restore normal operations quickly.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Detect incidents via monitoring tools (Azure Monitor, Grafana, App Insights).</li>
          <li> Prioritize based on severity (P1 - Critical, P2 - Major, etc.).</li>
          <li> Notify on-call engineers via ServiceNow, PagerDuty, or Teams alerts.</li>
          <li> Use runbooks or automation scripts for quick recovery.</li>
        </ul>
        <p><strong>In Practice:</strong> I handle production incidents through a ServiceNow workflow linked to Azure Alerts  ensuring fast response and accountability.</p>
      </div>`},{question:"What is Root Cause Analysis (RCA) and why is it important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Understanding Root Cause Analysis (RCA)</strong></h3>
        <p>
          <strong>RCA</strong> is a structured process to identify the underlying cause of an incident  not just fix the symptom.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Investigate logs, metrics, and timeline of the issue.</li>
          <li> Identify what failed  code, infra, or configuration.</li>
          <li> Document the root cause and preventive measures.</li>
        </ul>
        <p><strong>In Practice:</strong> After each major outage, I prepare a formal RCA document within 24 hours  reviewed in the weekly ops sync to ensure permanent fixes.</p>
      </div>`},{question:"How do you handle a production incident?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Production Incident Handling Steps</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 Detection  Monitoring tools trigger alerts (Azure Monitor / Grafana).</li>
          <li>2 Acknowledgement  On-call engineer gets notified via Teams / PagerDuty.</li>
          <li>3 Containment  Isolate the failing component to reduce blast radius.</li>
          <li>4 Mitigation  Apply workaround or rollback last deployment.</li>
          <li>5 Communication  Update stakeholders via incident channel.</li>
          <li>6 RCA  Analyze logs and identify the failure cause post recovery.</li>
        </ul>
        <p><strong>In Practice:</strong> For AKS outage incidents, I immediately scale backup pods, notify QA via Teams, and initiate rollback through YAML pipeline within 5 minutes.</p>
      </div>`},{question:"How do you document and present an RCA report?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Documenting RCA Reports</strong></h3>
        <p>
          RCA documentation ensures transparency and learning from incidents.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code> RCA Template Example:
Incident ID: INC-2025-0421
Date: 2025-11-07
Impact: 25% users unable to access App Service
Root Cause: Application Gateway backend probe failure due to expired SSL certificate
Resolution: Updated SSL binding and restarted App Service
Preventive Action: Added SSL expiry alert (7-day threshold)</code></pre>
        <p><strong>In Practice:</strong> I maintain all RCA reports in Confluence and review monthly to identify recurring issues and automate prevention.</p>
      </div>`},{question:"How do you automate incident detection and response?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated Incident Response</strong></h3>
        <p>
          Automation reduces MTTR (Mean Time to Recovery) and ensures faster resolutions.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Monitor</strong> + <strong>Logic Apps</strong> for alert-triggered actions.</li>
          <li>Auto-create incidents in <strong>ServiceNow</strong> or <strong>Jira</strong>.</li>
          <li>Trigger <strong>Runbooks</strong> to restart VMs, scale AKS nodes, or recycle App Services.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># Example Logic App Trigger
When Azure Alert fires:
 Check resource tag auto-remediate
 Run PowerShell to restart resource
 Post message in Teams channel</code></pre>
        <p><strong>In Practice:</strong> I built a Logic App flow where high CPU alerts automatically restart the VM and post status updates in our Teams war room channel.</p>
      </div>`},{question:"How do you perform post-incident analysis and continuous improvement?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Post-Incident Analysis & Continuous Improvement</strong></h3>
        <p>
          Post-incident reviews help identify systemic issues and build preventive culture.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Review incident trend metrics (MTTR, incident frequency).</li>
          <li> Conduct blameless post-mortems for collaborative learning.</li>
          <li> Create action items for long-term fixes and track completion.</li>
        </ul>
        <p><strong>In Practice:</strong> After a 2025 AKS network outage, we introduced health probes + pre-deployment checks to avoid similar issues in the future.</p>
      </div>`},{question:"What are your best practices for managing incidents effectively?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Incident Management Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Define SLAs for each alert severity (P1 = 15 mins response).</li>
          <li> Automate repetitive runbooks for common incidents (restart, cleanup).</li>
          <li> Maintain 24x7 on-call rotation with proper escalation matrix.</li>
          <li> Keep all incident SOPs and RCAs documented in one knowledge base.</li>
          <li> Measure KPIs: MTTA (Mean Time to Acknowledge) & MTTR (Mean Time to Resolve).</li>
        </ul>
        <p><strong>In Practice:</strong> Our DevOps team reduced MTTR by 40% after automating VM and App Service restart workflows through Azure Automation.</p>
      </div>`}]},{title:"14. Real-World Azure DevOps Troubleshooting Use Cases (Project-Level Scenarios)",questions:[{question:"Terraform apply failed in Azure DevOps  how did you fix it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Apply Failure</strong></h3>
        <p>
          Once, <strong>terraform apply</strong> failed in pipeline due to state file lock and missing RBAC permissions on the storage account used for backend.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Checked <code>terraform plan</code> output  verified backend state path.</li>
          <li> Used <code>az role assignment list</code> to ensure SPN had Storage Blob Contributor role.</li>
          <li> Ran <code>terraform force-unlock</code> to clear stale lock.</li>
          <li> Re-ran pipeline and validated state consistency.</li>
        </ul>
        <p><strong>In Practice:</strong> I automated state unlock and permission verification script before apply to prevent this recurring issue.</p>
      </div>`},{question:"Docker image build was failing in Azure DevOps  what was your approach?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Build Failure Case</strong></h3>
        <p>
          Build failed with error <code>failed to pull base image: unauthorized</code> due to ACR authentication issue.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Verified that <strong>Service Connection</strong> had ACR push/pull permissions.</li>
          <li> Re-authenticated using <code>az acr login --name &lt;registry&gt;</code>.</li>
          <li> Added <strong>ACR credentials</strong> under pipeline variables (secret).</li>
          <li> Finally ran <code>docker build</code> with <code>--pull</code> flag to refresh cache.</li>
        </ul>
        <p><strong>In Practice:</strong> I added a pre-step in pipeline to auto-login ACR before any docker command  eliminating manual authentication errors.</p>
      </div>`},{question:"Pipeline got stuck on 'Initialize job'  what did you do?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Job Initialization Stuck</strong></h3>
        <p>
          The job hung during <code>Initialize job</code> phase due to unavailable build agents and queue saturation.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Checked Agent Pool  found only 1 self-hosted agent, busy with long-running job.</li>
          <li> Scaled agent pool from 1  3 VMs using auto-scale logic in Azure VMSS.</li>
          <li> Configured concurrency limit (max parallel = 2) to avoid overload.</li>
        </ul>
        <p><strong>In Practice:</strong> I automated agent scaling via Azure Automation script  ensuring no job waits longer than 2 minutes in queue.</p>
      </div>`},{question:"Release pipeline failed during deployment to App Service  what was root cause?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>App Service Deployment Failure</strong></h3>
        <p>
          The deployment failed with 403 Forbidden  cause was incorrect publishing profile and expired Service Principal secret.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Rotated SPN secret in Azure AD and updated service connection.</li>
          <li> Regenerated App Service publish profile.</li>
          <li> Validated using <code>az webapp deployment list-publishing-profiles</code>.</li>
          <li> Re-triggered release and verified success in deployment logs.</li>
        </ul>
        <p><strong>In Practice:</strong> Now we store SPN secrets in Azure Key Vault and fetch dynamically at runtime to prevent secret expiry failures.</p>
      </div>`},{question:"Build pipeline failed due to missing NuGet dependencies  what did you do?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>NuGet Dependency Failure</strong></h3>
        <p>
          Build failed as private NuGet feed was inaccessible from build agent.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Added NuGet feed credentials under <strong>Service Connection  NuGet</strong>.</li>
          <li> Updated YAML to restore packages using auth token.</li>
          <li> Example:</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: NuGetAuthenticate@1
- script: dotnet restore MyApp.sln</code></pre>
        <p><strong>In Practice:</strong> I implemented <code>NuGetAuthenticate@1</code> as a reusable template across projects  fixing 90% package restore issues.</p>
      </div>`},{question:"Terraform state drift detected in pipeline  what steps did you take?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform State Drift Resolution</strong></h3>
        <p>
          Drift occurred because manual resource modification in Azure Portal caused mismatch between infra and state file.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Ran <code>terraform plan -refresh-only</code> to identify drift.</li>
          <li> Imported unmanaged resources using <code>terraform import</code>.</li>
          <li> Educated team to never manually change cloud resources.</li>
          <li> Enforced Policy-as-Code via Azure Policy and OPA to block direct portal changes.</li>
        </ul>
        <p><strong>In Practice:</strong> We enabled Azure Activity Logs integration  any manual infra change now triggers an alert and RCA task.</p>
      </div>`},{question:"Artifact publishing failed  whats your approach to debug?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Artifact Publishing Failure</strong></h3>
        <p>
          Failure happened due to incorrect artifact path and missing permissions to pipeline workspace.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Verified correct path in <code>publish</code> step.</li>
          <li> Checked if <code>$(Build.ArtifactStagingDirectory)</code> exists before publish.</li>
          <li> Fixed by adding:</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: PublishBuildArtifacts@1
  inputs:
    PathtoPublish: '$(Build.ArtifactStagingDirectory)'
    ArtifactName: 'drop'</code></pre>
        <p><strong>In Practice:</strong> I also added cleanup task post-build to remove large temp files  preventing storage overflows on agents.</p>
      </div>`},{question:"YAML variable substitution failed during deployment  how did you fix it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>YAML Variable Substitution Issue</strong></h3>
        <p>
          Pipeline failed to replace environment variables in <code>appsettings.json</code> because variable names didnt match case sensitivity.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Verified variables and replaced syntax using <code>$(variableName)</code>.</li>
          <li> Used variable groups and linked correctly to pipeline.</li>
          <li> Added explicit replacement task:</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: FileTransform@2
  inputs:
    folderPath: '$(System.DefaultWorkingDirectory)'
    fileType: 'json'</code></pre>
        <p><strong>In Practice:</strong> I created environment-specific variable groups (DEV, QA, PROD) and automated linking via templates to eliminate manual mismatches.</p>
      </div>`},{question:"Long-running build failed due to agent timeout  how to handle it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Build Timeout Issue</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Checked build duration  exceeded default 60 mins agent limit.</li>
          <li>Split large jobs into smaller stages  parallelized tasks (build, test, scan).</li>
          <li>Increased timeout using <code>timeoutInMinutes: 120</code> in YAML.</li>
        </ul>
        <p><strong>In Practice:</strong> After splitting large monolithic builds, total build time dropped from 95 mins to 34 mins  with zero timeouts.</p>
      </div>`},{question:"How do you perform RCA after a failed deployment in production?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>RCA for Failed Production Deployment</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Collected pipeline logs and Application Insights telemetry.</li>
          <li> Compared last successful and failed commit diffs.</li>
          <li> Identified config drift  missing Key Vault secret reference.</li>
          <li> Fixed YAML template and added validation job before deploy.</li>
          <li> Documented RCA in Confluence with preventive actions.</li>
        </ul>
        <p><strong>In Practice:</strong> I built an automated validation stage to check secret injection and service connectivity before deploying to PROD.</p>
      </div>`}]},{title:"15. Azure DevOps Advanced Topics  Approvals, Environments, Gates & Deployment Strategies",questions:[{question:"What are Environments in Azure DevOps and why use them?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure DevOps Environments</strong></h3>
        <p>
          <strong>Environments</strong> represent deployment targets (DEV, QA, UAT, PROD, AKS cluster, VM, etc.) in Azure DevOps.  
          They provide a single place to model targets, deploy using deployment jobs, and attach approvals, checks, and resource protections.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Track deployment history and who deployed what and when.</li>
          <li>Attach environment-specific secrets and service connections.</li>
          <li>Use environment-level checks (approvals, gates) to control deployments.</li>
        </ul>
        <p><strong>In Practice:</strong> I create Environments for each stage (dev/qa/uat/prod) and attach required service connections and post-deploy checks so PROD never gets an automated blind deploy.</p>
      </div>`},{question:"What are Approvals & Checks for Environments and how do they work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Approvals & Checks</strong></h3>
        <p>
          Approvals and checks are governance controls you attach to an environment to gate deployments. They include:
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Approvals</strong>  manual approval by individuals/groups before deployment proceeds.</li>
          <li><strong>Gates</strong>  automated checks like ARM template validation, Azure Resource Health, REST API checks, or required work items.</li>
          <li><strong>Branch controls</strong> and required reviewers can also be enforced via pipeline policies.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure 2 approvers for PROD (on-call + manager) and add an automated gate that validates App Insights health and successful smoke test endpoint before traffic is switched.</p>
      </div>`},{question:"How do you configure environment approvals in YAML pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environment Approvals in YAML Flow</strong></h3>
        <p>
          Approvals are configured in the Azure DevOps UI on the Environment itself (Approvals & checks)  YAML references the environment name.  
          The pipeline uses a <code>deployment</code> job that targets the environment, so checks are enforced automatically.
        </p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>stages:
- stage: DeployProd
  jobs:
  - deployment: DeployToProd
    environment: 'prod'
    strategy:
      runOnce:
        deploy:
          steps:
            - script: echo "Deploying to PROD"</code></pre>
        <p><strong>In Practice:</strong> I keep approval logic outside YAML (in environment settings) so non-dev teams (PMs, SecOps) can manage approvers without code changes.</p>
      </div>`},{question:"What are Deployment Gates and common gate types?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deployment Gates</strong></h3>
        <p>
          Gates are automated pre-deployment checks that must pass before an approval is considered complete. Common types:
        </p>
        <ul style="margin-left:1.2rem;">
          <li>REST API check (call a health endpoint or external system).</li>
          <li>Azure Resource Health / ARM template validation.</li>
          <li>Query-based checks (work items linked, successful smoke test results in DB).</li>
          <li>Invoke Logic Apps or Azure Functions for custom validation.</li>
        </ul>
        <p><strong>In Practice:</strong> I use a Gate that calls a smoke-test endpoint after the staging deploy; it returns pass/fail and only then the PROD approver sees a green check to approve.</p>
      </div>`},{question:"Explain Blue-Green deployment and how you implement it in Azure.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Blue-Green Deployment</strong></h3>
        <p>
          Blue-Green maintains two identical environments: <em>Blue</em> (current) and <em>Green</em> (new). Deploy to Green, test, then switch traffic to Green  rollback is just switch back.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Zero-downtime cutover by switching load balancer / DNS / Traffic Manager.</li>
          <li>Use health probes and smoke tests before switching traffic.</li>
          <li>Keep previous environment for quick rollback and post-deploy validation.</li>
        </ul>
        <p><strong>In Practice:</strong> For web apps I deploy to a slot (or separate App Service), run automated smoke tests, then swap slots or change Traffic Manager profile to cut traffic to the green environment.</p>
      </div>`},{question:"Explain Canary deployment and rollout strategies.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Canary Deployment</strong></h3>
        <p>
          Canary is a progressive rollout to a small portion of users initially, monitor metrics, then increase rollout percentage. Good for catching issues early without full blast.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Start 1-5% traffic  monitor errors, latency, resource usage.</li>
          <li>Use weighted routing (Ingress, App Gateway, Front Door, or service mesh) to control percentages.</li>
          <li>Automate promotion based on health metrics or manual approvals between steps.</li>
        </ul>
        <p><strong>In Practice:</strong> I implement canary using Argo Rollouts / Istio for AKS or Azure Front Door weights for App Services, with automatic rollback if error rate crosses threshold.</p>
      </div>`},{question:"What is Rolling / Ring deployment, and where is it useful?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rolling / Ring Deployment</strong></h3>
        <p>
          Rolling updates replace instances in batches (e.g., 10% at a time) until all are updated  reduces risk while maintaining capacity. Ring deployments are staged rollouts across user groups or regions (ring0 = internal, ring1 = beta, ring2 = prod).
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Use for stateful services or when gradual capacity change is required.</li>
          <li>Combine with health checks and min-availability to avoid service disruption.</li>
        </ul>
        <p><strong>In Practice:</strong> For large clusters I use rolling updates with maxUnavailable set to maintain N replicas and ring deployments for staged customer exposure (internal  staging  global).</p>
      </div>`},{question:"How do you implement automated rollback and health checks?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated Rollback & Health Checks</strong></h3>
        <p>
          Key elements: automated health validation, alert thresholds, and a rollback action if health degrades.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Run smoke tests (synthetic) immediately after deploy stage; gate promotion on pass.</li>
          <li>Monitor metrics (error rate, latency, CPU) for a short observation window post-deploy.</li>
          <li>If thresholds breached, trigger rollback job (re-deploy previous artifact or switch traffic back).</li>
        </ul>
        <p><strong>In Practice:</strong> I wire Application Insights and Prometheus metrics into deployment gates  if errorRate &gt; 1% for 5 mins, a Logic App triggers a rollback stage in the pipeline and notifies the incident channel.</p>
      </div>`},{question:"How do you secure deployments and enforce approvals programmatically?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Deployments & Programmatic Approvals</strong></h3>
        <p>
          Security controls are enforced at environment-level and via pipeline policies:
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Environment checks</strong> (approvals, ARM validation, required work items).</li>
          <li>Enforce branch policies (required reviewers, build validation) for pipeline-as-code changes.</li>
          <li>Use service principals with least privilege and rotate credentials via Key Vault.</li>
          <li>Require manual approval for deployments to sensitive environments and log every approver action for audit.</li>
        </ul>
        <p><strong>In Practice:</strong> I combine branch policies, environment approvals, and required gates (smoke tests + security scan) so no code can reach PROD without multi-party validation.</p>
      </div>`},{question:"What deployment strategy would you choose for a high-risk change and why?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Choosing a Deployment Strategy</strong></h3>
        <p>
          For a high-risk change (DB migration, critical API update), I prefer a staged approach:
        </p>
        <ul style="margin-left:1.2rem;">
          <li>1 Canary or ring deployment to expose change to small audience first.</li>
          <li>2 Run extensive automated integration and end-to-end tests in staging.</li>
          <li>3 Use feature flags for runtime toggle and rapid rollback without redeploy.</li>
          <li>4 If schema changes required, use backward-compatible migrations first and schedule final cutover in low-traffic window.</li>
        </ul>
        <p><strong>In Practice:</strong> For DB changes I deploy code that supports both old & new schema, rollout the schema migration using controlled scripts, then switch traffic after validation  minimizing outage and rollback complexity.</p>
      </div>`}]}];function Kw(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("azure-monitor"),a=um.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-purple-500 to-pink-500 shadow-glow",children:l.jsx(vu,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Azure Monitor & Observability"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Monitoring, Logging, Alerts & Observability Fundamentals"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Master Azure Monitor, KQL queries, Prometheus, Grafana, and complete observability practices."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:um.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:(u==null?void 0:u.question)||"",b=typeof u=="object"&&u&&"answer"in u&&!!u.answer,v=typeof u=="object"&&u&&"answerHtml"in u&&!!u.answerHtml,h=b?u.answer:null,w=b||v;return l.jsx("div",{className:"border-l-2 border-purple-500/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-purple-500 font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),w&&l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:u.answerHtml}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:h})})]})]})]})},f)})})})]},p))})]})]})}const Y3=Object.freeze(Object.defineProperty({__proto__:null,default:Kw},Symbol.toStringTag,{value:"Module"})),pm=[{title:"1. Personal Introduction & Role Summary",questions:[{question:"Tell me about yourself and your DevOps experience.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Personal Introduction</strong></h3>
        <p>
          Hi, Im <strong>Ritesh Sharma</strong>, a DevOps Engineer with around <strong>5 years of hands-on experience</strong> in automation, CI/CD pipelines, and infrastructure management across <strong>Azure Cloud</strong> and on-prem environments.
        </p>
        <p>
          I specialize in <strong>Azure DevOps, Terraform, Docker, Kubernetes, Jenkins, and GitHub Actions</strong>.  
          Currently, Im working at <strong>Litmus Information Systems LLP</strong>, where I manage build and release pipelines, VM provisioning, NFS setup, monitoring, and patching for a healthcare client in Europe.
        </p>
        <p>
          I focus on <strong>end-to-end delivery automation</strong>  from code commit to deployment, ensuring reliability, consistency, and security in every stage.
        </p>
      </div>`},{question:"Introduce yourself  describe your roles and responsibilities.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Roles & Responsibilities</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Designing and managing <strong>CI/CD pipelines</strong> using Azure DevOps (YAML & Classic).</li>
          <li> Building and deploying infrastructure using <strong>Terraform (IaC)</strong>.</li>
          <li> Containerizing applications using <strong>Docker</strong> and orchestrating through <strong>Kubernetes</strong>.</li>
          <li> Implementing <strong>SonarQube, Checkov, and Prometheus-Grafana</strong> for quality and monitoring.</li>
          <li> Performing patching, upgrades, and <strong>offline leapp migrations</strong> for RHEL servers.</li>
          <li> Managing <strong>Azure Repos</strong>, branching, approvals, and deployment gates.</li>
          <li> Collaborating with development and QA teams for smooth release cycles.</li>
        </ul>
        <p><strong>In short:</strong> My role ensures that software delivery is fast, secure, and reproducible with zero manual dependency.</p>
      </div>`},{question:"What are your day-to-day activities in your current project?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Day-to-Day DevOps Activities</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Monitoring daily <strong>CI/CD pipeline executions</strong> and resolving failed builds.</li>
          <li> Managing <strong>Terraform deployments</strong> for new environments (DEV/QA/UAT/PROD).</li>
          <li> Reviewing and merging PRs on <strong>Azure Repos</strong> with proper branch policies.</li>
          <li> Configuring <strong>service connections</strong> and secrets via Azure Key Vault.</li>
          <li> Performing <strong>Linux server patching and monitoring</strong> using Grafana & Prometheus.</li>
          <li> Coordinating with developers to optimize Dockerfiles and deployments.</li>
          <li> Documenting infrastructure changes and release notes for every sprint.</li>
        </ul>
        <p><strong>In Practice:</strong> I start the day by checking pipeline dashboards, reviewing alerts, and ensuring the previous nights deployments were successful.</p>
      </div>`},{question:"How do you get tasks assigned (Jira, ServiceNow, or other tools)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Task Management</strong></h3>
        <p>
          In our team, we primarily use <strong>ServiceNow</strong> for ticketing and <strong>Jira</strong> for sprint planning.  
          DevOps activities are tracked under Jira stories or ServiceNow change requests.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> I pick tickets assigned to DevOps from the daily sprint board.</li>
          <li> Each task has a clear description, acceptance criteria, and linked PR.</li>
          <li> Daily stand-up updates progress and blockers.</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain both operational (ServiceNow) and project (Jira) tasks so that changes are traceable for compliance audits.</p>
      </div>`},{question:"How do you communicate updates to your team, developers, and testers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Communication & Collaboration</strong></h3>
        <p>
          We follow a structured communication flow:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Daily stand-ups and sprint reviews through <strong>Microsoft Teams</strong>.</li>
          <li> Share deployment status via automated Teams webhook from Azure Pipelines.</li>
          <li> Maintain shared Confluence pages for environment setup and infra changes.</li>
          <li> Post-release reports and build summaries go to all stakeholders.</li>
        </ul>
        <p><strong>In Practice:</strong> I keep developers and testers updated on pipeline readiness or rollback actions instantly via Teams integrations.</p>
      </div>`},{question:"What was your biggest achievement in your last project?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Major Achievement</strong></h3>
        <p>
          Successfully automated the entire infrastructure provisioning and deployment for a multi-region healthcare system using <strong>Terraform + Azure DevOps YAML pipelines</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Reduced manual deployment time from 3 hours to 15 minutes.</li>
          <li>Integrated <strong>SonarQube, Checkov, and Key Vault</strong> into CI/CD pipelines.</li>
          <li>Built <strong>reusable YAML templates</strong> for DEV, QA, and PROD pipelines.</li>
        </ul>
        <p><strong>Impact:</strong> It improved consistency, reduced human error, and increased deployment frequency by 4x.</p>
      </div>`},{question:"What was your biggest learning from your DevOps journey?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Key Learnings from DevOps Journey</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Always automate repetitive tasks  it reduces human error.</li>
          <li> Security should be part of pipeline design, not an afterthought.</li>
          <li> Observability is as important as deployment  always monitor before and after release.</li>
          <li> Clear documentation and communication are the foundation of smooth CI/CD.</li>
        </ul>
        <p><strong>In Practice:</strong> I learned that DevOps success is 30% tools and 70% process, discipline, and collaboration.</p>
      </div>`},{question:"What made you interested in DevOps as a career?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Why DevOps?</strong></h3>
        <p>
          Ive always been passionate about automation and problem-solving.  
          When I saw how DevOps bridges the gap between development and operations  I knew this was my path.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> I love building solutions that reduce manual work and speed up delivery.</li>
          <li> DevOps gives end-to-end ownership  from code to production.</li>
          <li> Working with cloud, containers, and automation gives me daily challenges and learning.</li>
        </ul>
        <p><strong>In Practice:</strong> I started with system administration and gradually evolved into full DevOps because I enjoy seeing automation bring teams together and deliver faster with quality.</p>
      </div>`}]},{title:"2. Project Explanation & Tools",questions:[{question:"Describe your current or previous project in detail.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Project Overview  Centralized Patient Monitoring System (Healthcare Client, Europe)</strong></h3>
        <p>
          Im currently working on a <strong>Centralized Patient Monitoring System</strong> for a European healthcare provider.  
          The project involves collecting, processing, and visualizing real-time patient data from IoT-enabled medical devices hosted on Azure.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Infrastructure is built using <strong>Terraform</strong> with <strong>Azure as the cloud provider</strong>.</li>
          <li> Applications are containerized using <strong>Docker</strong> and deployed on <strong>Azure Kubernetes Service (AKS)</strong>.</li>
          <li> CI/CD implemented through <strong>Azure DevOps YAML pipelines</strong>.</li>
          <li> Monitoring with <strong>Azure Monitor, Grafana, and Prometheus</strong>.</li>
          <li> Secrets stored securely in <strong>Azure Key Vault</strong>.</li>
          <li> Logging handled via <strong>Log Analytics + Application Insights</strong>.</li>
        </ul>
        <p><strong>Objective:</strong> Achieve 99.9% uptime with automated deployment, infrastructure reproducibility, and continuous monitoring.</p>
      </div>`},{question:"What DevOps tools are you using and why?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Tools & Technologies Stack</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure DevOps</strong>  for CI/CD pipelines, Repos, and Artifacts.</li>
          <li> <strong>Terraform</strong>  for infrastructure provisioning (IaC).</li>
          <li> <strong>Docker</strong>  for application containerization and portability.</li>
          <li> <strong>AKS (Kubernetes)</strong>  for container orchestration and scaling.</li>
          <li> <strong>SonarQube</strong>  for code quality analysis and static scanning.</li>
          <li> <strong>Azure Key Vault</strong>  for secrets and credential management.</li>
          <li> <strong>Grafana & Prometheus</strong>  for monitoring and alerting.</li>
          <li> <strong>Jira</strong>  for sprint planning and tracking.</li>
          <li> <strong>Confluence</strong>  for project documentation and RCA reports.</li>
        </ul>
        <p><strong>In Practice:</strong> I chose Azure DevOps as the central automation platform because it integrates tightly with Azure and supports Terraform, Docker, and SonarQube plugins natively.</p>
      </div>`},{question:"Explain your project architecture (Azure, Terraform, AKS, etc.).",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Project Architecture  Azure Cloud & DevOps Flow</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Infrastructure:</strong> Created using Terraform  resource groups, VNets, subnets, NSGs, storage, AKS, and ACR.</li>
          <li> <strong>App Deployment:</strong> Docker images built in pipeline  pushed to ACR  deployed to AKS.</li>
          <li> <strong>State Management:</strong> Terraform backend stored in Azure Storage Account.</li>
          <li> <strong>Artifact Flow:</strong> Code  Build  SonarQube scan  Docker build  Push to ACR  Deploy via Helm.</li>
          <li> <strong>Monitoring:</strong> Azure Monitor + Prometheus integrated with Grafana dashboard.</li>
          <li> <strong>Security:</strong> Secrets fetched dynamically from Azure Key Vault during deployment.</li>
        </ul>
        <p><strong>In Practice:</strong> Each deployment follows the GitOps model  once code is merged, pipeline triggers infra + app deployment automatically to target environment.</p>
      </div>`},{question:"What environments do you manage (DEV, UAT, PROD)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environment Management</strong></h3>
        <p>
          We maintain four key environments: <strong>DEV  QA  UAT  PROD</strong>.  
          Each has its own variable group and state backend for isolation.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> DEV  Feature testing, runs multiple times daily.</li>
          <li> QA  Integration testing + SonarQube & Checkov scans.</li>
          <li> UAT  Pre-prod with limited data and manual approvals.</li>
          <li> PROD  Fully automated, triggered only after approvals + gates pass.</li>
        </ul>
        <p><strong>In Practice:</strong> Terraform state and YAML variable groups are environment-specific to ensure isolation and prevent accidental overwrites.</p>
      </div>`},{question:"What branching strategy do you follow, and why?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Branching Strategy  Trunk-Based + Feature Branch</strong></h3>
        <p>
          We follow a <strong>Trunk-based branching strategy</strong> with short-lived feature branches.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <code>main</code>  always stable and production-ready.</li>
          <li> <code>feature/*</code>  for individual features and bug fixes.</li>
          <li> <code>release/*</code>  for staging or UAT builds.</li>
          <li> <code>hotfix/*</code>  for emergency production patches.</li>
        </ul>
        <p><strong>In Practice:</strong> Every PR triggers a build validation pipeline and requires at least one reviewer before merge  this ensures quality and compliance.</p>
      </div>`},{question:"How do you integrate Terraform with pipelines in your project?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Integration with Azure DevOps Pipelines</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Terraform scripts stored under <code>/infra</code> folder in Azure Repos.</li>
          <li> Pipeline uses stages  Init  Validate  Plan  Apply.</li>
          <li> Remote backend configured with Azure Storage Account for state file.</li>
          <li> Secrets like clientId, subscriptionId, etc. pulled from Azure Key Vault.</li>
          <li> Plan output uploaded as an artifact for manual approval before apply.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>- task: TerraformTaskV4@4
  inputs:
    provider: 'azurerm'
    command: 'apply'
    environmentServiceNameAzureRM: 'my-service-connection'</code></pre>
        <p><strong>In Practice:</strong> I modularized Terraform  each module represents a resource stack (network, compute, storage) to improve reusability and pipeline efficiency.</p>
      </div>`},{question:"Which tools do you use for tracking, collaboration, and documentation (Jira, Confluence, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Tracking & Collaboration Tools</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Jira</strong>  sprint planning, tracking DevOps tasks, and bug tickets.</li>
          <li> <strong>Confluence</strong>  maintaining SOPs, RCA reports, and environment documentation.</li>
          <li> <strong>Microsoft Teams</strong>  daily stand-ups and incident alerts via webhook.</li>
          <li> <strong>ServiceNow</strong>  managing change requests and release approvals.</li>
        </ul>
        <p><strong>In Practice:</strong> I automate ticket creation via Azure Monitor  Logic App  ServiceNow integration  so alerts generate tickets automatically.</p>
      </div>`},{question:"How do you ensure CI/CD pipelines are secure and compliant?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline Security & Compliance</strong></h3>
        <p>
          Security is integrated at every stage of our CI/CD lifecycle.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Use <strong>Azure Key Vault</strong> for secrets  no hard-coded credentials.</li>
          <li> Enable <strong>RBAC & least privilege</strong> on all service connections.</li>
          <li> Run <strong>SonarQube</strong> and <strong>Checkov</strong> scans before deployment.</li>
          <li> Audit trails & logs retained for every release (ServiceNow change linked).</li>
          <li> All PROD pipelines have manual approvals + environment gates.</li>
          <li> Use <strong>branch policies + build validation</strong> before PR merge.</li>
        </ul>
        <p><strong>In Practice:</strong> Our pipelines comply with healthcare data standards (HIPAA)  ensuring security, traceability, and zero secret leakage across builds.</p>
      </div>`}]},{title:"3. Production Challenges & Troubleshooting",questions:[{question:"What challenges have you faced in production deployments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Production Challenges</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Pipeline failures due to expired service principal credentials or missing Key Vault secrets.</li>
          <li> Docker image pull failures from ACR because of authentication timeout.</li>
          <li> Permission issues  incorrect RBAC roles or locked Terraform state.</li>
          <li> Network latency between Azure regions causing AKS pod restarts.</li>
          <li> Terraform drift  manual infra changes not reflected in state file.</li>
          <li> VM patch reboots impacting application uptime.</li>
        </ul>
        <p><strong>In Practice:</strong> I always maintain pre-deployment checklists  verifying service connections, storage backend, and pipeline agents before triggering PROD release.</p>
      </div>`},{question:"Describe a production issue you handled under pressure.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Real Production Incident</strong></h3>
        <p>
          One night, our healthcare API gateway went down due to an expired SSL certificate  impacting all patient monitoring dashboards.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Identified failure via Azure Application Gateway health probe alerts.</li>
          <li> Renewed certificate in Key Vault and re-imported into the Gateway listener.</li>
          <li> Restarted gateway services and validated health probes.</li>
          <li> Application came online within 15 minutes, minimizing downtime.</li>
        </ul>
        <p><strong>Post-fix:</strong> I added an automated Azure Monitor alert for certificate expiry 15 days prior  preventing future impact.</p>
      </div>`},{question:"How do you handle deployment failures in CI/CD?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling CI/CD Deployment Failures</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Check pipeline logs  identify which stage or task failed.</li>
          <li>Validate variables, service connections, or environment access.</li>
          <li>Re-run failed job with <code>system.debug=true</code> enabled for verbose logs.</li>
          <li>Rollback last successful artifact automatically (if configured).</li>
          <li>Update stakeholders via Teams webhook or ServiceNow ticket.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code>condition: failed()
steps:
  - script: echo "Rolling back to previous stable release"</code></pre>
        <p><strong>In Practice:</strong> I implemented rollback stages in YAML pipelines  so if PROD deploy fails, pipeline redeploys last green build automatically.</p>
      </div>`},{question:"Tell me about a mistake you made in production and how you handled it.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Owning a Production Mistake</strong></h3>
        <p>
          Once, I applied Terraform changes without reviewing the plan properly  which accidentally deleted a storage account used for diagnostic logs.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Immediately stopped the apply job.</li>
          <li> Restored the deleted resource using Recovery Vault snapshot.</li>
          <li> Validated all pipelines and monitoring connections.</li>
          <li> Documented the RCA and added mandatory plan approval stage before apply.</li>
        </ul>
        <p><strong>Lesson:</strong> Never skip manual review of <code>terraform plan</code> output, especially in shared infra.</p>
      </div>`},{question:"How do you troubleshoot a pipeline or infrastructure issue?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Systematic Troubleshooting Approach</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 Identify whether failure is <strong>code, pipeline, or environment-related</strong>.</li>
          <li>2 Enable <code>system.debug=true</code> and re-run failed job.</li>
          <li>3 Check Azure Activity Logs and agent job logs.</li>
          <li>4 Validate resource health (VMs, AKS nodes, App Service) via Azure Portal.</li>
          <li>5 Compare configuration with last successful run.</li>
          <li>6 Document RCA in Confluence and implement preventive measure.</li>
        </ul>
        <p><strong>In Practice:</strong> My thumb rule  never guess; always reproduce, isolate, verify, then fix.</p>
      </div>`},{question:"If a VM shows running but RDP not working, how will you resolve it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>RDP Troubleshooting for Running VM</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Check NSG rules  ensure port 3389 inbound rule exists and public IP is reachable.</li>
          <li> Use <strong>Serial Console</strong> from Azure Portal to check firewall or RDP service.</li>
          <li> Run <strong>Network Watcher  IP Flow Verify</strong> to confirm port access.</li>
          <li> Restart RDP service from serial console:
            <pre style="background:#111;padding:.5rem;border-radius:.4rem;"><code>net start termservice</code></pre>
          </li>
          <li> If still blocked, reset NIC or create a new NIC and attach.</li>
        </ul>
        <p><strong>In Practice:</strong> I resolved similar cases by enabling Boot Diagnostics and resetting RDP config using Azure Run Command.</p>
      </div>`},{question:"How do you troubleshoot VM startup or connectivity issues?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VM Startup / Connectivity Troubleshooting</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Verify Boot Diagnostics screenshots  check if stuck on update or login screen.</li>
          <li> Review Activity Logs for recent power actions.</li>
          <li> Use Serial Console or Run Command (<code>Get-Service</code>, <code>ipconfig</code>).</li>
          <li> Check NIC health, public IP, and NSG inbound/outbound rules.</li>
          <li> Restart the VM or redeploy it (preserves disk and NIC).</li>
        </ul>
        <p><strong>In Practice:</strong> A VM in our environment boot-looped due to OS corruption; I attached its OS disk to another healthy VM, fixed registry, and redeployed successfully.</p>
      </div>`},{question:"Describe your approach to solving complex issues during downtime.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Downtime Under Pressure</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Immediately notify stakeholders and create an incident bridge (Teams/War room).</li>
          <li> Identify scope  which services or users are affected.</li>
          <li> Divide troubleshooting  infra, app, DB, and network teams in parallel.</li>
          <li> Use logs, Application Insights, and alerts to locate root cause.</li>
          <li> Apply quick workaround (rollback or scale out) to restore service.</li>
          <li> Post-incident  document RCA and preventive measures.</li>
        </ul>
        <p><strong>In Practice:</strong> During an AKS outage, I scaled the app via backup cluster and rerouted traffic via Front Door  restoring user access within 10 minutes.</p>
      </div>`}]},{title:"4. Automation, Scripting & Optimization",questions:[{question:"Describe a situation where you automated a manual process.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating a Repetitive Manual Task</strong></h3>
        <p>
          I automated the nightly snapshot & cleanup process for non-prod VMs and unattached disks which was earlier done manually every morning.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Built an Azure Automation Runbook (PowerShell) that:
            <ul>
              <li>Finds VMs tagged <code>env:dev</code> and takes snapshots.</li>
              <li>Deletes snapshots older than retention period (30 days).</li>
              <li>Writes status to a Log Analytics custom table and posts summary to Teams.</li>
            </ul>
          </li>
          <li> Scheduled runbook via TimeTrigger and secured automation account with managed identity.</li>
          <li> Result: Reduced manual work by weekly 6 hours and cut storage cost by removing stale snapshots.</li>
        </ul>
        <p><strong>In Practice:</strong> I always include idempotency and safe-guards (dry-run flag) for automation that modifies infra.</p>
      </div>`},{question:"What scripting languages have you used (Bash, PowerShell, Python)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Scripting Languages I Use</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Bash</strong>  quick automation on Linux agents, Docker build helpers, and small utilities.</li>
          <li> <strong>PowerShell</strong>  Azure Automation runbooks, VM management, and Windows-specific tasks.</li>
          <li> <strong>Python</strong>  complex orchestration scripts, API integrations, and log parsing (with rich libraries).</li>
        </ul>
        <p><strong>In Practice:</strong> I pick the language based on platform: Bash for Linux CI tasks, PowerShell for Azure/Windows automation, Python when parsing or structured logic is needed.</p>
      </div>`},{question:"How do you automate patching in Linux or Windows VMs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated Patching Strategy</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Windows:
            <ul>
              <li>Use <strong>Update Management</strong> in Azure Automation to schedule patch deployments.</li>
              <li>Define maintenance windows, pre/post scripts, and deployment targets via tags.</li>
            </ul>
          </li>
          <li>Linux:
            <ul>
              <li>Use Azure Automation or Ansible playbooks triggered by pipelines for controlled patch runs.</li>
              <li>Run pre-checks (disk, services) and post-verification (health probe, restart service).</li>
            </ul>
          </li>
          <li>General:
            <ul>
              <li>Perform patching in rings (canary hosts  rest) and run health checks after patch.</li>
              <li>Notify stakeholders and allow maintenance mode gating to avoid business impact.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> For production, I run weekly patch windows with canary VMs first and auto-rollback playbooks if smoke-tests fail.</p>
      </div>`},{question:"How do you ensure cost optimization in Azure environments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cost Optimization Steps</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Enable <strong>Cost Analysis</strong> and set budgets with alerts (80%/90% thresholds).</li>
          <li>Tagging policy: enforce cost-center/project tags to attribute spend.</li>
          <li>Right-size VMs  schedule auto-shutdown for non-prod using Automation or DevTest Labs.</li>
          <li>Use Reserved Instances / Savings Plans for steady-state workloads and Spot instances for batch jobs.</li>
          <li>Implement lifecycle rules for blobs (Hot  Cool  Archive) and delete unattached disks automatically.</li>
        </ul>
        <p><strong>In Practice:</strong> A combination of auto-shutdown + RI purchases reduced monthly bill by ~30% on one project.</p>
      </div>`},{question:"What steps do you take for resource utilization monitoring?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resource Utilization Monitoring</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Collect metrics (CPU, memory, disk I/O, network) using Azure Monitor and Prometheus for Kubernetes.</li>
          <li>Create dashboards in Grafana / Azure Workbooks showing 1h/24h/7d trends and heatmaps for hot resources.</li>
          <li>Set actionable alerts (not noisy)  e.g., CPU > 85% for 10 mins triggers scale or investigation.</li>
          <li>Run monthly review to identify idle or underutilized resources and schedule right-sizing or termination.</li>
        </ul>
        <p><strong>In Practice:</strong> I automate reports (cost + utilization) and create tickets for owners when resources are idle for > 14 days.</p>
      </div>`},{question:"How do you ensure post-deployment validation after automation?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Post-Deployment Validation</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Run smoke tests (HTTP endpoints, DB connectivity, auth) using test runners or curl scripts immediately after deploy.</li>
          <li>Verify health checks and readiness probes (AKS) and application metrics (latency, error rate).</li>
          <li>Use deployment gates to block promotion if validation fails  and attach automatic rollback runbooks.</li>
          <li>Store validation results as pipeline artifacts and publish a summary to Teams / ServiceNow.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;"><code># Example smoke step
- script: |
    STATUS=&dollar;(curl -s -o /dev/null -w "%{http_code}" https://myapp/health)
    if [ "&dollar;STATUS" -ne 200 ]; then
      echo "Health failed" && exit 1
    fi</code></pre>
        <p><strong>In Practice:</strong> I gate UAT  PROD promotion with automated smoke tests + manual approval for final sanity.</p>
      </div>`},{question:"How do you make your pipelines efficient (faster, secure, and reusable)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pipeline Efficiency & Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Modularize pipelines into templates (build.yml, deploy.yml) and reuse across projects.</li>
          <li>Use caching (npm, pip, Docker layer cache) to reduce build times.</li>
          <li>Parallelize tests with matrix strategies and run slower integration tests in separate stage.</li>
          <li>Store secrets in Key Vault and reference via service connections (no secrets in repo).</li>
          <li>Run heavy scans (Checkov/SonarQube) in PR validation but in parallel to avoid blocking builds unnecessarily.</li>
          <li>Measure pipeline duration and set SLAs; continuously optimize the slowest steps.</li>
        </ul>
        <p><strong>In Practice:</strong> By adding caching and splitting unit vs integration tests, we cut build time from ~20m to ~7m and increased developer feedback loop speed.</p>
      </div>`}]},{title:"5. Collaboration & Communication",questions:[{question:"How do you coordinate with developers and testers during release?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Coordinating Releases</strong></h3>
        <p>
          Release coordination is all about preparation, visibility and a single source of truth during the window.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Plan the release in <strong>Jira</strong> with clear owners, acceptance criteria and rollback plan.</li>
          <li> Pre-release checklist: service connections, infra health, Key Vault secrets, smoke-tests, and DB migrations.</li>
          <li> Create a temporary release channel (Teams/Slack)  pipeline posts, manual approvals, and quick decisions happen here.</li>
          <li> Keep developers & testers on standby for immediate validation and hotfixes; testers run a defined test pack right after deployment.</li>
          <li> Post-deploy report: summary of steps, success criteria and any follow-ups shared in channel and Confluence.</li>
        </ul>
        <p><strong>In Practice:</strong> For PROD releases I always run a pre-check runbook automatically, open a war-room channel, and only proceed if all checks pass.</p>
      </div>`},{question:"How do you handle escalations from clients or managers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Escalations</strong></h3>
        <p>
          Fast acknowledgement, clear ownership and continuous updates  thats the triage mantra.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Acknowledge immediately and create an incident with priority, impact and owner in ServiceNow (or the ticketing tool).</li>
          <li> Share initial findings and ETA on the same channel where stakeholders escalated (call or Teams)  set expectations.</li>
          <li> Contain first (rollback/scale/route traffic), then troubleshoot root cause in parallel.</li>
          <li> Provide frequent concise status (every 1015 mins for P1) until resolution and follow with formal RCA.</li>
        </ul>
        <p><strong>In Practice:</strong> I always name a single point-of-contact for stakeholder communication and a technical lead for remediation  keeps updates consistent and reliable.</p>
      </div>`},{question:"How do you manage cross-team dependencies?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Cross-Team Dependencies</strong></h3>
        <p>
          Make dependencies explicit early, own the coordination and automate verification where possible.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Capture dependencies in Jira stories and mark owners & SLAs for each dependency.</li>
          <li> Use integration tests and contract tests in CI to detect breaks early (API schema checks, consumer-driven contracts).</li>
          <li> Schedule syncs for high-risk changes and use pre-deploy sign-offs from dependent teams.</li>
          <li> Automate readiness checks (gates)  pipeline waits until dependent service health or a manual sign-off is present.</li>
        </ul>
        <p><strong>In Practice:</strong> For infra changes touching network or DB I require sign-off from Network and DBA owners before the apply step is allowed.</p>
      </div>`},{question:"What is your approach to working with multiple teams (Dev, QA, Ops, Network)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cross-Team Collaboration Approach</strong></h3>
        <p>
          Align on roles, make processes repeatable and automate handoffs to reduce friction.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Define RACI for major activities (whos Responsible / Accountable / Consulted / Informed).</li>
          <li> Automate handoffs: CI pipeline artifacts, test triggers, and deployment notifications to reduce manual steps.</li>
          <li> Keep shared documentation (runbooks, diagrams) in Confluence and link them in pipeline steps and tickets.</li>
          <li> Run regular cross-team retros and pre-release walkthroughs to surface blockers early.</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer contract tests and smoke tests as the API between teams  if tests pass, the consumer team can proceed confidently.</p>
      </div>`},{question:"How do you handle team conflicts or disagreements?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resolving Conflicts</strong></h3>
        <p>
          Focus on data, experiments and blameless collaboration  not opinions.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Bring evidence: logs, metrics, risk assessment, and small prototypes to validate approaches.</li>
          <li> Propose a short experiment (A/B or PoC) to prove which approach works in practice.</li>
          <li> If still ambiguous, follow an escalation path  involve leads and pick the safer option for production.</li>
          <li> Keep it blameless and document the decision rationale for future reference.</li>
        </ul>
        <p><strong>In Practice:</strong> I resolve many conflicts by running a quick canary and making decisions based on observed behavior rather than assumptions.</p>
      </div>`},{question:"How do you convince stakeholders to adopt a new tool or process?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Driving Tool / Process Adoption</strong></h3>
        <p>
          Start small, demonstrate ROI, reduce perceived risk and support the team through the transition.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Run a focused PoC with measurable KPIs (time saved, error reduction, cost). </li>
          <li> Present results with concrete metrics and a migration/rollback plan.</li>
          <li> Offer training, templates and documentation to lower adoption friction.</li>
          <li> Pilot with a friendly team and gather testimonials  then expand gradually.</li>
        </ul>
        <p><strong>In Practice:</strong> For CI template adoption, I provided ready-to-use YAML templates and a migration checklist  adoption jumped because devs could copy & paste and run immediately.</p>
      </div>`},{question:"How do you ensure transparency and communication during critical releases?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Transparency During Critical Releases</strong></h3>
        <p>
          Clear roles, live communication channels and regular precise updates are key to trust and smooth execution.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Open a dedicated war-room (Teams) with stakeholders and operators present for the window.</li>
          <li> Share a short runbook at the start: success criteria, rollback steps, owner contacts and monitoring dashboards links.</li>
          <li> Post structured updates at regular intervals (start, during critical milestones, end)  keep messages short and status-focused.</li>
          <li> After the release publish a concise post-mortem / summary with any follow-ups and timelines.</li>
        </ul>
        <p><strong>In Practice:</strong> During high-risk releases I appoint one communicator (not the implementer) to send scheduled status updates  it keeps the engineering team focused on execution.</p>
      </div>`}]},{title:"6. Leadership, Responsibility & Pressure Handling",questions:[{question:"How do you prioritize tasks during a sprint?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Task Prioritization Strategy</strong></h3>
        <p>
          I prioritize tasks based on <strong>business impact, dependencies, and risk level</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Use sprint planning to classify work: P1 (critical production), P2 (pipeline improvement), P3 (tech debt).</li>
          <li> Break large tasks into small deliverables so value is delivered incrementally.</li>
          <li> Keep buffer (~1015%) for urgent incidents or hotfixes.</li>
          <li> Sync daily with dev & QA to adjust priorities dynamically if new blockers appear.</li>
        </ul>
        <p><strong>In Practice:</strong> I follow the WSJF (Weighted Shortest Job First) logic  high-impact & short-duration tasks get picked first to show faster results.</p>
      </div>`},{question:"How do you handle tight deadlines or unplanned releases?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Tight Deadlines</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Identify critical path  whats mandatory vs what can be deferred.</li>
          <li> Increase communication frequency  quick syncs, no long meetings.</li>
          <li> Automate repetitive steps (build/test/deploy) to save manual time.</li>
          <li> Use feature flags or staged rollout if the release is risky.</li>
          <li> Keep all stakeholders informed on ETA and known risks to avoid surprises.</li>
        </ul>
        <p><strong>In Practice:</strong> During a sudden compliance fix, I created a hotfix pipeline using an existing YAML template  it went live in 4 hours without bypassing checks.</p>
      </div>`},{question:"How do you ensure deployment quality under pressure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Ensuring Quality Under Pressure</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Never skip mandatory checks  run smoke & health tests post-deploy.</li>
          <li> Keep rollback scripts & canary deployments pre-configured.</li>
          <li> Deploy smaller changes more frequently instead of one big risky push.</li>
          <li> Use automated gates (like build validation, approvers) even under tight windows.</li>
        </ul>
        <p><strong>In Practice:</strong> I once split a risky production change into 3 smaller patches, validated each with smoke tests, and avoided downtime entirely.</p>
      </div>`},{question:"What actions do you take to maintain stability after a major change?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Maintaining Stability Post-Change</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Monitor key metrics (CPU, latency, errors, queue length) for 2448 hours after deployment.</li>
          <li> Keep rollback plan & one-click restore ready if metrics deviate beyond threshold.</li>
          <li> Run regression tests and keep alerting slightly more aggressive post-deploy.</li>
          <li> Schedule post-change review (mini RCA) to capture learnings.</li>
        </ul>
        <p><strong>In Practice:</strong> After a DB schema update, I enabled query-level insights in App Insights  caught a slow query early before it impacted users.</p>
      </div>`},{question:"How do you handle on-call or escalation situations?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling On-Call Escalations</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Acknowledge within minutes and check dashboards/logs before making any change.</li>
          <li> Apply predefined runbooks (restart service, recycle pod, clear queue)  avoid ad-hoc fixes.</li>
          <li> Update incident ticket regularly with steps & observations  transparency builds trust.</li>
          <li> Once stable, conduct RCA to improve monitoring & automation.</li>
        </ul>
        <p><strong>In Practice:</strong> I keep automated self-heal scripts for common issues (like service restarts or disk cleanup)  reduces manual intervention drastically.</p>
      </div>`},{question:"How do you ensure proper documentation of your work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Documentation Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Maintain up-to-date runbooks, architecture diagrams & SOPs in Confluence.</li>
          <li> Embed documentation links directly in pipelines and alert messages.</li>
          <li> Update docs as part of definition of done  no task is closed without notes.</li>
          <li> Conduct peer-reviews for runbooks to ensure clarity and accuracy.</li>
        </ul>
        <p><strong>In Practice:</strong> Every new pipeline I create has a linked wiki page with setup, variables, and rollback steps  new team members onboard faster.</p>
      </div>`},{question:"Describe a time when you had to learn a new tool quickly to solve a business problem.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Quick Learning Example</strong></h3>
        <p>
          During a production compliance audit, I had to integrate <strong>Azure Sentinel</strong> logs with ServiceNow in under 3 days.  
          I quickly learned Sentinel's Log Analytics & Logic Apps, built a custom playbook to auto-create ServiceNow tickets when security alerts triggered.
        </p>
        <p> Result  incidents got logged automatically and response time reduced by 40%. The team continued using that flow permanently.</p>
        <p><strong>In Practice:</strong> My learning approach is hands-on: read docs  build PoC  test  document  productionize.</p>
      </div>`}]},{title:"7. Security, Compliance & Governance",questions:[{question:"How do you ensure security compliance in your DevOps pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Ensuring Security Compliance in DevOps Pipelines</strong></h3>
        <p>
          Security compliance starts right inside the CI/CD workflow  not after deployment.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Store all secrets in <strong>Azure Key Vault</strong>, never in plain YAML or code.</li>
          <li> Integrate <strong>SonarQube</strong> for code quality, <strong>Checkov</strong> or <strong>TFLint</strong> for IaC policy checks, and <strong>Trivy</strong> for image scans.</li>
          <li> Use pipeline gates to block merges when quality gates or security scans fail.</li>
          <li> Maintain signed build artifacts and enforce <strong>approvals</strong> for production deployments.</li>
          <li> Apply branch protection rules in Git  no direct commits to main/master.</li>
        </ul>
        <p><strong>In Practice:</strong> Every pipeline I build has static analysis, container scanning, and a mandatory approval gate before PROD deployment  ensuring compliance by design.</p>
      </div>`},{question:"How do you handle access control and permissions (RBAC, IAM, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Access Control & Permissions (RBAC/IAM)</strong></h3>
        <p>
          Follow the principle of <strong>least privilege</strong>  only grant whats required.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Use <strong>Azure RBAC</strong> roles like Reader, Contributor, Owner  assigned only at the lowest possible scope (resource-group or specific resource).</li>
          <li> For automation, use <strong>Managed Identities</strong> or short-lived Service Principals instead of shared credentials.</li>
          <li> Periodically review access assignments and remove stale permissions via <strong>Access Reviews</strong>.</li>
          <li> Audit changes using Azure Activity Logs & enable <strong>Privileged Identity Management (PIM)</strong> for just-in-time admin access.</li>
        </ul>
        <p><strong>In Practice:</strong> My pipelines run using managed identities bound to a resource group  no static credentials and full audit visibility.</p>
      </div>`},{question:"Whats your process for managing secrets, credentials, or tokens securely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Secrets & Credentials Securely</strong></h3>
        <p>
          Secrets lifecycle is handled using <strong>Azure Key Vault</strong> + <strong>pipeline integration</strong>.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Store all sensitive data (keys, passwords, tokens) in <strong>Key Vault</strong> with RBAC access policies.</li>
          <li> Rotate secrets automatically using Key Vault rotation policies or Azure Automation runbooks.</li>
          <li> Link Key Vault secrets directly into Azure DevOps Library variables.</li>
          <li> Disable variable logging  use masked variables so secrets never appear in build logs.</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain Key Vault per environment (DEV/QA/PROD) and connect it using service connection  secrets are injected dynamically at runtime only.</p>
      </div>`},{question:"How do you monitor policy compliance in Azure environments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring Policy Compliance</strong></h3>
        <p>
          Azure Policy and Blueprints ensure resources stay within organizational governance.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Use <strong>Azure Policy</strong> to enforce allowed SKUs, regions, and mandatory tags.</li>
          <li> Enable <strong>DeployIfNotExists</strong> rules to auto-remediate missing configs like Diagnostic Logs or Encryption.</li>
          <li> Track compliance score via <strong>Azure Policy dashboard</strong> or export to Log Analytics for reports.</li>
          <li> Configure alerts for non-compliance and integrate them with Logic Apps or Teams channels.</li>
        </ul>
        <p><strong>In Practice:</strong> I have automated remediation for missing diagnostics  if any resource violates policy, the fix runs automatically via a Logic App.</p>
      </div>`},{question:"What are your steps to ensure audit readiness in CI/CD?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Ensuring CI/CD Audit Readiness</strong></h3>
        <p>
          Every change in the pipeline must be traceable, authorized, and reproducible.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Enable <strong>build retention</strong> and store pipeline run logs & artifacts for at least 612 months.</li>
          <li> Maintain approval history  capture who approved deployments, with timestamps.</li>
          <li> Use <strong>ServiceNow Change IDs</strong> or ticket links within pipeline metadata for traceability.</li>
          <li> Keep all IaC and pipeline YAMLs version-controlled in Git for audit review.</li>
          <li> Periodic internal audits ensure that each pipeline aligns with organizational policy (e.g., no direct PROD deploys).</li>
        </ul>
        <p><strong>In Practice:</strong> For healthcare projects, I link every production deployment to a ServiceNow Change ID and retain the logs for 1 year.</p>
      </div>`},{question:"How do you implement change management for sensitive deployments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Change Management for Sensitive Deployments</strong></h3>
        <p>
          Sensitive deployments go through strict control  change tickets, approvals, and rollback readiness.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> All major releases are tied to <strong>ServiceNow change records</strong> with CAB approvals.</li>
          <li> Define <strong>pre-deploy checklist</strong>: backup confirmation, DR readiness, approval status, and rollback tested.</li>
          <li> Pipeline integration ensures deploy job only triggers if change ID is Approved.</li>
          <li> Communicate change window, risk, and rollback steps to all stakeholders before the window starts.</li>
          <li> After deployment, update the change record with outcome & attach logs for audit trail.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive integrated Azure Pipelines with ServiceNow  the deploy stage automatically checks for approved Change ID before executing any PROD step.</p>
      </div>`}]},{title:"8. Behavioral & Soft Skills",questions:[{question:"Tell me about a time you handled pressure calmly.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Pressure Calmly</strong></h3>
        <p>
          During a critical production outage, an app went down during peak hours. Instead of rushing, I followed our incident process  identified that the backend API was hitting a quota limit.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> I quickly enabled autoscaling and temporarily increased limits to restore service.</li>
          <li> After stabilization, performed RCA, created a new alert rule and documented preventive action.</li>
          <li> Stayed calm, communicated clearly to the team, and kept stakeholders updated every 10 minutes.</li>
        </ul>
        <p><strong>In Practice:</strong> I believe calm communication during pressure reduces mistakes and keeps the team aligned.</p>
      </div>`},{question:"How do you balance multiple priorities?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Balancing Multiple Priorities</strong></h3>
        <p>
          I manage priorities using clear visibility and impact-based planning.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Maintain a daily task board  separate P1s (urgent) from P2s (important but not urgent).</li>
          <li> Communicate early if new high-priority work appears  re-align tasks transparently with my lead.</li>
          <li> Use short bursts of focus (Pomodoro) for high-cognitive work and automate repetitive tasks to save time.</li>
        </ul>
        <p><strong>In Practice:</strong> I never multitask on critical items; instead, I schedule them and execute one at a time with full focus.</p>
      </div>`},{question:"Describe a time when you disagreed with your manager  how did you handle it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Disagreement Handled Professionally</strong></h3>
        <p>
          Once my manager wanted to skip automated security scans to meet a deployment deadline.  
          I respectfully disagreed because it could expose compliance risk.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> I presented scan history data showing 2 prior issues caught only by automation.</li>
          <li> Suggested a middle path  parallel scan with staged release instead of full skip.</li>
          <li> The compromise worked  no delay, and the pipeline remained compliant.</li>
        </ul>
        <p><strong>In Practice:</strong> I believe respectful disagreement backed by data leads to trust and better outcomes.</p>
      </div>`},{question:"What motivates you the most at work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Motivation at Work</strong></h3>
        <p>
          My motivation comes from <strong>impact and learning</strong>  seeing automation make real business difference.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Building something that saves time or reduces errors keeps me driven.</li>
          <li> Continuous learning  mastering new tools like Terraform, Checkov, or Sentinel excites me.</li>
          <li> Appreciating collaboration  when a team ships a release smoothly after a long sprint, its the best feeling.</li>
        </ul>
        <p><strong>In Practice:</strong> I stay motivated by tracking small wins  every automation I build adds value and satisfaction.</p>
      </div>`},{question:"How do you maintain focus and quality during repetitive work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Maintaining Focus in Repetitive Work</strong></h3>
        <p>
          Repetitive work is an opportunity to automate and improve. I ensure quality through consistency and systemization.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Automate wherever possible  scripts, templates, and standard tasks.</li>
          <li> Use checklists for repeat tasks to avoid human error.</li>
          <li> Take small breaks and track work visually  keeps mind fresh and focused.</li>
        </ul>
        <p><strong>In Practice:</strong> I automated 70% of a manual VM patching task; it turned a boring routine into a smooth monitored job.</p>
      </div>`},{question:"Tell me about a time you collaborated remotely or across time zones.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Remote Collaboration Across Time Zones</strong></h3>
        <p>
          In my current project, I work with teams in India and Europe. We follow a <strong>follow-the-sun</strong> model.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Maintain a shared task tracker (Azure Boards) and daily async updates to bridge time difference.</li>
          <li> Use Teams for structured handoffs  each region leaves end-of-day notes for the next shift.</li>
          <li> Record meetings or use summary notes so no one misses context.</li>
        </ul>
        <p><strong>In Practice:</strong> Clear documentation and async updates ensured 247 progress without waiting on others  especially during releases.</p>
      </div>`}]},{title:"9. HR & Career-Related Questions",questions:[{question:"Why do you want to switch from your current organization?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reason for Switching</strong></h3>
        <p>
          Ive learned a lot in my current role, especially in infrastructure automation and CI/CD pipeline management.  
          However, Im now looking for an opportunity where I can work on <strong>more complex DevOps environments</strong>  including advanced Azure services, Kubernetes, and security automation.
        </p>
        <p><strong>In short:</strong> My goal is to take on larger-scale challenges and grow technically, rather than just stay within maintenance tasks.</p>
      </div>`},{question:"What are your career goals for the next 23 years?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Career Goals</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Build expertise in <strong>Cloud DevOps & Infrastructure as Code</strong> with tools like Terraform, Helm, and Azure DevOps.</li>
          <li> Get certified in <strong>Azure DevOps Engineer Expert</strong> and <strong>CKA (Kubernetes Admin)</strong>.</li>
          <li> Contribute to building end-to-end CI/CD ecosystems and lead a small automation team.</li>
        </ul>
        <p><strong>In Practice:</strong> I want to grow from an executor to a solution designer who can own the full DevOps delivery pipeline.</p>
      </div>`},{question:"What kind of company culture do you prefer?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Preferred Company Culture</strong></h3>
        <p>
          I value a culture that encourages <strong>learning, ownership, and collaboration</strong>.  
          I like teams where communication is open, mistakes are treated as learning, and automation & process improvement are appreciated.
        </p>
        <p><strong>In short:</strong> I enjoy working in a transparent, DevOps-driven culture where innovation and continuous improvement are part of daily work.</p>
      </div>`},{question:"What are your salary expectations?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Salary Expectation</strong></h3>
        <p>
          Based on my experience in Azure, Terraform, and DevOps pipelines, Im looking for a <strong>competitive salary aligned with market standards</strong> and the responsibilities of the role.
        </p>
        <p>Im flexible as long as the overall opportunity supports my technical growth and career progression.</p>
      </div>`},{question:"Are you open to relocation or hybrid work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Relocation & Work Mode</strong></h3>
        <p>
          Yes, Im open to both relocation and hybrid work models if the project requires physical collaboration.  
          I believe face-to-face interaction helps during major releases and cross-team coordination.
        </p>
        <p>For regular operations, Im equally comfortable working remotely with clear communication and process tracking.</p>
      </div>`},{question:"How do you evaluate job satisfaction beyond salary?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Evaluating Job Satisfaction</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Learning and growth opportunities  chance to work on new tools and architectures.</li>
          <li> Healthy work culture  where feedback and recognition exist.</li>
          <li> Technical ownership  freedom to suggest and implement automation improvements.</li>
          <li> Clear goals and appreciation for impactful work.</li>
        </ul>
        <p><strong>In Practice:</strong> For me, the right work culture and learning curve matter more than just the compensation.</p>
      </div>`},{question:"Why should we hire you instead of other candidates?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Why Me?</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>  Hands-on experience in <strong>Azure, Terraform, YAML Pipelines, and Docker/Kubernetes</strong>.</li>
          <li> Strong CI/CD mindset  I build reusable, secure, and auditable pipelines.</li>
          <li> Analytical thinking  I focus on root-cause and optimization, not just quick fixes.</li>
          <li> Collaborative approach  I work seamlessly with dev, QA, and infra teams under tight deadlines.</li>
        </ul>
        <p><strong>In Practice:</strong> I can contribute from day one  not just in execution but also in improving pipeline efficiency, automation, and monitoring practices.</p>
      </div>`},{question:"Are you currently working somewhere?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Current Employment</strong></h3>
        <p>
          Yes, Im currently working at <strong>Litmus Information Systems LLP</strong> as a DevOps Engineer.  
          My responsibilities include VM management, patching, CI/CD pipeline automation, and infrastructure provisioning using Terraform and Azure.
        </p>
        <p><strong>In Practice:</strong> Ive been managing production-grade pipelines and ensuring stability and compliance across multiple environments.</p>
      </div>`},{question:"What was the reason for leaving your last organization?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reason for Leaving Previous Organization</strong></h3>
        <p>
          I left my previous organization to explore more modern DevOps practices.  
          My earlier role was stable but limited in terms of automation, containerization, and cloud-native exposure.
        </p>
        <p><strong>In Practice:</strong> I wanted to move from traditional system administration to full DevOps lifecycle work  which Im now doing and want to take further.</p>
      </div>`},{question:"Do you have any questions for us?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>You can ask following questions to the Interviewer</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> How mature is your current DevOps setup  are you using YAML pipelines or classic?</li>
          <li> What cloud automation tools are primarily used (Terraform, Bicep, ARM)?</li>
          <li> What kind of team structure and collaboration model do you follow for releases?</li>
          <li> How does the organization measure success in DevOps initiatives?</li>
        </ul>
        <p><strong>In Practice:</strong> Asking these shows curiosity and practical interest  not just in the role, but in improving their environment.</p>
      </div>`}]},{title:"10. Scenario-Based Problem Solving",questions:[{question:"A developer accidentally deleted a resource from production  how would you fix and prevent this permanently?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recover & Prevent Accidental Deletion</strong></h3>
        <p>
          Immediate recovery + long-term prevention are both required. Act fast to restore service, then close the process gaps.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Immediate steps:</strong>
            <ul>
              <li>1 Identify scope using <code>Azure Activity Logs</code> (who/what/time).</li>
              <li>2 Check for soft-delete / recovery point (Storage soft-delete, Key Vault soft-delete, Recovery Services Vault snapshots).</li>
              <li>3 If backup exists -> restore from snapshot/backup. If not, attempt <code>terraform import</code> to re-adopt resource or recreate from IaC (apply last known good plan).</li>
              <li>4 Communicate clearly to stakeholders: impact, ETA, and mitigation steps (open incident/ServiceNow ticket).</li>
            </ul>
          </li>
          <li><strong>Post-incident prevention:</strong>
            <ul>
              <li> Apply <strong>resource locks</strong> (CanNotDelete / ReadOnly) to critical resources.</li>
              <li> Enforce <strong>least-privilege RBAC</strong> and use <strong>PIM</strong> for elevated access.</li>
              <li> Keep all infra in <strong>Terraform</strong> (or IaC) and require plan approval before apply  block manual production changes.</li>
              <li> Add pre-apply validation: require ServiceNow Change ID and manual approver for destructive changes.</li>
              <li> Run a short training and publish a checklist on safe production operations.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> Restore from Recovery Vault if available; otherwise re-create using IaC and then lock the resource and tighten RBAC so it cannot reoccur.</p>
      </div>`},{question:"You found an API failing due to a WAF rule  how do you approach the fix?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Fixing WAF-Related API Failures</strong></h3>
        <p>
          Diagnose first, then implement the minimum-risk corrective action while maintaining security posture.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Investigate:</strong>
            <ul>
              <li>Query WAF logs in Log Analytics: <code>AzureDiagnostics | where Category == "ApplicationGatewayFirewallLog"</code> to get ruleId and sample requests.</li>
              <li>Reproduce the request (safe test) to confirm false positive vs real attack.</li>
            </ul>
          </li>
          <li><strong>Mitigate:</strong>
            <ul>
              <li>Temporarily change rule action to <code>Log</code> (not Disabled) or add a scoped exclusion for the specific rule/URI/client IP after risk assessment.</li>
              <li>Create a custom rule or request exception scoped to known good traffic rather than turning off the rule globally.</li>
            </ul>
          </li>
          <li><strong>Harden & Monitor:</strong>
            <ul>
              <li>Tune WAF rules, add positive security rules for known safe paths, and monitor blocked request trends.</li>
              <li>Document the change, add a regression test, and add an alert for repeated false positives.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I log and analyze offending requests, add precise exclusions or custom rules, then monitor for side effects  always preferring narrow changes over global disables.</p>
      </div>`},{question:"You deployed a new version and noticed performance degradation  what steps do you take?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Performance Regression Handling</strong></h3>
        <p>
          Quick containment, data-driven diagnosis, and safe rollback or fix-and-verify workflow.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Contain:</strong>
            <ul>
              <li>If impact is severe, immediately revert to last known good release (automatic rollback job or manual redeploy of previous artifact).</li>
              <li>Enable additional instances / scale-out temporarily if resource-constrained.</li>
            </ul>
          </li>
          <li><strong>Diagnose:</strong>
            <ul>
              <li>Compare metrics (App Insights traces, request rate, p95 latency, DB query times) between versions.</li>
              <li>Pinpoint slow components using distributed tracing and flamegraphs; inspect recent code changes, third-party library upgrades, and config changes.</li>
            </ul>
          </li>
          <li><strong>Fix & Verify:</strong>
            <ul>
              <li>Deploy targeted fixes to a canary group; run synthetic tests and load tests to validate improvement.</li>
              <li>Promote to full traffic once metrics stabilize.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I use canary  monitor  promote flow. If rollback was performed, I run postmortem and create a checklist to catch similar regressions earlier (pre-merge perf smoke).</p>
      </div>`},{question:"How would you plan a zero-downtime deployment for a production application?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Zero-Downtime Deployment Plan</strong></h3>
        <p>
          Use traffic-routing strategies, compatibility patterns, and automated validation to avoid service disruption.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Architecture & Compatibility:</strong> Ensure backward-compatible API changes or use feature flags for new behavior.</li>
          <li><strong>Deployment Strategy:</strong>
            <ul>
              <li>Blue-Green or Canary with weighted routing (Front Door, Traffic Manager, App Gateway, or service mesh).</li>
              <li>For DB schema changes, use backward-compatible migrations (expand-contract pattern) and run migrations in two phases.</li>
            </ul>
          </li>
          <li><strong>Validation & Safety:</strong>
            <ul>
              <li>Run automated smoke tests and synthetic transactions after each partial rollout.</li>
              <li>Use health gates and automatic rollback triggers based on error rate / latency thresholds.</li>
            </ul>
          </li>
          <li><strong>Communication & Rollback:</strong>
            <ul>
              <li>Open a deployment war-room, have rollback steps ready, and keep stakeholders informed.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I prefer canary with auto-promote on health metrics. For DB changes, I stage migration and switch flags after verification  ensuring zero downtime for users.</p>
      </div>`},{question:"You need to migrate VMs to the latest SKU due to Azure retirement  whats your plan?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VM SKU Migration Plan</strong></h3>
        <p>
          Plan, test, automate, and execute staged migrations with rollback and verification steps.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Assessment:</strong>
            <ul>
              <li>Inventory VMs, dependencies, OS compatibility, drivers, and performance profiles.</li>
              <li>Identify critical apps requiring high-availability handling or special drivers.</li>
            </ul>
          </li>
          <li><strong>Proof-of-Concept:</strong>
            <ul>
              <li>Create test VMs with new SKU and run smoke and performance tests (boot, disk I/O, app tests).</li>
            </ul>
          </li>
          <li><strong>Automation & Execution:</strong>
            <ul>
              <li>Prefer replacement strategy: provision new VM with new SKU, install app/config using scripts or image, join to load balancer, drain old VM, and decommission.</li>
              <li>For stateful VMs, snapshot OS/data disks and attach to new VM if needed (test carefully).</li>
            </ul>
          </li>
          <li><strong>Cutover & Validation:</strong>
            <ul>
              <li>Use staged rollouts (canary VMs) and monitor for 2448 hours, then proceed in waves.</li>
              <li>Automate the process in pipelines to ensure repeatability and logs for audit.</li>
            </ul>
          </li>
          <li><strong>Communication:</strong> Schedule maintenance windows for stateful migrations and notify stakeholders.</li>
        </ul>
        <p><strong>In Practice:</strong> I avoid in-place resize for critical servers; instead spin up new instances (immutable approach) to test and cut over safely.</p>
      </div>`},{question:"A pipeline is failing due to a service connection issue  how would you debug it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Debugging Service Connection Failures</strong></h3>
        <p>
          Service connection failures are usually auth or permission related  validate credentials, scope, and token state.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Check Logs:</strong> Inspect pipeline logs for HTTP codes (401/403) and error messages.</li>
          <li><strong>Validate Credentials:</strong>
            <ul>
              <li>Confirm service principal / PAT / certificate not expired; rotate if expired.</li>
              <li>Test connection locally via <code>az login --service-principal -u &lt;id&gt; -p &lt;secret&gt; --tenant &lt;tid&gt;</code>.</li>
            </ul>
          </li>
          <li><strong>Verify Permissions & Scope:</strong> Ensure the service principal has required RBAC roles (Contributor/Custom) at the correct scope (subscription/resource group).</li>
          <li><strong>Agent Environment:</strong> Ensure agent can reach the resource (network/NSG, firewall) and environment variables (tenant id, client id) are set correctly.</li>
          <li><strong>Fix & Test:</strong> Update service connection, re-run pipeline, and monitor success. Add synthetic test step in pipeline to validate auth early in job.</li>
        </ul>
        <p><strong>In Practice:</strong> I add a small auth-check job at the start of sensitive pipelines that fails fast with a clear error if the service connection is broken  saves wasted build minutes.</p>
      </div>`},{question:"Your manager asks to reduce monthly Azure cost by 20%  how do you proceed?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cost Reduction Plan (20%)</strong></h3>
        <p>
          Combine quick wins with medium-term architectural changes: measure first, then optimize.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Discovery:</strong>
            <ul>
              <li>Run Cost Analysis to identify top spenders (VMs, storage, networking, SQL).</li>
              <li>Group costs by project/team for accountability.</li>
            </ul>
          </li>
          <li><strong>Quick Wins (030 days):</strong>
            <ul>
              <li>Enable auto-shutdown for non-prod VMs and enforce schedules.</li>
              <li>Delete unattached disks and old snapshots; enable blob lifecycle policies.</li>
              <li>Move infrequently accessed data to Cool/Archive tiers.</li>
            </ul>
          </li>
          <li><strong>Mid-term (3090 days):</strong>
            <ul>
              <li>Right-size VMs and move steady workloads to Reserved Instances / Savings Plans.</li>
              <li>Use Spot instances for ephemeral batch jobs.</li>
              <li>Consolidate underutilized databases or move to serverless offerings where applicable.</li>
            </ul>
          </li>
          <li><strong>Governance:</strong>
            <ul>
              <li>Set budgets and alerts, enable cost tagging, and implement chargeback/showback per team.</li>
              <li>Automate reports and require justification for new high-cost resources.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> Quick wins often deliver 812% savings. Combined with RIs and right-sizing, 20% is achievable within 13 months while keeping performance SLAs intact.</p>
      </div>`}]}];function $w(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("behavioral"),a=pm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-rose-500 to-pink-500 shadow-glow",children:l.jsx(Av,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Behavioral & HR Questions"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Project Discussion, Scenario-Based & Career Questions"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Prepare for behavioral interviews, HR rounds, and scenario-based discussions."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:pm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:(u==null?void 0:u.question)||"",b=typeof u=="object"&&u&&"answer"in u&&!!u.answer,v=typeof u=="object"&&u&&"answerHtml"in u&&!!u.answerHtml,h=b?u.answer:null,w=b||v;return l.jsx("div",{className:"border-l-2 border-rose-500/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-rose-500 font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),w&&l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:u.answerHtml}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:h})})]})]})]})},f)})})})]},p))})]})]})}const J3=Object.freeze(Object.defineProperty({__proto__:null,default:$w},Symbol.toStringTag,{value:"Module"})),gm=[{title:"1. Docker Fundamentals",questions:[{question:"What is Docker and what problem does it solve?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Overview & the Problem It Solves</strong></h3>
        <p>
          Docker is a <strong>containerization platform</strong> that packages an application with its dependencies into a lightweight, isolated unit called a <em>container</em>.  
          It ensures the app runs the same way on every environment  dev, test, or production.
        </p>
        <h4> Problem Solved:</h4>
        <ul style="margin-left:1.2rem;">
          <li><em>It works on my machine</em> issue  eliminated because containers carry environment with them.</li>
          <li>Removes dependency conflicts and setup mismatches.</li>
          <li>Enables quick deployment and scaling with minimal resource use.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Docker to package APIs and microservices so developers, testers, and CI/CD pipelines run consistent builds across machines.</p>
      </div>`},{question:"What is the difference between a container and a virtual machine?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container vs Virtual Machine</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Feature</th><th>Container</th><th>Virtual Machine</th></tr>
          <tr><td>Architecture</td><td>Shares host OS kernel</td><td>Has its own OS via hypervisor</td></tr>
          <tr><td>Size</td><td>Lightweight (MBs)</td><td>Heavy (GBs)</td></tr>
          <tr><td>Startup Time</td><td>Seconds</td><td>Minutes</td></tr>
          <tr><td>Isolation</td><td>Process-level isolation</td><td>Full hardware-level isolation</td></tr>
          <tr><td>Use Case</td><td>Microservices, CI/CD, rapid scaling</td><td>Legacy workloads, OS diversity</td></tr>
        </table>
        <p><strong>In Practice:</strong> I prefer containers for microservices and pipelines, but VMs for heavy monolithic or mixed-OS workloads.</p>
      </div>`},{question:"Why do we use Docker in DevOps and CI/CD?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker in DevOps & CI/CD Pipelines</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Ensures consistent runtime environments across build, test, and deploy stages.</li>
          <li>Allows faster builds  containers start in seconds.</li>
          <li>Integrates easily with Jenkins, Azure DevOps, GitHub Actions for automated pipelines.</li>
          <li>Supports parallel testing with isolated containerized environments.</li>
          <li>Versioned Docker images simplify rollback and disaster recovery.</li>
        </ul>
        <p><strong>Example:</strong> I build Docker images in Azure DevOps pipelines and push them to ACR, which deploy directly to AKS clusters.</p>
      </div>`},{question:"What is Docker architecture  explain Docker daemon, CLI, and registry.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Architecture Components</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Docker Daemon (dockerd):</strong> The background service that manages containers, images, networks, and volumes.</li>
          <li> <strong>Docker CLI:</strong> Command-line interface to interact with the daemon using commands like <code>docker run</code>, <code>docker build</code>.</li>
          <li> <strong>Docker Registry:</strong> Stores Docker images  public (Docker Hub) or private (Azure Container Registry).</li>
        </ul>
        <p><strong>Flow:</strong> CLI  REST API  Daemon  Build/Run containers  Pull/Push to Registry.</p>
        <p><strong>In Practice:</strong> I run private ACR registries for enterprise builds, ensuring image security and compliance.</p>
      </div>`},{question:"What are images, containers, and layers in Docker?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Images, Containers & Layers</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Image:</strong> A read-only template built from a Dockerfile containing app code + dependencies.</li>
          <li> <strong>Container:</strong> A running instance of an image  isolated and ephemeral.</li>
          <li> <strong>Layers:</strong> Each Dockerfile instruction adds a new layer (cached for reusability).</li>
        </ul>
        <p><strong>Example:</strong> <em>Ubuntu  Python  Dependencies  App Code</em>  combined layers form a reusable image.</p>
        <p><strong>In Practice:</strong> Layer caching speeds up CI/CD builds by reusing unchanged dependencies.</p>
      </div>`},{question:"How is Docker different from traditional virtualization (Hyper-V, VMware)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker vs Traditional Virtualization</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Docker</th><th>VM (Hyper-V / VMware)</th></tr>
          <tr><td>Virtualization Level</td><td>OS-level (shares kernel)</td><td>Hardware-level (each VM has OS)</td></tr>
          <tr><td>Startup Time</td><td>Seconds</td><td>Minutes</td></tr>
          <tr><td>Performance</td><td>Lightweight, less overhead</td><td>Heavier due to full OS virtualization</td></tr>
          <tr><td>Resource Use</td><td>Efficient (many containers per host)</td><td>Limited (few VMs per host)</td></tr>
          <tr><td>Best For</td><td>Microservices & CI/CD</td><td>Monolithic apps & OS isolation</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use Docker for microservice workloads and VM-based environments only where full isolation or custom OS control is required.</p>
      </div>`}]},{title:"2. Docker  Hands-on & Advanced",questions:[{question:"Do you know Docker?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Yes, Ive hands-on experience with Docker</strong></h3>
        <p>
          I actively use Docker for containerizing microservices, building custom images, and deploying applications in CI/CD pipelines.  
          My typical workflow includes writing <code>Dockerfile</code>, using <code>docker-compose</code> for local orchestration,  
          and pushing built images to Azure Container Registry (ACR) for deployment to AKS or App Service.
        </p>
      </div>`},{question:"What is Docker and what problem does it solve?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Overview</strong></h3>
        <p>
          Docker is a containerization tool that packages an application and its dependencies into a single portable unit called a container.  
          It eliminates environment inconsistency issues between developer and production systems.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Ensures consistent environment across dev  QA  prod.</li>
          <li> Lightweight, fast startup  no full OS boot.</li>
          <li> Easy scaling and rollback using versioned images.</li>
        </ul>
        <p><strong>In Practice:</strong> I containerize APIs and push images to ACR  making deployments predictable and repeatable.</p>
      </div>`},{question:"What is the difference between a container and a virtual machine?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container vs Virtual Machine</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Feature</th><th>Container</th><th>Virtual Machine</th></tr>
          <tr><td>Isolation</td><td>Shares host OS kernel</td><td>Full OS isolation</td></tr>
          <tr><td>Startup</td><td>Seconds</td><td>Minutes</td></tr>
          <tr><td>Size</td><td>Lightweight (MBs)</td><td>Heavy (GBs)</td></tr>
          <tr><td>Use Case</td><td>Microservices, CI/CD</td><td>Legacy workloads, full OS control</td></tr>
        </table>
      </div>`},{question:"What is a Docker volume?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Volumes</strong></h3>
        <p>Volumes are used to persist data outside a containers writable layer, so data isnt lost when a container stops or restarts.</p>
        <p><strong>Example:</strong></p>
        <pre><code>docker volume create mydata
docker run -d -v mydata:/var/lib/mysql mysql</code></pre>
        <p><strong>In Practice:</strong> I use named volumes for DB persistence and shared mounts in multi-container setups (e.g., app + DB).</p>
      </div>`},{question:"Where do we store Docker images?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Image Storage</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Public Registry:</strong> Docker Hub.</li>
          <li> <strong>Private Cloud Registries:</strong> Azure Container Registry (ACR), AWS ECR, or GCP Artifact Registry.</li>
          <li> <strong>Local Registry:</strong> Self-hosted registry for on-prem builds.</li>
        </ul>
        <p><strong>In Practice:</strong> I push CI/CD build images to ACR using service connections in Azure DevOps pipelines.</p>
      </div>`},{question:"Write a Docker command to create a container from an image.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Run Command</strong></h3>
        <pre><code>docker run -d --name myapp -p 8080:80 nginx</code></pre>
        <p> Creates and runs a container named <code>myapp</code> from the <code>nginx</code> image, mapping port 8080 on host to 80 inside container.</p>
      </div>`},{question:"Write a Dockerfile to create a custom image.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sample Dockerfile</strong></h3>
        <pre><code># Base Image
FROM python:3.9

# Set working directory
WORKDIR /app

# Copy app files
COPY . .

# Install dependencies
RUN pip install -r requirements.txt

# Expose port
EXPOSE 5000

# Run the app
CMD ["python", "app.py"]</code></pre>
        <p><strong>In Practice:</strong> I use multi-stage builds to reduce image size and improve CI/CD performance.</p>
      </div>`},{question:"What is the difference between ADD and COPY in Dockerfile?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ADD vs COPY</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Command</th><th>Purpose</th><th>Special Features</th></tr>
          <tr><td><code>COPY</code></td><td>Copies files from host to container</td><td>Simple and preferred for static files</td></tr>
          <tr><td><code>ADD</code></td><td>Copies and auto-extracts compressed files (.tar, .gz)</td><td>Can also fetch URLs (less secure)</td></tr>
        </table>
        <p><strong>Best Practice:</strong> Use <code>COPY</code> unless auto-extraction or URL download is required.</p>
      </div>`},{question:"What is the difference between CMD and ENTRYPOINT?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CMD vs ENTRYPOINT</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>CMD</th><th>ENTRYPOINT</th></tr>
          <tr><td>Purpose</td><td>Provides default command</td><td>Defines main executable</td></tr>
          <tr><td>Override</td><td>Can be easily overridden</td><td>Arguments are appended</td></tr>
          <tr><td>Example</td><td><code>CMD ["npm", "start"]</code></td><td><code>ENTRYPOINT ["python"]</code></td></tr>
        </table>
        <p><strong>Best Practice:</strong> Use ENTRYPOINT for mandatory commands and CMD for default arguments.</p>
      </div>`},{question:"What is ARG in Docker and how is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ARG (Build-time Variables)</strong></h3>
        <p><code>ARG</code> defines variables that are used only during image build (not at runtime).</p>
        <pre><code>ARG APP_VERSION=1.0
RUN echo "Building version &dollar;APP_VERSION"</code></pre>
        <p><strong>In Practice:</strong> I pass ARGs like app version or environment during CI/CD builds for dynamic tagging.</p>
      </div>`},{question:"Can we make changes to a running Docker container?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Editing Running Containers</strong></h3>
        <p>Yes, you can <strong>exec</strong> into a running container and make changes, but theyre ephemeral.</p>
        <pre><code>docker exec -it myapp /bin/bash</code></pre>
        <p><strong>Note:</strong> Changes are lost after container restart unless you commit or mount persistent storage.</p>
      </div>`},{question:"Command to run an already created container.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Run Existing Container</strong></h3>
        <pre><code>docker start -a myapp</code></pre>
        <p><code>-a</code> attaches logs to the terminal.  
        Alternatively: <code>docker restart myapp</code> if it was previously stopped.</p>
      </div>`},{question:"Different networking types in Docker.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Network Types</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>bridge</strong>  Default; containers communicate via virtual bridge.</li>
          <li><strong>host</strong>  Shares host network; no isolation.</li>
          <li><strong>none</strong>  No network access.</li>
          <li><strong>overlay</strong>  Multi-host networking for Swarm or Kubernetes.</li>
        </ul>
        <p><strong>In Practice:</strong> I use custom bridge networks for microservice communication and overlay for multi-node clusters.</p>
      </div>`},{question:"What is Docker Swarm?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Swarm</strong></h3>
        <p>
          Swarm is Dockers native clustering and orchestration tool for running containers across multiple hosts.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Supports load balancing, service scaling, and rolling updates.</li>
          <li> Works via managers and workers (leader-based architecture).</li>
        </ul>
        <p><strong>In Practice:</strong> Although Swarm is simpler, I mostly use Kubernetes for orchestration in enterprise environments.</p>
      </div>`},{question:"What is multi-stage build in Docker?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-Stage Build</strong></h3>
        <p>Used to reduce image size by separating build and runtime stages.</p>
        <pre><code>FROM node:16 AS builder
WORKDIR /app
COPY . .
RUN npm install && npm run build

FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html</code></pre>
        <p><strong>In Practice:</strong> My production images are multi-stage  they keep only compiled binaries, no build tools.</p>
      </div>`},{question:"How do you clean up unused images and containers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Cleanup Commands</strong></h3>
        <pre><code>docker system prune -a -f</code></pre>
        <p>Removes stopped containers, unused images, and dangling networks to free space.</p>
      </div>`},{question:"What is the purpose of Docker Compose?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Compose</strong></h3>
        <p>Used to define and manage multi-container applications via a single YAML file.</p>
        <pre><code>version: "3"
services:
  web:
    image: nginx
  db:
    image: mysql
    environment:
      MYSQL_ROOT_PASSWORD: pass123</code></pre>
        <p><strong>In Practice:</strong> I use Compose for local testing of full stacks (app + DB + cache) before pushing to Kubernetes.</p>
      </div>`},{question:"What are the advantages of Docker over VMs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Advantages</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> Faster startup and smaller footprint.</li>
          <li> Efficient resource usage  multiple containers per host.</li>
          <li> Easier CI/CD automation and rollback.</li>
          <li> Portability  runs anywhere with Docker runtime.</li>
        </ul>
        <p><strong>In Practice:</strong> We replaced several dev VMs with Docker containers, cutting environment setup time by 80%.</p>
      </div>`},{question:"What is the docker create command used for?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>docker create Command</strong></h3>
        <p>Creates a container from an image but does not start it immediately.</p>
        <pre><code>docker create --name myapp nginx</code></pre>
        <p><strong>In Practice:</strong> I use this when I need to attach volumes or networks before running the container.</p>
      </div>`},{question:"What is docker push and docker pull?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>docker push / pull</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>docker push:</strong> Uploads an image from local system to a remote registry.</li>
          <li><strong>docker pull:</strong> Downloads an image from registry to local host.</li>
        </ul>
        <pre><code>docker push myacr.azurecr.io/app:v1
docker pull nginx:latest</code></pre>
      </div>`}]},{title:"3. Docker  Containers & Image Management",questions:[{question:"What is Docker?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Overview</strong></h3>
        <p>
          Docker is a containerization tool that packages applications with all dependencies into isolated containers.  
          It provides consistency across environments  whether its a developer laptop or a production server.
        </p>
        <p><strong>In Practice:</strong> I use Docker to build and deploy microservices, ensuring environment consistency and faster rollouts in CI/CD pipelines.</p>
      </div>`},{question:"Why do we use Docker in DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker in DevOps</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Ensures identical environments from development to production.</li>
          <li>Speeds up build and deployment cycles.</li>
          <li>Integrates easily with CI/CD tools like Jenkins, Azure DevOps, and GitHub Actions.</li>
          <li>Supports microservices-based architecture and horizontal scaling.</li>
        </ul>
        <p><strong>Example:</strong> We use Docker in Azure Pipelines to containerize APIs, push images to ACR, and deploy to AKS clusters automatically.</p>
      </div>`},{question:"What problem does Docker solve compared to VMs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Problems Docker Solves vs VMs</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Removes heavy OS overhead  containers share the host kernel.</li>
          <li>Faster startup  seconds vs minutes in VMs.</li>
          <li>Enables high density  100s of containers per host.</li>
          <li>Eliminates works on my machine issue by packaging dependencies inside the image.</li>
        </ul>
      </div>`},{question:"What is the difference between a container and a virtual machine?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container vs Virtual Machine</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Feature</th><th>Container</th><th>Virtual Machine</th></tr>
          <tr><td>Virtualization Level</td><td>OS-level (shared kernel)</td><td>Hardware-level (separate OS)</td></tr>
          <tr><td>Startup Time</td><td>Seconds</td><td>Minutes</td></tr>
          <tr><td>Size</td><td>Lightweight (MBs)</td><td>Heavy (GBs)</td></tr>
          <tr><td>Isolation</td><td>Process-level</td><td>Full OS isolation</td></tr>
        </table>
      </div>`},{question:"What is a Docker image and where are images stored?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Image & Storage</strong></h3>
        <p>
          A Docker image is a read-only template containing the app code and dependencies.  
          Images are stored locally (in Docker cache) or in remote registries like DockerHub or ACR.
        </p>
        <p><strong>Example:</strong> I push my images to <code>myapp.azurecr.io</code> for deployment across environments.</p>
      </div>`},{question:"What is a Docker container?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Container</strong></h3>
        <p>
          A container is a running instance of a Docker image  its lightweight, isolated, and ephemeral.  
          It runs as a process on the host machine sharing the OS kernel.
        </p>
      </div>`},{question:"What is the difference between Dockerfile and Docker Compose?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Dockerfile vs Docker Compose</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Dockerfile:</strong> Defines steps to build a single image.</li>
          <li><strong>Docker Compose:</strong> Manages multi-container apps using YAML configuration.</li>
        </ul>
        <p><strong>Example:</strong> I use Dockerfile for API build and Compose to run API + DB + Redis together locally.</p>
      </div>`},{question:"Explain Dockerfile structure.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Dockerfile Structure</strong></h3>
        <pre><code>FROM node:18
WORKDIR /app
COPY . .
RUN npm install
EXPOSE 3000
CMD ["npm", "start"]</code></pre>
        <ul>
          <li><strong>FROM:</strong> Base image</li>
          <li><strong>WORKDIR:</strong> Sets working directory</li>
          <li><strong>COPY / RUN:</strong> Copies files and installs dependencies</li>
          <li><strong>CMD:</strong> Defines default process to run</li>
        </ul>
      </div>`},{question:"What is the difference between ADD and COPY commands?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ADD vs COPY</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Command</th><th>Function</th><th>Special Behavior</th></tr>
          <tr><td>COPY</td><td>Copies files/directories from host to image</td><td>Simple, preferred method</td></tr>
          <tr><td>ADD</td><td>Same as COPY + auto-extracts archives + fetches URLs</td><td>Useful but less secure</td></tr>
        </table>
      </div>`},{question:"What is the difference between CMD and ENTRYPOINT?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CMD vs ENTRYPOINT</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>CMD</th><th>ENTRYPOINT</th></tr>
          <tr><td>Purpose</td><td>Provides default command</td><td>Defines main executable</td></tr>
          <tr><td>Override</td><td>Can be overridden at runtime</td><td>Arguments appended to ENTRYPOINT</td></tr>
        </table>
        <p><strong>Best Practice:</strong> Use ENTRYPOINT for main command, CMD for default args.</p>
      </div>`},{question:"What is a multi-stage Dockerfile and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-stage Builds</strong></h3>
        <p>Used to reduce final image size by separating build and runtime stages.</p>
        <pre><code>FROM golang:1.19 AS builder
WORKDIR /src
COPY . .
RUN go build -o app

FROM alpine
COPY --from=builder /src/app /app
ENTRYPOINT ["/app"]</code></pre>
      </div>`},{question:"What are the different networking modes in Docker?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Network Modes</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>bridge</strong>  Default, private internal network.</li>
          <li><strong>host</strong>  Shares hosts network stack.</li>
          <li><strong>none</strong>  No networking.</li>
          <li><strong>overlay</strong>  For multi-host (Swarm/K8s) networks.</li>
        </ul>
      </div>`},{question:"What is a volume in Docker and why is it important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Volumes</strong></h3>
        <p>Volumes store data outside the containers writable layer  ensuring persistence across restarts.</p>
        <pre><code>docker run -v mydata:/var/lib/mysql mysql</code></pre>
      </div>`},{question:"What is Docker Swarm?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Swarm</strong></h3>
        <p>Swarm is Dockers built-in orchestration tool for clustering multiple Docker hosts and managing services.</p>
        <p><strong>In Practice:</strong> Used for simpler orchestration; Kubernetes is preferred for production-grade orchestration.</p>
      </div>`},{question:"What is Docker Compose and how is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Compose</strong></h3>
        <p>Used to define and run multi-container setups using <code>docker-compose.yml</code>.</p>
        <pre><code>version: "3"
services:
  web:
    image: nginx
  db:
    image: mysql</code></pre>
      </div>`},{question:"Where do we keep Docker images (e.g. ACR, DockerHub)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Image Repositories</strong></h3>
        <ul>
          <li>Docker Hub  public registry.</li>
          <li>Azure Container Registry (ACR)  enterprise use.</li>
          <li>Amazon ECR, GCP Artifact Registry  cloud-specific.</li>
        </ul>
      </div>`},{question:"How to push and pull Docker images?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Push / Pull Commands</strong></h3>
        <pre><code>docker tag myapp:latest myacr.azurecr.io/myapp:v1
docker push myacr.azurecr.io/myapp:v1
docker pull myacr.azurecr.io/myapp:v1</code></pre>
      </div>`},{question:"Can we modify a running Docker container?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Modifying Containers</strong></h3>
        <p>You can <code>exec</code> into a container and make temporary changes:</p>
        <pre><code>docker exec -it myapp bash</code></pre>
        <p>To make permanent changes: <code>docker commit myapp newimage:v1</code></p>
      </div>`},{question:"Command to run an already created container.",answerHtml:`
      <div class="answer-rich">
        <pre><code>docker start -a myapp</code></pre>
        <p><code>-a</code> attaches logs to your terminal; you can also use <code>docker restart myapp</code>.</p>
      </div>`},{question:"What is ARG in Dockerfile?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ARG  Build-time Variables</strong></h3>
        <pre><code>ARG APP_VERSION=1.0
RUN echo "Building version &dollar;APP_VERSION"</code></pre>
      </div>`},{question:"How to clean unused Docker containers, networks, and images?",answerHtml:`
      <div class="answer-rich">
        <pre><code>docker system prune -a -f</code></pre>
        <p>Removes stopped containers, dangling images, and unused networks.</p>
      </div>`},{question:"What is a build context in Docker?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Build Context</strong></h3>
        <p>Build context is the directory sent to the Docker daemon when running <code>docker build</code>.  
        It contains all files referenced in the Dockerfile.</p>
      </div>`},{question:"What is the purpose of .dockerignore file?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>.dockerignore</strong></h3>
        <p>Used to exclude unnecessary files (e.g., <code>node_modules</code>, logs, secrets) from the build context to reduce image size.</p>
      </div>`},{question:"What is difference between container restart policies (no, always, on-failure)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Restart Policies</strong></h3>
        <ul>
          <li><strong>no:</strong> Never restarts container.</li>
          <li><strong>always:</strong> Always restarts if stopped.</li>
          <li><strong>on-failure:</strong> Restarts only if exit code  0.</li>
        </ul>
        <pre><code>docker run --restart always nginx</code></pre>
      </div>`},{question:"How do you debug failing Docker builds?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Build Debugging</strong></h3>
        <ul>
          <li>Use <code>--progress=plain --no-cache</code> to view build logs.</li>
          <li>Insert <code>RUN echo</code> or <code>RUN ls</code> for debugging steps.</li>
          <li>Build locally with <code>docker build -t test .</code> and test intermediate layers.</li>
        </ul>
      </div>`}]},{title:"4. Dockerfile  Core Concepts & Instructions",questions:[{question:"What is a Dockerfile and why do we use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>What is a Dockerfile?</strong></h3>
        <p>
          A Dockerfile is a plain-text recipe that contains the steps required to build a Docker image.  
          It defines base image, copies source, installs dependencies, runs build steps, and sets the runtime command.
        </p>
        <p><strong>Why use it?</strong> Because it makes image builds reproducible, versionable (stored in Git), and automatable in CI/CD pipelines.</p>
        <p><strong>In Practice:</strong> Every microservice I build has a Dockerfile in the repo  CI runs <code>docker build</code> and produces immutable images.</p>
      </div>`},{question:"What are the main instructions in a Dockerfile (FROM, RUN, CMD, COPY, EXPOSE, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Dockerfile Instructions</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>FROM</strong>  base image for a stage (mandatory).</li>
          <li><strong>WORKDIR</strong>  set working directory inside image.</li>
          <li><strong>COPY</strong> / <strong>ADD</strong>  copy files from build context.</li>
          <li><strong>RUN</strong>  execute commands at build time (install packages, compile).</li>
          <li><strong>ENV</strong>  set environment variables available at build & runtime.</li>
          <li><strong>ARG</strong>  build-time variables (not persisted to runtime env by default).</li>
          <li><strong>EXPOSE</strong>  document which ports the container listens on (metadata only).</li>
          <li><strong>VOLUME</strong>  declare mount points for persistent data.</li>
          <li><strong>ENTRYPOINT</strong>  executable that will always run (entry binary).</li>
          <li><strong>CMD</strong>  default arguments for ENTRYPOINT or default command if ENTRYPOINT not set.</li>
          <li><strong>USER</strong>  switch to a non-root user.</li>
          <li><strong>HEALTHCHECK</strong>  define a container health probe.</li>
        </ul>
      </div>`},{question:"What is the difference between ADD and COPY commands?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ADD vs COPY</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>COPY</strong>  simple, predictable: copy files from build context into image. Use this for most cases.</li>
          <li><strong>ADD</strong>  does everything COPY does + can automatically extract local tar archives and can fetch remote URLs. Because of side-effects, use ADD only when you need its extra features.</li>
        </ul>
        <p><strong>Best practice:</strong> Prefer <code>COPY</code> for clarity and security; use <code>ADD</code> only for tar extraction or trusted remote fetches (rare).</p>
      </div>`},{question:"What is the difference between CMD and ENTRYPOINT?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CMD vs ENTRYPOINT</strong></h3>
        <p>
          Both define what runs in the container, but they serve different purposes:
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>ENTRYPOINT</strong> sets the fixed command/executable (the containers main purpose).</li>
          <li><strong>CMD</strong> provides default arguments to ENTRYPOINT or a fallback command if ENTRYPOINT is not defined.</li>
        </ul>
        <p><strong>Examples:</strong></p>
        <pre><code>ENTRYPOINT ["/usr/bin/python", "app.py"]   # fixed executable
CMD ["--port", "8080"]                         # default args, overridable</code></pre>
        <p><strong>Rule:</strong> Use ENTRYPOINT for the main binary and CMD for default options you might want to override at runtime.</p>
      </div>`},{question:"What is .dockerignore and why is it important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>.dockerignore Purpose</strong></h3>
        <p>
          <code>.dockerignore</code> excludes files from the build context sent to the Docker daemon (like <code>node_modules</code>, logs, local creds).  
          This reduces build size, speeds up build, and prevents secrets from accidentally being included.
        </p>
        <p><strong>Practical tip:</strong> Always include <code>.git</code>, <code>.env</code>, and heavy directories in <code>.dockerignore</code>.</p>
      </div>`},{question:"What is the difference between ARG and ENV in Dockerfile?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ARG vs ENV</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>ARG</strong>  build-time variable, available only during image build. Not persisted to runtime environment unless explicitly set to ENV.</li>
          <li><strong>ENV</strong>  sets environment variables inside the final image and available at runtime.</li>
        </ul>
        <p><strong>Use case:</strong> Use ARG for build-time options (version tags, build flags) and ENV for configuration needed at runtime or for documentation.</p>
      </div>`},{question:"How do you reduce Docker image size?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reducing Image Size  Practical Tips</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use minimal base images (alpine, distroless) when possible.</li>
          <li>Use multi-stage builds  build artifacts in one stage, copy only final artifacts into runtime stage.</li>
          <li>Combine RUN commands with <code>&amp;&amp;</code> and clean package manager caches in same RUN step (reduces intermediate layers).</li>
          <li>Add appropriate <code>.dockerignore</code> so context excludes unnecessary files.</li>
          <li>Remove build tools and dev dependencies from final image.</li>
          <li>Use explicit <code>--no-install-recommends</code> when installing packages (Debian/Ubuntu).</li>
        </ul>
      </div>`},{question:"Can we create a Docker image without pulling a base image?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create Image Without Base?</strong></h3>
        <p>
          In Dockerfile, the first instruction is typically <code>FROM</code>. You can create a scratch-based image using <code>FROM scratch</code> (an empty image) and add binaries  but then you must provide everything (static binary + dependencies).  
          For most apps, you use an existing base image (alpine, debian, distroless).
        </p>
        <p><strong>Use case for <code>scratch</code>:</strong> Small static Go binaries or minimal container images where you want absolute minimal surface.</p>
      </div>`},{question:"What is a multi-stage build, and why/when is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-stage Builds  Why Use Them?</strong></h3>
        <p>
          Multi-stage builds let you separate build-time steps from runtime image. You compile/build in a heavy image (with build tools) and then copy only the final artifact into a lightweight runtime image  resulting in much smaller final images.
        </p>
        <p><strong>When to use:</strong> When compile toolchain is large (e.g., Node, Java, Go) but runtime only needs compiled binary or static files.</p>
      </div>`},{question:"What are best practices for writing a Dockerfile?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Dockerfile Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Keep images small  use multi-stage builds and minimal base images.</li>
          <li>Order layers by change frequency (put frequently changing files later so earlier layers are cached).</li>
          <li>Avoid secrets in Dockerfile and build context  use build-time secrets or Key Vault/secret store in CI.</li>
          <li>Use non-root user with <code>USER</code> where possible.</li>
          <li>Pin dependency versions for reproducibility.</li>
          <li>Use <code>.dockerignore</code> to reduce context size.</li>
          <li>Provide HEALTHCHECK to help orchestrators detect unhealthy containers.</li>
        </ul>
      </div>`},{question:"Write a Dockerfile for a Node.js application using multi-stage builds.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Node.js Multi-stage Dockerfile</strong></h3>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># Stage 1  builder
FROM node:18-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --production=false
COPY . .
RUN npm run build

# Stage 2  runtime
FROM node:18-alpine
WORKDIR /app
# copy only production deps and build output
COPY --from=builder /app/package*.json ./
RUN npm ci --production=true
COPY --from=builder /app/dist ./dist
ENV NODE_ENV=production
EXPOSE 3000
USER node
CMD ["node", "dist/server.js"]</code></pre>
        <p><strong>Why:</strong> builder has dev deps and build chain; runtime contains only production deps + built assets  smaller final image.</p>
      </div>`},{question:"Write a Dockerfile to build a custom image for any application.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Generic Custom Dockerfile (Python Flask Example)</strong></h3>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>FROM python:3.11-slim

# create non-root user
RUN groupadd -r appgroup && useradd -r -g appgroup appuser

WORKDIR /app

# copy only requirements first for caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# copy application
COPY . .

# expose and run
EXPOSE 5000
USER appuser
ENV FLASK_ENV=production
CMD ["gunicorn", "app:app", "-b", "0.0.0.0:5000", "--workers", "3"]</code></pre>
        <p><strong>Notes:</strong> Using slim base, non-root user, pip cache disabled, layering optimized for build caching.</p>
      </div>`},{question:"How do you debug failing Docker builds?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Debugging Docker Builds</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Build with verbose/plain output: <code>docker build --progress=plain --no-cache .</code>.</li>
          <li>Re-run failing RUN step locally by placing commands in a temporary container (or use an interactive builder stage):</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># build to specific stage and start shell
docker build --target builder -t tmp-builder .
docker run --rm -it tmp-builder /bin/sh</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Insert debug lines like <code>RUN ls -la /app</code> or <code>RUN cat /path/file</code> to inspect layer content.</li>
          <li>Check network/proxy issues when fetching packages  ensure build environment has outbound access or use caches.</li>
          <li>Check file permissions and non-root user problems.</li>
        </ul>
      </div>`}]},{title:"5. Docker Image Management",questions:[{question:"How do you build and tag Docker images?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Build & Tag Workflow</strong></h3>
        <p>
          Build the image from a Dockerfile and tag it with a meaningful repository/name:tag so you can push/pull it from a registry.
        </p>
        <p><strong>Typical steps:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Write a Dockerfile in the project root.</li>
          <li>Run <code>docker build</code> with a tag (name:tag).</li>
          <li>Optionally retag for the target registry (e.g., ACR) before push.</li>
        </ol>
        <pre><code># Build and tag locally
docker build -t myapp:1.0 .

# Retag for registry
docker tag myapp:1.0 myacr.azurecr.io/myapp:1.0</code></pre>
        <p><strong>In Practice:</strong> I let CI generate a build number (e.g., 1.0.23 or commit SHA) and use that as the image tag to ensure immutability.</p>
      </div>`},{question:"Write the build command to build a Docker image.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>docker build Example</strong></h3>
        <p>Basic build command:</p>
        <pre><code>docker build -t myapp:1.0 .</code></pre>
        <p>With build-arg and no-cache (debug):</p>
        <pre><code>docker build --build-arg APP_ENV=prod --no-cache -t myapp:1.0 .</code></pre>
        <p><strong>Note:</strong> The trailing dot <code>.</code> is the build context (directory sent to daemon).</p>
      </div>`},{question:"What is the purpose of the docker build -t command?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Purpose of <code>-t</code></strong></h3>
        <p>
          The <code>-t</code> flag tags the image during build with a name and optional tag (<code>name:tag</code>), making it easy to reference the image later for running, pushing, or retagging.
        </p>
        <p><strong>Example:</strong> <code>docker build -t myrepo/myapp:1.0 .</code> creates an image you can run with <code>docker run myrepo/myapp:1.0</code>.</p>
      </div>`},{question:"How do you push Docker images to Azure Container Registry (ACR)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Push Images to ACR (common approaches)</strong></h3>
        <p>Recommended: use <code>az acr login</code> or CI service connection to avoid manual creds.</p>
        <pre><code># 1) Tag image for ACR
docker tag myapp:1.0 myacr.azurecr.io/myapp:1.0

# 2) Login (developer machine)
az acr login --name myacr

# 3) Push
docker push myacr.azurecr.io/myapp:1.0</code></pre>
        <p><strong>CI note:</strong> In Azure DevOps/GitHub Actions use a service principal or managed identity and the built-in ACR task to push images securely (no local docker login required).</p>
      </div>`},{question:"How do you pull images from Docker Hub or ACR?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pull Commands</strong></h3>
        <pre><code># Pull from Docker Hub
docker pull nginx:latest

# Pull from ACR (after az acr login or docker login)
docker pull myacr.azurecr.io/myapp:1.0</code></pre>
        <p><strong>Tip:</strong> For private registries ensure your CI/CD agent has proper auth (service principal, managed identity, or docker login credentials).</p>
      </div>`},{question:"How do you reduce the size of Docker images?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Image Size Reduction Techniques</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use minimal base images (e.g., <code>alpine</code> or distroless) where possible.</li>
          <li>Use multi-stage builds  keep only runtime artifacts in final image.</li>
          <li>Combine and minimize <code>RUN</code> steps (chain with <code>&amp;&amp;</code>) to reduce intermediate layers.</li>
          <li>Clear package caches in same RUN step (e.g., <code>apt-get clean && rm -rf /var/lib/apt/lists/*</code>).</li>
          <li>Add a proper <code>.dockerignore</code> to exclude heavy files (node_modules, .git).</li>
          <li>Remove build tools and dev dependencies in final stage.</li>
          <li>Use specific small runtime images (e.g., <code>node:alpine</code> or distroless) for production.</li>
        </ul>
        <p><strong>In Practice:</strong> I measure image size in CI and fail build if it exceeds a threshold  enforces discipline.</p>
      </div>`},{question:"What are best practices for image versioning and tagging?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Image Tagging Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use immutable tags: <code>myapp:1.2.3</code> (semantic versioning) or <code>myapp:sha-<commit-sha></code>.</li>
          <li>Keep <code>latest</code> for convenience but avoid relying on it in production (it's mutable).</li>
          <li>Include CI build metadata: <code>myapp:1.2.3-build.42</code> or <code>myapp:sha-abcdef</code> for traceability.</li>
          <li>Use image digests (sha256:@digest) in deployment manifests to ensure exact image is pulled: <code>myacr.azurecr.io/myapp@sha256:...</code>.</li>
          <li>Automate tagging in CI: tag by branch, tag name, and commit SHA (e.g., <code>feature/x</code>, <code>v1.2.3</code>, <code>sha</code>).</li>
          <li>Retire old tags with lifecycle policies in the registry (ACR retention or purge policies) to save storage.</li>
        </ul>
        <p><strong>In Practice:</strong> I push both semver tag and commit-SHA tag, and deployments use the SHA or digest for immutability while QA uses semver tags.</p>
      </div>`}]},{title:"6. Containers  Run, Manage & Inspect",questions:[{question:"How do you create and run a container from an image?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create & Run Container</strong></h3>
        <p>
          Use <code>docker run</code> to create and start a container from an image in one step.
        </p>
        <pre><code># Run nginx in detached mode
docker run -d --name webserver -p 8080:80 nginx:latest

# Run interactively
docker run -it --name myapp ubuntu /bin/bash</code></pre>
        <ul>
          <li><code>-d</code>  detached mode (background)</li>
          <li><code>-p</code>  map host:container port</li>
          <li><code>-it</code>  interactive terminal</li>
        </ul>
        <p><strong>In Practice:</strong> I use this pattern during debugging or local testing before committing CI pipeline steps.</p>
      </div>`},{question:"Do you know the command to create a container manually?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create Container (Without Starting)</strong></h3>
        <pre><code># Create but do not start
docker create --name mydb mysql:8.0

# Then start it manually
docker start mydb</code></pre>
        <p><strong>Note:</strong> <code>docker run</code> = create + start in one step. <code>docker create</code> only defines the container metadata before first start.</p>
      </div>`},{question:"What is the difference between docker stop and docker kill?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>docker stop vs docker kill</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Command</th><th>Signal</th><th>Description</th></tr>
          <tr><td><strong>docker stop</strong></td><td>SIGTERM  SIGKILL (graceful)</td><td>Gives process time to clean up before stopping.</td></tr>
          <tr><td><strong>docker kill</strong></td><td>SIGKILL (immediate)</td><td>Forcefully terminates process instantly.</td></tr>
        </table>
        <p><strong>Best practice:</strong> Always use <code>docker stop</code> first to let the app shut down gracefully.</p>
      </div>`},{question:"How do you check running containers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>List Containers</strong></h3>
        <pre><code># Show running containers
docker ps

# Show all (including stopped)
docker ps -a</code></pre>
        <p><strong>Tip:</strong> Use <code>--filter</code> and <code>--format</code> for focused output:
        <pre><code>docker ps --filter "status=running" --format "{{.Names}} - {{.Image}}"</code></pre>
      </div>`},{question:"How do you check logs for a container?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>View Container Logs</strong></h3>
        <pre><code># View logs for container
docker logs myapp

# Follow logs live
docker logs -f myapp</code></pre>
        <p><strong>Note:</strong> Use <code>--tail</code> to limit output or integrate logs with external systems (e.g., Azure Monitor or ELK stack).</p>
      </div>`},{question:"How do you debug a container that has exited?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Debugging Exited Containers</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Check exit code: <code>docker ps -a</code> or <code>docker inspect --format='{{.State.ExitCode}}' myapp</code></li>
          <li>View logs: <code>docker logs myapp</code></li>
          <li>Start an interactive shell to inspect filesystem:
          <pre><code>docker run -it --entrypoint /bin/bash myapp</code></pre></li>
          <li>Alternatively, commit container for inspection:
          <pre><code>docker commit myapp debug:latest
docker run -it debug:latest /bin/bash</code></pre></li>
        </ol>
        <p><strong>Pro tip:</strong> For crash loops, use <code>--restart=on-failure</code> and attach logs to a monitoring system.</p>
      </div>`},{question:"How do you inspect a containers configuration (env, ports, volumes)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Inspect Container Configuration</strong></h3>
        <pre><code># Full inspection (JSON)
docker inspect myapp

# View specific data using format
docker inspect -f '{{.Config.Env}}' myapp
docker inspect -f '{{.NetworkSettings.Ports}}' myapp</code></pre>
        <p><strong>Tip:</strong> Combine with <code>jq</code> for pretty JSON filtering:
        <pre><code>docker inspect myapp | jq '.[0].Config.Entrypoint'</code></pre>
      </div>`},{question:"How do you clean up unused containers and images?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Cleanup</strong></h3>
        <p>Clean up stopped containers, dangling images, and unused networks:</p>
        <pre><code># Remove stopped containers, unused images, and networks
docker system prune -f

# Remove everything (including volumes)
docker system prune -a --volumes -f</code></pre>
        <p><strong>Best practice:</strong> Schedule cleanup on self-hosted agents or dev servers periodically to free space.</p>
      </div>`},{question:"What is the command to remove dangling images and stopped containers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Remove Dangling Resources</strong></h3>
        <pre><code># Remove only dangling (untagged) images
docker image prune -f

# Remove stopped containers
docker container prune -f</code></pre>
        <p><strong>Dangling =</strong> images without tags, often leftover after rebuilds.</p>
      </div>`}]},{title:"7. Networking & Communication Between Containers",questions:[{question:"How do you create a Docker network?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create a Docker Network</strong></h3>
        <p>Use <code>docker network create</code> to make a custom network (bridge or overlay).</p>
        <pre><code># Create a user-defined bridge network (single host)
docker network create my-bridge-net

# Create an overlay network (multi-host, requires Swarm/K8s integration)
docker network create -d overlay my-overlay-net</code></pre>
        <p><strong>In Practice:</strong> I create user-defined bridge networks for isolation and service discovery on a single host  they give automatic DNS resolution between containers.</p>
      </div>`},{question:"If you want two containers to communicate, how can you do it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container-to-Container Communication</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Create (or use) the same network: <code>docker network create my-net</code></li>
          <li>Run containers on that network:
            <pre><code>docker run -d --name app --network my-net myapp
docker run -d --name redis --network my-net redis</code></pre>
          </li>
          <li>Use container name as hostname (DNS): <code>ping redis</code> or connect in app via <code>redis:6379</code>.</li>
        </ol>
        <p><strong>Tip:</strong> Use user-defined networks  default bridge does not provide automatic DNS by container name.</p>
      </div>`},{question:"How do you connect multiple containers in a custom network?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Connect Multiple Containers</strong></h3>
        <p>Either start containers with <code>--network</code> or connect an existing container to a network with <code>docker network connect</code>.</p>
        <pre><code># Start containers on network
docker run -d --name web --network my-net nginx
docker run -d --name api --network my-net myapi

# Connect an already running container to a network
docker network connect my-net existing-container</code></pre>
        <p><strong>In Practice:</strong> I attach app containers to a shared network and attach monitoring/logging sidecars to the same network when needed.</p>
      </div>`},{question:"What kind of network policies can be used to restrict container communication?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Network Policies & Restrictions</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>On single-host Docker: use <strong>iptables</strong> rules or custom Docker network drivers that support ACLs.</li>
          <li>With Kubernetes: use <strong>NetworkPolicy</strong> CRDs to allow/deny pod-to-pod traffic (Calico, Cilium, etc.).</li>
          <li>In Docker Swarm: use encrypted overlay networks and control published ports; for more advanced ACLs use an external CNI or firewall.</li>
          <li>Use <code>docker network inspect</code> and host-level firewall rules for tighter control.</li>
        </ul>
        <p><strong>Pro tip:</strong> For production microservices, prefer Kubernetes network policies (Calico/Cilium)  they are purpose-built for fine-grained traffic control.</p>
      </div>`},{question:"What are the different types of Docker networks (bridge, host, overlay, none)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Network Types</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>bridge</strong>  Default single-host network; user-defined bridge adds DNS-based service discovery.</li>
          <li><strong>host</strong>  Container shares host network namespace (no network isolation).</li>
          <li><strong>overlay</strong>  Multi-host network for Swarm/K8s; enables containers on different hosts to talk over encrypted vxlan.</li>
          <li><strong>none</strong>  No network stack; container isolated from network.</li>
        </ul>
        <p><strong>When to use:</strong> bridge for most single-host apps, host for high-performance networking (but risky), overlay for multi-host clusters.</p>
      </div>`},{question:"What is the default Docker network driver?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Default Driver</strong></h3>
        <p>The default driver is <code>bridge</code> for containers started without a <code>--network</code> option. Docker also creates a <code>bridge</code> network named <code>bridge</code> and a default <code>none</code> network.</p>
        <p><strong>Note:</strong> For predictable DNS and discovery, create your own user-defined bridge network instead of relying on the default.</p>
      </div>`},{question:"How do you manage DNS inside a Docker network?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DNS & Service Discovery in Docker</strong></h3>
        <p>User-defined networks provide an embedded DNS server  container names resolve to IPs within that network.</p>
        <pre><code># Example: containers started on my-net can resolve each other by name
docker run -d --name db --network my-net postgres
docker run -d --name web --network my-net myweb
# from web: ping db</code></pre>
        <p>Advanced options:</p>
        <ul style="margin-left:1.2rem;">
          <li>Use <code>--dns</code> to set custom DNS servers for containers.</li>
          <li>Use <code>docker network inspect</code> to see network settings and container IPs.</li>
          <li>In multi-host setups, use external DNS/Service Discovery (Consul, etcd) or orchestrator-provided DNS (Kubernetes CoreDNS).</li>
        </ul>
        <p><strong>In Practice:</strong> For multi-host clusters I prefer orchestrator DNS (k8s CoreDNS) or a dedicated service discovery tool  built-in Docker DNS is great for single-host setups.</p>
      </div>`}]},{title:"8. Volumes & Data Persistence",questions:[{question:"What is a Docker volume and why do we use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>What is a Docker Volume?</strong></h3>
        <p>
          A Docker <strong>volume</strong> is a managed persistent storage area that lives outside a container's writable layer.  
          Volumes are the recommended way to persist data because they survive container restarts and removals, and are managed by Docker.
        </p>
        <p><strong>Why use volumes?</strong> Data persistence, isolation from image lifecycle, better performance than some bind mounts, and portability (can be attached to other containers).</p>
      </div>`},{question:"How do you persist data using volumes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Persist Data with Volumes  Example</strong></h3>
        <p>Create a named volume and mount it into a container:</p>
        <pre><code># create volume
docker volume create mydata

# run container with volume
docker run -d --name mydb -v mydata:/var/lib/mysql mysql:8.0</code></pre>
        <p>The DB writes to <code>/var/lib/mysql</code> inside the container, but data is stored in the <code>mydata</code> volume on the host (managed by Docker).</p>
      </div>`},{question:"What happens if a container is deleted  how do you prevent data loss?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container Deletion & Data Safety</strong></h3>
        <p>If a container is removed, named volumes remain by default  data is safe. Only removing volumes explicitly (or using prune) deletes the data.</p>
        <ul style="margin-left:1.2rem;">
          <li>Use named volumes (not anonymous) so you can reattach them later.</li>
          <li>Avoid <code>docker run --rm</code> for containers that hold important data.</li>
          <li>Set up scheduled backups (see backup section below) and/or store data on network-backed volumes (NFS, cloud file storage).</li>
        </ul>
      </div>`},{question:"Suppose a container is accidentally deleted  what steps can you take to protect or recover the data?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recovering Data After Accidental Container Deletion</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Check if the named volume still exists:
            <pre><code>docker volume ls
docker volume inspect mydata</code></pre>
          </li>
          <li>If volume exists, recreate a container and reattach the volume:
            <pre><code>docker run -d --name newdb -v mydata:/var/lib/mysql mysql:8.0</code></pre>
          </li>
          <li>If the container had no named volume (anonymous), try to find dangling volumes and inspect them before pruning:
            <pre><code>docker volume ls -qf dangling=true</code></pre>
          </li>
          <li>If the container was removed without the volume (and volume deleted), restore from backups (see below).</li>
        </ol>
        <p><strong>Pro tip:</strong> Always use named volumes and ensure backups are automated to avoid single-point data loss.</p>
      </div>`},{question:"How do you back up volume data?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Backing up a Docker Volume</strong></h3>
        <p>Classic backup pattern: run a temporary container that mounts the volume and creates a tarball on the host or remote storage.</p>
        <pre><code># backup volume to host directory (POSIX example)
mkdir -p /opt/backups
docker run --rm -v mydata:/data -v /opt/backups:/backup alpine   sh -c "cd /data && tar czf /backup/mydata-&dollar;(date +%F).tar.gz ."</code></pre>
        <p>Alternatively send tarball to S3/ACR/remote share by using a container with the appropriate client tools.</p>
      </div>`},{question:"How do you restore volume data?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Restore Volume From Backup</strong></h3>
        <p>Restore by mounting the volume and extracting the tarball back into it:</p>
        <pre><code># create/ensure target volume exists
docker volume create mydata

# restore into volume from host backup
docker run --rm -v mydata:/data -v /opt/backups:/backup alpine   sh -c "cd /data && tar xzf /backup/mydata-2025-01-10.tar.gz"</code></pre>
        <p>Then start your container mounting <code>mydata</code> as usual.</p>
      </div>`},{question:"What options exist for copying files between host and a container/volume?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Copying Files In/Out</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>docker cp</code>  copy files between host and container filesystem:
            <pre><code>docker cp mycontainer:/var/lib/mysql/dump.sql /host/path/</code></pre>
          </li>
          <li>Temporary helper container (useful for volumes) as shown in backup/restore examples.</li>
          <li>Mount host bind mounts for easy access (but be careful with permissions and portability).</li>
        </ul>
      </div>`},{question:"How do you back up volumes when using orchestrators like Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Backups in Kubernetes</strong></h3>
        <p>
          Use CSI volume snapshots (if supported by the storage class) or tools like Velero to snapshot and backup persistent volumes and metadata.  
          For cloud mounts, use cloud provider snapshot features (Azure Disk snapshot, AWS EBS snapshot).
        </p>
      </div>`},{question:"What are bind mounts vs named volumes  pros and cons?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Bind Mounts vs Named Volumes</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Bind mounts</strong> (host directory mounted into container): simple, great for development, but less portable and can cause permission/ownership issues.</li>
          <li><strong>Named volumes</strong> (managed by Docker): portable across containers, safer defaults, stored in Docker-managed location, and easier to backup/inspect.</li>
        </ul>
        <p><strong>Rule:</strong> Use bind mounts for local dev (quick edits), use named volumes for production data persistence.</p>
      </div>`},{question:"How to automate backups and prevent data loss in production?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated Backup Strategies</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Schedule periodic backups with cron jobs or CI agents that run the temporary-container tar approach and push to offsite/long-term storage (S3, Azure Blob).</li>
          <li>Use cloud-native volume drivers (Azure File, Azure Disk, EBS) and rely on provider snapshots for point-in-time recovery.</li>
          <li>Implement retention policies and test restores regularly (DR drills).</li>
          <li>Monitor volume usage and set alerts to prevent running out of disk causing corruption.</li>
        </ul>
      </div>`},{question:"What about volume drivers and external storage integrations?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Volume Drivers & External Storage</strong></h3>
        <p>
          For production you may use volume drivers that integrate with NFS, cloud file services, or storage plugins (e.g., local-persist, rexray, cloud-specific CSI drivers).  
          These give advanced features: network-backed storage, snapshots, replication, and better portability across hosts.
        </p>
      </div>`}]},{title:"9. Docker Compose & Multi-Container Setup",questions:[{question:"What is Docker Compose and how does it work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Compose  Multi-container orchestration for dev/test</strong></h3>
        <p>
          Docker Compose is a tool to define and run multi-container applications using a YAML file (<code>docker-compose.yml</code>).  
          You declare services, networks, volumes, and Compose starts them in the correct order, wiring networking and volumes automatically.
        </p>
        <p><strong>How it works:</strong> Compose reads the YAML, creates required networks/volumes, builds images (if requested), and runs containers. Under the hood it uses the Docker Engine API.</p>
        <p><strong>In Practice:</strong> I use Compose to launch an app + DB + cache locally and in CI integration tests before deploying to Kubernetes.</p>
      </div>`},{question:"How do you define multiple services in a docker-compose.yml file?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Example docker-compose.yml with multiple services</strong></h3>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>version: "3.8"
services:
  web:
    build: ./web
    ports:
      - "8080:80"
    depends_on:
      - api
    networks:
      - app-net

  api:
    image: myapi:latest
    environment:
      - DATABASE_URL=postgres://postgres:pass@db:5432/appdb
    networks:
      - app-net

  db:
    image: postgres:14
    volumes:
      - db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_PASSWORD: pass
    networks:
      - app-net

volumes:
  db-data:

networks:
  app-net:</code></pre>
        <p><strong>Notes:</strong> <code>depends_on</code> controls start order but not readiness  use healthchecks or wait-for scripts for true readiness.</p>
      </div>`},{question:"How do you bring up and shut down multi-container applications?",answerHtml:`
      <div class="answer-rich">
        <h3> /  <strong>Compose lifecycle commands</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>docker compose up -d</code>  build (if needed) and start all services in background.</li>
          <li><code>docker compose ps</code>  list running services for the compose project.</li>
          <li><code>docker compose logs -f</code>  follow logs for all services.</li>
          <li><code>docker compose down</code>  stop and remove containers, networks (preserves named volumes by default).</li>
          <li><code>docker compose down --volumes --remove-orphans</code>  remove volumes and orphan containers tied to the project.</li>
        </ul>
        <p><strong>CI pattern:</strong> <code>docker compose -f docker-compose.ci.yml up --build --abort-on-container-exit</code> then capture exit code and <code>docker compose down --volumes</code>.</p>
      </div>`},{question:"What is the difference between Docker Compose and Docker Swarm?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Compose vs Swarm  comparison</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Docker Compose</strong>  developer/local & CI tool for defining multi-service stacks on a single host (though versioned Compose can target Swarm/K8s with transforms).</li>
          <li><strong>Docker Swarm</strong>  built-in Docker cluster orchestration for running services across multiple hosts with service scheduling, scaling, and built-in load balancing.</li>
        </ul>
        <p><strong>When to use:</strong> Compose for local dev, integration tests, and simple setups; Swarm (or preferably Kubernetes) for production multi-node orchestration and scaling.</p>
      </div>`},{question:"What are advantages of Compose for CI/CD setups?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Why Compose is great for CI/CD</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Reproducible environment: same stack locally and in CI.</li>
          <li>Fast spin-up of dependent services (DB, cache, mock services) for integration tests.</li>
          <li>Supports build-time image creation, so CI can build and test images before pushing to registry.</li>
          <li>Easy teardown ensures CI agents are clean after tests (<code>down --volumes</code>).</li>
          <li>Can run service-level tests by using <code>--abort-on-container-exit</code> to fail CI if tests inside containers fail.</li>
        </ul>
        <p><strong>Example CI flow:</strong> Checkout  docker compose -f docker-compose.ci.yml up --build --abort-on-container-exit  run result validations  docker compose down --volumes.</p>
      </div>`},{question:"How do you connect containers from different Docker Compose projects?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Connecting across Compose projects</strong></h3>
        <p>Use a shared external network or attach running containers to another project's network.</p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># 1) create a shared network
docker network create shared-net

# 2) in compose A (docker-compose-A.yml)
networks:
  shared-net:
    external: true

services:
  svcA:
    networks:
      - shared-net

# 3) in compose B (docker-compose-B.yml) also declare the same external network
networks:
  shared-net:
    external: true

# 4) bring both projects up; services can now resolve each other by container name</code></pre>
        <p>Alternatively, use <code>docker network connect</code> to attach containers from one compose project to a different network at runtime.</p>
        <p><strong>Note:</strong> Keep project names and service names predictable; use aliases in networks to simplify hostnames.</p>
      </div>`}]},{title:"10. Security & Secret Management",questions:[{question:"How do you handle secrets inside containers (Key Vault, ENV, mounted files)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Secrets Inside Containers</strong></h3>
        <p>
          In production, secrets are <strong>never baked into images</strong>.  
          Instead, they are injected securely at runtime using environment variables, mounted files, or secret stores.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Environment Variables</strong>  Simple, but avoid committing in Dockerfile. Pass via <code>docker run -e</code> or Compose <code>env_file</code>.</li>
          <li><strong>Mounted Files</strong>  Mount secrets as files from secret volumes (<code>-v /secrets:/run/secrets</code>).</li>
          <li><strong>Azure Key Vault</strong>  Best for production; fetch dynamically through managed identity or init script.</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer Key Vault integration + environment injection at runtime. It ensures credentials never exist in the image or Git repo.</p>
      </div>`},{question:"How do you fetch secrets from Azure Key Vault into your container?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Fetching Secrets from Azure Key Vault</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Assign a <strong>Managed Identity</strong> to the container or VM where Docker runs.</li>
          <li>Grant Key Vault <strong>Secret Get/List</strong> permissions to that identity.</li>
          <li>Use Azure CLI or SDK inside container to fetch secrets securely:
            <pre><code>az keyvault secret show --name db-password --vault-name myvault --query value -o tsv</code></pre>
          </li>
          <li>Alternatively, inject secrets via pipeline before deployment (e.g. Azure DevOps variable group linked with Key Vault).</li>
        </ol>
        <p><strong>Example:</strong> In ADO pipeline, link Key Vault  inject secrets as environment variables  pass them to Docker container at runtime.</p>
      </div>`},{question:"How do you handle secrets for PHP  MySQL connection in Docker?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>PHP  MySQL Secret Management</strong></h3>
        <p>Never hardcode DB credentials in PHP config or Dockerfile. Use env variables or secret mounts:</p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>version: "3.8"
services:
  web:
    image: php:8.2-apache
    environment:
      - DB_HOST=mysql
      - DB_USER=&dollar;{DB_USER}
      - DB_PASS=&dollar;{DB_PASS}
    env_file: .env
    depends_on:
      - mysql

  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: &dollar;{DB_PASS}
    volumes:
      - mysql-data:/var/lib/mysql

volumes:
  mysql-data:</code></pre>
        <p><strong>In Practice:</strong> I store <code>.env</code> in secure pipeline storage (ADO Library or Key Vault). Its injected at runtime and never checked into Git.</p>
      </div>`},{question:"What is the best way to inject secrets securely in runtime?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Runtime Secret Injection</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Docker Secrets</strong> (Swarm/K8s) or external stores like <strong>Azure Key Vault</strong>.</li>
          <li>Inject via orchestrator or pipeline  never store in image layers.</li>
          <li>Use <strong>Managed Identity</strong> for automatic token-based authentication.</li>
          <li>Mount secrets read-only in container (e.g., <code>/run/secrets/db_password</code>).</li>
          <li>Rotate credentials regularly and ensure containers restart on rotation.</li>
        </ul>
        <p><strong>Best Practice:</strong> Keep runtime secrets ephemeral  use short-lived tokens (AAD, Vault references) rather than static passwords.</p>
      </div>`},{question:"What are the recommended practices to scan images for vulnerabilities?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container Image Vulnerability Scanning</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Docker Scout</strong>, <strong>Trivy</strong>, or <strong>Anchore</strong> to scan images during build or CI.</li>
          <li>Enable <strong>ACR Vulnerability Scanning</strong> for Azure Container Registry.</li>
          <li>Scan both base and application layers regularly.</li>
          <li>Keep base images updated and use minimal base images (e.g. <code>alpine</code>).</li>
          <li>Integrate scans in CI/CD pipelines  fail build if critical CVEs found.</li>
        </ul>
        <p><strong>In Practice:</strong> I run Trivy as a pre-deployment step and configured ACR to auto-scan pushed images. Reports are reviewed during release gates.</p>
      </div>`},{question:"Scenario: You wrote a Dockerfile for NGINX (latest version). It was working fine earlier, but now shows vulnerability issues  what might have happened?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Latest Tag  The Hidden Trap</strong></h3>
        <p>
          The <code>:latest</code> tag always pulls the most recent image from the registry.  
          That means if NGINX released a newer base image with CVEs, your pipeline fetched it automatically.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Latest tag changed  new image pulled with new vulnerabilities.</li>
          <li>Always <strong>pin versions</strong> (e.g. <code>nginx:1.25.3-alpine</code>).</li>
          <li>Regularly re-scan and rebuild pinned images to patch known CVEs.</li>
          <li>Use trusted registries and sign images for integrity validation.</li>
        </ul>
        <p><strong>In Practice:</strong> I lock Dockerfile base versions and have a monthly base image refresh pipeline to update & rescan images safely.</p>
      </div>`}]},{title:"11. Troubleshooting & Debugging Containers",questions:[{question:"How do you troubleshoot container startup issues?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container startup troubleshooting  quick checklist</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Check container status & exit code:</strong>
            <pre><code>docker ps -a
docker inspect --format '{{.State.Status}} {{.State.ExitCode}}' mycontainer</code></pre>
          </li>
          <li><strong>View logs:</strong> first thing  container logs often show config/env errors:
            <pre><code>docker logs mycontainer
docker logs --since 5m --tail 200 -f mycontainer</code></pre>
          </li>
          <li><strong>Common root causes:</strong> wrong CMD/ENTRYPOINT, missing env vars, volume mount errors (file not found / permission), healthcheck failures, missing config files, or port conflicts.</li>
          <li><strong>Reproduce interactively:</strong> start shell instead of default process to inspect filesystem:
            <pre><code>docker run --rm -it --entrypoint /bin/sh myimage</code></pre>
          </li>
          <li><strong>Build-time issues:</strong> use verbose build logs:
            <pre><code>docker build --progress=plain --no-cache -t debug-image .</code></pre>
          </li>
          <li><strong>Inspect metadata:</strong> check environment, mounts, and entrypoint:
            <pre><code>docker inspect mycontainer | jq '.[0].Config'
docker inspect -f '{{.Mounts}} {{.Config.Env}}' mycontainer</code></pre>
          </li>
        </ol>
        <p><strong>Pro tip:</strong> Add small healthchecks and verbose startup logs in your app  saves 80% of debugging time in CI.</p>
      </div>`},{question:"How do you check logs or live attach to a running container?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Logs & attach</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>View logs</strong>:
            <pre><code>docker logs mycontainer            # full logs
docker logs -f mycontainer          # follow (live)
docker logs --since 1m mycontainer  # logs since 1 minute</code></pre>
          </li>
          <li><strong>Attach to main process</strong>:
            <pre><code>docker attach mycontainer</code></pre>
            <p>Warning: <code>attach</code> attaches to the container's main PID  detaching requires <code>Ctrl+P Ctrl+Q</code> or you'll kill the process.</p>
          </li>
          <li><strong>Prefer exec for debugging</strong> (safer, spawns a new shell/process):
            <pre><code>docker exec -it mycontainer /bin/bash</code></pre>
          </li>
        </ul>
      </div>`},{question:"How do you use docker exec to debug inside a container?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using <code>docker exec</code> for debugging</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Start an interactive shell:
            <pre><code>docker exec -it mycontainer /bin/bash   # or /bin/sh</code></pre>
          </li>
          <li>Inspect filesystem, permissions and config files:
            <pre><code>ls -la /app
cat /app/config.yaml
ps aux</code></pre>
          </li>
          <li>Run network checks from inside:
            <pre><code>curl -sv http://backend:8080/health
nslookup redis</code></pre>
          </li>
          <li>Check process tree or open files:
            <pre><code>top or htop
lsof -p <pid></code></pre>
          </li>
          <li>If container image lacks tools, run a debug container that mounts the same volumes:
            <pre><code>docker run --rm -it -v mydata:/data --network container:mycontainer alpine /bin/sh</code></pre>
          </li>
        </ol>
        <p><strong>Note:</strong> <code>exec</code> spawns a new process; it does not disturb the main PID and is the preferred debug method.</p>
      </div>`},{question:"How do you investigate high CPU/memory usage of a container?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Investigate resource spikes</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Live metrics:</strong> use docker stats for quick view:
            <pre><code>docker stats mycontainer            # live CPU/memory/IO</code></pre>
          </li>
          <li><strong>Top processes inside container:</strong>
            <pre><code>docker exec -it mycontainer top   # or ps aux --sort=-%cpu</code></pre>
          </li>
          <li><strong>Inspect cgroup limits and usage:</strong>
            <pre><code>docker inspect -f '{{.HostConfig.Memory}} {{.HostConfig.CpuShares}}' mycontainer</code></pre>
          </li>
          <li><strong>Check application logs & GC (for Java/Node):</strong> look for memory leaks, frequent GC pauses, or runaway loops.</li>
          <li><strong>Profile the app:</strong> attach profiler (e.g., pprof, Java Flight Recorder) or enable app-level metrics.</li>
          <li><strong>Temporary mitigation:</strong> restart container or scale out replicas and then perform full root-cause analysis.</li>
        </ol>
        <p><strong>Prevention:</strong> set resource limits at runtime (<code>--memory</code>, <code>--cpus</code>) and add monitoring/alerts (Prometheus/Grafana or cloud metrics).</p>
      </div>`},{question:"How do you resolve port already in use errors?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Fixing port conflicts</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Check which container/process uses the port:
            <pre><code># Docker containers
docker ps --format 'table {{.ID}}	{{.Names}}	{{.Ports}}'

# Host process (Linux)
ss -tulpn | grep :8080
# or
lsof -i :8080</code></pre>
          </li>
          <li>If another container is using the port, stop it:
            <pre><code>docker stop <container-name-or-id></code></pre>
          </li>
          <li>Change host port mapping when running the container:
            <pre><code>docker run -p 8081:80 myimage</code></pre>
          </li>
          <li>If host service uses the port, either stop the host service or choose a different host port for the container mapping.</li>
          <li>In CI, ensure ephemeral ports or random port allocation to avoid collisions on shared agents.</li>
        </ol>
      </div>`},{question:"How do you troubleshoot networking issues between containers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container networking troubleshooting</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Confirm network membership:</strong>
            <pre><code>docker network inspect my-net   # see attached containers and IPs</code></pre>
          </li>
          <li><strong>Ping / curl between containers:</strong>
            <pre><code>docker exec -it web ping api
docker exec -it web curl -v http://api:8080/health</code></pre>
          </li>
          <li><strong>DNS issues:</strong> ensure containers are on a user-defined network (gives automatic DNS), or check /etc/hosts inside container.</li>
          <li><strong>Check port mapping vs container port:</strong> ensure service binds to 0.0.0.0 inside container (not 127.0.0.1).</li>
          <li><strong>Inspect iptables / host firewall:</strong> host-level firewall could block traffic between containers or to the host ports.</li>
          <li><strong>Overlay/multi-host issues:</strong> verify VXLAN/overlay encryption & Swarm/K8s control plane connectivity; check overlay network logs.</li>
        </ol>
        <p><strong>Debug pattern:</strong> reproduce from a debug container on same network, <code>tcpdump</code> if needed, and inspect docker daemon logs for network driver errors.</p>
      </div>`}]},{title:"12. Scripting & Automation",questions:[{question:"Can you write a shell script to copy a file to another server where source/destination are provided by user input?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Interactive shell script to copy file via SCP</strong></h3>
        <p>Simple, safe script that prompts the user for source path, target user@host:path and optional SSH key. Validates inputs and uses <code>scp</code>.</p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>#!/usr/bin/env bash
set -euo pipefail

read -rp "Source file (local path): " SRC
read -rp "Destination (user@host:/path): " DEST
read -rp "Use SSH key? (path or leave empty): " SSH_KEY

# Basic validation
if [[ ! -f "&dollar;SRC" ]]; then
  echo "ERROR: Source file does not exist: &dollar;SRC" >&2
  exit 2
fi

SCP_CMD=(scp -o StrictHostKeyChecking=accept-new -q)

if [[ -n "&dollar;SSH_KEY" ]]; then
  SCP_CMD+=(-i "&dollar;SSH_KEY")
fi

# Run scp
"&dollar;{SCP_CMD[@]}" "&dollar;SRC" "&dollar;DEST" && echo "File copied to &dollar;DEST"</code></pre>
        <p><strong>Notes / production tips:</strong> use SSH keys (no password), wrap in retry logic for flaky networks, or use rsync for resumable transfers (<code>rsync -avz -e 'ssh -i key' src user@host:/dest</code>).</p>
      </div>`},{question:"How do you automate image builds using scripts or pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated image build patterns</strong></h3>
        <p>Typical CI flow: checkout  run lints/tests  build image  tag with semver + commit SHA  push to registry  run image scan  promote.</p>
        <h4>Shell script example (CI-friendly)</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># build-and-push.sh
set -euo pipefail

IMAGE_NAME="myacr.azurecr.io/myapp"
TAG="&dollar;{1:- &dollar;(git rev - parse--short HEAD)}"   # default to short SHA

docker build - t "&dollar;{IMAGE_NAME}:&dollar;{TAG}".
docker build - t "&dollar;{IMAGE_NAME}:latest".
docker push "&dollar;{IMAGE_NAME}:&dollar;{TAG}"
docker push "&dollar;{IMAGE_NAME}:latest"

# Optional scan(using trivy)
trivy image--exit - code 1 --severity CRITICAL "&dollar;{IMAGE_NAME}:&dollar;{TAG}" || {
    echo "Vulnerability found! Failing pipeline."
  exit 1
} < /code></pre >
  <p><strong>CI note: </strong> Make CI agent login to registry via service principal / managed identity rather than storing creds on runner.</p>
  </div>`},{question:"How do you integrate Docker with CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CI/CD integration  short & practical snippets</strong></h3>

        <h4>Azure DevOps (YAML)  build & push to ACR</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>trigger:
  branches:
    include: [ main ]

pool:
  vmImage: 'ubuntu-latest'

variables:
  imageName: 'myapp'

steps:
- task: Docker@2
  displayName: Build and push
  inputs:
    command: buildAndPush
    repository: &dollar;(acrLoginServer)/&dollar;(imageName)
    dockerfile: Dockerfile
    tags: |
      &dollar;(Build.SourceVersion)
      latest
    containerRegistry: 'my-acr-service-connection'</code></pre>

        <h4>Jenkins (Declarative)  build & push</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>pipeline {
  agent any
  stages {
    stage('Build') {
      steps {
        script {
          def image = docker.build("myacr.azurecr.io/myapp:&dollar;{env.GIT_COMMIT.take(7)}")
          docker.withRegistry('https://myacr.azurecr.io', 'acr-creds') {
            image.push()
            image.push('latest')
          }
        }
      }
    }
  }
}</code></pre>

        <h4>GitHub Actions  build & push</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: azure/docker-login@v1
        with:
          login-server: myacr.azurecr.io
          username: &dollar;{{ secrets.ACR_USER }}
          password: &dollar;{{ secrets.ACR_PASS }}
      - name: Build and push
        run: |
          docker build -t myacr.azurecr.io/myapp:&dollar;{{ github.sha }} .
          docker push myacr.azurecr.io/myapp:&dollar;{{ github.sha }}</code></pre>

        <p><strong>Best practice:</strong> use service connections, managed identities, or built-in tokens  never echo registry creds in logs.</p>
      </div>`},{question:"How do you automate cleanup of old images/containers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated cleanup strategies</strong></h3>
        <p>Combine local host cleanup + registry retention policy to control disk and storage consumption.</p>

        <h4>Local cleanup (hosts / CI agents)</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># simple cron job script: /usr/local/bin/cleanup-docker.sh
#!/usr/bin/env bash
set -euo pipefail

# remove stopped containers
docker container prune -f

# remove dangling images
docker image prune -f

# remove unused images & volumes older than 72 hours
docker system prune -a --volumes --filter "until=72h" -f</code></pre>
        <p>Schedule it in cron (e.g., nightly): <code>0 3 * * * /usr/local/bin/cleanup-docker.sh &gt;/var/log/docker-cleanup.log 2>&1</code></p>

        <h4>Registry-side cleanup (ACR / Docker Registry)</h4>
        <ul style="margin-left:1.2rem;">
          <li>Use ACR retention / purge policies to automatically delete untagged images or images older than N days.</li>
          <li>Use lifecycle rules: tag immutability for semver/digest-based deploys, and keep only X latest tags for non-production branches.</li>
          <li>For on-prem registry, run scheduled scripts that call registry API to delete older images by tag/date.</li>
        </ul>

        <h4>Example: keep only last N tags in CI</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># pseudo-script idea: list tags, skip top N, delete older tags using registry API or az acr repository delete</code></pre>

        <p><strong>Pro tip:</strong> Never delete images that production might rely on  use immutability and promote-by-digest to ensure deployed images remain available even if tags are pruned.</p>
      </div>`}]},{title:"13. Git Integration & Code Management",questions:[{question:"How can you copy code from one branch to another in GitHub?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Copy code between branches  practical methods</strong></h3>
        <p>There are multiple ways depending on what you want (entire branch, a single commit, or selected files):</p>
        <ul style="margin-left:1.2rem;">
          <li>
            <strong>Merge entire branch</strong>  best when you want all changes:
            <pre><code># from target branch
git checkout target-branch
git merge source-branch
# resolve conflicts, commit, push
git push origin target-branch</code></pre>
          </li>
          <li>
            <strong>Cherry-pick one or more commits</strong>  pick specific commits:
            <pre><code>git checkout target-branch
git cherry-pick <commit-sha>       # repeat for multiple commits
git push origin target-branch</code></pre>
          </li>
          <li>
            <strong>Checkout specific files from another branch</strong>  copy selected files only:
            <pre><code>git checkout source-branch -- path/to/file1 path/to/dir/
git commit -m "Bring file1 from source-branch"
git push origin target-branch</code></pre>
          </li>
          <li>
            <strong>Interactive patch (select hunks)</strong>  when you need fine-grained control:
            <pre><code>git checkout target-branch
git checkout -p source-branch -- path/to/file   # choose hunks interactively
git commit -m "Selective patch from source-branch"
git push</code></pre>
          </li>
          <li>
            <strong>Via Pull Request (recommended for teams)</strong>  open a PR from source  target, run CI, get approvals, merge. Gives audit trail and review.
          </li>
        </ul>
        <p><strong>Pro tip:</strong> Use PRs for code review and traceability. Use cherry-pick for urgent hotfixes where PR workflows are too slow.</p>
      </div>`},{question:"How do you connect Docker build automation with GitHub repositories?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automate Docker builds from GitHub  common approaches</strong></h3>
        <p>Typical patterns: GitHub Actions (native), CI server (Jenkins/Azure DevOps), or Docker Hub automated builds.</p>

        <h4>1) GitHub Actions (recommended)</h4>
        <p>Example workflow: build image on push, tag with SHA, push to ACR or Docker Hub, run vulnerability scan.</p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>name: CI - Build & Push
on:
  push:
    branches: [ main ]
  pull_request:

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up QEMU (optional for multi-arch)
        uses: docker/setup-qemu-action@v2
      - name: Log in to ACR
        uses: azure/docker-login@v1
        with:
          login-server: myacr.azurecr.io
          username: &dollar;{{ secrets.ACR_USER }}
          password: &dollar;{{ secrets.ACR_PASS }}
      - name: Build and push
        run: |
          docker build -t myacr.azurecr.io/myapp:&dollar;{{ github.sha }} .
          docker push myacr.azurecr.io/myapp:&dollar;{{ github.sha }}</code></pre>

        <h4>2) Azure DevOps / Jenkins</h4>
        <p>Use pipeline tasks or Jenkins Docker plugin to build/push images. Use service connections or credentials stored securely in pipeline secrets.</p>

        <h4>3) Docker Hub Automated Builds</h4>
        <p>Link GitHub repo to Docker Hub and configure build rules  convenient but less flexible than Actions/ADO for complex workflows.</p>

        <p><strong>Best practices:</strong> tag images by commit SHA, use signed images or content digest in manifests, run image scans in pipeline, and avoid using <code>latest</code> in production deploys.</p>
      </div>`},{question:"How do you use Docker Hub or ACR as an artifact repository?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using Docker Hub / ACR as artifact registries</strong></h3>
        <p>Both Docker Hub and Azure Container Registry act as artifact stores for images. Use them to store, version, and distribute images to environments.</p>

        <h4>Push / Pull flow</h4>
        <pre><code># tag for registry
docker tag myapp:1.0 myacr.azurecr.io/myapp:1.0

# login (ACR example)
az acr login --name myacr

# push image
docker push myacr.azurecr.io/myapp:1.0

# pull image on target host
docker pull myacr.azurecr.io/myapp:1.0</code></pre>

        <h4>Registry features & best practices</h4>
        <ul style="margin-left:1.2rem;">
          <li><strong>Authentication:</strong> Use service principals / managed identities (ACR) or tokens/secrets. Avoid storing plaintext creds in code.</li>
          <li><strong>Tagging:</strong> Use semantic versions + commit SHA; use image digests in deployment manifests for immutability.</li>
          <li><strong>Retention/Lifecycle:</strong> Configure retention policies (ACR) to purge old, untagged images and save storage costs.</li>
          <li><strong>Scanning:</strong> Enable vulnerability scanning (ACR Tasks or integrated scanners like Trivy/Anchore).</li>
          <li><strong>Geo-replication:</strong> ACR Premium supports geo-replication for faster pulls; Docker Hub has rate limits  consider private registries for heavy usage.</li>
          <li><strong>Immutability:</strong> Prefer promoting images by digest instead of mutable tags in production deployments.</li>
        </ul>

        <p><strong>In Practice:</strong> I push CI images to ACR with both <code>semver</code> and <code>sha</code> tags, scans run automatically, and K8s manifests reference the image digest for deployment immutability.</p>
      </div>`}]},{title:"14. Advanced Docker Topics",questions:[{question:"What is a multi-stage build and when is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-stage builds  optimize build size & security</strong></h3>
        <p>
          Multi-stage builds let you use multiple <code>FROM</code> statements in a single Dockerfile, 
          copying only the required artifacts from the builder image to the final runtime image.
        </p>
        <p><strong>Why:</strong> reduces image size, removes compilers/tools, and improves security by keeping only production binaries.</p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># Example: Node.js build  lightweight runtime
FROM node:18 AS build
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build

FROM node:18-alpine
WORKDIR /app
COPY --from=build /app/dist ./dist
COPY --from=build /app/package*.json ./
CMD ["node", "dist/server.js"]</code></pre>
        <p><strong>In Practice:</strong> I use this for every production image  clean final layer, smaller attack surface, and faster pull times in CI/CD.</p>
      </div>`},{question:"What is Docker Swarm? How does it differ from Compose?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Docker Swarm vs Compose</strong></h3>
        <p><strong>Docker Swarm</strong> is Docker's native container orchestration tool  it manages multiple nodes, handles service discovery, scaling, and rolling updates.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Docker Compose</strong>  local multi-container setup, single-host only, great for development.</li>
          <li><strong>Swarm</strong>  multi-host clustering, high availability, load balancing, secrets, and rolling deployments.</li>
        </ul>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># Deploy a stack using Docker Swarm
docker swarm init
docker stack deploy -c docker-compose.yml myapp</code></pre>
        <p><strong>In Practice:</strong> I used Swarm in small-scale setups before moving to Kubernetes  Swarm is simpler but lacks advanced scheduling and ecosystem integration.</p>
      </div>`},{question:"What is the role of Docker in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Dockers role in Kubernetes</strong></h3>
        <p>
          Docker acts as the <strong>container runtime</strong> for Kubernetes  it builds and runs containers from images.
        </p>
        <p>
          Kubernetes abstracts the runtime using the <strong>Container Runtime Interface (CRI)</strong>.
          Historically, Kubernetes used Docker as its default runtime (via dockershim), 
          but now supports <strong>containerd</strong> and <strong>CRI-O</strong> natively.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Docker builds  produces OCI-compliant images</li>
          <li>K8s pulls those images  runs them via containerd</li>
          <li>Docker CLI  developer-friendly build/test interface</li>
        </ul>
        <p><strong>In Practice:</strong> I still use Docker for local builds and testing, then deploy those images to AKS via containerd runtime  both are fully compatible.</p>
      </div>`},{question:"How do you optimize Docker image builds for speed and security?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Optimizing image builds  speed + security</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>multi-stage builds</strong> to remove build-time dependencies.</li>
          <li>Leverage <strong>Docker layer caching</strong>  place <code>RUN apt-get install</code> and <code>COPY package.json</code> early in the file.</li>
          <li>Use minimal base images (<code>alpine</code>, <code>distroless</code>).</li>
          <li>Run as <strong>non-root user</strong> (<code>USER appuser</code>).</li>
          <li>Use <strong>.dockerignore</strong> to exclude unnecessary files from the build context.</li>
          <li>Scan for vulnerabilities with <strong>Trivy</strong> or <strong>Docker Scout</strong>.</li>
          <li>Use <strong>BuildKit</strong> for parallel builds:
            <pre><code>DOCKER_BUILDKIT=1 docker build -t app:latest .</code></pre>
          </li>
          <li>Sign images with <strong>cosign</strong> or <strong>notary</strong> for supply-chain integrity.</li>
        </ul>
        <p><strong>In Practice:</strong> I always keep base images lightweight and rebuild only when dependencies change. 
        BuildKit + Trivy in CI reduces build time by ~40% and ensures CVEs are caught early.</p>
      </div>`},{question:"How do you handle image versioning and dependency updates?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Versioning strategy for images</strong></h3>
        <p><strong>Image tagging patterns:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><code>app:1.0.0</code>  semantic version for releases.</li>
          <li><code>app:1.0.0-commitSHA</code>  immutable reference per build.</li>
          <li><code>app:latest</code>  development/testing only (never for prod).</li>
        </ul>
        <p><strong>Dependency updates:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Automate base image updates using Dependabot or Renovate Bot.</li>
          <li>Use <code>FROM ubuntu:22.04</code> instead of <code>FROM ubuntu:latest</code> to avoid breaking builds.</li>
          <li>Rebuild regularly with CI to pull latest patches even if app code hasnt changed.</li>
          <li>Run vulnerability scans after rebuilds and fail builds for critical CVEs.</li>
        </ul>
        <p><strong>In Practice:</strong> Every image tag in my pipelines follows <code>vX.Y.Z-<SHA></code>. 
        We promote images by digest between environments (DEV  QA  PROD) for full traceability.</p>
      </div>`},{question:"How do you integrate vulnerability scanning into image pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating image scanning into CI/CD</strong></h3>
        <p>Security scanning should run after build and before deployment. You can use CLI tools or cloud-native scanners.</p>
        <h4>Example using <strong>Trivy</strong> in CI pipeline:</h4>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code>steps:
  - name: Scan image for vulnerabilities
    run: |
      trivy image --exit-code 1 --severity HIGH,CRITICAL myacr.azurecr.io/myapp:&dollar;{{ github.sha }}</code></pre>
        <h4>Azure DevOps built-in scanning:</h4>
        <p>ACR can automatically scan pushed images (with Defender for Cloud).  
        You can also trigger ADO gate policies based on scan results before promoting images to PROD.</p>
        <ul style="margin-left:1.2rem;">
          <li>Integrate scanning task post-build.</li>
          <li>Fail pipeline if critical CVEs are found.</li>
          <li>Store scan reports as pipeline artifacts for audit.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate Trivy in Docker build stage + ACR Defender scans in PROD registry  gives double assurance and traceability.</p>
      </div>`}]},{title:"15. Real-World Scenarios & Interview Situations",questions:[{question:"Scenario: Container startup fails due to missing environment variables  how do you fix it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Container failed due to missing environment variables</strong></h3>
        <p><strong>Diagnosis Steps:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Check logs: <code>docker logs &lt;container_id&gt;</code>  look for <em>key not found</em> or <em>undefined variable</em> errors.</li>
          <li>Inspect envs inside container: <code>docker exec -it &lt;container_id&gt; env</code></li>
          <li>Review <code>docker run</code> or Compose YAML  see if <code>environment:</code> or <code>--env-file</code> is missing.</li>
        </ul>
        <p><strong>Fixes:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Add env vars in Compose:</li>
          <pre><code>environment:
  DB_HOST: mysql
  DB_USER: admin
  DB_PASS: &dollar;{DB_PASS}</code></pre>
          <li>Or load via <code>--env-file .env</code> when running manually.</li>
          <li>For secrets, mount from Azure Key Vault or secret manager instead of plaintext envs.</li>
        </ul>
        <p><strong>Pro tip:</strong> Add validation logic (like <code>ENTRYPOINT ["/bin/sh", "-c", "test -n \\"&dollar;DB_HOST\\" || exit 1; exec node app.js"]</code>) to fail fast and detect missing vars early in CI.</p>
      </div>`},{question:"Scenario: Application container is unreachable  how do you check its network configuration?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>App unreachable  debugging Docker networking</strong></h3>
        <p><strong>Step-by-step troubleshooting:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Check container status: <code>docker ps</code>  ensure its running.</li>
          <li>Inspect port mapping: <code>docker port &lt;container_id&gt;</code>  confirm host:container ports (e.g. 8080:80).</li>
          <li>Verify app binding: inside container run <code>netstat -tulnp</code> or <code>curl localhost:&lt;port&gt;</code>  app must listen on <code>0.0.0.0</code>.</li>
          <li>Check Docker network connectivity: <code>docker network inspect bridge</code>  ensure container IP reachable.</li>
          <li>Run a test container in same network: <code>docker run -it --network app-net busybox ping myapp</code></li>
          <li>For Compose, confirm all services share same <code>networks:</code> section.</li>
        </ul>
        <p><strong>Root causes:</strong> app listening only on 127.0.0.1, wrong port exposed, container not attached to network, or firewall blocking traffic.</p>
      </div>`},{question:"Scenario: Disk space is full  how do you clean up unused images?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling full disk due to Docker artifacts</strong></h3>
        <p><strong>Quick clean-up commands:</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># Remove stopped containers
docker container prune -f

# Remove dangling (untagged) images
docker image prune -f

# Remove all unused images, networks, and build cache
docker system prune -a -f --volumes</code></pre>
        <p><strong>Preventive actions:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Set up cron cleanup scripts on self-hosted agents.</li>
          <li>Use smaller base images (alpine/distroless) to reduce cache size.</li>
          <li>Implement retention policies for images in ACR/Docker Hub.</li>
        </ul>
        <p><strong>In Practice:</strong> I monitor <code>/var/lib/docker</code> size via Prometheus alerts and auto-trigger prune jobs on CI runners when usage crosses 80%.</p>
      </div>`},{question:"Scenario: Vulnerability scanner flags your image  what actions do you take?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Responding to image vulnerability alerts</strong></h3>
        <p><strong>Remediation process:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Review scanner report (Trivy, ACR Defender, Aqua, etc.)  identify <em>critical/high</em> CVEs.</li>
          <li>Check if vulnerability is from base image (e.g., <code>ubuntu:20.04</code>) or app libraries.</li>
          <li>Rebuild image with updated base layer or dependencies:
            <pre><code>docker build --no-cache -t app:patched .</code></pre>
          </li>
          <li>Apply package updates (<code>apt-get update && apt-get upgrade -y</code> or <code>npm audit fix</code>).</li>
          <li>Scan again post-build to validate fix.</li>
          <li>For false positives  cross-check NVD or vendor site and suppress only verified safe ones.</li>
        </ul>
        <p><strong>Preventive measures:</strong> Pin base images by digest, use Renovate/Dependabot for version bumps, integrate image scanning in every CI build.</p>
      </div>`},{question:"Scenario: Multi-stage build image isnt copying final artifact  how do you debug the build?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Debugging multi-stage build copy issues</strong></h3>
        <p><strong>Common issue:</strong> <code>COPY --from=build /app/dist ./</code> fails or missing files in final image.</p>
        <p><strong>Debugging steps:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Inspect intermediate build layers with <code>--target</code>:
            <pre><code>docker build --target build -t build-stage .</code></pre>
          </li>
          <li>Run the intermediate image to verify files exist:
            <pre><code>docker run -it build-stage ls /app/dist</code></pre>
          </li>
          <li>Ensure correct paths and spelling  e.g. <code>/app/dist</code> vs <code>/app/build</code>.</li>
          <li>Confirm <code>WORKDIR</code> consistency between stages.</li>
          <li>Use <code>--progress=plain</code> for verbose build logs.</li>
        </ul>
        <p><strong>In Practice:</strong> I always specify absolute paths and use <code>ls</code> checks in build stage before final copy. Most multi-stage issues are path mismatches or missing artifacts after npm/mvn build steps.</p>
      </div>`}]}];function Qw(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("docker"),a=gm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-cyan-500 to-blue-500 shadow-glow",children:l.jsx(Ua,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Docker & Containerization"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Containerization, Images, Networking & Docker Compose"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Comprehensive Docker interview preparation covering container fundamentals, commands, images, volumes, networking, and Compose."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:gm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:(u==null?void 0:u.question)||"",b=typeof u=="object"&&u&&"answer"in u&&!!u.answer,v=typeof u=="object"&&u&&"answerHtml"in u&&!!u.answerHtml,h=b?u.answer:null,w=b||v;return l.jsx("div",{className:"border-l-2 border-cyan-500/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-cyan-500 font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),w&&l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:u.answerHtml}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:h})})]})]})]})},f)})})})]},p))})]})]})}const Z3=Object.freeze(Object.defineProperty({__proto__:null,default:Qw},Symbol.toStringTag,{value:"Module"})),mm=[{title:"1. Monitoring Overview & Tools",questions:[{question:"What monitoring tools have you worked on (Azure Monitor, Log Analytics, Grafana, Prometheus, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring Tools Ive Worked With</strong></h3>
        <p><strong>Cloud-native:</strong> Azure Monitor, Log Analytics, Application Insights, Azure Alerts</p>
        <p><strong>Open-source:</strong> Prometheus (metrics), Grafana (visualization), Loki (logs), Alertmanager (alert routing)</p>
        <p><strong>Security/Performance:</strong> Microsoft Defender for Cloud, Splunk, SonarQube for code quality.</p>
        <p><strong>In Practice:</strong> I integrate Azure Monitor for resource health + Prometheus for container metrics + Grafana dashboards for visualization.  
        Application Insights handles app-level tracing and failures for AKS-hosted microservices.</p>
      </div>`},{question:"What is Azure Monitor, and how does it collect metrics and logs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Monitor Overview</strong></h3>
        <p><strong>Azure Monitor</strong> is a unified service that collects, analyzes, and visualizes telemetry data from Azure resources, applications, and infrastructure.</p>
        <ul style="margin-left:1.2rem;">
          <li>Collects <strong>metrics</strong> (performance counters, CPU, memory, disk).</li>
          <li>Collects <strong>logs</strong> (activity logs, diagnostic logs, app traces).</li>
          <li>Stores logs in <strong>Log Analytics Workspace</strong> and metrics in <strong>Azure Metrics Database</strong>.</li>
        </ul>
        <p><strong>Architecture:</strong> Agents (AMA or Log Analytics Agent) collect telemetry  sent to workspace  visualized in dashboards or Grafana.</p>
        <p><strong>In Practice:</strong> I enable Monitor for all core resources (VMs, AKS, App Gateway) and forward data to centralized workspace for alerting and retention control.</p>
      </div>`},{question:"What is the purpose of a Log Analytics Workspace?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Log Analytics Workspace  centralized logging backend</strong></h3>
        <p>Log Analytics Workspace is the <strong>data storage and query engine</strong> for Azure Monitor logs.</p>
        <ul style="margin-left:1.2rem;">
          <li>Stores logs from VMs, AKS, App Gateway, Key Vault, etc.</li>
          <li>Used for querying via <strong>KQL (Kusto Query Language)</strong>.</li>
          <li>Acts as a data source for Azure Dashboards, Sentinel, and Grafana.</li>
        </ul>
        <p><strong>In Practice:</strong> I use a single workspace per environment (DEV, UAT, PROD) with retention set based on compliance.  
        Dashboards and alerts are built directly over the workspace queries.</p>
      </div>`},{question:"How do you configure diagnostic settings to capture activity logs efficiently?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Diagnostic Settings Configuration</strong></h3>
        <p>Diagnostic settings determine where logs/metrics from Azure resources are sent.</p>
        <ul style="margin-left:1.2rem;">
          <li>Enable via <strong>Azure Portal  Resource  Diagnostic Settings</strong>.</li>
          <li>Choose log categories (Administrative, Security, Performance).</li>
          <li>Send to:
            <ul>
              <li> Log Analytics Workspace (for analysis)</li>
              <li> Storage Account (for archival)</li>
              <li> Event Hub (for streaming to SIEM/Splunk)</li>
            </ul>
          </li>
        </ul>
        <p><strong>Best Practice:</strong> Capture only required categories to reduce ingestion cost  avoid all-logs by default.</p>
        <p><strong>CLI Example:</strong></p>
        <pre><code>az monitor diagnostic-settings create \\
  --name appDiag \\
  --resource &lt;resource-id&gt; \\
  --workspace &lt;log-analytics-id&gt; \\
  --logs '[{"category":"AppServiceHTTPLogs","enabled":true}]'</code></pre>
      </div>`},{question:"How do you monitor VMs, Application Gateway, AKS, or Storage Accounts in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resource-Specific Monitoring Setup</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>VMs</strong>  Enable <strong>Azure Monitor Agent</strong> for metrics/logs; track CPU, memory, and disk via <code>Insights  VM Performance</code>.</li>
          <li> <strong>Application Gateway</strong>  Enable <strong>access, performance, WAF logs</strong> in diagnostics; view via Log Analytics.</li>
          <li> <strong>AKS</strong>  Integrate with <strong>Container Insights</strong> to collect pod/node metrics (CPU, memory, restarts).</li>
          <li> <strong>Storage Accounts</strong>  Enable <strong>read/write/delete metrics</strong> and use <strong>Storage Insights</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I centralize all logs into one workspace and use dashboards for node CPU, pod restarts, and App Gateway errors (4xx/5xx tracking).</p>
      </div>`},{question:"What is Application Insights, and how is it used for end-to-end tracing?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Application Insights  deep app telemetry</strong></h3>
        <p><strong>Application Insights</strong> is part of Azure Monitor that provides distributed tracing, performance, and usage telemetry for apps.</p>
        <ul style="margin-left:1.2rem;">
          <li>Tracks requests, dependencies (DB/API calls), and exceptions.</li>
          <li>Correlates transactions across microservices using <strong>operationId</strong>.</li>
          <li>Visualizes response time, failure rates, and live metrics.</li>
        </ul>
        <p><strong>In Practice:</strong> I instrument Node.js and .NET apps using the Application Insights SDK for tracing slow APIs and dependency bottlenecks, linking to dashboards for live performance visibility.</p>
      </div>`},{question:"Whats the difference between metrics-based alerts and log-based alerts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Metrics vs Log Alerts  key difference</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Metrics-based alerts:</strong> real-time, numeric, low latency (< 1 min).  
          Used for CPU%, Memory, Latency thresholds.</li>
          <li><strong>Log-based alerts:</strong> KQL query-driven, more flexible but slower (510 min delay).  
          Used for error patterns, event count, custom logic.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code># Metrics alert  CPU > 80%
az monitor metrics alert create --name highCPU --resource vm1 ...

# Log alert  failed requests > 50 in 5 min
requests | where resultCode >= 500
| summarize count() by bin(TimeGenerated, 5m)</code></pre>
      </div>`},{question:"How do you reduce the data ingestion and retention costs in Azure Monitor?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cost Optimization in Azure Monitor</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Collect only required categories (disable verbose diagnostic logs).</li>
          <li>Use <strong>Data Collection Rules (DCR)</strong> to filter metrics/logs before ingestion.</li>
          <li>Reduce <strong>log retention period</strong> (default 30 days  set to 7/14 days).</li>
          <li>Archive old logs to <strong>Storage Account</strong> (cheaper long-term).</li>
          <li>Use <strong>Azure Monitor Cost Analysis Workbook</strong> to visualize ingestion cost by table/resource.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive reduced ingestion cost by 40% by filtering unnecessary VM logs (Heartbeat, Perf) using DCRs and archiving raw logs beyond 30 days.</p>
      </div>`},{question:"What are best practices to avoid duplicate or unnecessary logs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Best Practices to Avoid Log Noise</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Disable overlapping diagnostic settings on the same resource.</li>
          <li>Use centralized workspaces  avoid sending same logs to multiple workspaces.</li>
          <li>Filter with DCRs and categories.</li>
          <li>Log only <strong>Warning</strong> and <strong>Error</strong> level unless debugging.</li>
          <li>Set structured logging (JSON format) for easy filtering.</li>
        </ul>
        <p><strong>In Practice:</strong> I use structured logs and custom log categories in AKS/Function Apps to keep ingestion focused and searchable.</p>
      </div>`},{question:"How do you correlate logs across multiple services in Log Analytics?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cross-service log correlation</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>operation_Id</strong> or <strong>correlation_Id</strong> fields from Application Insights.</li>
          <li>Join logs from multiple tables with KQL joins:
            <pre><code>AppRequests
| join kind=inner AppDependencies on operation_Id
| project TimeGenerated, name, dependencyType, duration</code></pre>
          </li>
          <li>Use <strong>Transaction Search</strong> or <strong>End-to-End View</strong> in App Insights for visual trace.</li>
          <li>Link multiple workspaces via <strong>cross-resource queries</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I tag every transaction with a unique <code>correlationId</code> (passed via headers), making it easy to trace requests across API, DB, and downstream microservices.</p>
      </div>`}]},{title:"2. FinOps  Cloud Cost Optimization & Governance",questions:[{question:"Have you been involved in FinOps activities to help clients save cloud costs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>FinOps involvement  optimizing Azure spend</strong></h3>
        <p>Yes  Ive been actively involved in <strong>FinOps (Financial Operations)</strong> to monitor and optimize cloud costs.</p>
        <ul style="margin-left:1.2rem;">
          <li>Analyzed <strong>Azure Cost Management + Billing</strong> dashboards weekly.</li>
          <li>Right-sized VMs and storage tiers using Azure Advisor recommendations.</li>
          <li>Set up <strong>budgets, alerts</strong>, and <strong>tag-based chargeback</strong> for project owners.</li>
          <li>Enabled <strong>auto-shutdown</strong> for non-prod VMs and scaled AKS clusters based on usage.</li>
        </ul>
        <p><strong>Impact:</strong> Reduced monthly Azure cost by 2530% across DEV/UAT environments.</p>
      </div>`},{question:"What is FinOps, and why is it important in cloud cost governance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>FinOps  bridging Finance & DevOps</strong></h3>
        <p><strong>FinOps (Financial Operations)</strong> is a collaborative practice that brings together Finance, Engineering, and Operations to manage cloud spending efficiently.</p>
        <ul style="margin-left:1.2rem;">
          <li>Ensures <strong>cost visibility</strong> across teams.</li>
          <li>Promotes <strong>accountability</strong>  teams own their cloud usage.</li>
          <li>Drives <strong>optimization</strong> using automation, governance, and usage insights.</li>
        </ul>
        <p><strong>In Practice:</strong> I implement FinOps with tagging standards, cost reports, and right-sizing policies to maintain cost control and forecast accuracy.</p>
      </div>`},{question:"How do you track and analyze monthly Azure spend for various subscriptions?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Tracking monthly Azure spend</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Cost Management + Billing</strong> for subscription-wise analysis.</li>
          <li>Apply <strong>filters by resource group, tag, service, or location</strong>.</li>
          <li>Download reports (CSV/Excel) for trend analysis.</li>
          <li>Enable <strong>Cost Analysis Workbook</strong> for real-time dashboards.</li>
          <li>Automate exports to <strong>Storage Account</strong> for long-term tracking.</li>
        </ul>
        <p><strong>In Practice:</strong> I compare actual vs forecast spend monthly and present optimization recommendations (e.g., VM size reduction, auto-scaling, or reserved instances).</p>
      </div>`},{question:"What tools or dashboards do you use for cost visibility (Cost Management + Billing, Azure Advisor, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Tools for Cloud Cost Visibility</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure Cost Management + Billing</strong>  for overall spend analysis and trend monitoring.</li>
          <li><strong>Azure Advisor</strong>  recommends right-sizing, idle resource cleanup, and reserved instance savings.</li>
          <li><strong>Azure Policy</strong>  to enforce tagging and cost governance.</li>
          <li><strong>Power BI</strong>  for custom cost visualization dashboards.</li>
        </ul>
        <p><strong>In Practice:</strong> I use a shared Power BI dashboard for management to visualize spend by department, environment, and region in real time.</p>
      </div>`},{question:"How do you allocate costs between projects, environments, or departments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cost Allocation Strategy</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>tagging standards</strong> like <code>project=app1</code>, <code>env=prod</code>, <code>owner=teamA</code>.</li>
          <li>Group resources under <strong>dedicated resource groups or subscriptions</strong>.</li>
          <li>Generate cost reports by tag in <strong>Cost Management</strong> portal.</li>
          <li>Export cost data to <strong>Power BI</strong> for departmental allocation.</li>
        </ul>
        <p><strong>In Practice:</strong> I ensure every resource is tagged properly and use tag-based filtering for accurate chargeback to business units.</p>
      </div>`},{question:"How do you use tags and resource groups for effective cost tracking?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Tagging & Resource Group Strategy</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Mandatory tags: <code>Environment</code>, <code>Project</code>, <code>Owner</code>, <code>CostCenter</code>.</li>
          <li>Applied via <strong>Azure Policy</strong> to enforce tagging rules.</li>
          <li>Resource Groups logically separate billing boundaries (per app or environment).</li>
        </ul>
        <p><strong>In Practice:</strong> I use policy-based automation to ensure every resource has tags before deployment  this makes cost roll-up simple and auditable.</p>
      </div>`},{question:"What is the use of Azure Budgets, and how do you configure alerts for threshold breaches?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Budgets & Alerts</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Create budgets under <strong>Cost Management  Budgets</strong>.</li>
          <li>Define amount, time range (monthly/quarterly), and scope (subscription/resource group).</li>
          <li>Set alert thresholds  e.g., 80%, 100%, 120%.</li>
          <li>Notifications sent via <strong>Action Groups (email, Teams, webhook)</strong>.</li>
        </ul>
        <p><strong>CLI Example:</strong></p>
        <pre><code>az consumption budget create --name DevBudget \\
  --amount 500 --time-grain monthly \\
  --resource-group dev-rg --category cost</code></pre>
        <p><strong>In Practice:</strong> I configure monthly budgets per environment and send alerts to finance & DevOps leads once usage crosses 80%.</p>
      </div>`},{question:"How do you ensure cost accountability within DevOps teams?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cost Accountability Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Each DevOps team owns its resource groups and associated tags.</li>
          <li>Enable <strong>role-based access (RBAC)</strong>  owners can view but not overspend.</li>
          <li>Share monthly cost reports and trends per team.</li>
          <li>Link optimization KPIs to sprint retrospectives.</li>
        </ul>
        <p><strong>In Practice:</strong> Teams are responsible for deleting idle resources post-release and optimizing workloads within their own budgets.</p>
      </div>`},{question:"What are Reserved Instances and Savings Plans in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reserved Instances vs Savings Plans</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Reserved Instances (RIs):</strong> Commit to a specific VM type and region for 1 or 3 years for up to 70% savings.</li>
          <li><strong>Savings Plans:</strong> Commit to spend (e.g., &dollar;500/month) instead of a specific VM type  offers more flexibility with 3060% savings.</li>
        </ul>
        <p><strong>In Practice:</strong> I analyze steady workloads (Prod VMs, App Services) for RIs and use Savings Plans for variable or evolving environments.</p>
      </div>`},{question:"How do you choose between Pay-As-You-Go and Reserved pricing models?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Choosing the right pricing model</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Pay-As-You-Go:</strong> For non-production or short-term workloads.</li>
          <li><strong>Reserved Instances:</strong> For long-running, predictable workloads.</li>
          <li><strong>Hybrid:</strong> Combine  PAYG for dynamic scaling + RI for baseline capacity.</li>
        </ul>
        <p><strong>In Practice:</strong> I analyze usage patterns via Azure Advisor before switching workloads from PAYG to Reserved models.</p>
      </div>`},{question:"How do you automate cost reports or set alerts for anomalies?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Cost Governance</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Enable <strong>Scheduled Exports</strong> from Cost Management to Storage Account or Log Analytics.</li>
          <li>Use <strong>Logic Apps</strong> or <strong>Power Automate</strong> to send cost reports via email/Teams.</li>
          <li>Set anomaly detection alerts using <strong>Azure Monitor Metrics</strong> or <strong>FinOps dashboards</strong>.</li>
          <li>Integrate <strong>Power BI</strong> for automated visualization refreshes.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive built weekly Logic App flows that email top 10 costly resources and highlight anomalies if spend rises >20% week-over-week.</p>
      </div>`}]},{title:"3. Cost Optimization of Azure Infrastructure",questions:[{question:"What are your strategies for cost optimization in Azure infrastructure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>High-level strategies</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Right-size resources (VMs, DBs) based on telemetry, not guesswork.</li>
          <li>Use reserved instances / savings plans for predictable workloads.</li>
          <li>Auto-scale services and schedule automatic shutdown for non-prod.</li>
          <li>Choose appropriate storage tiers and lifecycle policies.</li>
          <li>Remove orphaned resources (disks, IPs, snapshots) and enforce tagging + policies.</li>
          <li>Use Spot instances for fault-tolerant, non-critical workloads.</li>
          <li>Optimise networking to reduce egress and NAT Gateway usage.</li>
          <li>Automate cost reviews, budgets and alerts via Azure Cost Management + Advisor.</li>
        </ul>
        <p><strong>In Practice:</strong> I run weekly reports (metrics + cost) and apply Advisor recommendations after validation  this gave 2030% savings in my projects.</p>
      </div>`},{question:"How do you decide on the right VM size and SKU for workloads?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Right-sizing methodology</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Collect performance metrics (CPU, memory, disk IO, network) for 24 weeks using Azure Monitor/Insights.</li>
          <li>Identify steady-state and peak usage  focus on 95th percentile, not spikes.</li>
          <li>Map workload characteristics to VM families (memory-optimized for DBs, compute-optimized for CPU-heavy jobs).</li>
          <li>Test a smaller SKU in staging and run load tests to validate performance.</li>
          <li>Apply auto-scale rules (scale-out) rather than over-provisioning a single large VM where possible.</li>
        </ol>
        <p><strong>Commands / Tools:</strong> use Azure Advisor and VM Insights; example to get VM metrics:</p>
        <pre><code>az monitor metrics list --resource /subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.Compute/virtualMachines/{vmName} --metric "Percentage CPU"</code></pre>
      </div>`},{question:"How do you utilize auto-scaling and shutdown schedules for non-prod environments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Auto-scale & Scheduled Shutdown</strong></h3>
        <p><strong>Auto-scale:</strong> Configure VMSS/AKS Horizontal Pod Autoscaler/App Service autoscale rules based on CPU/memory/queue length.</p>
        <pre><code># Example: create autoscale rule for VMSS (simplified)
az monitor autoscale create --resource-group rg --resource /subscriptions/{sub}/resourceGroups/rg/providers/Microsoft.Compute/virtualMachineScaleSets/myvmss --name autoscale --min-count 1 --max-count 5 --count 1</code></pre>
        <p><strong>Scheduled shutdown (non-prod):</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use Azure Automation Start/Stop VMs solution or DevTest Labs auto-shutdown.</li>
          <li>Use tags (e.g., <code>env:dev</code>) and automation runbooks to discover and stop VMs at 20:00 and start at 08:00.</li>
        </ul>
        <pre><code># Example: schedule via Automation Runbook (pseudo)
- query VMs with tag env=dev
- stop VMs at 20:00, start at 08:00</code></pre>
        <p><strong>In Practice:</strong> Scheduling non-prod environments reduced costs ~40% without affecting productivity.</p>
      </div>`},{question:"How do you identify and remove idle or underutilized resources?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Detect & cleanup idle resources</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use Azure Advisor (Idle VM recommendations), Cost Analysis (spike detection), and activity logs to find low-usage resources.</li>
          <li>Query metrics for zero CPU/low network for a defined period (e.g., 30 days).</li>
          <li>Identify unattached disks, unused public IPs, orphaned NICs and snapshots using CLI or Resource Graph.</li>
        </ul>
        <pre><code># Find unattached managed disks
az disk list --query "[?managedBy==null].{name:name,resourceGroup:resourceGroup}" -o table

# Find unused public IPs
az network public-ip list --query "[?ipAddress==null].{name:name,rg:resourceGroup}" -o table</code></pre>
        <p><strong>Cleanup process:</strong> tag candidates as <code>review: true</code>, notify owners, wait 7 days, then delete via automation.</p>
      </div>`},{question:"What is the benefit of using Azure Spot VMs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Spot VMs  cost-effective compute</strong></h3>
        <p>Spot VMs offer large discounts (up to 90%) for interruptible workloads where eviction is acceptable.</p>
        <ul style="margin-left:1.2rem;">
          <li>Best for: batch jobs, CI workers, stateless workloads, and large compute tasks that can resume.</li>
          <li>Trade-off: Azure can evict Spot VMs when capacity is needed  architect to handle interruptions.</li>
          <li>Use cases: auto-scale pools for CI, big-data transient compute, worker nodes for non-critical processing.</li>
        </ul>
      </div>`},{question:"How do you optimize storage costs (access tiers, lifecycle management, redundancy)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Storage cost optimization</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Choose correct access tier: Hot for frequently accessed, Cool/Archive for infrequent data.</li>
          <li>Enable lifecycle management rules to move blobs to cooler tiers or delete after retention period.</li>
          <li>Choose redundancy level appropriately (LRS vs ZRS vs GRS) depending on RPO/RTO and cost trade-offs.</li>
          <li>Enable soft delete and snapshot lifecycle but clean orphaned snapshots regularly to avoid hidden costs.</li>
        </ul>
        <pre><code># Example: set lifecycle rule via az (pseudo)
az storage account management-policy create --account-name mystorage --resource-group rg --policy '{
  "rules":[
    {"name":"move-to-cool","enabled":true,"definition":{...}}
  ]
}'</code></pre>
        <p><strong>In Practice:</strong> I applied lifecycle rules to move backups older than 30 days to Archive and reduced storage costs by ~60% for backup blobs.</p>
      </div>`},{question:"How do you optimize networking costs (bandwidth, NAT Gateway, data egress)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Networking cost controls</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Minimize cross-region data egress by colocating resources in same region or using VNet peering with care.</li>
          <li>Use Azure CDN to cache static content and reduce egress from origin storage or App Service.</li>
          <li>Reduce NAT Gateway costs by using per-subnet NAT or shared NAT (consolidate outbound via fewer NATs).</li>
          <li>Use Private Link/Service Endpoints to avoid public egress when accessing PaaS services.</li>
          <li>Monitor data transfer patterns and identify heavy egress flows; optimize or compress payloads where possible.</li>
        </ul>
        <p><strong>In Practice:</strong> Moving large file transfers to an internal data plane and using CDN cut bandwidth cost significantly for static assets.</p>
      </div>`},{question:"How do you monitor unattached disks, orphaned IPs, and unused PIPs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Detect unattached resources</strong></h3>
        <pre><code># Unattached managed disks
az disk list --query "[?managedBy==null].{name:name,rg:resourceGroup}" -o table

# Public IPs without IP address assigned (or not attached)
az network public-ip list --query "[?ipAddress==null]|[?ipConfiguration==null].{name:name,rg:resourceGroup}" -o table

# Unassociated NICs
az network nic list --query "[?virtualMachine==null].{name:name,rg:resourceGroup}" -o table</code></pre>
        <p><strong>Automation:</strong> schedule Resource Graph queries + Logic App or Runbook to notify owners and auto-delete after approval/TTL.</p>
      </div>`},{question:"How do you use Azure Advisor recommendations for cost savings?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Advisor for cost recommendations</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Enable Advisor and review Cost recommendations: right-size VMs, shutdown unused resources, buy reserved instances.</li>
          <li>Validate recommendations  Advisor is a guide, not always 100% accurate; test smaller sizes in staging before applying.</li>
          <li>Use Advisor API to fetch recommendations and automate tagging or ticket creation for owners to action.</li>
        </ul>
        <pre><code># get advisor recommendations (example)
az advisor recommendation list --category Cost</code></pre>
        <p><strong>In Practice:</strong> We used Advisor to discover low-util VMs and saved by switching them to burstable sizes and applying RIs to baseline workloads.</p>
      </div>`},{question:"How do you optimize backup and monitoring costs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Optimize backup & monitoring costs</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>For backups: choose proper retention windows and tiering; use incremental/differential backups and storage tiering for old backups.</li>
          <li>For snapshots: schedule cleanups and avoid excessive snapshot frequency for non-critical disks.</li>
          <li>Monitoring: filter diagnostic settings (avoid sending verbose logs everywhere), reduce retention in Log Analytics for non-prod, and archive old logs to Storage.</li>
          <li>Use sampling for Application Insights (reduce telemetry ingestion) and route raw logs to cheaper blob storage for long-term archival.</li>
        </ul>
        <pre><code># Set Log Analytics retention (example)
az monitor log-analytics workspace update --resource-group rg --workspace-name workspace --retention-time 30</code></pre>
        <p><strong>In Practice:</strong> I lowered non-prod retention to 7 days and archived raw logs, reducing monitoring cost without losing critical telemetry for production.</p>
      </div>`}]},{title:"4. Monitoring Cost Optimization Scenarios",questions:[{question:"After implementing Azure Monitor, the cost increased  how would you optimize it without disabling monitoring?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Option A : Optimize Azure Monitor costs without turning off monitoring</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Filter ingestion at source:</strong> Use Data Collection Rules (DCR) or resource diagnostic settings to only ingest required log categories (avoid noisy debug/verbose logs).</li>
          <li><strong>Use sampling:</strong> Enable sampling in Application Insights for high-volume telemetry (reduce ingestion while keeping visibility into errors/trends).</li>
          <li><strong>Shorten retention for non-prod:</strong> Lower Log Analytics retention for DEV/UAT (e.g., 714 days) and keep PROD longer.</li>
          <li><strong>Route selectively:</strong> Send detailed logs to Storage Account for archival, forward only necessary logs to Log Analytics, and stream critical events to Event Hubs/SIEM as needed.</li>
          <li><strong>Reduce metric frequency:</strong> Increase collection interval for low-value metrics or disable very-frequent custom metrics.</li>
          <li><strong>Use dedicated workspaces strategically:</strong> Use separate workspaces for noisy systems (or non-prod) to avoid high-cost aggregation in a single workspace, or use workspace per environment pattern where it makes sense.</li>
          <li><strong>Review alerts & thresholds:</strong> Consolidate alert rules, increase evaluation intervals, and avoid firing noisy alerts  use metric alerts for realtime thresholds and log alerts for complex patterns.</li>
          <li><strong>Archive old data:</strong> Export older logs to cheap Blob storage (archive) instead of keeping long retention in Log Analytics.</li>
          <li><strong>Automate and monitor cost impact:</strong> Build a small dashboard showing ingestion & retention costs and alert when ingestion spikes; iterate on filters accordingly.</li>
        </ol>
        <p><strong>Quick wins:</strong> filter diagnostic categories, enable Application Insights sampling, and shorten non-prod retention  these three often cut ingestion quickly without losing critical observability.</p>
      </div>
      <li>
      <div class="answer-rich">
        <h3> <strong>Option B : Optimizing Azure Monitor cost without losing observability</strong></h3>
        <p>Azure Monitor is powerful, but improper configuration leads to high ingestion and retention costs. Instead of turning it off, I would fine-tune the setup:</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Use custom log ingestion filters:</strong> Only collect logs that add value  disable verbose categories like AuditLogs, Heartbeat, or Metrics from non-prod.</li>
          <li> <strong>Adjust retention periods:</strong> Reduce retention to 714 days for DEV/UAT while keeping 3090 days for PROD-critical systems.</li>
          <li> <strong>Route logs selectively:</strong> Send detailed logs to Azure Storage (cheap archival), and forward only alerts or critical logs to Log Analytics.</li>
          <li> <strong>Enable sampling in Application Insights:</strong> Reduce telemetry ingestion by keeping representative data samples (e.g., 2030%)  helps maintain insights with lower costs.</li>
          <li> <strong>Store only critical metrics:</strong> Keep performance metrics (CPU, latency, failure rate) and drop low-value diagnostic noise from testing components.</li>
          <li> <strong>Workspace design optimization:</strong> Split large centralized workspaces  use separate workspaces for DEV, QA, and PROD to isolate and control cost-heavy sources.</li>
          <li> <strong>Review metric alert frequency:</strong> Reduce evaluation frequency (e.g., from 1 min  5 min) and disable redundant alerts on test environments.</li>
        </ul>
        <p><strong>In Practice:</strong> After applying these optimizations for a healthcare client, the Azure Monitor ingestion cost dropped by <strong>~45%</strong> without losing visibility.</p>
      </div>`},{question:"Your companys cloud costs are rising rapidly  how would you reduce them without affecting performance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Option A : Practical cost-reduction steps that preserve performance</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Identify top spenders:</strong> Use Azure Cost Management and Cost Analysis to list top resources/subscriptions by spend and drill into contributors (VMs, storage, networking).</li>
          <li><strong>Right-size compute:</strong> Analyze usage (95th percentile) and move oversized VMs to more appropriate SKUs; prefer scale-out over single large instance when possible.</li>
          <li><strong>Apply auto-scaling & schedules:</strong> Autoscale stateless apps and schedule shutdowns for non-prod environments (night/weekend shutdowns).</li>
          <li><strong>Use discounts:</strong> Purchase Reserved Instances or Savings Plans for stable workloads; use Spot VMs for interruptible jobs (CI runners, batch).</li>
          <li><strong>Optimize storage:</strong> Move infrequently accessed data to Cool/Archive, enable lifecycle rules, and remove orphaned snapshots/blobs.</li>
          <li><strong>Network cost controls:</strong> Reduce cross-region egress, use CDN for static assets, consolidate NAT gateways and review expensive PIP/NAT usage.</li>
          <li><strong>Remove zombie resources:</strong> Detect and reclaim unattached disks, unused IPs, idle VMs, and stale load balancer rules via Resource Graph/automation runbooks.</li>
          <li><strong>Enforce governance:</strong> Use tagging, policies, and chargeback to hold teams accountable; block creation of expensive SKUs in non-approved subscriptions via Azure Policy.</li>
          <li><strong>Automate reporting & anomaly detection:</strong> Schedule cost exports, create FinOps dashboards, and enable anomaly detection alerts to catch sudden spend spikes early.</li>
          <li><strong>Incremental rollout & validate:</strong> Test changes (right-sizing, RIs) in a staging window, measure performance and user impact, then roll out at scale to avoid regressions.</li>
        </ol>
        <p><strong>Outcome-focused approach:</strong> prioritize high-impact items (top 10 spenders) first, apply automation and governance to prevent recurrence, and keep stakeholders informed with cost dashboards & weekly reports.</p>
      </div>
      <li>
      <div class="answer-rich">
        <h3> <strong>Option B :Practical steps to reduce Azure cost without degrading performance</strong></h3>
        <p>Rather than random cuts, I follow a structured FinOps-driven approach  visibility  optimization  governance:</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Identify top spenders:</strong> Use Azure Cost Management & Billing dashboard to list top consuming services, resource groups, or subscriptions.</li>
          <li> <strong>Auto-shutdown idle VMs:</strong> Apply schedules (Azure Automation or Logic Apps) to stop DEV/QA VMs during off-hours automatically.</li>
          <li> <strong>Use auto-scaling:</strong> Implement horizontal auto-scaling for App Services, AKS, or VMSS so compute scales dynamically with workload demand.</li>
          <li> <strong>Region selection:</strong> Deploy non-critical or DR workloads in cost-effective Azure regions with similar latency profiles.</li>
          <li> <strong>Reserved & Spot Instances:</strong> Convert steady workloads to Reserved Instances or Savings Plans and use Spot VMs for CI or batch jobs.</li>
          <li> <strong>Cleanup unused assets:</strong> Periodically audit and remove unattached disks, stale snapshots, unused NICs, idle load balancers, and public IPs.</li>
          <li> <strong>Network optimization:</strong> Consolidate NAT Gateways, limit public egress, and use Private Links to avoid costly data transfer.</li>
          <li> <strong>FinOps governance:</strong> Implement dashboards, budgets, and anomaly alerts to catch sudden cost spikes early and ensure team accountability.</li>
        </ul>
        <p><strong>Result:</strong> With these actions, our DevOps team achieved ~30% sustained monthly savings while maintaining 99.9% SLA and full observability.</p>
      </div>`}]},{title:"5. Agent Management & Pipeline Performance vs Cost",questions:[{question:"How many agents would you create to run 50 pipelines or applications as fast as possible while minimizing cost?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sizing agent fleet  balance speed vs cost</strong></h3>
        <p>
          Theres no one-size-fits-all number, but for ~50 pipelines the pragmatic approach is to optimize concurrency and reuse rather than provisioning 50 agents.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Rule of thumb:</strong> Start with <strong>610 high-performance self-hosted agents</strong> sized for your workloads (e.g., 8 vCPU / 16 GB RAM) and allow multiple parallel jobs per agent if CPU/RAM permits (24 concurrent jobs). This gives ~1240 parallel job capacity without 50 separate VMs.</li>
          <li> <strong>Use agent pools:</strong> Create dedicated pools for critical pipelines (release, prod) and shared pools for low-priority or experimental jobs to avoid resource contention.</li>
          <li> <strong>Prefer parallel job configuration over more agents:</strong> Tune job-level parallelism (matrix builds, job dependencies) so a single agent runs several light jobs concurrently rather than one heavy job blocking many agents.</li>
          <li> <strong>Use ephemeral/container agents:</strong> Use containerized agents or ephemeral VMs for fast, repeatable environments  this reduces drift and speeds up cold-starts.</li>
          <li> <strong>Hybrid model:</strong> Keep Microsoft-hosted (burst) or cloud-hosted agents for unpredictable spikes and self-hosted pool for steady baseline throughput.</li>
          <li> <strong>Cost optimizations:</strong> Run self-hosted agents on Spot VMs or scale-sets with autoscale rules, shut down agents during nights/weekends for non-critical pools, and consolidate build images to reduce startup time.</li>
        </ul>
        <p><strong>Example calc:</strong> If average pipeline job takes 10 minutes and each agent can safely run 3 jobs concurrently, 8 agents  ~24 parallel jobs  can complete 50 typical short jobs in 23 waves. For long-running builds, increase agent count or use burst capacity (cloud-hosted) selectively.</p>
      </div>`},{question:"How do you ensure pipeline performance while maintaining cost efficiency?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Performance without waste  practical patterns</strong></h3>
        <p>Focus on reducing work and improving parallelism so pipelines run faster with fewer resources.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Cache dependencies:</strong> Use pipeline caching (npm/ Maven/ pip/cache tasks or Azure Pipelines Cache task) to avoid re-downloading dependencies every run.</li>
          <li> <strong>Use container-based agents:</strong> Prebuild agent images with common toolchains so startup time is minimal.</li>
          <li> <strong>Incremental builds:</strong> Build only changed modules (monorepo partial builds), use build output caching and remote cache (e.g., Gradle/ Bazel remote cache) to reduce build time.</li>
          <li> <strong>Parallelize smartly:</strong> Use matrix and parallel jobs for unit tests/linters while serializing slow operations (integration tests, deployments).</li>
          <li> <strong>Profile pipelines:</strong> Monitor pipeline duration and identify the slowest tasks. Optimize or split long tasks into smaller parallelizable units.</li>
          <li> <strong>Remove or schedule expensive jobs:</strong> Run heavy E2E or security scans on nightly or pre-release pipelines instead of every push; keep fast quality gates on PRs.</li>
          <li> <strong>Autoscale & ephemeral agents:</strong> Scale self-hosted agents up during peak (use scale sets) and down afterwards; use ephemeral agents for burst capacity on demand (cost-effective vs keeping idle agents).</li>
          <li> <strong>Use spot/ephemeral for non-critical:</strong> Run CI-heavy but non-prod jobs on Spot instances or ephemeral containers  big cost wins if you accept interruptions.</li>
          <li> <strong>Track & alert:</strong> Collect pipeline metrics (queue time, agent utilization, job duration) and alert when agent utilization is low (wasted) or queue time high (need more/better agents).</li>
        </ul>
        <p><strong>YAML example snippet (Azure Pipelines):</strong></p>
        <pre style="background:#111;padding:.6rem;border-radius:.4rem;color:#fff;"><code># use container job + cache task example
jobs:
- job: Build
  pool:
    name: 'self-hosted-pool'
  container: mcr.microsoft.com/dotnet/sdk:7.0
  steps:
    - task: Cache@2
      inputs:
        key: 'npm | "&dollar;(Agent.OS)" | package-lock.json'
        path: &dollar;(Pipeline.Workspace)/.npm
    - script: npm ci
    - script: npm run build</code></pre>
        <p><strong>Summary:</strong> combine caching, containerized agents, autoscaling, and smarter scheduling  that gives you high pipeline throughput with controlled cost. Monitor agent utilization and tune the fleet iteratively based on real telemetry.</p>
      </div>`}]},{title:"6. Governance, Automation & Reporting",questions:[{question:"How do you automate cost and utilization reports for stakeholders?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Azure cost & utilization reporting</strong></h3>
        <p>
          Automated reporting is essential for FinOps maturity. I configure periodic cost exports and integrate them with visualization tools for actionable insights.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Schedule exports:</strong> Enable <strong>Azure Cost Management  Scheduled Exports</strong> to automatically send cost & usage data (CSV/JSON) to a Storage Account or Log Analytics workspace daily or weekly.</li>
          <li> <strong>Automation Runbooks:</strong> Use <strong>Azure Automation or Logic Apps</strong> to fetch data via REST API (<code>/providers/Microsoft.CostManagement/exports</code>) and email or post summaries to Teams/Slack channels.</li>
          <li> <strong>Visualization:</strong> Connect <strong>Power BI</strong> or <strong>Grafana</strong> with exported data for trend reports  e.g., cost per RG, team, or environment.</li>
          <li> <strong>Custom KPIs:</strong> Include metrics like VM utilization %, cost per environment, savings via Reserved Instances, and underutilized resources.</li>
          <li> <strong>Alerts:</strong> Trigger notifications if cost exceeds budgets or anomalies are detected using <strong>Azure Monitor Alerts</strong>.</li>
        </ul>
        <p><strong>In my current project:</strong> We send automated weekly cost reports to stakeholders via Logic App + Power BI, reducing manual tracking by 90%.</p>
      </div>`},{question:"How do you ensure governance and policy enforcement using Azure Policy?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Policy-based governance and compliance enforcement</strong></h3>
        <p>
          Governance starts with defining compliance rules at the subscription or management group level. I use <strong>Azure Policy</strong> to automate guardrails.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Define & assign policies:</strong> Restrict resource locations, enforce tagging, control SKU usage, and ensure diagnostic settings are enabled.</li>
          <li> <strong>Policy initiatives:</strong> Group related policies (e.g., Security Baseline, Networking Standards) into a single initiative assigned to management groups.</li>
          <li> <strong>Remediation tasks:</strong> Enable automatic remediation to fix non-compliant resources  e.g., auto-apply tags or enable encryption.</li>
          <li> <strong>Compliance reporting:</strong> Use the Policy Compliance dashboard to track adherence across subscriptions.</li>
          <li> <strong>Integration:</strong> Connect with <strong>Azure Defender</strong> and <strong>Log Analytics</strong> for security & compliance visibility.</li>
        </ul>
        <p><strong>Example:</strong> We enforced cost governance by policy  blocking premium SKUs and forcing cost-center tag at creation. This ensured accountability per project.</p>
      </div>`},{question:"How do you handle multi-subscription cost governance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-subscription cost governance strategy</strong></h3>
        <p>
          I implement centralized management using <strong>Management Groups</strong> and shared policies while maintaining cost visibility per subscription.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Management hierarchy:</strong> Organize subscriptions under <strong>Management Groups</strong> (Production, Non-Prod, Sandbox) for unified control.</li>
          <li> <strong>Centralized billing:</strong> Enable consolidated billing for aggregated reporting while tagging resources for cost ownership.</li>
          <li> <strong>Tagging strategy:</strong> Enforce cost-center, owner, environment, and project tags for chargeback.</li>
          <li> <strong>Shared policies:</strong> Apply consistent governance policies via initiatives across all subscriptions.</li>
          <li> <strong>Cross-sub reporting:</strong> Use Power BI or Cost Management API to merge data across subscriptions for a unified dashboard.</li>
        </ul>
        <p><strong>Result:</strong> This structure enabled us to maintain granular cost control while ensuring governance at scale.</p>
      </div>`},{question:"How do you forecast cloud costs for upcoming projects?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Forecasting Azure cost for new projects</strong></h3>
        <p>Forecasting combines estimation tools, historical data, and usage patterns.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Pricing Calculator:</strong> Estimate compute, storage, and network costs before provisioning resources.</li>
          <li> <strong>Use historical patterns:</strong> Analyze previous workloads (usage trends, scaling metrics) to project monthly consumption.</li>
          <li> <strong>Scenario simulation:</strong> Model cost impact for different scaling profiles (steady vs burst).</li>
          <li> <strong>Reserved Instance planning:</strong> Simulate saving options for predictable workloads using RI & Savings Plan calculators.</li>
          <li> <strong>Communicate buffer:</strong> Always plan a 1015% buffer for region price variance or unexpected scale events.</li>
        </ul>
      </div>`},{question:"What are the best FinOps KPIs (unit cost per app, utilization rate, reserved instance coverage)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Key FinOps KPIs for Cloud Cost Efficiency</strong></h3>
        <p>Tracking these metrics ensures financial accountability and operational efficiency:</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Unit cost per app/service:</strong> Cost per environment, team, or feature delivered.</li>
          <li> <strong>Utilization rate:</strong> CPU/Memory/Storage utilization percentage  helps identify idle resources.</li>
          <li> <strong>Reserved Instance coverage:</strong> % of workloads covered by RI/Savings Plans vs on-demand.</li>
          <li> <strong>Idle resource ratio:</strong> Portion of resources with <10% utilization for potential cleanup.</li>
          <li> <strong>Cost per user/session:</strong> Especially for SaaS workloads to track profitability.</li>
        </ul>
      </div>`},{question:"How do you enable chargeback or showback models for cost transparency?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Chargeback/Showback cost transparency</strong></h3>
        <p>To promote accountability, I implement tagging and reporting for team-wise visibility.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Mandatory tagging policy:</strong> Enforce Project, Environment, Owner, and CostCenter tags on all resources.</li>
          <li> <strong>Centralized cost collection:</strong> Aggregate costs via Azure Cost Management APIs and group them by tag values.</li>
          <li> <strong>Dashboards:</strong> Create Power BI dashboards to display per-team or per-application spending.</li>
          <li> <strong>Showback:</strong> Monthly reports highlight each teams cost to drive optimization discussions.</li>
          <li> <strong>Chargeback:</strong> Optionally integrate with billing to deduct internal cost allocations for shared infrastructure.</li>
        </ul>
      </div>`},{question:"How do you integrate monitoring dashboards with cost dashboards (Grafana, Power BI, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating monitoring and cost visibility</strong></h3>
        <p>Integration ensures that performance metrics and financial data align for better decision-making.</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Grafana:</strong> Connect Azure Monitor and Cost Management APIs as data sources to create unified dashboards (e.g., CPU utilization vs cost per service).</li>
          <li> <strong>Power BI:</strong> Merge exported cost data with monitoring logs from Log Analytics for a combined operational-financial view.</li>
          <li> <strong>Azure Workbook integration:</strong> Use Azure Workbooks to correlate resource health with cost anomalies.</li>
          <li> <strong>Automated refresh:</strong> Configure daily or hourly refresh using Power BI Gateway or Grafana scheduling.</li>
        </ul>
        <p><strong>Outcome:</strong> Stakeholders can visualize both performance and cost impact in one place  enabling data-driven optimization decisions.</p>
      </div>`}]},{title:"7. Real-Time FinOps Automation  Alerts, Budgets & Logic Apps Integration",questions:[{question:"How do you create a budget in Azure and trigger real-time alerts when thresholds are breached?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create Budgets & Real-time Alerts</strong></h3>
        <p>Use <strong>Azure Cost Management  Budgets</strong> to create a budget scoped to subscription / resource group / tag and attach <strong>Action Groups</strong> for notifications or automation.</p>
        <p><strong>Steps (Portal):</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Go to Cost Management  Budgets  + Add.</li>
          <li>Scope: choose subscription / resource group / tag filter.</li>
          <li>Set amount & recurrence (monthly/quarterly).</li>
          <li>Add alerts at thresholds (e.g., 70%, 85%, 100%).</li>
          <li>Attach an Action Group (email / webhook / Logic App / Automation Runbook).</li>
        </ol>
        <p><strong>CLI example to create budget (simplified):</strong></p>
        <pre><code>az consumption budget create \\
  --name "ProdBudget" \\
  --amount 5000 \\
  --time-grain Monthly \\
  --scope /subscriptions/{subId} \\
  --notifications '[{"enabled":true,"threshold":80,"operator":"GreaterThan","contactEmails":["ops@company.com"]}]'</code></pre>
        <p><strong>Why:</strong> Budgets = first line of defense. Attach Action Groups to call Logic Apps or webhooks for automated remediation (scale down, tag, or notify finance + owners).</p>
      </div>`},{question:"What is an Action Group and how do you use it to automate FinOps responses?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Action Groups for automated responses</strong></h3>
        <p>Action Groups are reusable notification/webhook configurations used by Alerts & Budgets to send notifications or call automation endpoints.</p>
        <p><strong>Common actions:</strong> email, SMS, webhook, Logic App, Azure Function, ARM template deployment, Automation Runbook.</p>
        <p><strong>Typical pattern:</strong> Budget alert -> Action Group (calls Logic App webhook) -> Logic App executes remediation (stop VMs / create ticket / notify chat).</p>
        <pre><code># create action group (example)
az monitor action-group create \\
  --name FinOpsActionGroup \\
  --resource-group infra-rg \\
  --short-name finops \\
  --action webhook myLogicAppWebhook https://prod-00.westeurope.logic.azure.com/... </code></pre>
        <p><strong>In Practice:</strong> I make separate Action Groups per environment (prod / non-prod) so automation differs by env (notify-only in prod vs auto-shutdown in non-prod).</p>
      </div>`},{question:"How do you build a Logic App that automatically remediates cost spikes (example: stop non-prod VMs when budget threshold reached)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Logic App: Budget Alert  Auto-shutdown non-prod VMs</strong></h3>
        <p><strong>High-level flow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Trigger: HTTP request (called by Action Group) or built-in "When a budget threshold is exceeded".</li>
          <li>Parse payload to get scope and threshold.</li>
          <li>Query resources by tag (e.g., env=dev) using Azure Resource Manager connector or call REST API.</li>
          <li>For each VM found: call Azure REST / Runbook to stop VM (or create ticket if production).</li>
          <li>Send summary to Teams/Email and create audit log entry in Storage or Log Analytics.</li>
        </ol>
        <p><strong>Example pseudo-steps in Logic App designer:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>HTTP Request trigger (schema matching budget alert).</li>
          <li>Condition: if scope contains 'non-prod' or check tag via "List resources".</li>
          <li>For each resource: "Invoke Azure Automation Runbook" (Stop-VM runbook) or "Call Azure REST API" to POST /providers/Microsoft.Compute/virtualMachines/{vm}/powerOff?api-version=... .</li>
          <li>Post message to Teams channel with results.</li>
        </ol>
        <p><strong>Runbook snippet (PowerShell) that Logic App can call:</strong></p>
        <pre><code>param(
  [string]&dollar;ResourceGroup,
  [string]&dollar;VMName
)

# Stop VM (assumes Run As account or Managed Identity with rights)
Stop-AzVM -ResourceGroupName &dollar;ResourceGroup -Name &dollar;VMName -Force -ErrorAction Stop</code></pre>
        <p><strong>Why:</strong> This pattern gives immediate remediation and audit trail while allowing safe exceptions for prod resources.</p>
      </div>`},{question:"How do you detect cost anomalies automatically and route them to a FinOps playbook?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automatic anomaly detection  FinOps playbook</strong></h3>
        <p>Use Azure Cost Management's anomaly detection + Azure Monitor anomaly detection (custom logs/metrics) to raise alerts and trigger playbooks.</p>
        <p><strong>Flow:</strong> Cost anomaly detected  Action Group/Alert  Logic App / Function  Playbook executes steps (tag, scale, notify, create ticket).</p>
        <p><strong>Implementation tips:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Enable Cost Management anomaly detection and connect to an Action Group webhook.</li>
          <li>Build a Logic App playbook that runs investigation queries (Resource Graph queries, Cost API calls) to identify top services causing spike.</li>
          <li>Auto-create a Jira/ServiceNow ticket with details, owner, remediation suggestions, and attach runbook logs.</li>
        </ul>
        <p><strong>Quick example Resource Graph query (find top cost by resource group):</strong></p>
        <pre><code>Resources
| where type =~ 'microsoft.compute/virtualmachines' 
| project name, resourceGroup, subscriptionId, properties
| summarize count() by resourceGroup</code></pre>
      </div>`},{question:"How do you secure and authorize automation (Logic Apps / Runbooks) so remediation actions run safely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure automation & least privilege</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use Managed Identities (system-assigned or user-assigned) for Logic Apps and Runbooks  avoid using service principal secrets.</li>
          <li>Grant least-privilege RBAC roles (e.g., Virtual Machine Contributor on a resource-group scope) to the identity.</li>
          <li>Use approval gates: for prod actions, have Logic App send approval request to on-call before executing remediation.</li>
          <li>Audit every action: log inputs, outputs, and identity used to Log Analytics or Storage for compliance.</li>
          <li>Use Key Vault to store any credentials and reference them via Managed Identity from Logic App / Runbook.</li>
        </ul>
        <p><strong>In Practice:</strong> I never give auto-remediation Runbooks subscription-wide rights  scope them to resource groups tagged for automation and require manual approval for production.</p>
      </div>`},{question:"How do you export cost data and automate reports for stakeholders (daily/weekly/monthly)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated cost exports & reports</strong></h3>
        <p>Use Scheduled Exports in Cost Management to push cost data to a Storage Account or Log Analytics, then trigger Logic App / Function to format and mail or push to Power BI.</p>
        <p><strong>Steps:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Create Scheduled Export in Cost Management (daily/weekly)  destination Storage Account / CSV.</li>
          <li>Use Logic App to pick up new blob, transform into a summary (top 10 spenders), and push to Teams/Email or to Power BI dataset via REST API.</li>
          <li>Alternatively, use Cost Management REST API to query costs in near real-time and construct dashboards programmatically.</li>
        </ol>
        <pre><code># example: create an export via az (pseudo)
az costmanagement export create --name weekly-export --scope /subscriptions/{sub} --recurrence Weekly --definition '{...}'</code></pre>
        <p><strong>Outcome:</strong> Stakeholders receive digestible reports automatically and C-level sees weekly trend visuals without manual work.</p>
      </div>`},{question:"How do you implement safe automated RI/Savings Plan recommendations or tagging based on usage patterns?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automate RI planning & tagging safely</strong></h3>
        <p>Automated purchase of RIs is risky  I implement an assisted automation approach:</p>
        <ul style="margin-left:1.2rem;">
          <li>Run periodic analysis (Cost Management API or Advisor) to find stable VM usage candidate groups.</li>
          <li>Logic App compiles recommendations and creates a draft proposal (savings %, beneficiaries, cost impact).</li>
          <li>Send proposal to FinOps queue (Teams/Email) for human approval. After approval, automation can call Reservation APIs to purchase RIs via a service principal with finance approval.</li>
          <li>Alternatively, auto-tag candidate resources as <code>ri_candidate:true</code> to track before purchase.</li>
        </ul>
        <p><strong>Why:</strong> This ensures the business/finance sign-off before committing to 13 year contracts.</p>
      </div>`},{question:"What monitoring / observability should you have for the automation itself (playbooks & runbooks)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Observability for automation</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Log all automation runs to a central Log Analytics workspace with run details, identity used, duration, success/failure, and error message.</li>
          <li>Create alerts on failed runbooks or Logic App runs (e.g., >3 failures in 1 hour) and notify on-call.</li>
          <li>Implement retries with exponential backoff for transient failures and record retry attempts.</li>
          <li>Keep a human-readable audit trail (blob or table) for compliance and post-mortem.</li>
        </ul>
        <p><strong>In Practice:</strong> I build a small Automation Health dashboard in Grafana/Power BI showing run success rate, average remediation time, and costs saved by automation actions.</p>
      </div>`}]}];function Yw(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("finops"),a=mm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-emerald-500 to-teal-500 shadow-glow",children:l.jsx(yu,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Monitoring & FinOps"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Cost Optimization, Cloud Governance & Financial Operations"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Master FinOps practices, cost optimization strategies, and cloud financial management."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:mm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:u.question,b=typeof u=="object"&&"answer"in u?u.answer:null,v=typeof u=="object"&&"answerHtml"in u?u.answerHtml:null;return l.jsx("div",{className:"border-l-2 border-emerald-500/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-emerald-500 font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),b||v?l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:v.replace(/\$\{/g,"$${")}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:b})})]}):null]})]})},f)})})})]},p))})]})]})}const X3=Object.freeze(Object.defineProperty({__proto__:null,default:Yw},Symbol.toStringTag,{value:"Module"})),hm=[{title:"1. Git Fundamentals",questions:[{question:"What is Git and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git  Distributed Version Control System</strong></h3>
        <p>Git is a <strong>distributed version control system (DVCS)</strong> that tracks changes in source code, allowing multiple developers to collaborate efficiently.</p>
        <p><strong>Why we use Git:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Tracks full history of code changes for rollback and audit.</li>
          <li>Enables team collaboration with branching and merging.</li>
          <li>Supports offline work  local commits sync later to remote.</li>
          <li>Resolves conflicts when multiple developers modify same codebase.</li>
          <li>Integrates easily with CI/CD tools and platforms like GitHub or Azure DevOps.</li>
        </ul>
        <p><strong>In practice:</strong> I use Git daily for feature branching, PR reviews, tagging releases, and maintaining clean commit history across environments.</p>
      </div>`},{question:"What is GitHub?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>GitHub  Cloud Hosting for Git Repositories</strong></h3>
        <p>GitHub is a <strong>cloud-based platform</strong> that hosts Git repositories and provides collaboration features for developers.</p>
        <ul style="margin-left:1.2rem;">
          <li>Acts as the <strong>remote repository</strong> where code is stored centrally.</li>
          <li>Supports <strong>Pull Requests, Issues, Actions (CI/CD)</strong>, and Project Boards.</li>
          <li>Enables <strong>team access control</strong> and <strong>branch protection rules</strong>.</li>
        </ul>
        <p><strong>Example:</strong> Developers push local changes  GitHub triggers Actions pipeline  deploys to Azure or Kubernetes.</p>
      </div>`},{question:"What is GitFlow?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>GitFlow  Branching Strategy for Release Management</strong></h3>
        <p>GitFlow is a <strong>branching model</strong> that defines how features, releases, and hotfixes are managed in a structured workflow.</p>
        <p><strong>Typical branches:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>main/master:</strong> production-ready code.</li>
          <li><strong>develop:</strong> integration branch for upcoming release.</li>
          <li><strong>feature/*:</strong> each feature development.</li>
          <li><strong>release/*:</strong> pre-production stabilization.</li>
          <li><strong>hotfix/*:</strong> urgent bug fixes on production.</li>
        </ul>
        <p><strong>In practice:</strong> I follow GitFlow for enterprise projects  ensures stable main branch, and controlled release cycles with PR approvals.</p>
      </div>`},{question:"What problem does Git solve in software development?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Problems Git Solves</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Eliminates code overwrites  multiple developers can safely work on same project.</li>
          <li>Provides full <strong>change tracking</strong>  who changed what and when.</li>
          <li>Supports <strong>branch-based development</strong>  isolating features or bug fixes.</li>
          <li>Makes <strong>rollback</strong> and version comparison easy.</li>
          <li>Improves <strong>collaboration & deployment pipelines</strong> through integration with CI/CD.</li>
        </ul>
        <p><strong>Example:</strong> I can revert a single faulty commit or cherry-pick specific changes to fix production without affecting other branches.</p>
      </div>`},{question:"How does Git differ from other version control systems like SVN?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git vs SVN</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Distributed vs Centralized:</strong> Git has full local copy of repo; SVN depends on central server.</li>
          <li><strong>Offline work:</strong> Git supports commits and diffs offline; SVN requires constant connectivity.</li>
          <li><strong>Branching:</strong> Git branches are lightweight and instant; SVN branching is heavy and slow.</li>
          <li><strong>Performance:</strong> Git operations are faster due to local computation.</li>
          <li><strong>History & Merge:</strong> Gits SHA-based commit history and 3-way merge make conflict resolution efficient.</li>
        </ul>
        <p><strong>In real use:</strong> Git offers better scalability and parallel development  Ive used it for large infra-as-code projects with 30+ active contributors.</p>
      </div>`},{question:"What are commits, branches, and repositories in Git?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Key Git Concepts</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Commit:</strong> A snapshot of staged changes with a unique SHA ID  forms the project history.</li>
          <li><strong>Branch:</strong> A movable pointer to commits, used for parallel feature or bug fix work.</li>
          <li><strong>Repository (repo):</strong> The complete project structure tracked by Git  contains commits, branches, and tags.</li>
        </ul>
        <p><strong>Example:</strong> I usually create a new branch for each feature, commit regularly with atomic messages, and raise a pull request for merge.</p>
      </div>`},{question:"What is the difference between local repository and remote repository?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Local vs Remote Repository</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Local repo:</strong> Exists on your machine. You can commit, branch, and revert offline.</li>
          <li><strong>Remote repo:</strong> Hosted on a server (like GitHub, GitLab, or Azure DevOps) for collaboration.</li>
          <li><strong>Sync commands:</strong> 
            <ul>
              <li><code>git push</code>  send local commits to remote.</li>
              <li><code>git pull</code>  fetch + merge latest remote changes.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In real usage:</strong> I always sync my local dev branch before pushing to avoid conflicts during PR merges.</p>
      </div>`},{question:"What is the basic workflow of Git (init  add  commit  push  pull)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Basic Git Workflow</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>git init</strong>  Initialize a new Git repository.</li>
          <li><strong>git add .</strong>  Stage changes for commit.</li>
          <li><strong>git commit -m "message"</strong>  Save snapshot locally.</li>
          <li><strong>git remote add origin &lt;url&gt;</strong>  Link to remote repo.</li>
          <li><strong>git push origin main</strong>  Push commits to remote branch.</li>
          <li><strong>git pull</strong>  Fetch and merge latest remote updates.</li>
        </ol>
        <pre><code># Example
git init
git add .
git commit -m "initial commit"
git remote add origin https://github.com/org/repo.git
git push -u origin main</code></pre>
        <p><strong>In practice:</strong> This is my daily dev loop  commit small, meaningful changes and push regularly to maintain clean sync with remote.</p>
      </div>`}]},{title:"2. Git & Version Control",questions:[{question:"What is Git and why do we use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git  Distributed Version Control</strong></h3>
        <p>Git is a <strong>distributed version control system</strong> used to manage code history and collaboration. It keeps a full local copy of the repository so developers can work offline and merge changes later.</p>
        <ul style="margin-left:1.2rem;">
          <li>Tracks every change in code with commit history.</li>
          <li>Supports parallel development via branches.</li>
          <li>Enables rollback, conflict resolution, and audit trails.</li>
          <li>Integrates easily with CI/CD pipelines for automation.</li>
        </ul>
        <p><strong>In practice:</strong> I use Git daily to manage infrastructure code, feature branches, and PR-based deployments with CI/CD triggers.</p>
      </div>`},{question:"What is Git cherry-pick?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Cherry-pick  Apply Specific Commits</strong></h3>
        <p><code>git cherry-pick</code> is used to <strong>apply a specific commit</strong> from one branch onto another without merging the full branch.</p>
        <pre><code># Example
git checkout main
git cherry-pick a1b2c3d</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Useful for migrating only critical fixes or features.</li>
          <li>Common in production hotfix workflows.</li>
        </ul>
        <p><strong>In real use:</strong> I cherry-pick production hotfix commits from release branch to main without merging the full dev history.</p>
      </div>`},{question:"Difference between Git pull and Git fetch.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Pull vs Git Fetch</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>git fetch:</strong> Downloads commits/branches from remote but does NOT merge  safe to review changes first.</li>
          <li><strong>git pull:</strong> Fetches + automatically merges into current branch.</li>
        </ul>
        <pre><code># Example
git fetch origin
git merge origin/main  # Manual merge
# vs
git pull origin main   # Fetch + merge in one step</code></pre>
        <p><strong>Best practice:</strong> I prefer <code>fetch</code> first in production repos to review diffs before merging.</p>
      </div>`},{question:"What is Git merge?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Merge  Combine Branches</strong></h3>
        <p><code>git merge</code> integrates changes from one branch into another, creating a new merge commit.</p>
        <pre><code>git checkout develop
git merge feature/login</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Keeps history of both branches (non-linear).</li>
          <li>Used for stable integration in GitFlow.</li>
        </ul>
        <p><strong>In practice:</strong> I use merge for feature  develop consolidation with code review via pull requests.</p>
      </div>`},{question:"What is Git rebase?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Rebase  Linear History Rewrite</strong></h3>
        <p><code>git rebase</code> re-applies commits from one branch on top of another to create a linear commit history.</p>
        <pre><code>git checkout feature/login
git rebase develop</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Makes history cleaner and easier to read.</li>
          <li>Avoid rebase on shared/public branches (can rewrite history).</li>
        </ul>
        <p><strong>In practice:</strong> I rebase feature branches before raising PRs to keep commit history linear and conflict-free.</p>
      </div>`},{question:"What does git commit --amend do?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Amend Last Commit</strong></h3>
        <p><code>git commit --amend</code> lets you modify the most recent commit (message or staged files).</p>
        <pre><code>git add missed-file.txt
git commit --amend --no-edit</code></pre>
        <p><strong>In practice:</strong> I use it to fix typos or missed files before pushing to remote  avoids unnecessary commits.</p>
      </div>`},{question:"How to resolve Git merge conflicts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resolving Merge Conflicts</strong></h3>
        <p>Conflicts occur when two branches modify the same lines of code. Git stops merge until manually resolved.</p>
        <ol style="margin-left:1.2rem;">
          <li>Open conflicted files  look for <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code> markers.</li>
          <li>Choose correct changes or merge manually.</li>
          <li>Stage resolved files using <code>git add .</code>.</li>
          <li>Commit with <code>git commit</code> to complete merge.</li>
        </ol>
        <p><strong>In practice:</strong> I prefer VS Code merge editor  shows side-by-side diff and reduces manual mistakes.</p>
      </div>`},{question:"What is Git Flow and branching strategy?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Flow  Standard Branching Model</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>main:</strong> stable production branch.</li>
          <li><strong>develop:</strong> integration branch for upcoming releases.</li>
          <li><strong>feature/*:</strong> individual feature work.</li>
          <li><strong>release/*:</strong> staging branch before production.</li>
          <li><strong>hotfix/*:</strong> urgent fix directly from main.</li>
        </ul>
        <p><strong>In practice:</strong> I follow GitFlow for large projects  helps keep main stable and supports controlled CI/CD promotion.</p>
      </div>`},{question:"How to recover deleted commits in Git?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recovering Deleted Commits</strong></h3>
        <p>Git keeps history references even after deletion. Use <code>git reflog</code> to find lost commits.</p>
        <pre><code>git reflog
git checkout &lt;commit-id&gt;</code></pre>
        <p><strong>In practice:</strong> Ive used reflog multiple times to recover accidentally reset or amended commits safely.</p>
      </div>`},{question:"What is a pull request and why is it important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pull Request (PR)  Code Review Mechanism</strong></h3>
        <p>A Pull Request is a <strong>merge request</strong> raised to integrate changes from one branch into another, with peer review.</p>
        <ul style="margin-left:1.2rem;">
          <li>Enables <strong>code review and approval workflow</strong>.</li>
          <li>Runs CI/CD checks before merge.</li>
          <li>Ensures branch protection and auditability.</li>
        </ul>
        <p><strong>In practice:</strong> I use PR templates enforcing reviewers, tags, and linked Jira tickets for traceability.</p>
      </div>`},{question:"How to secure credentials committed by mistake?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Removing Sensitive Data</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Delete secret file and commit change.</li>
          <li>Use <code>git filter-branch</code> or <code>BFG Repo Cleaner</code> to purge from history.</li>
          <li>Revoke exposed credentials immediately.</li>
          <li>Force-push cleaned branch if required.</li>
        </ol>
        <pre><code>bfg --delete-files 'secrets.json'</code></pre>
        <p><strong>Best practice:</strong> Always use <code>.gitignore</code> and store secrets in Key Vault or environment variables.</p>
      </div>`},{question:"How do you push, pull and clone repositories?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Push, Pull & Clone</strong></h3>
        <pre><code># Clone a remote repo
git clone https://github.com/org/project.git

# Pull latest changes
git pull origin develop

# Push local commits
git push origin feature/login</code></pre>
        <p><strong>In practice:</strong> I always clone using SSH for secure access and consistent CI/CD integration.</p>
      </div>`},{question:"What is Git Stash and when is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Stash  Save Work Temporarily</strong></h3>
        <p><code>git stash</code> temporarily saves uncommitted changes so you can switch branches without committing.</p>
        <pre><code>git stash save "WIP"
git checkout develop
git stash pop</code></pre>
        <p><strong>Use case:</strong> During urgent hotfixes, I stash local work, switch branch, fix issue, and later restore my changes.</p>
      </div>`},{question:"How to rename a branch locally and remotely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Renaming Branches</strong></h3>
        <pre><code># Rename locally
git branch -m old-name new-name

# Delete old remote branch
git push origin --delete old-name

# Push new branch
git push origin new-name

# Reset upstream tracking
git push --set-upstream origin new-name</code></pre>
        <p><strong>In practice:</strong> I rename branches to align with Jira IDs or release naming conventions.</p>
      </div>`},{question:"What are tags in Git and when are they used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Tags  Version Snapshots</strong></h3>
        <p>Tags mark specific commits (commonly used for release versions).</p>
        <pre><code>git tag -a v1.0 -m "First stable release"
git push origin v1.0</code></pre>
        <p><strong>Use case:</strong> I tag releases for deployment tracking and rollback points in production pipelines.</p>
      </div>`},{question:"What is the difference between Feature and Hotfix branches?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Feature vs Hotfix Branches</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Feature branch:</strong> Created from <code>develop</code> for new enhancements or modules.</li>
          <li><strong>Hotfix branch:</strong> Created from <code>main</code> to fix urgent production issues.</li>
          <li>Hotfix merges back into both <code>main</code> and <code>develop</code> to keep codebase consistent.</li>
        </ul>
        <p><strong>In practice:</strong> I treat hotfix branches as high-priority paths with immediate testing and CI/CD trigger for rollback readiness.</p>
      </div>`}]},{title:"3. GitHub Actions",questions:[{question:"What is GitHub Actions workflow?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>GitHub Actions Workflow  CI/CD as Code</strong></h3>
        <p>A <strong>workflow</strong> in GitHub Actions is a YAML-based automation pipeline that runs on specific triggers (push, PR, schedule, etc.).</p>
        <ul style="margin-left:1.2rem;">
          <li>Stored inside <code>.github/workflows/</code> directory.</li>
          <li>Defines <strong>jobs, steps, and actions</strong> to build, test, deploy, or automate tasks.</li>
          <li>Executes in isolated GitHub-hosted or self-hosted runners.</li>
        </ul>
        <pre><code># Example: simple CI workflow
name: CI Build
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: npm ci
      - run: npm test</code></pre>
        <p><strong>In practice:</strong> I use separate workflows for CI (build/test) and CD (deploy), each triggered by different GitHub events.</p>
      </div>`},{question:"What is workflow dispatch in GitHub?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Workflow Dispatch  Manual Trigger</strong></h3>
        <p><code>workflow_dispatch</code> allows users to manually trigger workflows from the GitHub UI or via API.</p>
        <pre><code>on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deploy target'
        required: true
        default: 'dev'</code></pre>
        <p><strong>Use case:</strong> I use <code>workflow_dispatch</code> for on-demand deployments  e.g., promoting a build from dev to prod manually.</p>
      </div>`},{question:"What are jobs, steps, and actions in workflow files?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Workflow Structure  Jobs, Steps & Actions</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Job:</strong> A set of steps that run on a single runner (e.g., build, test, deploy).</li>
          <li><strong>Step:</strong> A single task inside a job (shell command or an action).</li>
          <li><strong>Action:</strong> A pre-built or custom reusable component performing a task.</li>
        </ul>
        <pre><code>jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run tests
        run: npm test</code></pre>
        <p><strong>In practice:</strong> I modularize workflows by splitting build/test/deploy into separate jobs to run in parallel.</p>
      </div>`},{question:"What are reusable workflows and composite actions?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reusable Workflows & Composite Actions</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Reusable workflows:</strong> Centralized workflows that can be called by other repos using <code>workflow_call</code>.</li>
          <li><strong>Composite actions:</strong> Custom actions that bundle multiple steps into one logical unit (defined in <code>action.yml</code>).</li>
        </ul>
        <pre><code># Reusable workflow call
uses: org/repo/.github/workflows/deploy.yml@main</code></pre>
        <p><strong>In practice:</strong> I create reusable workflows for shared CI/CD templates across multiple microservice repos  ensures consistency.</p>
      </div>`},{question:"How do you write GitHub Actions YAML files?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Writing Workflow YAMLs</strong></h3>
        <p>Workflows are defined using YAML syntax in <code>.github/workflows/</code>.</p>
        <pre><code>name: Build and Deploy
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: npm install
      - run: npm run build</code></pre>
        <p><strong>Best practices:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Always pin action versions (<code>@v4</code>) for stability.</li>
          <li>Use matrix builds for multiple Node or Python versions.</li>
          <li>Define environment variables and secrets in one place.</li>
        </ul>
      </div>`},{question:"What is the difference between on: push and on: workflow_dispatch?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Trigger Differences</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>on: push</strong>  Runs automatically when commits are pushed to specified branches.</li>
          <li><strong>on: workflow_dispatch</strong>  Runs only when manually triggered via UI or API.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>on:
  push:
    branches: [main]
  workflow_dispatch:</code></pre>
        <p><strong>In practice:</strong> I use <code>push</code> for CI builds and <code>workflow_dispatch</code> for controlled production deployments.</p>
      </div>`},{question:"What action do you use for checkout and what version?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Checkout Action  Fetch Repository Code</strong></h3>
        <p>I use <code>actions/checkout</code> to pull code from the repo inside the runner.</p>
        <pre><code>- name: Checkout code
  uses: actions/checkout@v4</code></pre>
        <p><strong>Why v4:</strong> Supports better submodule handling, caching, and performance improvements.</p>
      </div>`},{question:"What is the latest version of actions/checkout used in your pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Latest Checkout Action Version</strong></h3>
        <p>The latest stable version is <code>actions/checkout@v4</code> (as of 2025).</p>
        <p><strong>In my pipelines:</strong> Ive migrated from <code>@v2</code>  <code>@v4</code> for enhanced Git LFS and multi-fetch improvements.</p>
      </div>`},{question:"What are build metrics in GitHub Actions?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Build Metrics & Insights</strong></h3>
        <p>GitHub Actions provides workflow run metrics such as:</p>
        <ul style="margin-left:1.2rem;">
          <li>Job duration and success rate.</li>
          <li>Queue time and concurrency usage.</li>
          <li>Artifact size and cache hit ratio.</li>
        </ul>
        <p><strong>In practice:</strong> I use GitHubs Usage & Insights tab to analyze pipeline performance and optimize build times by caching dependencies.</p>
      </div>`},{question:"How do you use secrets in GitHub Actions?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using Secrets Securely</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Define secrets under <strong>Repo  Settings  Secrets  Actions</strong>.</li>
          <li>Access using <code>&dollar;{'{{ secrets.VAR_NAME }}'}</code> syntax inside workflows.</li>
        </ul>
        <pre><code>- name: Deploy
  run: az login --service-principal -u &dollar;{{ secrets.AZURE_ID }} -p &dollar;{{ secrets.AZURE_SECRET }} --tenant &dollar;{{ secrets.TENANT_ID }}</code></pre>
        <p><strong>In practice:</strong> I keep environment credentials and API keys strictly in GitHub Secrets or Org-level secrets for shared workflows.</p>
      </div>`},{question:"How do you handle parallel jobs in GitHub workflows?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Parallel Job Execution</strong></h3>
        <p>GitHub Actions runs jobs in parallel by default unless dependencies are defined using <code>needs:</code>.</p>
        <pre><code>jobs:
  build:
    runs-on: ubuntu-latest
  test:
    runs-on: ubuntu-latest
  deploy:
    runs-on: ubuntu-latest
    needs: [build, test]</code></pre>
        <p><strong>In practice:</strong> I parallelize linting, testing, and build jobs to reduce total CI time from 10 mins to ~4 mins.</p>
      </div>`},{question:"What are GitHub events and syntax-based triggers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Events & Triggers in GitHub Actions</strong></h3>
        <p>Workflows can trigger on <strong>events</strong> such as:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>push / pull_request:</strong> Code changes or PRs.</li>
          <li><strong>schedule:</strong> Cron-based triggers.</li>
          <li><strong>release:</strong> Tag creation events.</li>
          <li><strong>workflow_dispatch:</strong> Manual trigger.</li>
        </ul>
        <pre><code>on:
  schedule:
    - cron: '0 2 * * *'  # Every night 2AM</code></pre>
        <p><strong>In practice:</strong> I use time-based triggers for nightly builds and event-based triggers for CI/CD pipelines.</p>
      </div>`},{question:"How do you integrate GitHub Actions with Azure pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating GitHub Actions with Azure Pipelines</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use GitHub  Azure DevOps Service Connection for pipeline triggers.</li>
          <li>Trigger Azure Pipeline via REST API from GitHub Action.</li>
        </ul>
        <pre><code>- name: Trigger Azure Pipeline
  run: |
    curl -u USER:TOKEN \\
    -X POST "https://dev.azure.com/org/project/_apis/pipelines/{id}/runs?api-version=6.0"</code></pre>
        <p><strong>In practice:</strong> I trigger Azure Pipelines for infra deployments post successful GitHub CI builds  keeps CI in GitHub and CD in Azure DevOps.</p>
      </div>`}]},{title:"4. Git Commands & Operations",questions:[{question:"What is the difference between Git Pull, Fetch, and Clone?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Pull vs Fetch vs Clone</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>git clone:</strong> Copies a remote repository (all branches, commits, and history) to your local machine.</li>
          <li><strong>git fetch:</strong> Downloads latest changes (commits, branches, tags) from remote <em>without merging</em> into local branch.</li>
          <li><strong>git pull:</strong> Combination of <code>fetch + merge</code>. It fetches and immediately merges remote changes into your current branch.</li>
        </ul>
        <pre><code># Examples
git clone https://github.com/user/repo.git
git fetch origin
git pull origin main</code></pre>
        <p><strong>In practice:</strong> I use <code>fetch</code> before merging to review incoming changes safely.</p>
      </div>`},{question:"What is the difference between Git Merge and Rebase?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Merge vs Rebase  Combining Branches</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Merge:</strong> Creates a new "merge commit" combining two branch histories  preserves branch structure.</li>
          <li><strong>Rebase:</strong> Reapplies commits from one branch on top of another  results in a cleaner, linear history.</li>
        </ul>
        <pre><code># Merge Example
git checkout main
git merge feature

# Rebase Example
git checkout feature
git rebase main</code></pre>
        <p><strong>Rule of thumb:</strong> Use <code>merge</code> for shared branches, <code>rebase</code> for local cleanup before pushing.</p>
      </div>`},{question:"What is Git Stash and when do you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Stash  Save Work Temporarily</strong></h3>
        <p><strong>git stash</strong> temporarily stores uncommitted changes without committing them. Useful when switching branches mid-work.</p>
        <pre><code># Commands
git stash           # Save current changes
git stash list      # View stashes
git stash apply     # Reapply latest stash
git stash pop       # Apply and remove from stash</code></pre>
        <p><strong>In practice:</strong> I stash when I need to pull updates or switch branches without losing ongoing edits.</p>
      </div>`},{question:"What is the difference between git reset, git revert, and git checkout?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reset vs Revert vs Checkout</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>git reset:</strong> Moves HEAD to a specific commit  can modify commit history (dangerous for shared repos).</li>
          <li><strong>git revert:</strong> Creates a new commit that undoes a previous one  safe for shared branches.</li>
          <li><strong>git checkout:</strong> Switches branches or restores files from a specific commit.</li>
        </ul>
        <pre><code># Examples
git reset --hard HEAD~1   # Remove last commit
git revert 123abc         # Safely undo commit
git checkout dev          # Switch branch</code></pre>
        <p><strong>Pro Tip:</strong> Use <code>revert</code> for public history, <code>reset</code> for local corrections.</p>
      </div>`},{question:"What is a detached HEAD state and how do you fix it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Detached HEAD  What It Means & Fix</strong></h3>
        <p>Occurs when you checkout a commit (not a branch), so HEAD points directly to a commit instead of a branch.</p>
        <pre><code># Example of detached HEAD
git checkout a1b2c3d</code></pre>
        <p><strong>Fix:</strong> Create or switch to a branch to reattach HEAD.</p>
        <pre><code>git checkout -b new-branch    # Create new branch from commit
# or
git switch main</code></pre>
        <p><strong>In practice:</strong> I sometimes use detached HEAD to test old commits safely without affecting branches.</p>
      </div>`},{question:"How do you check commit history in Git?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Viewing Commit History</strong></h3>
        <p>Use <strong>git log</strong> and its options for various formats.</p>
        <pre><code>git log --oneline --graph --decorate
git log --author="Ritesh"
git log -p            # Show patch details</code></pre>
        <p><strong>In practice:</strong> I use <code>--oneline --graph</code> to quickly visualize branch merges and commit flows.</p>
      </div>`},{question:"How do you view differences between commits or branches (git diff)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Comparing Changes with git diff</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Unstaged changes:</strong> <code>git diff</code></li>
          <li><strong>Staged changes:</strong> <code>git diff --cached</code></li>
          <li><strong>Between commits/branches:</strong> <code>git diff commit1 commit2</code> or <code>git diff main dev</code></li>
        </ul>
        <pre><code># Example
git diff HEAD~1 HEAD
git diff main feature</code></pre>
        <p><strong>Pro Tip:</strong> Combine with <code>--stat</code> for summarized output.</p>
      </div>`},{question:"What is the difference between git push and git pull?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Push vs Pull</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>git push:</strong> Uploads your local commits to remote repository (e.g., GitHub).</li>
          <li><strong>git pull:</strong> Downloads and merges remote commits into your current branch.</li>
        </ul>
        <pre><code># Examples
git push origin main
git pull origin main</code></pre>
        <p><strong>In practice:</strong> I always pull before pushing to avoid non-fast-forward conflicts.</p>
      </div>`}]},{title:"5. Branching & Collaboration",questions:[{question:"What are branching strategies  Feature, Hotfix, Release, and Trunk-Based?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Branching Strategies in Real Projects</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Feature Branch:</strong> Each feature or story is developed in its own branch (e.g., <code>feature/login-api</code>) and merged after review.</li>
          <li><strong>Hotfix Branch:</strong> Used to patch production issues quickly  branched from <code>main</code> and merged back to both <code>main</code> & <code>develop</code>.</li>
          <li><strong>Release Branch:</strong> Created from <code>develop</code> for final testing and version tagging before deploying to production.</li>
          <li><strong>Trunk-Based:</strong> Minimal branching  developers commit small, frequent changes directly to <code>main</code> (CI/CD heavy model).</li>
        </ul>
        <p><strong>In practice:</strong> I usually go with <strong>Git Flow</strong> or <strong>Feature branching</strong> for team collaboration to ensure clean isolation of work.</p>
      </div>`},{question:"Which branching model have you implemented in your project?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Implemented Model  Git Flow</strong></h3>
        <p>In my project, we implemented <strong>Git Flow</strong> because of multi-environment releases and parallel feature development.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Main (master):</strong> Always production-ready.</li>
          <li><strong>Develop:</strong> Ongoing integration branch for QA testing.</li>
          <li><strong>Feature branches:</strong> Created from <code>develop</code>.</li>
          <li><strong>Release branches:</strong> Created before going live.</li>
          <li><strong>Hotfix branches:</strong> Created from <code>main</code> to fix critical issues.</li>
        </ul>
        <p><strong>Why:</strong> Ensures parallel work, clean merges, and predictable releases.</p>
      </div>`},{question:"What is the Git Flow model?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Flow  Structured Branching Strategy</strong></h3>
        <p><strong>Git Flow</strong> defines a consistent branching model for managing feature, release, and hotfix workflows.</p>
        <pre><code>Main  Stable production branch
Develop  Integration branch
Feature/*  New features
Release/*  Final testing
Hotfix/*  Urgent fixes on production</code></pre>
        <p><strong>Commands (Git Flow CLI):</strong></p>
        <pre><code>git flow init
git flow feature start login
git flow release start v1.0
git flow hotfix start urgent-fix</code></pre>
        <p><strong>In practice:</strong> I prefer Git Flow for enterprise repos where multiple teams deploy in parallel.</p>
      </div>`},{question:"How do you delete a branch locally and remotely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deleting Branches (Local + Remote)</strong></h3>
        <pre><code># Delete local branch
git branch -d feature/login
# Force delete if not merged
git branch -D feature/login

# Delete remote branch
git push origin --delete feature/login</code></pre>
        <p><strong>In practice:</strong> I always delete feature branches post-merge to keep repo clean and maintain hygiene.</p>
      </div>`},{question:"How do you rename a branch (local + remote)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Renaming Branches Safely</strong></h3>
        <pre><code># Rename local branch
git branch -m old-name new-name

# Push new branch and delete old remote
git push origin new-name
git push origin --delete old-name</code></pre>
        <p><strong>Pro Tip:</strong> Always inform your team after renaming remote branches to avoid fetch errors.</p>
      </div>`},{question:"How do you create a new branch and switch to it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create & Switch Branch</strong></h3>
        <pre><code># Create new branch
git branch feature/ui-update

# Create and switch in one command
git checkout -b feature/ui-update</code></pre>
        <p><strong>In practice:</strong> I use descriptive names (e.g., <code>feature/terraform-cost-optimization</code>) for clarity in team repos.</p>
      </div>`},{question:"How do you merge one branch into another?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Merging Branches</strong></h3>
        <pre><code># Switch to target branch
git checkout develop
# Merge source branch into it
git merge feature/api-integration</code></pre>
        <p><strong>In practice:</strong> I prefer merging via <strong>Pull Requests</strong> to ensure code review, build checks, and approvals before integration.</p>
      </div>`},{question:"What are merge conflicts and how do you resolve them?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Merge Conflicts  Cause & Resolution</strong></h3>
        <p>Conflicts occur when Git cannot automatically merge changes due to overlapping edits in the same file.</p>
        <pre><code># Steps to resolve
1 Identify conflict markers (<<<<<<<, =======, >>>>>>>)
2 Manually edit file and keep the correct version
3 Mark resolved and commit
git add .
git commit</code></pre>
        <p><strong>In practice:</strong> I use VS Code merge tools or GitKraken to visually resolve conflicts faster and cleaner.</p>
      </div>`},{question:"How do you perform cherry-pick in Git?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Cherry-Pick  Selective Commit Transfer</strong></h3>
        <p>Cherry-pick lets you apply a specific commit from one branch to another without merging the whole branch.</p>
        <pre><code># Example
git checkout develop
git cherry-pick 9fceb02</code></pre>
        <p><strong>Use case:</strong> Useful when a single bug fix or feature needs to be moved quickly to a different environment branch.</p>
      </div>`}]},{title:"6. Commits, History & Recovery",questions:[{question:"How do you undo the last commit without losing your changes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Undo Last Commit (Keep Changes)</strong></h3>
        <p>Use <strong>--soft</strong> reset to move HEAD back one commit but keep changes in the working directory and staging area.</p>
        <pre><code># Undo last commit but keep files
git reset --soft HEAD~1</code></pre>
        <p><strong>In practice:</strong> I use this when I forget to update the commit message or want to add one more file before recommitting.</p>
      </div>`},{question:"How do you revert a bad commit that broke the build?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reverting a Bad Commit</strong></h3>
        <p>Use <strong>git revert</strong> to safely undo a commit by creating a new commit that reverses its changes (no history rewrite).</p>
        <pre><code># Revert a commit by hash
git revert <commit-hash></code></pre>
        <p><strong>In practice:</strong> Ideal for shared branches where others may have already pulled the bad commit.</p>
      </div>`},{question:"How do you squash multiple commits into one?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Squashing Commits for Clean History</strong></h3>
        <p>Use <strong>interactive rebase</strong> to combine multiple commits into a single logical commit.</p>
        <pre><code># Squash last 3 commits
git rebase -i HEAD~3</code></pre>
        <p>Then mark all but the first commit as <code>squash</code> or <code>s</code> in the editor and save.</p>
        <p><strong>In practice:</strong> I always squash before merging feature branches to keep history readable and meaningful.</p>
      </div>`},{question:"What is interactive rebase and how do you use it to clean commit history?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Interactive Rebase  History Cleanup Tool</strong></h3>
        <p><strong>Interactive rebase</strong> lets you edit, reorder, squash, or drop commits before pushing them.</p>
        <pre><code># Open rebase editor for last 5 commits
git rebase -i HEAD~5</code></pre>
        <p><strong>Options inside editor:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>pick:</strong> Keep commit as is.</li>
          <li><strong>reword:</strong> Edit commit message.</li>
          <li><strong>squash:</strong> Combine with previous commit.</li>
          <li><strong>drop:</strong> Remove commit.</li>
        </ul>
        <p><strong>In practice:</strong> I use it before merging to <code>main</code> to ensure linear, well-labeled commit history.</p>
      </div>`},{question:"How do you recover deleted commits or branches?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recover Deleted Commits or Branches</strong></h3>
        <p>Git keeps a record of all recent HEAD positions via <strong>reflog</strong>.</p>
        <pre><code># Show recent actions
git reflog

# Recover a branch
git checkout -b recovered-branch <commit-hash></code></pre>
        <p><strong>In practice:</strong> Reflog is a life-saver when commits or branches get lost during a bad rebase or reset.</p>
      </div>`},{question:"How do you remove sensitive data (like passwords or tokens) from Git history?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Removing Sensitive Data from History</strong></h3>
        <p>Use <strong>git filter-repo</strong> (recommended) or <strong>BFG Repo-Cleaner</strong> to remove secrets permanently from history.</p>
        <pre><code># Using git filter-repo
git filter-repo --path secret.txt --invert-paths</code></pre>
        <p><strong>Steps:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Remove sensitive file from history.</li>
          <li>Force push rewritten branch.</li>
          <li>Invalidate old clones (rotate credentials).</li>
        </ol>
        <p><strong>In practice:</strong> After removing secrets, I always update CI/CD secrets store and revoke old tokens.</p>
      </div>`},{question:"How do you recover an accidentally deleted branch?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recover Deleted Branch</strong></h3>
        <p>Git reflog tracks the last commit pointer for deleted branches, allowing easy recovery.</p>
        <pre><code># Find the last commit hash
git reflog

# Recreate the branch
git branch restore-feature <commit-hash></code></pre>
        <p><strong>In practice:</strong> Ive used this multiple times to recover branches deleted after premature cleanup.</p>
      </div>`},{question:"What is Git reflog and how can it help you recover lost commits?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Reflog  The Recovery Command</strong></h3>
        <p><strong>git reflog</strong> records every movement of HEAD  commits, rebases, resets, checkouts, etc. Its your recovery log.</p>
        <pre><code># View reflog
git reflog

# Restore lost commit
git checkout -b restore HEAD@{2}</code></pre>
        <p><strong>In practice:</strong> Reflog is my go-to command when a commit disappears after reset, rebase, or accidental deletion.</p>
      </div>`}]},{title:"7. Tags & Versioning",questions:[{question:"What is a Git Tag and why is it useful?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Tag  Snapshot for a Specific Commit</strong></h3>
        <p>A <strong>Git Tag</strong> marks a specific point in history  usually for a <strong>release version</strong> like <code>v1.0.0</code>. Its a read-only reference to a commit that doesnt change over time.</p>
        <p><strong>Why its useful:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Identifies stable releases for deployment or rollback.</li>
          <li>Helps trace code versions in production.</li>
          <li>Used by CI/CD pipelines to trigger versioned builds.</li>
        </ul>
        <p><strong>In practice:</strong> I use tags to label deployment-ready commits like <code>v1.2.3</code> before triggering automation in GitHub Actions or Azure DevOps.</p>
      </div>`},{question:"What is the difference between lightweight and annotated tags?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Lightweight vs Annotated Tags</strong></h3>
        <p><strong>Lightweight Tag:</strong> Simple pointer to a commit  no metadata, no message.</p>
        <pre><code>git tag v1.0.0</code></pre>
        <p><strong>Annotated Tag:</strong> Full tag object stored in Git DB with tagger info, date, and message.</p>
        <pre><code>git tag -a v1.0.0 -m "Release version 1.0.0"</code></pre>
        <p><strong>Key Difference:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Annotated tags are preferred for official releases.</li>
          <li>Lightweight tags are good for local checkpoints.</li>
        </ul>
        <p><strong>In practice:</strong> I always use annotated tags in CI/CD for traceability and signed release notes.</p>
      </div>`},{question:"How do you create, view, and delete tags?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create, View & Delete Tags</strong></h3>
        <p><strong>Create Tag:</strong></p>
        <pre><code># Annotated
git tag -a v1.1.0 -m "Release 1.1.0"</code></pre>
        <p><strong>View All Tags:</strong></p>
        <pre><code>git tag</code></pre>
        <p><strong>View Tag Details:</strong></p>
        <pre><code>git show v1.1.0</code></pre>
        <p><strong>Delete Tag:</strong></p>
        <pre><code># Local delete
git tag -d v1.1.0

# Remote delete
git push origin :refs/tags/v1.1.0</code></pre>
        <p><strong>In practice:</strong> I tag each production release and maintain old tags for rollback traceability.</p>
      </div>`},{question:"How do you push tags to a remote repository?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pushing Tags to Remote</strong></h3>
        <p>By default, <code>git push</code> does not push tags automatically. You must push them explicitly.</p>
        <pre><code># Push a specific tag
git push origin v1.1.0

# Push all tags
git push origin --tags</code></pre>
        <p><strong>In practice:</strong> I push version tags as part of release automation so CI/CD picks the correct build version.</p>
      </div>`},{question:"What is the purpose of tagging in release management?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Tags in Release Management</strong></h3>
        <p>Tags act as <strong>immutable version identifiers</strong> in a release cycle  mapping deployed artifacts to source code state.</p>
        <p><strong>Key Purposes:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Trace back exact code version used in production.</li>
          <li>Enable rollback to a known stable commit.</li>
          <li>Automate semantic versioning (<code>v1.2.3</code>) in CI/CD pipelines.</li>
        </ul>
        <p><strong>In practice:</strong> My pipelines auto-tag builds on successful deployments using versioning rules like <code>major.minor.patch</code>.</p>
      </div>`}]},{title:"8. Merge, Rebase & Squash Explained",questions:[{question:"Difference between merge and rebase  when should you use each?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Merge vs Rebase  Key Difference</strong></h3>
        <p><strong>Merge:</strong> Combines changes from one branch into another, preserving full commit history with a new merge commit.</p>
        <p><strong>Rebase:</strong> Moves or reapplies commits from one branch on top of another branch  creating a cleaner, linear history.</p>
        <p><strong>When to use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Use Merge</strong> when you want to retain full history and context of all merges (preferred in shared branches like <code>main</code>).</li>
          <li><strong>Use Rebase</strong> for personal feature branches to keep history clean before merging to <code>develop</code>.</li>
        </ul>
        <pre><code># Merge example
git checkout develop
git merge feature/login

# Rebase example
git checkout feature/login
git rebase develop</code></pre>
        <p><strong>In practice:</strong> I rebase my feature branches on develop before merge to avoid unnecessary merge commits.</p>
      </div>`},{question:"What happens internally during a rebase?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>What Happens Internally During Rebase</strong></h3>
        <p>Rebase temporarily removes your branch commits, moves the branch pointer to the target base, and re-applies your commits one by one.</p>
        <p><strong>Steps internally:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Git stores your commits temporarily in a buffer.</li>
          <li>Moves your branch pointer to the latest commit of the base branch (e.g., <code>develop</code>).</li>
          <li>Re-applies your commits sequentially over it.</li>
          <li>If conflicts occur, you must resolve them manually before continuing with <code>git rebase --continue</code>.</li>
        </ol>
        <p><strong>In practice:</strong> I often use rebase before pushing to keep a linear, readable history in team repos.</p>
      </div>`},{question:"What is the difference between squash merge and regular merge?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Squash Merge vs Regular Merge</strong></h3>
        <p><strong>Regular Merge:</strong> Preserves all commits from the feature branch.</p>
        <p><strong>Squash Merge:</strong> Combines all commits from the feature branch into a single commit on the target branch.</p>
        <pre><code># Regular merge
git merge feature/login

# Squash merge
git merge --squash feature/login
git commit -m "Feature: login implemented"</code></pre>
        <p><strong>When to use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use squash merge for small feature branches to keep main branch history clean.</li>
          <li>Use regular merge for large, collaborative branches where history is important.</li>
        </ul>
        <p><strong>In practice:</strong> I prefer squash merges in PRs for single-owner features  cleaner audit trail.</p>
      </div>`},{question:"What is a Git conflict and how do you handle it during rebase?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Git Conflicts During Rebase</strong></h3>
        <p><strong>Conflict:</strong> Happens when the same lines of code are modified differently in two branches being merged or rebased.</p>
        <p><strong>During rebase:</strong> Git pauses and lets you resolve manually.</p>
        <pre><code># Typical rebase conflict resolution
git status          # See conflicting files
git add <file>      # After fixing conflicts
git rebase --continue</code></pre>
        <p>If you want to abort rebase:</p>
        <pre><code>git rebase --abort</code></pre>
        <p><strong>In practice:</strong> I always resolve conflicts locally, test build once, then continue rebase  avoids broken merges in main.</p>
      </div>`},{question:"How do you combine multiple commits using git squash or interactive rebase?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Combining Commits  Squash & Interactive Rebase</strong></h3>
        <p>Use <strong>interactive rebase</strong> to combine multiple commits into a single, meaningful one.</p>
        <pre><code># Combine last 3 commits
git rebase -i HEAD~3</code></pre>
        <p>In the editor, change all but the first commit from <code>pick</code>  <code>squash</code> (or <code>s</code>).</p>
        <p>Then, edit the commit message and save.</p>
        <p><strong>In practice:</strong> I always squash commits before PR  ensures clean, review-friendly history without "fix typo" commits.</p>
      </div>`},{question:"What are the advantages and risks of using rebase?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Advantages & Risks of Rebase</strong></h3>
        <p><strong>Advantages:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Cleaner, linear commit history.</li>
          <li>Easier to trace bugs and review changes.</li>
          <li>No unnecessary merge commits cluttering logs.</li>
        </ul>
        <p><strong>Risks:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Rewriting history  can cause issues if commits are already pushed/shared.</li>
          <li>Potential for conflicts when rebasing large branches.</li>
        </ul>
        <p><strong>Best Practice:</strong> Never rebase public branches. Rebase only your local feature branches before merge.</p>
        <p><strong>In practice:</strong> I follow rebase local, merge shared  keeps team workflow safe and clean.</p>
      </div>`}]},{title:"9. Git Configuration & .gitignore",questions:[{question:"What is a .gitignore file and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>.gitignore  Ignore Unnecessary or Sensitive Files</strong></h3>
        <p>The <strong>.gitignore</strong> file tells Git which files or directories to ignore  meaning they wont be tracked, staged, or committed.</p>
        <p><strong>Why it's used:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>To prevent committing unnecessary build artifacts (e.g., <code>dist/</code>, <code>node_modules/</code>).</li>
          <li>To protect sensitive info like credentials or local configs (<code>.env</code>, <code>.terraform/</code>).</li>
          <li>To keep repo clean and lightweight for collaborators.</li>
        </ul>
        <p><strong>In practice:</strong> Every project I create has a well-defined <code>.gitignore</code> template aligned with its tech stack  e.g., Node, Python, Terraform, etc.</p>
      </div>`},{question:"What types of files should go in .gitignore (examples: .env, node_modules, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Files/Folders to Add in .gitignore</strong></h3>
        <p><strong>Typical examples:</strong></p>
        <pre><code># Environment and secrets
.env
.env.local
*.pem

# Dependencies and builds
node_modules/
dist/
build/
__pycache__/
.terraform/

# Logs & cache
*.log
.cache/
coverage/
.idea/
.vscode/

# System files
.DS_Store
Thumbs.db</code></pre>
        <p><strong>In practice:</strong> I customize .gitignore per environment  for example, Terraform state files (<code>terraform.tfstate*</code>) in infra repos and build outputs in app repos.</p>
      </div>`},{question:"How do you apply changes in .gitignore after the file has already been committed?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Apply .gitignore Changes Retroactively</strong></h3>
        <p>By default, Git continues to track files even if they are later added to <code>.gitignore</code>. To untrack already-committed files:</p>
        <pre><code># Stop tracking existing files now ignored
git rm -r --cached .
git add .
git commit -m "Apply updated .gitignore rules"</code></pre>
        <p>This removes files from version control (not from local disk) and applies updated ignore rules.</p>
        <p><strong>In practice:</strong> I always review <code>git status</code> after this step to ensure no critical file is mistakenly untracked.</p>
      </div>`},{question:"What is .gitattributes and how is it different from .gitignore?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>.gitattributes vs .gitignore</strong></h3>
        <p><strong>.gitignore:</strong> Tells Git which files to ignore  they wont be committed or tracked.</p>
        <p><strong>.gitattributes:</strong> Defines how Git handles files that are tracked  controlling behaviors like line endings, diff/merge drivers, and binary handling.</p>
        <p><strong>Example:</strong></p>
        <pre><code># Normalize line endings
*.sh text eol=lf

# Treat images as binary
*.png binary

# Custom diff driver
*.tf diff=terraform</code></pre>
        <p><strong>In practice:</strong> I use <code>.gitattributes</code> to enforce consistent line endings (LF/CRLF) across teams using Windows & macOS  avoids random diffs in commits.</p>
      </div>`},{question:"How do you configure Git username and email globally or per repo?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Configuring Git Identity (Global & Local)</strong></h3>
        <p>Git uses <strong>username</strong> and <strong>email</strong> to tag commits. You can configure them globally or per repository.</p>
        <p><strong>Set globally (applies to all repos):</strong></p>
        <pre><code>git config --global user.name "Ritesh Sharma"
git config --global user.email "ritesh@example.com"</code></pre>
        <p><strong>Set locally (specific to a repo):</strong></p>
        <pre><code>git config user.name "ProjectUser"
git config user.email "project@example.com"</code></pre>
        <p><strong>Verify configuration:</strong></p>
        <pre><code>git config --list</code></pre>
        <p><strong>In practice:</strong> I maintain global identity for personal repos and override per project when using corporate GitHub or Azure DevOps accounts.</p>
      </div>`}]},{title:"10. Authentication & Security",questions:[{question:"How do you handle Git authentication in CI/CD pipelines?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Git Authentication in CI/CD Pipelines</strong></h3>
        <p>In CI/CD, authentication is handled using <strong>secure tokens or service connections</strong>  never raw credentials.</p>
        <p><strong>Common patterns:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>GitHub Actions:</strong> Use <code>GITHUB_TOKEN</code> or personal PAT stored in <strong>Secrets</strong>.</li>
          <li><strong>Azure DevOps:</strong> Use <strong>Service Connections</strong> or PAT in secure variable groups.</li>
          <li><strong>Self-hosted agents:</strong> Store tokens as environment variables or secret files (not in repo).</li>
        </ul>
        <p><strong>Example (GitHub Actions):</strong></p>
        <pre><code>git config user.name "ci-bot"
git config user.email "ci-bot@company.com"
git push https://x-access-token:&dollar;{{ secrets.GITHUB_TOKEN }}@github.com/org/repo.git HEAD:main</code></pre>
        <p><strong>In practice:</strong> I ensure CI/CD jobs use temporary tokens with least privilege  just enough to clone/push code or tag releases.</p>
      </div>`},{question:"What are the ways to authenticate Git access in Azure DevOps or GitHub (PAT, SSH keys, OAuth)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Authentication Methods  Azure DevOps & GitHub</strong></h3>
        <p><strong>1 Personal Access Token (PAT):</strong> Token-based authentication, scoped for APIs and Git operations. Common in CI/CD and automation.</p>
        <p><strong>2 SSH Keys:</strong> Key-based authentication for developers  ideal for local dev or self-hosted runners.</p>
        <p><strong>3 OAuth / SSO:</strong> Used for user logins and organization-based access policies (SSO enforced by Azure AD or GitHub Enterprise).</p>
        <p><strong>In practice:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>SSH</strong> for dev access (no password prompts).</li>
          <li>Use <strong>PATs</strong> in CI/CD pipelines with limited scopes (read/write).</li>
          <li>Use <strong>OAuth</strong> for integrations like GitHub Apps or third-party services.</li>
        </ul>
      </div>`},{question:"What is a Personal Access Token (PAT) and when should you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Personal Access Token (PAT)</strong></h3>
        <p>A <strong>PAT</strong> is a secure token that replaces username/password for Git authentication or API access.</p>
        <p><strong>When to use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Automating Git operations in pipelines or scripts.</li>
          <li>Accessing private repos via REST API.</li>
          <li>Integrating external tools (Terraform, Jenkins, GitHub Actions).</li>
        </ul>
        <p><strong>Best Practices:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>fine-grained PATs</strong> with minimal scope and expiration.</li>
          <li>Never hardcode PATs  store in <strong>Secrets / Key Vault / Variable Groups</strong>.</li>
        </ul>
        <p><strong>In practice:</strong> I create short-lived PATs for CI jobs and rotate them via service principal automation every 90 days.</p>
      </div>`},{question:"How do you securely store credentials for Git automation?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Credential Storage for Git Automation</strong></h3>
        <p><strong>Never store credentials in plain text or code.</strong> Always use secret management solutions provided by your CI/CD platform.</p>
        <p><strong>Recommended secure stores:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>GitHub Actions:</strong> <code>Settings  Secrets and Variables  Actions</code></li>
          <li><strong>Azure DevOps:</strong> Secure <strong>Variable Groups</strong> or <strong>Azure Key Vault</strong> linked to pipeline.</li>
          <li><strong>AWS / GCP:</strong> Use Secrets Manager or KMS with IAM roles.</li>
        </ul>
        <p><strong>In practice:</strong> My pipelines fetch credentials dynamically from Key Vault using service principal  no secrets exposed in YAML or logs.</p>
      </div>`},{question:"How do you revoke or rotate credentials in a secure pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Revoking & Rotating Credentials Securely</strong></h3>
        <p>Regular credential rotation reduces risk from leaked tokens or old service accounts.</p>
        <p><strong>Steps to rotate securely:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Revoke old PAT or key from GitHub / Azure DevOps settings.</li>
          <li>Generate a new one with minimal permissions.</li>
          <li>Update it in secret store (GitHub Secrets / Key Vault / DevOps Variable Group).</li>
          <li>Trigger a test run to validate new credentials.</li>
        </ol>
        <p><strong>Automation Tip:</strong> Use scripts or scheduled Logic Apps to auto-rotate credentials every 90 days and notify via Teams/Slack.</p>
        <p><strong>In practice:</strong> I maintain a rotation policy using Azure Key Vault expiration alerts integrated with a Logic App for notification & update workflow.</p>
      </div>`}]},{title:"11. Git in CI/CD & DevOps",questions:[{question:"How is Git integrated with Azure DevOps, Jenkins, or GitHub Actions?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Integration in CI/CD Tools</strong></h3>
        <p>Git acts as the <strong>source of truth</strong> for all CI/CD platforms. Each tool integrates through webhooks or service connections.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure DevOps:</strong> Pipelines connect directly to Azure Repos or GitHub via <strong>Service Connections</strong>. Each commit or PR triggers a pipeline run.</li>
          <li><strong>GitHub Actions:</strong> Uses <code>on: push</code> or <code>on: pull_request</code> events to trigger workflows defined in <code>.github/workflows/</code>.</li>
          <li><strong>Jenkins:</strong> Uses <strong>Git plugin</strong> + <strong>webhooks</strong> to poll or automatically trigger builds on commits.</li>
        </ul>
        <p><strong>In practice:</strong> I usually configure pipelines to auto-trigger on <code>main</code> merges and create separate workflows for feature and release branches.</p>
      </div>`},{question:"How do you automate deployments triggered by Git commits?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated Deployments via Git Commits</strong></h3>
        <p>Automated deployments are triggered using <strong>Git push  CI  CD</strong> workflow.</p>
        <p><strong>Typical flow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Developer pushes code or merges PR to <code>main</code> / <code>release</code> branch.</li>
          <li>Pipeline auto-triggers (via webhook).</li>
          <li>CI builds and runs tests.</li>
          <li>CD stage deploys to environment (Dev  Staging  Prod).</li>
        </ol>
        <p><strong>Example (GitHub Actions):</strong></p>
        <pre><code>on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - run: npm ci && npm run build
      - run: az webapp deploy ...</code></pre>
        <p><strong>In practice:</strong> I set branch-based environment rules  <code>develop</code>  dev, <code>main</code>  production  using environment protection rules in Actions or DevOps.</p>
      </div>`},{question:"What is the workflow between Git commits  pipeline trigger  deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>End-to-End Git  Pipeline  Deployment Workflow</strong></h3>
        <p>This is the backbone of DevOps automation:</p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Commit Stage:</strong> Developer commits & pushes code to Git repo.</li>
          <li><strong>Trigger Stage:</strong> Git event (push/PR) triggers CI pipeline via webhook.</li>
          <li><strong>CI Stage:</strong> Pipeline builds code, runs lint/test, and publishes artifacts (Docker image, zip, etc.).</li>
          <li><strong>CD Stage:</strong> Deployment pipeline pulls the artifact and deploys to environment.</li>
          <li><strong>Feedback Stage:</strong> Deployment status + logs pushed back to Git PR checks or Teams notification.</li>
        </ol>
        <p><strong>In practice:</strong> I design pipelines as multi-stage YAMLs in Azure DevOps (Build  Test  Deploy) linked to Git branches.</p>
      </div>`},{question:"How do you handle merge conflicts in a multi-developer CI/CD environment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Merge Conflicts in Teams</strong></h3>
        <p>Merge conflicts arise when multiple developers edit the same file regions.</p>
        <p><strong>Best practices:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>feature branches</strong> + frequent <strong>pulls from main</strong> to stay updated.</li>
          <li>Enforce <strong>PR validation pipelines</strong> before merging.</li>
          <li>Use <strong>code owners</strong> and <strong>branch protection rules</strong> for controlled merges.</li>
          <li>Use <code>git rebase</code> on latest main to flatten history and minimize conflicts.</li>
        </ul>
        <p><strong>In CI/CD:</strong> Conflicts are caught early during PR builds  pipelines fail, forcing resolution before merge.</p>
        <p><strong>In practice:</strong> I always rebase or merge main  feature before opening PR to avoid pipeline merge failures.</p>
      </div>`},{question:"How do you manage Git credentials in pipelines securely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Git Credential Management in CI/CD</strong></h3>
        <p>Never hardcode credentials in YAML files  use platform secrets.</p>
        <p><strong>Recommended methods:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>GitHub Actions:</strong> Store in <code>Settings  Secrets  Actions</code>, access via <code>&dollar;{'{{ secrets.PAT_TOKEN }}'}</code>.</li>
          <li><strong>Azure DevOps:</strong> Use <strong>Variable Groups</strong> linked with <strong>Azure Key Vault</strong>.</li>
          <li><strong>Jenkins:</strong> Use <strong>Credentials Plugin</strong> and access via environment variables.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>git push https://x-access-token:&dollar;{'{'}{ secrets.GITHUB_TOKEN }{'}'}@github.com/org/repo.git HEAD:main</code></pre>
        <p><strong>In practice:</strong> I use temporary, least-privileged tokens with 90-day rotation and audit logs enabled for all access.</p>
      </div>`}]},{title:"12. Real-World Scenarios & Troubleshooting",questions:[{question:"You merged a PR and later realized a mistake  how do you fix it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Fixing a Merged PR Mistake</strong></h3>
        <p><strong>Option 1  Revert Merge Commit:</strong></p>
        <pre><code># Find merge commit hash
git log --oneline

# Revert merge safely
git revert -m 1 <merge-commit-hash>
git push origin main</code></pre>
        <p><strong>Option 2  Reset (if private branch, no one pulled yet):</strong></p>
        <pre><code>git reset --hard HEAD~1
git push -f origin main</code></pre>
        <p><strong>In practice:</strong> I prefer <code>git revert</code> for shared branches to avoid rewriting history and breaking others' clones.</p>
      </div>`},{question:"You committed secrets accidentally  how do you remove them from history and push?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Removing Sensitive Data from Git History</strong></h3>
        <p>Use <strong>git filter-repo</strong> (modern approach) or <strong>BFG Repo-Cleaner</strong).</p>
        <pre><code># Remove secret file from history
git filter-repo --path .env --invert-paths

# Force push rewritten history
git push origin --force

# Rotate secrets in environment</code></pre>
        <p><strong>In practice:</strong> After removing secrets, I always revoke old keys/tokens and replace them in CI/CD secret stores.</p>
      </div>`},{question:"Youre in a detached HEAD state after a rebase  what should you do?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Fixing Detached HEAD</strong></h3>
        <p>Detached HEAD means you are not on any branch  commits can be lost if you switch branches.</p>
        <pre><code># Check current HEAD
git status

# Create a new branch to preserve work
git checkout -b fix-detached-head

# Push to remote
git push origin fix-detached-head</code></pre>
        <p><strong>In practice:</strong> Always create a branch immediately to save work before switching or rebasing.</p>
      </div>`},{question:"How do you identify who made a change in a file (git blame)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Tracking Changes with git blame</strong></h3>
        <pre><code># Show commit and author info per line
git blame app/main.js

# Optional: limit to specific lines
git blame -L 10,20 app/main.js</code></pre>
        <p><strong>In practice:</strong> I use git blame for troubleshooting production issues or understanding why a specific change was introduced.</p>
      </div>`},{question:"How do you recover files after running git reset --hard accidentally?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recover Files after git reset --hard</strong></h3>
        <p>Use <strong>git reflog</strong> to find the previous HEAD and restore files.</p>
        <pre><code># Check reflog
git reflog

# Recover lost commit or files
git checkout HEAD@{2} -- path/to/file</code></pre>
        <p><strong>In practice:</strong> Reflog is my lifesaver when someone resets the branch or rewrites history accidentally.</p>
      </div>`},{question:"How do you resolve a large merge conflict between feature and release branches?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resolving Large Merge Conflicts</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Identify conflicting files: <code>git status</code></li>
          <li>Open conflict markers (<<<<<, =====, >>>>>) in files.</li>
          <li>Manually merge changes or use IDE merge tool.</li>
          <li>Add resolved files: <code>git add &lt;file&gt;</code></li>
          <li>Continue merge/rebase: <code>git commit</code> or <code>git rebase --continue</code></li>
          <li>Test build locally before pushing.</li>
        </ol>
        <p><strong>In practice:</strong> I often do feature  release rebase, fix conflicts locally, test thoroughly, then merge to release  avoids breaking CI/CD.</p>
      </div>`},{question:"How do you manage Git hooks (pre-commit, post-merge, etc.) in your projects?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Git Hooks</strong></h3>
        <p>Git hooks are scripts executed at certain points in Git workflow:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>pre-commit:</strong> Run linters, tests, or code formatters before commit.</li>
          <li><strong>post-merge:</strong> Run scripts after merging (e.g., dependency install).</li>
          <li><strong>pre-push:</strong> Run CI tests before push.</li>
        </ul>
        <p><strong>Setup:</strong></p>
        <pre><code># Add a pre-commit hook
echo "npm run lint" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit</code></pre>
        <p><strong>In practice:</strong> I standardize hooks across team using <code>husky</code> or <code>lefthook</code> so everyone enforces lint/tests automatically.</p>
      </div>`}]},{title:"13. Advanced Git Concepts",questions:[{question:"What is Git submodule and when do you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Submodule  Nested Repositories</strong></h3>
        <p>A <strong>Git submodule</strong> allows you to embed one Git repository inside another. Useful for managing dependencies or shared libraries.</p>
        <p><strong>When to use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Include a shared library in multiple projects while keeping its history separate.</li>
          <li>Keep external dependencies tracked but isolated.</li>
        </ul>
        <p><strong>Commands:</strong></p>
        <pre><code># Add a submodule
git submodule add https://github.com/org/lib.git libs/lib

# Initialize & update
git submodule update --init --recursive</code></pre>
        <p><strong>In practice:</strong> I use submodules for common Terraform modules across multiple infrastructure repos.</p>
      </div>`},{question:"What is Git Worktree?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Git Worktree  Multiple Working Directories</strong></h3>
        <p>Git Worktree lets you have multiple working directories for the same repository, each on a different branch.</p>
        <p><strong>Use cases:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Work on multiple features simultaneously without switching branches.</li>
          <li>Test a release branch while continuing development on main.</li>
        </ul>
        <pre><code># Create new worktree for branch
git worktree add ../feature-branch feature/new-feature

# Remove worktree
git worktree remove ../feature-branch</code></pre>
        <p><strong>In practice:</strong> I often use worktrees to test hotfixes on production branch while developing feature branches locally.</p>
      </div>`},{question:"How do you rebase your feature branch with main while preserving your local commits?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rebase Feature Branch on Main</strong></h3>
        <pre><code># Switch to feature branch
git checkout feature/login

# Rebase on main
git fetch origin
git rebase origin/main

# Resolve conflicts if any
git rebase --continue

# Push updated branch
git push --force-with-lease</code></pre>
        <p><strong>In practice:</strong> I rebase frequently to keep my feature branch up-to-date with main while maintaining clean commit history.</p>
      </div>`},{question:"What are bare repositories in Git?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Bare Repositories</strong></h3>
        <p>A <strong>bare repository</strong> contains only the Git database (no working directory). It's typically used as a central repo for collaboration.</p>
        <p><strong>Key points:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Cannot edit files directly  no working tree.</li>
          <li>Used as remote repo for push/pull.</li>
          <li>Created using: <code>git init --bare repo.git</code></li>
        </ul>
        <p><strong>In practice:</strong> Central repos on GitHub or Azure DevOps are effectively bare  we push/pull, never directly edit.</p>
      </div>`},{question:"How do you split a large Git repository into smaller ones (monorepo to multirepo)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Splitting a Large Repository</strong></h3>
        <p>Use <strong>git filter-repo</strong> or <strong>git subtree</strong> to split directories into separate repos while preserving history.</p>
        <pre><code># Example: split directory into new repo
git filter-repo --path apps/frontend/ --path-rename frontend/:/

# Push new repo
git remote add new-frontend <url>
git push new-frontend main</code></pre>
        <p><strong>In practice:</strong> I split large monorepos into multiple repos to reduce CI/CD build times and isolate teams.</p>
      </div>`},{question:"What are signed commits and how are they verified?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Signed Commits (GPG/SSH)</strong></h3>
        <p>Signed commits ensure integrity and authenticity  verify that the commit was made by a trusted developer.</p>
        <p><strong>Create a signed commit:</strong></p>
        <pre><code>git commit -S -m "Implement login feature"</code></pre>
        <p><strong>Verify a commit:</strong></p>
        <pre><code>git log --show-signature</code></pre>
        <p><strong>In practice:</strong> In enterprise projects, all commits to main or release branches are signed and verified to prevent unauthorized code injection.</p>
      </div>`}]}];function Jw(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("git"),a=hm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-orange-500 to-red-500 shadow-glow",children:l.jsx(wu,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Git & Version Control"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Git, Azure Repos, GitHub Actions & Branching Strategies"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Master version control with Git, Azure Repos integration, and GitHub Actions workflows."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:hm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:u.question,b=typeof u=="object"&&"answer"in u?u.answer:null,v=typeof u=="object"&&"answerHtml"in u?u.answerHtml:null;return l.jsx("div",{className:"border-l-2 border-orange-500/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-orange-500 font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),b||v?l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:v.replace(/\$\{/g,"$${")}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:b})})]}):null]})]})},f)})})})]},p))})]})]})}const eD=Object.freeze(Object.defineProperty({__proto__:null,default:Jw},Symbol.toStringTag,{value:"Module"})),tD=()=>l.jsx("div",{className:"flex min-h-screen items-center justify-center bg-background",children:l.jsxs("div",{className:"text-center",children:[l.jsx("h1",{className:"mb-4 text-4xl font-bold",children:"Welcome to Your Blank App"}),l.jsx("p",{className:"text-xl text-muted-foreground",children:"Start building your amazing project here!"})]})}),rD=Object.freeze(Object.defineProperty({__proto__:null,default:tD},Symbol.toStringTag,{value:"Module"})),fm=[{title:"1. Kubernetes Fundamentals",questions:[{question:"What is Kubernetes and why is it used in production?",answerHtml:`<div class='answer-rich'> 
      <h3> <strong>What & Why of Kubernetes</strong></h3> 
      <p><strong>Kubernetes (K8s)</strong> is a powerful <strong>container orchestration platform</strong> that automates deployment, scaling, and management of containerized applications.</p> 
      <p><strong>In production:</strong> it provides auto-scaling, self-healing, zero-downtime deployments, and efficient resource utilization.</p>
      <ul style='margin-left:1.2rem;'>
        <li>Automates container scheduling across cluster nodes.</li>
        <li>Restarts failed containers automatically (self-healing).</li>
        <li>Load balances incoming traffic via Services & Ingress.</li>
        <li>Supports rolling updates and rollbacks.</li>
        <li>Integrates easily with CI/CD, monitoring, and service mesh.</li>
      </ul>
      <p><strong>Real-world use:</strong> In AKS, I deploy microservices via YAML manifests, expose via LoadBalancer services, and scale pods automatically based on CPU/memory metrics.</p>
      </div>`},{question:"What problem does Kubernetes solve compared to VMs or Docker Compose?",answerHtml:`<div class='answer-rich'> 
      <h3> <strong>Problem Kubernetes Solves</strong></h3> 
      <p>Before Kubernetes, managing multiple Docker containers manually or via Docker Compose was painful  especially in production.</p>
      <p><strong>Problems solved:</strong></p>
      <ul style='margin-left:1.2rem;'>
        <li><strong>Scalability:</strong> Kubernetes auto-scales pods based on load (unlike static Compose setup).</li>
        <li><strong>High Availability:</strong> Automatically reschedules failed pods on healthy nodes.</li>
        <li><strong>Zero-Downtime Deployments:</strong> Handles rolling updates natively.</li>
        <li><strong>Resource Efficiency:</strong> Schedules containers dynamically to best use CPU/RAM across nodes.</li>
        <li><strong>Service Discovery:</strong> DNS-based internal service communication  Compose lacks this level of automation.</li>
      </ul>
      <p><strong>Example:</strong> In AKS, Ive replaced multiple VM-hosted apps with a single Kubernetes cluster  managing scaling, updates, and health checks centrally.</p>
      </div>`},{question:"What are Pods, Deployments, and ReplicaSets?",answerHtml:`<div class='answer-rich'> 
      <h3> <strong>Pods, Deployments & ReplicaSets</strong></h3> 
      <p><strong>Pod:</strong> Smallest deployable unit in Kubernetes  usually runs one container (or tightly coupled containers).</p>
      <p><strong>ReplicaSet:</strong> Ensures a specific number of Pod replicas are running at all times.</p>
      <p><strong>Deployment:</strong> Manages ReplicaSets and defines desired application state  handles rollout, rollback, and versioning.</p>
      <ul style='margin-left:1.2rem;'>
        <li>Pod = Single running instance.</li>
        <li>ReplicaSet = Maintains desired pod count.</li>
        <li>Deployment = Declarative management layer over ReplicaSet.</li>
      </ul>
      <pre><code>kubectl get pods
kubectl get deployments
kubectl get rs
      </code></pre>
      <p><strong>In my workflow:</strong> I define Deployments in YAML with rolling update strategy, ensuring zero downtime during releases.</p>
      </div>`},{question:"Explain the difference between Deployment and StatefulSet.",answerHtml:`<div class='answer-rich'> 
      <h3> <strong>Deployment vs StatefulSet</strong></h3> 
      <p><strong>Deployment:</strong> Used for stateless workloads  all pods are identical and can be replaced freely.</p>
      <p><strong>StatefulSet:</strong> Used for stateful workloads  pods have stable identities, network names, and persistent storage.</p>
      <table style='width:100%; border-collapse:collapse; margin:1rem 0;'>
        <tr><th style='text-align:left;'>Feature</th><th style='text-align:left;'>Deployment</th><th style='text-align:left;'>StatefulSet</th></tr>
        <tr><td>Use Case</td><td>Web apps, APIs</td><td>Databases, Kafka, Redis</td></tr>
        <tr><td>Pod Identity</td><td>Random</td><td>Fixed (pod-0, pod-1...)</td></tr>
        <tr><td>Storage</td><td>Ephemeral</td><td>PersistentVolumeClaim per pod</td></tr>
      </table>
      <p><strong>Example:</strong> I use Deployments for stateless microservices and StatefulSets for MongoDB or Redis with persistent storage in AKS.</p>
      </div>`},{question:"What is a DaemonSet and why is it used?",answerHtml:`<div class='answer-rich'> 
      <h3> <strong>DaemonSet  Cluster-wide Background Pods</strong></h3> 
      <p><strong>DaemonSet</strong> ensures that a copy of a Pod runs on <strong>every node</strong> in the cluster (or selected nodes).</p>
      <p><strong>Use cases:</strong></p>
      <ul style='margin-left:1.2rem;'>
        <li>Running logging agents (Fluentd, Filebeat).</li>
        <li>Running monitoring agents (Node Exporter, Datadog, Prometheus node-exporter).</li>
        <li>Custom network plugins or security agents on each node.</li>
      </ul>
      <pre><code>kubectl get daemonsets -n kube-system</code></pre>
      <p><strong>Real-world:</strong> I deploy Prometheus Node Exporter as a DaemonSet to collect node-level metrics from every AKS node.</p>
      </div>`},{question:"What are Namespaces in Kubernetes and why are they important?",answerHtml:`<div class='answer-rich'> 
      <h3> <strong>Namespaces  Logical Isolation</strong></h3> 
      <p><strong>Namespace</strong> logically separates resources within a cluster.</p>
      <p><strong>Why important:</strong></p>
      <ul style='margin-left:1.2rem;'>
        <li>Segregates environments (dev, qa, prod) in the same cluster.</li>
        <li>Applies role-based access control (RBAC) per team or project.</li>
        <li>Enables resource quotas and network policies for isolation.</li>
      </ul>
      <pre><code>kubectl create namespace dev
kubectl get all -n prod
      </code></pre>
      <p><strong>In my setup:</strong> Each project team gets its own namespace in AKS with defined resource limits and RBAC policies.</p>
      </div>`},{question:"What are Labels and Selectors? How do they relate to Pods and Services?",answerHtml:`<div class='answer-rich'> 
      <h3> <strong>Labels & Selectors  The Glue of Kubernetes</strong></h3> 
      <p><strong>Labels:</strong> Key-value pairs attached to Kubernetes objects (like pods, nodes, services) to organize and select them.</p>
      <p><strong>Selectors:</strong> Used by Services, ReplicaSets, or Deployments to filter and manage objects with specific labels.</p>
      <p><strong>Example:</strong></p>
      <pre><code>labels:
  app: frontend
  env: prod

selector:
  matchLabels:
    app: frontend
      </code></pre>
      <p><strong>In action:</strong> My frontend service uses selector <code>app=frontend</code> to automatically route traffic to all matching pods. This decouples pod naming from service configuration.</p>
      </div>`},{question:"What are your day-to-day activities in Azure Kubernetes Service (AKS)?",answerHtml:`<div class='answer-rich'> 
      <h3> <strong>Day-to-Day in AKS</strong></h3> 
      <p>I manage and maintain multiple AKS clusters for microservices-based applications. Typical daily tasks include:</p>
      <ul style='margin-left:1.2rem;'>
        <li>Monitoring cluster health via Azure Monitor & Grafana dashboards.</li>
        <li>Deploying apps via YAML manifests and Azure DevOps CI/CD pipelines.</li>
        <li>Managing secrets and configs using <strong>Azure Key Vault</strong> & Kubernetes Secrets.</li>
        <li>Scaling deployments using <strong>Horizontal Pod Autoscaler (HPA)</strong>.</li>
        <li>Rolling updates and canary deployments via Azure DevOps pipelines.</li>
        <li>Debugging pod logs using <code>kubectl logs</code> and checking events.</li>
        <li>Integrating Prometheus + Grafana for metrics and alerting.</li>
      </ul>
      <p><strong>Hands-on example:</strong> Recently implemented a pipeline to deploy backend API to AKS with automatic scaling and health probes, reducing downtime during peak traffic.</p>
      </div>`}]},{title:"2. Kubernetes  Cluster, Pods, Deployments & Scaling",questions:[{question:"What is Kubernetes?",answerHtml:`<div class='answer-rich'>
          <h3> < strong > Overview < /strong></h3 >
      <p><strong>Kubernetes(K8s) < /strong> is an open-source container orchestration platform developed by Google. It automates <strong>deployment, scaling, load balancing, and self-healing</strong > of containerized applications.</p>
      < p > <strong>In Production: </strong> It ensures zero downtime, high availability, and smooth CI/CD rollouts.I use Kubernetes primarily for microservice orchestration in Azure AKS.</p>
        < ul style='margin-left:1.2rem;' >
        <li>Automatic scaling and rolling updates.</li>
          < li > Load balancing through Services and Ingress.</li>
            < li > Efficient resource scheduling across nodes.</li>
              < li > Self - healing(failed pods auto - recreated).</li>
              </ul>
              </div>`},{question:"Explain Kubernetes architecture.",answerHtml:`<div class='answer-rich'>
      <h3> < strong > Kubernetes Architecture < /strong></h3 >
  <p>Kubernetes follows a < strong > MasterWorker(Control PlaneNode) < /strong> architecture.</p >
  <ul style='margin-left:1.2rem;' >
  <li><strong>Control Plane: </strong> Manages cluster state, scheduling, API, and controller logic.</li >
  <li><strong>Worker Nodes: </strong> Run the actual application pods.</li >
  </ul>
  < p > <strong>Main components: </strong></p >
  <ul style='margin-left:1.2rem;' >
  <li><strong>API Server: </strong> Central communication hub between users and cluster.</li >
  <li><strong>etcd: </strong> Key-value store maintaining cluster state.</li >
  <li><strong>Controller Manager: </strong> Ensures desired state (e.g., ReplicaSet count).</li >
  <li><strong>Scheduler: </strong> Assigns pods to nodes based on resource availability.</li >
  <li><strong>Kubelet: </strong> Node agent ensuring pod containers run correctly.</li >
  <li><strong>Kube - proxy: </strong> Manages network routing and load balancing on nodes.</li >
  </ul>
  < p > <strong>In AKS: </strong> Control plane is managed by Azure; we manage worker nodes only.</p >
  </div>`},{question:"What are master and worker nodes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Master & Worker Nodes < /strong></h3 >
        <p><strong>Master(Control Plane): </strong> Responsible for cluster decisions  scheduling, health checks, desired state management.</p >
          <p><strong>Worker Node: </strong> Hosts Pods. Each node runs Kubelet, container runtime (like containerd), and kube-proxy.</p >
            <p><strong>In AKS: </strong> Control plane is abstracted  we interact via <code>kubectl</code > or Azure Portal; worker nodes scale automatically via Node Pools.</p>
              </div>`},{question:"What is a Pod in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Pod Basics < /strong></h3 >
        <p>A < strong > Pod < /strong> is the smallest deployable unit in Kubernetes. It encapsulates one or more tightly coupled containers that share the same network and storage.</p >
        <ul style='margin-left:1.2rem;' >
          <li>Each Pod gets a unique IP within the cluster.</li>
            < li > Containers inside a Pod communicate via localhost.</li>
              < li > Pods are ephemeral  recreated if they crash or during updates.</li>
                </ul>
                < p > <strong>In AKS: </strong> I usually deploy one container per Pod to isolate services and simplify autoscaling.</p >
                  </div>`},{question:"How many containers can you run in a Pod?",answerHtml:`<div class='answer-rich'>
      < p > You can run multiple containers in a Pod, but the < strong > best practice < /strong> is to keep <strong>one container per Pod</strong > unless containers are tightly coupled(like sidecar pattern).</p>
        < p > <strong>Example: </strong> A main app container and a sidecar container (like a logging or metrics agent).</p >
          </div>`},{question:"What are ReplicaSets and Deployments?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>ReplicaSets & Deployments < /strong></h3 >
        <ul style='margin-left:1.2rem;' >
          <li><strong>ReplicaSet: </strong> Ensures a specified number of Pod replicas are running at all times.</li >
            <li><strong>Deployment: </strong> Higher-level object that manages ReplicaSets, enabling rolling updates and rollbacks.</li >
              </ul>
              < p > <strong>Example: </strong> I define Deployments in YAML  set replicas, image version, and update strategy (rolling update) to ensure smooth zero-downtime releases.</p >
                </div>`},{question:"What is the difference between Stateful and Stateless applications?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Stateful vs Stateless < /strong></h3 >
        <p><strong>Stateless apps: </strong> Dont maintain session/data locally(e.g., APIs, web frontends).Any replica can handle a request.</p>
          < p > <strong>Stateful apps: </strong> Maintain state/data consistency(e.g., Databases, Kafka).Each pod has its own identity and storage.</p>
            < p > <strong>In Kubernetes: </strong> Stateless apps  Deployments; Stateful apps  StatefulSets with Persistent Volumes.</p >
              </div>`},{question:"What is a StatefulSet in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>StatefulSet  For Stateful Workloads < /strong></h3 >
        <p>Manages deployment of stateful applications that require < strong > stable network identity and persistent storage < /strong>.</p >
          <ul style='margin-left:1.2rem;' >
            <li>Pods have unique names(e.g., db - 0, db - 1).</li>
              < li > Each Pod has its own < strong > Persistent Volume Claim(PVC) < /strong>.</li >
                <li>Used for databases like MySQL, MongoDB, Cassandra.</li>
                  </ul>
                  < p > <strong>Example: </strong> I deployed MongoDB replica sets using StatefulSet with Azure Disk as persistent volume.</p >
                    </div>`},{question:"What is a DaemonSet?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>DaemonSet < /strong></h3 >
        <p>Ensures one Pod runs on < strong > every node < /strong> in the cluster (or selected nodes).</p >
          <p><strong>Use cases: </strong></p >
            <ul style='margin-left:1.2rem;' >
              <li>Node monitoring(Node Exporter).</li>
                < li > Logging(Fluent Bit / Filebeat).</li>
                < li > Security scanning or network plugins.</li>
                  </ul>
                  < p > <strong>In AKS: </strong> I run Fluent Bit as a DaemonSet to collect logs from all worker nodes.</p >
                    </div>`},{question:"What is a Namespace in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Namespace < /strong></h3 >
        <p>Provides logical isolation between resources in the same cluster.</p>
          < ul style = 'margin-left:1.2rem;' >
            <li>Separates environments like dev, qa, prod.</li>
              < li > Allows applying resource quotas and network policies.</li>
                < li > Supports RBAC for team - level access control.</li>
                  </ul>
                  < p > <strong>Example: </strong> I create separate namespaces per environment and assign roles using ClusterRoleBinding.</p >
                    </div>`},{question:"What is ClusterRole and ClusterRoleBinding?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>ClusterRole & ClusterRoleBinding < /strong></h3 >
        <p><strong>ClusterRole: </strong> Defines permissions (verbs like get, list, create) at the cluster level.</p >
          <p><strong>ClusterRoleBinding: </strong> Binds that role to a user, group, or service account.</p >
            <pre><code>kubectl create clusterrolebinding dev - admin \\
  --clusterrole=cluster - admin \\
  --user=devops @company.com</code></pre >
    <p><strong>In AKS: </strong> I use ClusterRoles for admin-level access across namespaces and normal Roles for namespace-specific tasks.</p >
      </div>`},{question:"What are ConfigMaps and Secrets?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>ConfigMaps & Secrets < /strong></h3 >
        <p><strong>ConfigMaps: </strong> Store non-confidential configuration data (key-value pairs).</p >
          <p><strong>Secrets: </strong> Store sensitive data (passwords, tokens, certificates)  base64 encoded.</p >
            <p><strong>In Practice: </strong> I mount ConfigMaps as environment variables or volumes for app configs, and Secrets for credentials.</p >
              </div>`},{question:"How do you manage secrets in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Managing Secrets Securely < /strong></h3 >
        <ul style='margin-left:1.2rem;' >
          <li>Create secrets using < code > kubectl create secret generic < /code>.</li >
            <li>Use < strong > Azure Key Vault < /strong> integration with AKS for secure secret injection.</li >
              <li>Mount secrets as environment variables or files in Pods.</li>
                < li > Enable < strong > Encryption at Rest < /strong> for secret data.</li >
                  </ul>
                  < p > <strong>Example: </strong> I use Azure Key Vault CSI driver to sync secrets directly into Pods  no plain YAML exposure.</p >
                    </div>`},{question:"What is a Headless Service and when is it used?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Headless Service  Direct Pod Access < /strong></h3 >
        <p>Headless Service(ClusterIP = None) allows clients to directly access individual Pods instead of load - balancing traffic.</p>
          < p > <strong>Used when: </strong></p >
            <ul style='margin-left:1.2rem;' >
              <li>Pods need to communicate directly(like StatefulSets).</li>
                < li > Applications like databases(MongoDB, Cassandra) need stable DNS names for each Pod.</li>
                  </ul>
                  < p > <strong>Example: </strong> I used headless service for MongoDB StatefulSet, allowing replica pods to discover each other using DNS entries like <code>mongo-0.db-svc.default.svc.cluster.local</code >.</p>
                    </div>`},{question:"What is the difference between ClusterIP, NodePort, and LoadBalancer services?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Service Types Comparison < /strong></h3 >
        <table style='width:100%; border-collapse:collapse; margin:1rem 0;' >
          <tr><th>Type < /th><th>Scope</th > <th>Use Case < /th></tr >
            <tr><td><strong>ClusterIP < /strong></td > <td>Accessible only within cluster < /td><td>Internal microservice communication</td > </tr>
              < tr > <td><strong>NodePort < /strong></td > <td>Exposes service on each node IP: Port < /td><td>For local or limited external testing</td > </tr>
                < tr > <td><strong>LoadBalancer < /strong></td > <td>Creates external load balancer(Azure LB) < /td><td>Expose apps publicly with stable IP</td > </tr>
                  </table>
                  < p > <strong>In AKS: </strong> I use LoadBalancer for external APIs, ClusterIP for backend services, and NodePort only for debugging.</p >
                    </div>`},{question:"What is Ingress Controller and Ingress resource?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Ingress  Smart HTTP Routing < /strong></h3 >
        <p><strong>Ingress Controller < /strong> manages inbound HTTP/HTTPS traffic and routes it to services based on rules defined in <strong>Ingress resources < /strong>.</p >
          <p><strong>Example: </strong></p >
            <pre><code>host: api.company.com
              / app1 -> service app1
                / app2 -> service app2 < /code></pre >
                  <p><strong>Popular Ingress Controllers: </strong> NGINX, Azure Application Gateway (AGIC), Traefik.</p >
                    <p><strong>In AKS: </strong> I use NGINX Ingress Controller for path-based routing and SSL termination. Its integrated with Azure DNS and certificates via cert-manager.</p >
                      </div>`},{question:"How do you debug a Pod not running or in pending state?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Debugging Pods < /strong></h3 >
        <ul style='margin-left:1.2rem;' >
          <li>Check pod events and describe details: </li>
            < pre > <code>kubectl describe pod < pod - name > </code></pre >
              <li>Check logs for errors: </li>
                < pre > <code>kubectl logs < pod - name > </code></pre >
                  <li>Check node status and resources: </li>
                    < pre > <code>kubectl get nodes - o wide < /code></pre >
                      <li>Use ephemeral debug container: </li>
                        < pre > <code>kubectl debug pod / <pod-name > -it--image = busybox < /code></pre >
                          </ul>
                          < p > <strong>In real cases: </strong> I mostly found pending pods due to insufficient CPU/memory or node taints.For CrashLoopBackOff, logs usually show wrong config / env issue.</p>
                            </div>`},{question:"What are possible reasons for Pod stuck in Pending state?",answerHtml:`<div class='answer-rich'>
      < ul style = 'margin-left:1.2rem;' >
        <li>Insufficient cluster resources(CPU / RAM).</li>
          < li > Node selector or affinity mismatch.</li>
            < li > PersistentVolumeClaim not bound.</li>
              < li > Taints on nodes preventing scheduling.</li>
                < li > Network or image pull issues.</li>
                  </ul>
                  < p > <strong>Real Example: </strong> In AKS, my pod was pending due to missing node pool label that Deployment selector required  fixed by adding correct labels.</p >
                    </div>`},{question:"How do you check Pod logs and events?",answerHtml:`<div class='answer-rich'>
      < pre > <code>kubectl logs < pod - name >
        kubectl logs < pod - name > -c < container - name >
          kubectl describe pod < pod - name > </code></pre >
            <p><strong>In practice: </strong> I also integrate <strong>Azure Monitor + Log Analytics</strong > with AKS, so I can query logs using KQL or view real-time logs from Grafana.</p>
              </div>`},{question:"What is Horizontal Pod Autoscaler (HPA)?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Horizontal Pod Autoscaler(HPA) < /strong></h3 >
        <p>Automatically scales the number of pods based on CPU / memory usage or custom metrics.</p>
          < pre > <code>kubectl autoscale deployment webapp--min = 2 --max = 5 --cpu - percent=70 < /code></pre >
            <p><strong>In AKS: </strong> I use HPA with Azure Monitor metrics to handle traffic spikes dynamically  for example, scaling API pods from 3  10 during load testing.</p >
              </div>`},{question:"How does Kubernetes perform service discovery?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Service Discovery in Kubernetes < /strong></h3 >
        <p>Kubernetes uses < strong > CoreDNS < /strong> for service discovery. Every service gets a DNS entry inside the cluster.</p >
          <p><strong>Example: </strong> <code>backend.default.svc.cluster.local</code > resolves to backend services ClusterIP.</p>
            < p > <strong>In AKS: </strong> I often use internal DNS for microservice-to-microservice communication without hardcoding IPs.</p >
              </div>`},{question:"How does auto-scaling work in AKS?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>AKS Auto - Scaling < /strong></h3 >
        <ul style='margin-left:1.2rem;' >
          <li><strong>Pod - level scaling: </strong> via HPA.</li >
            <li><strong>Node - level scaling: </strong> via Cluster Autoscaler  adds/removes worker nodes based on workload demand.</li>
              </ul>
              < p > <strong>Example: </strong> In production, I configured AKS autoscaler to scale node pool between 310 nodes automatically when HPA increases replicas.</p >
                </div>`},{question:"How to configure AKS in Azure Portal?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Configuring AKS in Portal < /strong></h3 >
        <ol style='margin-left:1.2rem;' >
          <li>Go to < strong > Azure Portal  Kubernetes Services  + Create.< /strong></li >
            <li>Specify resource group, cluster name, and region.</li>
              < li > Choose < strong > Node Size < /strong>, node count, and enable auto-scaling.</li >
                <li>Enable monitoring with <strong>Azure Monitor + Log Analytics.< /strong></li >
                  <li>Integrate with <strong>Azure AD < /strong> for RBAC.</li >
                    </ol>
                    < p > <strong>Pro Tip: </strong> I always use separate node pools for system and user workloads  helps manage scaling efficiently.</p >
                      </div>`},{question:"What is Taint and Toleration?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Taints & Tolerations < /strong></h3 >
        <p><strong>Taints: </strong> Applied on nodes to restrict pod scheduling.</p >
          <p><strong>Tolerations: </strong> Applied on pods to allow them to be scheduled on tainted nodes.</p >
            <pre><code>kubectl taint nodes node1 key = value: NoSchedule < /code></pre >
              <p><strong>Example: </strong> I taint GPU nodes so only ML workloads with proper tolerations can schedule there.</p >
                </div>`},{question:"What are Liveness and Readiness probes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Probes for Health Checks < /strong></h3 >
        <p><strong>Liveness Probe: </strong> Checks if container is healthy  restarts if failed.</p >
          <p><strong>Readiness Probe: </strong> Checks if container is ready to receive traffic.</p >
            <pre><code>livenessProbe:
  httpGet:
  path: /health
  port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5 < /code></pre >
    <p><strong>In my workloads: </strong> Proper probes ensure zero-downtime rollouts  AKS waits until pods are fully ready before routing traffic.</p >
      </div>`},{question:"What is Helm in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Helm  Kubernetes Package Manager < /strong></h3 >
        <p><strong>Helm < /strong> simplifies app deployment by packaging YAML manifests into reusable charts.</p >
        <p><strong>Benefits: </strong></p >
          <ul style='margin-left:1.2rem;' >
            <li>Version - controlled deployments.</li>
              < li > Parameterization via < code > values.yaml < /code>.</li >
                <li>Easy rollback and upgrades.</li>
                  </ul>
                  < p > <strong>Example: </strong> I deploy NGINX Ingress and Prometheus using official Helm charts with custom values files.</p >
                    </div>`},{question:"What are Helm charts and their types?",answerHtml:`<div class='answer-rich'>
      < p > <strong>Helm Chart: </strong> A collection of templates + configuration files for deploying an app.</p >
        <p><strong>Types: </strong></p >
          <ul style='margin-left:1.2rem;' >
            <li><strong>Official Charts: </strong> Maintained by community (e.g., prometheus-community).</li >
              <li><strong>Custom Charts: </strong> Built in-house for internal apps.</li >
                </ul>
                < pre > <code>helm install myapp./ mychart - f values.yaml < /code></pre >
                  </div>`},{question:"What is a Replica in Kubernetes?",answerHtml:`<div class='answer-rich'>
      < p > <strong>Replica: </strong> Refers to a running instance of a Pod. Kubernetes maintains replicas using ReplicaSets to ensure desired count.</p >
        <p><strong>Example: </strong> If replica count = 3, Kubernetes ensures 3 pods are always running even if one crashes.</p >
          </div>`},{question:"What is a deployment strategy (Rolling, Recreate, Blue-Green, Canary)?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Deployment Strategies < /strong></h3 >
        <ul style='margin-left:1.2rem;' >
          <li><strong>Rolling Update: </strong> Replaces pods gradually  zero downtime (default in AKS).</li >
            <li><strong>Recreate: </strong> Deletes old pods first  short downtime.</li >
              <li><strong>Blue - Green: </strong> Run two environments (blue=current, green=new), then switch traffic.</li >
                <li><strong>Canary: </strong> Release new version to a small subset first.</li >
                  </ul>
                  < p > <strong>In AKS: </strong> I often use rolling updates and canary via Azure DevOps pipeline with traffic splitting in Ingress.</p >
                    </div>`},{question:"What is Persistent Volume (PV) and Persistent Volume Claim (PVC)?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Persistent Storage in Kubernetes < /strong></h3 >
        <ul style='margin-left:1.2rem;' >
          <li><strong>PV: </strong> Actual storage provisioned in cluster (e.g., Azure Disk, NFS).</li >
            <li><strong>PVC: </strong> Request by a Pod to use specific PV.</li >
              </ul>
              < p > <strong>Example: </strong> In AKS, I use PVCs to attach Azure Managed Disks for stateful applications like databases.</p >
                </div>`},{question:"What is a NodePort service?",answerHtml:`<div class='answer-rich'>
      < p > <strong>NodePort < /strong> exposes a service on a static port (3000032767) across all nodes, allowing external access via <code>NodeIP:NodePort</code >.</p>
      < p > <strong>In practice: </strong> I use NodePort for internal debugging or temporary access, not for production traffic.</p >
        </div>`},{question:"How do you perform rolling update and rollback?",answerHtml:`<div class='answer-rich'>
      < pre > <code>kubectl rollout status deployment / webapp
kubectl rollout undo deployment / webapp < /code></pre >
    <p><strong>Rolling update: </strong> Gradually replaces old pods with new ones.</p >
      <p><strong>Rollback: </strong> Returns to previous ReplicaSet version instantly.</p >
        <p><strong>In CI / CD: </strong> I trigger rolling updates via pipeline; AKS automatically handles rollback if health probes fail.</p >
          </div>`},{question:"How do you monitor Kubernetes clusters?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Monitoring Clusters < /strong></h3 >
        <p><strong>Tools used: </strong> Prometheus, Grafana, Azure Monitor, Kube-State-Metrics.</p >
          <ul style='margin-left:1.2rem;' >
            <li>Monitor CPU, memory, pod restarts, and node health.</li>
              < li > Set alerts via Prometheus or Azure Alerts.</li>
                < li > Use Grafana dashboards for visual trends.</li>
                  </ul>
                  < p > <strong>In AKS: </strong> Integrated Azure Monitor and custom Grafana dashboards per namespace and service.</p >
                    </div>`},{question:"How do you integrate Prometheus and Grafana for monitoring?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Prometheus + Grafana Integration < /strong></h3 >
        <ol style='margin-left:1.2rem;' >
          <li>Deploy Prometheus via Helm chart.</li>
            < li > Expose metrics endpoints using ServiceMonitors.</li>
              < li > Deploy Grafana and configure Prometheus datasource.</li>
                < li > Import pre - built dashboards(CPU, pod status, etc.).</li>
                  </ol>
                  < p > <strong>In practice: </strong> I use Helm for both and integrate with Azure AD SSO for dashboard authentication.</p >
                    </div>`},{question:"What command checks resource usage in Kubernetes?",answerHtml:`<div class='answer-rich'>
      < pre > <code>kubectl top nodes
kubectl top pods - n < namespace > </code></pre >
    <p><strong>In AKS: </strong> Metrics Server must be installed  I use it with HPA and custom dashboards for real-time resource visibility.</p >
      </div>`},{question:"What is the smallest component in AKS?",answerHtml:`<div class='answer-rich'>
      < p > <strong>Pod < /strong> is the smallest deployable component in AKS  it runs one or more containers with shared network and storage context.</p >
      </div>`},{question:"How do you manage network policies in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Network Policies  Pod - level Firewall < /strong></h3 >
        <p>Used to control communication between pods and namespaces(based on labels and ports).</p>
          < pre > <code>apiVersion: networking.k8s.io / v1
  kind: NetworkPolicy
  metadata:
  name: allow - frontend
  spec:
  podSelector:
  matchLabels:
  app: frontend
  ingress:
  - from:
  - podSelector:
  matchLabels:
  app: backend < /code></pre >
    <p><strong>In AKS: </strong> I enable Azure CNI and apply NetworkPolicies to restrict inter-service communication, improving cluster security.</p >
      </div>`}]},{title:"3. Kubernetes (AKS / K8s)",questions:[{question:"What is Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Kubernetes  The Container Orchestrator</strong></h3>
      <p><strong>Kubernetes (K8s)</strong> is an open-source platform that automates deployment, scaling, and management of containerized applications.</p>
      <p><strong>Why it's used:</strong></p>
      <ul style='margin-left:1.2rem;'>
        <li>Manages containerized workloads across multiple nodes.</li>
        <li>Provides self-healing (restarts crashed pods automatically).</li>
        <li>Handles service discovery, load balancing, and auto-scaling.</li>
        <li>Enables rolling updates and zero-downtime deployments.</li>
      </ul>
      <p><strong>In AKS:</strong> I use Kubernetes for hosting microservices, managing deployments through YAML and Azure DevOps pipelines, and auto-scaling with metrics.</p>
      </div>`},{question:"What is the architecture of Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Kubernetes Architecture  Control Plane & Worker Nodes</strong></h3>
      <p>Kubernetes has a <strong>Control Plane (Master)</strong> that manages the cluster and <strong>Worker Nodes</strong> that run the actual workloads.</p>
      <h4> Control Plane Components:</h4>
      <ul style='margin-left:1.2rem;'>
        <li><strong>API Server:</strong> The entry point for all cluster operations (<code>kubectl</code> commands interact here).</li>
        <li><strong>etcd:</strong> Key-value store maintaining the clusters desired and current state.</li>
        <li><strong>Controller Manager:</strong> Ensures desired state (like number of replicas) is maintained.</li>
        <li><strong>Scheduler:</strong> Assigns pods to appropriate nodes based on resource availability.</li>
      </ul>
      <h4> Worker Node Components:</h4>
      <ul style='margin-left:1.2rem;'>
        <li><strong>Kubelet:</strong> Node agent ensuring that pods are running as expected.</li>
        <li><strong>Kube-proxy:</strong> Handles networking and service routing inside the cluster.</li>
        <li><strong>Container Runtime:</strong> (Docker / containerd) runs the containers themselves.</li>
      </ul>
      <p><strong>In AKS:</strong> Azure manages the Control Plane  I only manage Worker Nodes, Deployments, and Networking.</p>
      </div>`},{question:"What are different services in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Types of Kubernetes Services</strong></h3>
      <p>Kubernetes Services expose applications running on Pods to internal or external traffic.</p>
      <ul style='margin-left:1.2rem;'>
        <li><strong>ClusterIP:</strong> Default type; internal access only (used for service-to-service communication).</li>
        <li><strong>NodePort:</strong> Exposes service on a static port on all nodes (<code>NodeIP:NodePort</code>).</li>
        <li><strong>LoadBalancer:</strong> Integrates with cloud load balancer (like Azure Load Balancer) for external access.</li>
        <li><strong>Headless Service:</strong> No load balancing, DNS resolves directly to Pod IPs  used with StatefulSets.</li>
      </ul>
      <p><strong>In AKS:</strong> I generally use LoadBalancer for APIs and ClusterIP for internal microservices.</p>
      </div>`},{question:"What is stateless vs stateful in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Stateless vs Stateful Workloads</strong></h3>
      <p><strong>Stateless:</strong> Applications that dont maintain session or data locally. Any replica can handle any request.</p>
      <ul style='margin-left:1.2rem;'>
        <li>Examples: Web frontends, APIs, microservices.</li>
        <li>Managed using Deployments.</li>
      </ul>
      <p><strong>Stateful:</strong> Applications that need persistent data and stable network identity.</p>
      <ul style='margin-left:1.2rem;'>
        <li>Examples: Databases (MySQL, MongoDB), Kafka.</li>
        <li>Managed using StatefulSets and PersistentVolumes.</li>
      </ul>
      <p><strong>In AKS:</strong> I deploy stateless microservices as Deployments and databases using StatefulSets with Azure Disks.</p>
      </div>`},{question:"What is a StatefulSet in Kubernetes?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>StatefulSet  Managing Stateful Applications</strong></h3>
      <p><strong>StatefulSet</strong> provides stable, persistent Pod identity and storage. Each Pod has its own <strong>Persistent Volume Claim (PVC)</strong> and stable hostname.</p>
      <ul style='margin-left:1.2rem;'>
        <li>Used for databases, queues, or storage systems.</li>
        <li>Pods follow a strict startup/termination order (pod-0, pod-1, etc.).</li>
        <li>Supports persistent data through PVCs even after restarts.</li>
      </ul>
      <p><strong>In AKS:</strong> I use StatefulSets for PostgreSQL and Redis deployments with Azure Managed Disks for data durability.</p>
      </div>`},{question:"What is a Deployment Set?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Deployment  Declarative Pod Management</strong></h3>
      <p><strong>Deployment</strong> defines the desired state for stateless applications. It manages ReplicaSets and handles rolling updates, scaling, and rollbacks automatically.</p>
      <ul style='margin-left:1.2rem;'>
        <li>Ensures a specified number of replicas are always running.</li>
        <li>Performs zero-downtime rolling updates.</li>
        <li>Allows easy rollback to previous versions.</li>
      </ul>
      <pre><code>kubectl rollout status deployment/webapp
kubectl rollout undo deployment/webapp</code></pre>
      <p><strong>In practice:</strong> I deploy microservices as Deployments using Azure DevOps YAML pipelines and manage blue-green or rolling deployments via AKS.</p>
      </div>`},{question:"What is a Namespace in K8s?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>Namespace  Logical Segmentation</strong></h3>
      <p><strong>Namespace</strong> helps in logically isolating resources (Pods, Services, Deployments) inside a cluster.</p>
      <ul style='margin-left:1.2rem;'>
        <li>Commonly used for separating environments  dev, qa, prod.</li>
        <li>Supports applying resource quotas and access controls per team or project.</li>
        <li>Improves resource visibility and security.</li>
      </ul>
      <pre><code>kubectl create namespace staging
kubectl get all -n prod</code></pre>
      <p><strong>In AKS:</strong> I maintain separate namespaces for different environments and link each with unique Azure AD RBAC permissions.</p>
      </div>`},{question:"What is ClusterRoleBinding?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>ClusterRoleBinding  Cluster-wide Access Control</strong></h3>
      <p><strong>ClusterRoleBinding</strong> assigns a <strong>ClusterRole</strong> (set of permissions) to a user, group, or service account at the cluster level.</p>
      <pre><code>kubectl create clusterrolebinding devops-admin \\
  --clusterrole=cluster-admin \\
  --user=ritesh@ritti.com</code></pre>
      <p><strong>Example:</strong> I use ClusterRoleBinding to grant cluster-admin access to DevOps team members while restricting developers to specific namespaces.</p>
      </div>`},{question:"What is ReplicaSet?",answerHtml:`<div class='answer-rich'>
      <h3> <strong>ReplicaSet  Ensuring Desired Pod Count</strong></h3>
      <p><strong>ReplicaSet</strong> ensures that a specified number of identical Pods are running at all times. If a Pod fails, it creates a new one automatically.</p>
      <ul style='margin-left:1.2rem;'>
        <li>Used by Deployments under the hood for scaling and versioning.</li>
        <li>Can be manually created for simple workloads, but Deployments are preferred for lifecycle management.</li>
      </ul>
      <pre><code>kubectl get rs
kubectl scale rs webapp --replicas=5</code></pre>
      <p><strong>In my AKS environment:</strong> ReplicaSets handle pod redundancy for high availability and work together with HPA for auto-scaling during peak load.</p>
      </div>`},{question:"What is Pod?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pod  Smallest Deployable Unit in Kubernetes</strong></h3>
        <p>A <strong>Pod</strong> is the smallest deployable object in Kubernetes that can host one or more containers sharing the same network namespace, storage, and lifecycle.</p>
        <p><strong>In Practice:</strong> Each Pod has its own IP; containers inside a Pod communicate via localhost, making them tightly coupled (useful for sidecar patterns).</p>
      </div>`},{question:"What is a Headless Service?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Headless Service  Direct Pod-to-Pod Discovery</strong></h3>
        <p>A <strong>Headless Service</strong> (created with <code>clusterIP: None</code>) exposes Pod IPs directly instead of load-balancing traffic.</p>
        <p><strong>Use Case:</strong> Stateful apps (like databases) where client needs to directly resolve individual Pod endpoints using DNS (e.g., StatefulSets).</p>
      </div>`},{question:"What is a DaemonSet?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DaemonSet  Runs Pod on Every Node</strong></h3>
        <p>A <strong>DaemonSet</strong> ensures a copy of a specific Pod runs on all (or selected) nodes in the cluster.</p>
        <ul style="margin-left:1.2rem;">
          <li>Used for cluster-level agents like log collectors (Fluentd), monitoring (Prometheus Node Exporter), or networking (CNI).</li>
          <li>Automatically deploys Pods on new nodes as they join the cluster.</li>
        </ul>
      </div>`},{question:"What is Taint and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Taints & Tolerations  Scheduling Control</strong></h3>
        <p><strong>Taints</strong> are applied on nodes to repel certain Pods unless they have matching <strong>tolerations</strong>.</p>
        <pre><code>kubectl taint nodes node1 key=value:NoSchedule</code></pre>
        <p><strong>In Practice:</strong> Used to isolate workloads  e.g., run production-only Pods on dedicated nodes and block dev Pods.</p>
      </div>`},{question:"What is the smallest component in AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pod  Smallest Executable Unit</strong></h3>
        <p>In AKS (Azure Kubernetes Service), the smallest schedulable component is a <strong>Pod</strong>  it runs one or more containers as a logical unit.</p>
        <p><strong>Note:</strong> Containers are not scheduled directly; Pods abstract container lifecycle & resource management.</p>
      </div>`},{question:"How do you manage secrets in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Secrets Securely</strong></h3>
        <p>Kubernetes <strong>Secrets</strong> are used to store sensitive data like passwords, tokens, and keys (base64 encoded by default).</p>
        <pre><code>kubectl create secret generic db-secret --from-literal=DB_PASS='P@ssw0rd'</code></pre>
        <p><strong>Best Practices:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use external secret stores (Azure Key Vault, HashiCorp Vault).</li>
          <li>Integrate with <strong>Secrets Store CSI Driver</strong> for runtime injection.</li>
          <li>Restrict RBAC access to Secrets API.</li>
        </ul>
      </div>`},{question:"What is Helm?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Helm  Package Manager for Kubernetes</strong></h3>
        <p><strong>Helm</strong> simplifies deployment by packaging K8s manifests into versioned bundles called <strong>charts</strong>.</p>
        <p><strong>In Practice:</strong> I use Helm to deploy complex apps (NGINX, Prometheus, ArgoCD) using parameterized <code>values.yaml</code> for different environments.</p>
      </div>`},{question:"What are Helm Charts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Helm Charts  Reusable Deployment Templates</strong></h3>
        <p>Helm charts are directories containing templates, values, and metadata for deploying Kubernetes resources in a repeatable way.</p>
        <pre><code>helm install myapp ./chart --values values-prod.yaml</code></pre>
        <p><strong>Benefit:</strong> Enables consistent, parameterized deployments across environments (dev, QA, prod).</p>
      </div>`},{question:"What is Stateful Deployment in K8s?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>StatefulSet  Persistent & Ordered Deployment</strong></h3>
        <p>Used for applications requiring stable network identities, persistent storage, and ordered scaling (e.g., databases, Kafka).</p>
        <p><strong>In Practice:</strong> Each Pod gets a predictable DNS name (e.g., <code>mysql-0.mysql</code>), and volumes are preserved across restarts.</p>
      </div>`},{question:"Have you deployed Kubernetes clusters? What all did you deploy?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Hands-on with AKS Deployments</strong></h3>
        <p>Yes, deployed and managed multiple AKS clusters for production workloads.</p>
        <ul style="margin-left:1.2rem;">
          <li>Set up AKS with Azure CNI, AAD integration, and managed identities.</li>
          <li>Deployed apps using Helm & GitHub Actions CI/CD.</li>
          <li>Configured Ingress (NGINX) + cert-manager for SSL automation.</li>
          <li>Integrated monitoring with Azure Monitor + Prometheus + Grafana.</li>
        </ul>
      </div>`},{question:"How do you maintain a Kubernetes cluster?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cluster Maintenance  Keeping AKS Healthy</strong></h3>
        <p>I focus on proactive maintenance to keep the cluster secure, performant, and compliant:</p>
        <ul style="margin-left:1.2rem;">
          <li>Regularly apply <strong>node OS patches</strong> and <strong>Kubernetes version upgrades</strong>.</li>
          <li>Monitor cluster health via <strong>Azure Monitor, Prometheus, and Grafana</strong>.</li>
          <li>Use <strong>Cluster Autoscaler</strong> and <strong>Horizontal Pod Autoscaler</strong> to balance workloads.</li>
          <li>Audit RBAC, network policies, and secrets for security drift.</li>
          <li>Perform periodic <strong>etcd snapshot backups</strong> for disaster recovery.</li>
        </ul>
        <p><strong>In Practice:</strong> I automate node image upgrades and apply policy validation using <strong>Azure Policy for AKS</strong>.</p>
      </div>`},{question:"How to upgrade a Kubernetes cluster?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cluster Upgrades  Safe Rolling Strategy</strong></h3>
        <p>Upgrades in AKS are handled via Azure CLI or Portal with zero-downtime strategy:</p>
        <pre><code>az aks upgrade --resource-group myRG --name myAKS --kubernetes-version 1.30.2</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Upgrade <strong>control plane</strong> first, then <strong>node pools</strong>.</li>
          <li>Use <strong>staging environments</strong> to test workload compatibility.</li>
          <li>Monitor during upgrade via <code>kubectl get nodes -w</code> and Azure portal logs.</li>
        </ul>
        <p><strong>Tip:</strong> Always use <strong>maintenance windows</strong> and <strong>node image auto-upgrade</strong> for controlled rollouts.</p>
      </div>`},{question:"How to configure AKS with Azure Portal?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS Setup via Azure Portal</strong></h3>
        <p>Through Azure Portal  Create Kubernetes Service wizard:</p>
        <ol style="margin-left:1.2rem;">
          <li>Select subscription & resource group.</li>
          <li>Configure node pools (VM size, scaling, OS type).</li>
          <li>Enable AAD integration & managed identity.</li>
          <li>Choose network model (Azure CNI / Kubenet).</li>
          <li>Integrate with <strong>Azure Monitor</strong> for logging & metrics.</li>
        </ol>
        <p><strong>In Practice:</strong> I also integrate <strong>Azure Key Vault CSI driver</strong> and link ACR during creation.</p>
      </div>`},{question:"How does Kubernetes handle service discovery?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Service Discovery  DNS & ClusterIP</strong></h3>
        <p>Kubernetes uses an internal <strong>DNS service (CoreDNS)</strong> to resolve service names to ClusterIPs.</p>
        <ul style="margin-left:1.2rem;">
          <li>Each Service gets a stable virtual IP (ClusterIP).</li>
          <li>Pods can reach other services via <code>myservice.mynamespace.svc.cluster.local</code>.</li>
          <li>For external access, use <strong>LoadBalancer</strong> or <strong>Ingress Controller</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> DNS is auto-managed; I validate using <code>kubectl exec -it pod -- nslookup service-name</code>.</p>
      </div>`},{question:"How to integrate AKS with ACR (Azure Container Registry)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS + ACR Integration</strong></h3>
        <p>Grant AKS permission to pull images from ACR using role assignment:</p>
        <pre><code>az aks update -n myAKS -g myRG --attach-acr myACR</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>This creates a Managed Identity link between AKS and ACR.</li>
          <li>No manual docker login or secret creation needed.</li>
        </ul>
        <p><strong>In Practice:</strong> For multi-tenant environments, I use separate ACRs and scoped RBAC for image access.</p>
      </div>`},{question:"How to troubleshoot a pod in Pending state?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Troubleshooting Pending Pods</strong></h3>
        <p>Common causes: insufficient resources, missing node selector match, or scheduling taints.</p>
        <pre><code>kubectl describe pod &lt;pod-name&gt;</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Check <strong>Events</strong> section for errors like 0/3 nodes available.</li>
          <li>Verify node capacity (<code>kubectl get nodes -o wide</code>).</li>
          <li>Ensure correct <strong>tolerations</strong> or <strong>storage class</strong> availability.</li>
        </ul>
        <p><strong>In Practice:</strong> I usually scale node pool or adjust resource requests to resolve Pending Pods.</p>
      </div>`},{question:"How much should be the ideal pod-to-container ratio?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pod-to-Container Ratio  Design Best Practice</strong></h3>
        <p>Typically, a Pod hosts <strong>12 containers</strong> (main app + sidecar). Avoid overpacking containers in a Pod.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>1 container:</strong> independent microservices.</li>
          <li><strong>2 containers:</strong> sidecar pattern (e.g., Envoy proxy, Fluent Bit).</li>
          <li><strong>Never</strong> mix unrelated containers inside a single Pod.</li>
        </ul>
        <p><strong>In Practice:</strong> I follow one responsibility per Pod rule for observability and scaling clarity.</p>
      </div>`},{question:"How to monitor a Kubernetes cluster?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cluster Monitoring & Observability</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Monitor for Containers</strong> for AKS integration.</li>
          <li>Install <strong>Prometheus + Grafana</strong> via Helm for custom metrics dashboards.</li>
          <li>Enable <strong>Container Insights</strong> for CPU, memory, and node-level metrics.</li>
          <li>Use <strong>kubectl top</strong> or <strong>metrics-server</strong> for quick checks.</li>
        </ul>
        <p><strong>In Practice:</strong> I set up alert rules for Pod restarts, node pressure, and resource thresholds integrated with Slack or Teams via webhooks.</p>
      </div>`},{question:"What are ConfigMaps and Secrets?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ConfigMaps & Secrets  Managing App Configs</strong></h3>
        <p>They externalize configuration from container images.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>ConfigMap:</strong> Stores non-sensitive data (like URLs, env vars).</li>
          <li><strong>Secret:</strong> Stores sensitive data (like passwords, tokens) in base64 format.</li>
        </ul>
        <pre><code>kubectl create configmap app-config --from-literal=env=prod
kubectl create secret generic db-secret --from-literal=password=Pass@123</code></pre>
        <p><strong>In Practice:</strong> I mount them as env vars or volumes, and integrate Secrets with <strong>Azure Key Vault CSI driver</strong> for auto-sync.</p>
      </div>`},{question:"What is Horizontal Pod Autoscaler (HPA)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>HPA  Scaling Pods Automatically</strong></h3>
        <p>HPA automatically scales the number of pods based on CPU, memory, or custom metrics.</p>
        <pre><code>kubectl autoscale deployment myapp --cpu-percent=70 --min=2 --max=10</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Uses <strong>metrics-server</strong> to fetch real-time utilization.</li>
          <li>Continuously adjusts pod replicas based on defined thresholds.</li>
        </ul>
        <p><strong>In Practice:</strong> I use <strong>Prometheus Adapter</strong> for custom metrics-based autoscaling (like QPS or latency).</p>
      </div>`},{question:"What is the use of Network Policies?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Network Policies  Controlling Pod Communication</strong></h3>
        <p>They define how pods communicate with each other and external resources within the cluster.</p>
        <pre><code>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
spec:
  podSelector:
    matchLabels:
      app: frontend
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: backend</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Restrict lateral movement & isolate workloads.</li>
          <li>Implemented using CNI plugins (like Calico or Azure CNI).</li>
        </ul>
        <p><strong>In Practice:</strong> I enforce default-deny rules and allow only required ingress/egress paths.</p>
      </div>`},{question:"How do you perform canary deployment in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Canary Deployment  Gradual Rollout of New Version</strong></h3>
        <p>Deploys new version of app to small subset of users before full rollout.</p>
        <pre><code>apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 10
  selector:
    matchLabels: app: myapp
  template:
    metadata:
      labels:
        version: v2</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Use two Deployments (stable + canary) with <strong>weighted services</strong>.</li>
          <li>Managed via <strong>Ingress Controller (NGINX/Traefik)</strong> or <strong>Service Mesh (Istio/Linkerd)</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Helm hooks or Argo Rollouts for controlled traffic shifting & auto rollback.</p>
      </div>`},{question:"Have you worked with auto-scaling in AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS Auto-Scaling  Node & Pod Levels</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Cluster Autoscaler:</strong> Scales node pools when pods cant be scheduled.</li>
          <li><strong>HPA:</strong> Scales pods based on resource usage.</li>
          <li><strong>VPA:</strong> (Vertical Pod Autoscaler) Adjusts pod resource requests.</li>
        </ul>
        <p><strong>In Practice:</strong> I enable cluster-autoscaler via Azure CLI and configure HPA for microservices  both work together seamlessly.</p>
      </div>`},{question:"How does auto-scaling work internally?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Autoscaling Internals  HPA & Cluster Autoscaler</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Metrics Server</strong> collects CPU/memory data from Kubelet.</li>
          <li><strong>HPA Controller</strong> compares actual metrics vs target.</li>
          <li>If pods exceed target utilization  increase replicas.</li>
          <li>If pods cant schedule  <strong>Cluster Autoscaler</strong> adds new nodes automatically.</li>
        </ul>
        <p><strong>In Practice:</strong> I monitor these via <strong>Azure Monitor Insights</strong> and tune scaling thresholds per workload type.</p>
      </div>`},{question:"What is kubectl and some useful commands?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>kubectl  The Kubernetes CLI Tool</strong></h3>
        <p>kubectl interacts with the Kubernetes API server for managing cluster resources.</p>
        <ul style="margin-left:1.2rem;">
          <li><code>kubectl get pods</code>  list all pods</li>
          <li><code>kubectl describe pod &lt;name&gt;</code>  show details/events</li>
          <li><code>kubectl logs &lt;pod&gt;</code>  view logs</li>
          <li><code>kubectl exec -it &lt;pod&gt; -- /bin/bash</code>  shell into pod</li>
          <li><code>kubectl apply -f file.yaml</code>  create/update resource</li>
          <li><code>kubectl top pods</code>  show pod resource usage</li>
        </ul>
        <p><strong>In Practice:</strong> I create aliases like <code>k get po</code> or use <strong>kubectx + kubens</strong> for faster context switching.</p>
      </div>`}]},{title:"4. Services, Networking & Ingress in Kubernetes",questions:[{question:"What are Services in Kubernetes and what types exist?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Services  Stable Networking for Pods</strong></h3>
        <p>Services provide a stable IP and DNS name to access pods, even if pod IPs change dynamically.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>ClusterIP:</strong> Default type  exposes service within cluster only.</li>
          <li><strong>NodePort:</strong> Exposes service on a static port of each node.</li>
          <li><strong>LoadBalancer:</strong> Provisions external LB (e.g., Azure LB or AWS ELB).</li>
          <li><strong>ExternalName:</strong> Maps service to external DNS name.</li>
        </ul>
        <p><strong>In Practice:</strong> I use ClusterIP for internal microservices, LoadBalancer for public APIs.</p>
      </div>`},{question:"Explain ClusterIP, NodePort, and LoadBalancer  when do you use each?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Service Types  Internal vs External Exposure</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>ClusterIP:</strong> Internal-only communication (default).</li>
          <li><strong>NodePort:</strong> Opens a port (3000032767) on each node for external access.</li>
          <li><strong>LoadBalancer:</strong> Integrates with cloud LB (Azure/AWS/GCP) for public endpoint.</li>
        </ul>
        <p><strong>Usage:</strong> ClusterIP  backend, NodePort  dev/test, LoadBalancer  production apps.</p>
      </div>`},{question:"What happens behind the scenes when you choose Service type = LoadBalancer in AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS LoadBalancer  Behind the Scenes</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>AKS talks to <strong>Azure Resource Manager</strong> via cloud controller manager.</li>
          <li>Automatically provisions an <strong>Azure Load Balancer</strong> and assigns a public IP.</li>
          <li>Creates backend pool entries pointing to each node.</li>
          <li>kube-proxy ensures requests are routed to correct pod on that node.</li>
        </ul>
        <p><strong>In Practice:</strong> I often combine LoadBalancer + Ingress for better path-based routing.</p>
      </div>`},{question:"What is the difference between Ingress and Ingress Controller?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Ingress vs Ingress Controller</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Ingress:</strong> Kubernetes object that defines routing rules (like /api  backend-svc).</li>
          <li><strong>Ingress Controller:</strong> The actual implementation (e.g., NGINX, Traefik, Azure Application Gateway) that reads Ingress rules and configures routing.</li>
        </ul>
        <p><strong>In Practice:</strong> I use NGINX Ingress Controller with TLS, rewrite rules, and path-based routing.</p>
      </div>`},{question:"Why do we use Ingress Controller and what is its purpose?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Ingress Controller  The Traffic Manager</strong></h3>
        <p>It manages external HTTP/HTTPS traffic and routes it to services inside the cluster based on rules.</p>
        <ul style="margin-left:1.2rem;">
          <li>Enables domain-based & path-based routing.</li>
          <li>Handles SSL/TLS termination.</li>
          <li>Supports rewrite, rate limiting, auth, etc.</li>
        </ul>
        <p><strong>In Practice:</strong> It replaces multiple LBs  single entry point for all microservices.</p>
      </div>`},{question:"How do you expose a Kubernetes application to external traffic?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Expose Kubernetes App Externally</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Option 1:</strong> Service of type <code>LoadBalancer</code>.</li>
          <li><strong>Option 2:</strong> Ingress + Ingress Controller (preferred for many apps).</li>
          <li><strong>Option 3:</strong> NodePort (temporary/testing use).</li>
        </ul>
        <p><strong>In Practice:</strong> I expose APIs via Ingress with NGINX controller and Azure public IP.</p>
      </div>`},{question:"What is kube-proxy and how does it route traffic?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>kube-proxy  Service Networking Layer</strong></h3>
        <p>kube-proxy runs on each node and maintains network rules (iptables/ipvs) to forward traffic to correct pod endpoints.</p>
        <ul style="margin-left:1.2rem;">
          <li>Watches API server for Service & Endpoint changes.</li>
          <li>Implements load-balancing across pods.</li>
          <li>Supports three modes: <strong>userspace, iptables, ipvs</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer ipvs mode for high-performance clusters.</p>
      </div>`},{question:"Explain complete traffic flow: from user  Ingress  Service  Pod.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Traffic Flow  From User to Pod</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>User sends request to app DNS (e.g., myapp.com).</li>
          <li>Request hits <strong>Ingress Controller</strong> (e.g., NGINX).</li>
          <li>Ingress routes based on rules  forwards to <strong>Service</strong>.</li>
          <li>Service forwards to a <strong>Pod endpoint</strong> (via kube-proxy).</li>
          <li>Response flows back same path.</li>
        </ol>
        <p><strong>In Practice:</strong> I visualize this flow using Azure Network Watcher + Kiali dashboards.</p>
      </div>`},{question:"How would you secure application traffic (TLS termination, HTTPS redirection)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Application Traffic</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Enable <strong>TLS termination</strong> at Ingress Controller using a secret (cert + key).</li>
          <li>Use <strong>cert-manager</strong> for auto-renewal via Let's Encrypt.</li>
          <li>Force HTTPS redirection with Ingress annotations.</li>
        </ul>
        <pre><code>tls:
- hosts:
  - myapp.com
  secretName: myapp-cert</code></pre>
        <p><strong>In Practice:</strong> I integrate cert-manager + Azure DNS for full auto-cert provisioning.</p>
      </div>`},{question:"If your AKS cluster is private, how can you connect to it securely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securely Connecting to Private AKS</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Bastion</strong> or <strong>Jumpbox VM</strong> inside same VNet.</li>
          <li>Configure <strong>Private Endpoint</strong> for AKS API Server.</li>
          <li>Access via <strong>Azure VPN Gateway</strong> or <strong>ExpressRoute</strong>.</li>
          <li>kubectl uses local kubeconfig with private FQDN.</li>
        </ul>
        <p><strong>In Practice:</strong> I connect via Azure Bastion + managed identity authentication.</p>
      </div>`}]},{title:"5. Configuration, Secrets & Environment Management",questions:[{question:"What are ConfigMaps and Secrets?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ConfigMaps & Secrets  Externalized Configuration</strong></h3>
        <p><strong>ConfigMaps</strong> store non-sensitive data like environment configs, URLs, or feature flags.</p>
        <p><strong>Secrets</strong> store sensitive data like passwords, tokens, or certificates (Base64 encoded).</p>
        <ul style="margin-left:1.2rem;">
          <li>ConfigMap  plaintext configuration.</li>
          <li>Secret  encoded & access-controlled via RBAC.</li>
        </ul>
        <p><strong>In Practice:</strong> I use ConfigMaps for app settings and Secrets for connection strings or API keys.</p>
      </div>`},{question:"What is the difference between ConfigMap and Secret?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ConfigMap vs Secret</strong></h3>
        <table class="table-auto border-collapse border border-gray-300">
          <tr><th>Aspect</th><th>ConfigMap</th><th>Secret</th></tr>
          <tr><td>Data Type</td><td>Plain text config</td><td>Sensitive (Base64 encoded)</td></tr>
          <tr><td>Access Control</td><td>Normal RBAC</td><td>Strict RBAC with secret resource type</td></tr>
          <tr><td>Encryption</td><td>No encryption</td><td>Encrypted at rest (if enabled)</td></tr>
        </table>
        <p><strong>In Practice:</strong> Use ConfigMap for config, Secret for credentials  never mix them.</p>
      </div>`},{question:"How do you inject ConfigMap and Secret values into Pods?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Injecting ConfigMaps & Secrets into Pods</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>As environment variables:</strong>
            <pre><code>envFrom:
  - configMapRef:
      name: app-config
  - secretRef:
      name: app-secret</code></pre>
          </li>
          <li><strong>As mounted files:</strong>
            <pre><code>volumeMounts:
  - name: config
    mountPath: /app/config</code></pre>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I mount secrets read-only and rotate them via Key Vault references.</p>
      </div>`},{question:"How do you secure Secrets in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Secrets in Kubernetes</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Enable <strong>encryption at rest</strong> for Secrets via KMS provider (Azure Key Vault).</li>
          <li>Restrict access via <strong>RBAC</strong>  only apps or namespaces that need it.</li>
          <li>Use <strong>Secrets Store CSI Driver</strong> for direct Key Vault integration.</li>
          <li>Avoid printing secrets in logs or kubectl outputs.</li>
        </ul>
        <p><strong>In Practice:</strong> I disable plaintext secret access and rely on CSI driver-managed secrets.</p>
      </div>`},{question:"How do you fetch secrets from Azure Key Vault into AKS Pods?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating Azure Key Vault with AKS</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Deploy <strong>Azure Key Vault CSI Driver</strong> and <strong>Secrets Store Provider for Azure</strong>.</li>
          <li>Create a <strong>SecretProviderClass</strong> manifest mapping Key Vault secrets to pod volume.</li>
          <li>Use <strong>Managed Identity</strong> to authenticate AKS to Key Vault.</li>
          <li>Mount secrets into pods as files or environment variables.</li>
        </ol>
        <pre><code>volumeMounts:
  - name: secrets-store
    mountPath: "/mnt/secrets"</code></pre>
        <p><strong>In Practice:</strong> This approach removes the need to store credentials in Kubernetes at all.</p>
      </div>`},{question:"What are best practices for storing sensitive configuration in AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Best Practices  Sensitive Config Management</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Key Vault</strong> + CSI driver (no plaintext in cluster).</li>
          <li>Enable <strong>Encryption at Rest</strong> for Secrets.</li>
          <li>Use <strong>Managed Identities</strong> instead of storing credentials.</li>
          <li>Limit access via <strong>RBAC + Network Policies</strong>.</li>
          <li>Audit access via Azure Monitor + Defender for Cloud.</li>
        </ul>
        <p><strong>In Practice:</strong> I standardize all AKS environments with Key Vault-based secret retrieval to meet compliance.</p>
      </div>`}]},{title:"6. Probes, Health Checks & Scaling",questions:[{question:"What are Liveness and Readiness Probes in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Liveness & Readiness Probes  Application Health Checks</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Liveness Probe:</strong> Checks if the container is alive. If it fails, Kubelet restarts the container.</li>
          <li><strong>Readiness Probe:</strong> Checks if the container is ready to serve traffic. If it fails, pod is removed from Service endpoints.</li>
        </ul>
        <pre><code>livenessProbe:
  httpGet:
    path: /health
    port: 8080
readinessProbe:
  httpGet:
    path: /ready
    port: 8080</code></pre>
        <p><strong>In Practice:</strong> I always set both probes for production workloads  liveness for crash recovery, readiness for load balancing.</p>
      </div>`},{question:"What types of health probes exist (HTTP, TCP, Exec)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Probe Types in Kubernetes</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>HTTP Probe:</strong> Calls an HTTP endpoint (common for web apps).</li>
          <li><strong>TCP Probe:</strong> Checks if a TCP socket is open (useful for DB or services without HTTP).</li>
          <li><strong>Exec Probe:</strong> Executes a command inside the container and checks exit code 0/1.</li>
        </ul>
        <p><strong>In Practice:</strong> I use HTTP for APIs, TCP for Redis/MQ, and Exec for internal scripts or agents.</p>
      </div>`},{question:"Have you worked on configuring readiness and liveness probes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Hands-on: Configuring Probes</strong></h3>
        <p>Yes  I define both probes in my Deployment manifests with proper delay and threshold values to avoid false restarts.</p>
        <pre><code>livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 20
  periodSeconds: 10
  failureThreshold: 3</code></pre>
        <p><strong>In Practice:</strong> For .NET and Node apps, I tune probe delays to match app startup time (avoid premature restarts).</p>
      </div>`},{question:"What is HPA (Horizontal Pod Autoscaler) and how does it work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Horizontal Pod Autoscaler (HPA)</strong></h3>
        <p>HPA automatically scales the number of pod replicas based on observed metrics like CPU or memory usage.</p>
        <pre><code>kubectl autoscale deployment webapp \\
  --cpu-percent=70 --min=2 --max=10</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Uses <strong>metrics-server</strong> for resource metrics.</li>
          <li>Continuously monitors and updates replicas to maintain target utilization.</li>
          <li>Can integrate with custom metrics (Prometheus, KEDA).</li>
        </ul>
        <p><strong>In Practice:</strong> I use HPA + KEDA combo for event-driven scaling in AKS.</p>
      </div>`},{question:"How do you scale deployments manually and automatically?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Manual vs Automatic Scaling</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Manual Scaling:</strong> <code>kubectl scale deployment webapp --replicas=5</code></li>
          <li><strong>Automatic Scaling:</strong> via <strong>HPA</strong> or <strong>KEDA</strong> (based on metrics or events).</li>
          <li><strong>Cluster Autoscaler:</strong> scales node pools when pods cannot be scheduled.</li>
        </ul>
        <p><strong>In Practice:</strong> I combine HPA for pod scaling and Cluster Autoscaler for node scaling on AKS.</p>
      </div>`},{question:"How do you host a highly available, auto-scalable web app on AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Hosting HA & Auto-Scalable Web App on AKS</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Deploy app as a <strong>Deployment</strong> with min 3 replicas.</li>
          <li>Expose via <strong>Service (LoadBalancer)</strong> + Ingress for routing.</li>
          <li>Enable <strong>HPA</strong> for pod scaling and <strong>Cluster Autoscaler</strong> for nodes.</li>
          <li>Distribute workloads across multiple zones (availability zones in AKS).</li>
          <li>Use <strong>readiness/liveness probes</strong> for self-healing.</li>
        </ol>
        <p><strong>In Practice:</strong> I design multi-zone AKS clusters with managed identities + Key Vault + auto-scaling for peak resilience.</p>
      </div>`},{question:"What is kubectl rollout undo used for?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>kubectl rollout undo  Rollback to Previous Deployment</strong></h3>
        <p>Used to revert to the last known stable version of a deployment after a failed release.</p>
        <pre><code>kubectl rollout undo deployment/webapp</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Maintains revision history for each rollout.</li>
          <li>Useful for immediate rollback during CI/CD failures.</li>
        </ul>
        <p><strong>In Practice:</strong> I automate rollbacks via GitHub Actions using <code>kubectl rollout undo</code> after failed smoke tests.</p>
      </div>`}]},{title:"7. Deployments & Strategies",questions:[{question:"What is a Deployment in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deployment  Declarative Management of Pods</strong></h3>
        <p>A <strong>Deployment</strong> in Kubernetes is a higher-level abstraction that manages ReplicaSets and ensures the desired number of pods are running with the correct version.</p>
        <ul style="margin-left:1.2rem;">
          <li>Enables rolling updates, rollback, and scaling.</li>
          <li>Ensures self-healing  restarts unhealthy pods automatically.</li>
          <li>Declarative model  define desired state in YAML, K8s reconciles it continuously.</li>
        </ul>
        <pre><code>kubectl apply -f deployment.yaml
kubectl get deployments</code></pre>
        <p><strong>In Practice:</strong> I use Deployments for stateless services  APIs, frontends, or workers  with HPA and probes for reliability.</p>
      </div>`},{question:"What are different deployment strategies available in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deployment Strategies in Kubernetes</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Recreate:</strong> Stops old pods before starting new ones (downtime possible).</li>
          <li><strong>Rolling Update:</strong> Default strategy  gradually replaces old pods with new ones.</li>
          <li><strong>Blue-Green Deployment:</strong> Two environments (blue & green)  one live, one standby.</li>
          <li><strong>Canary Deployment:</strong> Gradually routes traffic to new version (controlled rollout).</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer rolling updates for APIs, blue-green for critical backend changes, and canary for production experiments.</p>
      </div>`},{question:"What is a Rolling Update and how do you perform rollback?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rolling Update & Rollback</strong></h3>
        <p>Rolling update replaces pods gradually  ensuring zero downtime during deployment.</p>
        <pre><code>kubectl rollout status deployment/webapp
kubectl rollout undo deployment/webapp</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>New pods are created before old ones are terminated.</li>
          <li>If an update fails, use <strong>rollback</strong> to revert to previous stable version.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate rolling updates with Azure DevOps pipelines using <code>kubectl apply</code> and post-deployment checks.</p>
      </div>`},{question:"What is a Blue-Green Deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Blue-Green Deployment  Zero Downtime Rollouts</strong></h3>
        <p>Two identical environments (Blue = current, Green = new). After verification, traffic switches to Green.</p>
        <ul style="margin-left:1.2rem;">
          <li>Blue stays active until Green is fully validated.</li>
          <li>Rollback is instant  just redirect traffic back to Blue.</li>
          <li>Useful for production-critical apps with strict uptime SLAs.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Nginx Ingress + Azure Traffic Manager for blue-green routing in AKS.</p>
      </div>`},{question:"What is a Canary Deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Canary Deployment  Gradual Rollout of New Version</strong></h3>
        <p>Releases new versions to a small subset of users first, monitoring performance before full rollout.</p>
        <ul style="margin-left:1.2rem;">
          <li>Canary pods run alongside stable ones.</li>
          <li>Traffic gradually increases from 5%  50%  100%.</li>
          <li>Metrics and alerts decide if rollout continues or reverts.</li>
        </ul>
        <p><strong>In Practice:</strong> I combine Canary with Prometheus metrics or Azure Monitor alerts to automate safe rollout decisions.</p>
      </div>`},{question:"What happens if deployment fails midway? How do you recover or rollback?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Failure Handling & Rollback</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Deployment pauses if pods fail health checks.</li>
          <li>Use <code>kubectl rollout undo</code> to revert to last stable version.</li>
          <li>Logs and <code>kubectl describe deployment</code> show root cause (CrashLoop, imagePull errors, etc.).</li>
          <li>HPA + probes minimize downtime during recovery.</li>
        </ul>
        <p><strong>In Practice:</strong> My CI/CD pipelines detect failure & trigger rollback jobs automatically in Azure DevOps.</p>
      </div>`},{question:"How do you manage CI/CD integration for AKS deployments (with Azure DevOps)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CI/CD Integration with Azure DevOps</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>CI:</strong> Build Docker image  push to ACR.</li>
          <li><strong>CD:</strong> Pipeline applies manifests via <code>kubectl</code> or Helm.</li>
          <li>Use Azure service connection for secure kubeconfig authentication.</li>
          <li>Post-deployment validation via probes & smoke tests.</li>
        </ol>
        <p><strong>In Practice:</strong> My YAML pipeline includes rollback, HPA verification, and tag-based release triggers.</p>
      </div>`},{question:"What are the application deployment types youve worked with (YAML, Helm, pipeline)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Application Deployment Types</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>YAML Manifests:</strong> Direct <code>kubectl apply -f</code> (used for quick testing or PoCs).</li>
          <li><strong>Helm Charts:</strong> Parameterized and reusable deployment templates for consistent releases.</li>
          <li><strong>CI/CD Pipelines:</strong> Full automation via Azure DevOps with versioned artifacts and rollback steps.</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain a hybrid model  Helm for config-driven apps, pipelines for multi-env deployments (Dev  QA  Prod).</p>
      </div>`}]},{title:"8. Storage & Volumes",questions:[{question:"What are Persistent Volumes (PV) and Persistent Volume Claims (PVC)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Persistent Volumes (PV) & Persistent Volume Claims (PVC)</strong></h3>
        <p><strong>PV (Persistent Volume):</strong> Cluster-level resource representing a piece of storage provisioned either manually or dynamically.</p>
        <p><strong>PVC (Persistent Volume Claim):</strong> Request made by a pod to use a PV  similar to how a pod requests CPU/RAM resources.</p>
        <pre><code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mypvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: managed-premium</code></pre>
        <p><strong>In Practice:</strong> I use dynamic provisioning with Azure Disk/Files using <code>storageClassName</code> for automated PV creation.</p>
      </div>`},{question:"How do you mount Azure Disk or Azure File Share in AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Mounting Azure Disk or File Share in AKS</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure Disk:</strong> Block storage, used for single-pod attach (ReadWriteOnce). Ideal for databases or stateful pods.</li>
          <li><strong>Azure File:</strong> SMB-based file share, supports multiple pods (ReadWriteMany).</li>
        </ul>
        <pre><code>volumeMounts:
- name: mydisk
  mountPath: /data
volumes:
- name: mydisk
  persistentVolumeClaim:
    claimName: azure-disk-pvc</code></pre>
        <p><strong>In Practice:</strong> I prefer Azure File for shared access (web pods) and Azure Disk for stateful DB components.</p>
      </div>`},{question:"How do you protect data from accidental pod deletion?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Protecting Data from Pod Deletion</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>PVs are independent of pod lifecycle  data persists even if the pod is deleted.</li>
          <li>Use <strong>PersistentVolumeClaim</strong>  it ensures data survives pod recreation.</li>
          <li>Set <strong>reclaimPolicy</strong> to <code>Retain</code> (default is Delete).</li>
          <li>For critical workloads  use StatefulSet with PVC templates.</li>
        </ul>
        <pre><code>reclaimPolicy: Retain</code></pre>
        <p><strong>In Practice:</strong> I ensure all DB and backend pods use PVCs with Retain policy + scheduled snapshot backups in Azure.</p>
      </div>`},{question:"What happens when a StatefulSet Pod restarts  how is its data preserved?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>StatefulSet Pod Restart  Data Persistence</strong></h3>
        <p>Each StatefulSet Pod gets its own unique, stable storage through <strong>volumeClaimTemplates</strong>.</p>
        <ul style="margin-left:1.2rem;">
          <li>When a pod restarts, it reattaches to the same volume claim.</li>
          <li>Pod identity and data are maintained  ideal for databases or message queues.</li>
          <li>Kubernetes ensures volume mount consistency even during node rescheduling.</li>
        </ul>
        <pre><code>volumeClaimTemplates:
- metadata:
    name: data
  spec:
    accessModes: [ "ReadWriteOnce" ]
    resources:
      requests:
        storage: 10Gi</code></pre>
        <p><strong>In Practice:</strong> I use StatefulSets for MySQL, MongoDB, and Kafka to preserve unique pod identities with persistent data.</p>
      </div>`}]},{title:"9. Node Pools, Scheduling & Cluster Management",questions:[{question:"What is a Node Pool in AKS and how do you manage it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Node Pools in AKS  Overview & Management</strong></h3>
        <p>A <strong>Node Pool</strong> is a set of nodes (VMs) within an AKS cluster that share the same configuration  OS type, VM size, and scaling policies.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Default Node Pool:</strong> Created automatically during cluster setup. Hosts system-critical Pods (e.g., CoreDNS, kube-proxy).</li>
          <li><strong>User Node Pools:</strong> Custom pools used for application workloads, can be Linux/Windows, GPU-enabled, or spot instances.</li>
        </ul>
        <p><strong>Management Commands:</strong></p>
        <pre><code>az aks nodepool list -g myRG --cluster-name myAKS
az aks nodepool add -g myRG --cluster-name myAKS -n apppool --node-count 3
az aks nodepool delete -g myRG --cluster-name myAKS -n oldpool
az aks nodepool scale -g myRG --cluster-name myAKS -n apppool --node-count 5</code></pre>
        <p><strong>In Practice:</strong> I use separate pools for system vs app workloads, e.g.:
        <br> systempool (Core components)
        <br> apppool (Web/API workloads)
        <br> gpupool (AI/ML inference)</p>
      </div>`},{question:"What are Taints and Tolerations?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Taints & Tolerations  Controlling Pod Placement</strong></h3>
        <p><strong>Taints</strong> are applied on nodes to repel Pods unless those Pods have a matching <strong>Toleration</strong>.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Taint Example:</strong> <code>kubectl taint nodes node1 key=value:NoSchedule</code></li>
          <li><strong>Toleration Example (Pod spec):</strong></li>
        </ul>
        <pre><code>tolerations:
  - key: "key"
    operator: "Equal"
    value: "value"
    effect: "NoSchedule"</code></pre>
        <p><strong>Use Case:</strong> Isolate workloads  e.g., GPU workloads only run on GPU node pool, not on system nodes.</p>
        <p><strong>In Practice:</strong> I taint system nodes with <code>CriticalAddonsOnly=true:NoSchedule</code> and apply tolerations to system Pods only.</p>
      </div>`},{question:"What are common issues with pod scheduling (Pending state, Evicted Pods)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pod Scheduling Issues  Causes & Fixes</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Pending Pods:</strong> Scheduler cant find a suitable node.
            <ul>
              <li> Not enough CPU/Memory.</li>
              <li> Node taints without matching tolerations.</li>
              <li> PVC cant bind (storage issue).</li>
              <li> Node selectors/affinities too restrictive.</li>
            </ul>
          </li>
          <li><strong>Evicted Pods:</strong> Node runs out of resources or disk.
            <ul>
              <li> Check <code>kubectl describe pod</code> for eviction reason.</li>
              <li> Increase node size or add more nodes.</li>
              <li> Clean up unused images/logs on node.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I use Azure Monitor and <code>kubectl get events --sort-by=.metadata.creationTimestamp</code> to trace exact scheduling bottlenecks.</p>
      </div>`},{question:"How do you monitor the health of AKS nodes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Node Health Monitoring in AKS</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>kubectl get nodes -o wide</strong> and <strong>kubectl describe node</strong> for status.</li>
          <li>Check node metrics in <strong>Azure Monitor for Containers</strong> or <strong>Log Analytics</strong>.</li>
          <li>Enable <strong>Azure Policy</strong> for node compliance and drift detection.</li>
          <li>Use <strong>Node Problem Detector (NPD)</strong> DaemonSet for kernel/disk/network issues.</li>
        </ul>
        <p><strong>In Practice:</strong> I monitor node CPU/memory pressure alerts in Azure Monitor and trigger auto-scaling or draining scripts accordingly.</p>
      </div>`},{question:"How do you upgrade AKS clusters safely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Safe AKS Cluster Upgrades</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Review available versions:
            <pre><code>az aks get-upgrades -n myAKS -g myRG</code></pre>
          </li>
          <li>Backup manifests & verify CI/CD rollback support.</li>
          <li>Drain and upgrade node pools one by one:
            <pre><code>az aks nodepool upgrade --cluster-name myAKS -g myRG -n apppool --kubernetes-version 1.30.3</code></pre>
          </li>
          <li>Validate workloads post-upgrade and check node health.</li>
        </ol>
        <p><strong>In Practice:</strong> I perform upgrades in non-prod first, use <strong>Blue-Green node pools</strong> for production zero-downtime upgrades.</p>
      </div>`},{question:"How do you drain nodes safely before upgrade?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Safe Node Draining</strong></h3>
        <p>Before upgrading or decommissioning a node, drain workloads safely:</p>
        <pre><code>kubectl drain node-name --ignore-daemonsets --delete-emptydir-data</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><strong>--ignore-daemonsets:</strong> Keeps system Pods running (e.g., kube-proxy).</li>
          <li><strong>--delete-emptydir-data:</strong> Deletes ephemeral storage data safely.</li>
        </ul>
        <p>After upgrade, uncordon the node to allow scheduling:</p>
        <pre><code>kubectl uncordon node-name</code></pre>
        <p><strong>In Practice:</strong> I drain nodes in rolling batches and use <strong>PodDisruptionBudgets (PDB)</strong> to avoid downtime.</p>
      </div>`},{question:"How do you handle image caching and OS patching in AKS nodes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Image Caching & OS Patching</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Image Caching:</strong>
            <ul>
              <li>Use <strong>Azure Container Registry (ACR)</strong> close to AKS region for faster pulls.</li>
              <li>Leverage <strong>AKS Node Image Cache</strong> or <code>crictl pull</code> preloads.</li>
              <li>For CI/CD: pre-pull base images during deployment warmup jobs.</li>
            </ul>
          </li>
          <li><strong>OS Patching:</strong>
            <ul>
              <li>AKS automatically handles <strong>node OS security updates</strong> via Azure-managed images.</li>
              <li>For manual patching:
                <pre><code>az aks nodepool upgrade --node-image-only</code></pre>
              </li>
              <li>Recreates VMs with the latest patched OS image without affecting K8s version.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I schedule monthly OS image upgrades and automate image pulls in pipelines to reduce cold-start latency.</p>
      </div>`}]},{title:"10. RBAC & Security",questions:[{question:"What is RBAC (Role-Based Access Control) in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>RBAC  Role-Based Access Control</strong></h3>
        <p>RBAC is Kubernetes' authorization model that grants permissions to users, groups, or service accounts by binding them to <strong>Roles</strong> or <strong>ClusterRoles</strong>. It enforces <em>who</em> can do <em>what</em> on which resources.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Role</strong>  namespaced permissions (e.g., allow pods in namespace foo).</li>
          <li><strong>ClusterRole</strong>  cluster-wide permissions (e.g., view nodes).</li>
          <li><strong>RoleBinding</strong> / <strong>ClusterRoleBinding</strong>  attach role to subject (user/serviceAccount/group).</li>
        </ul>
        <p><strong>In Practice:</strong> I use RBAC to separate CI/CD bot permissions (deploy only) from human admin permissions (cluster-level ops).</p>
      </div>`},{question:"How do you configure RBAC roles and bindings?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Configure Roles & Bindings</strong></h3>
        <p>Define a Role or ClusterRole YAML and bind it to subjects using RoleBinding/ClusterRoleBinding.</p>
        <pre><code># Role (namespace-scoped)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-deployer
  namespace: prod
rules:
- apiGroups: [""]
  resources: ["pods","services","deployments"]
  verbs: ["get","list","create","update","patch"]

# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: bind-deployer
  namespace: prod
subjects:
- kind: ServiceAccount
  name: ci-bot
  namespace: ci
roleRef:
  kind: Role
  name: app-deployer
  apiGroup: rbac.authorization.k8s.io
</code></pre>
        <p><strong>Tip:</strong> Use <code>kubectl auth can-i</code> to validate permissions.</p>
      </div>`},{question:"How do you implement least privilege access in AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Least Privilege  Practical Steps</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Apply the principle of least privilege: grant only required verbs (e.g., <code>get,list,watch</code> vs <code>create,delete</code>).</li>
          <li>Use <strong>namespaces</strong> to isolate teams/environments.</li>
          <li>Use <strong>service accounts</strong> for automation (CI/CD) and avoid using user tokens.</li>
          <li>Prefer <strong>ClusterRoles</strong> only when truly necessary  otherwise use namespaced Roles.</li>
          <li>Use Azure AD integration (or Azure RBAC) to manage human access centrally and enforce MFA/SSO.</li>
          <li>Regularly audit (via Azure Monitor / Kubernetes audit logs) and review role bindings.</li>
        </ul>
        <p><strong>In Practice:</strong> I create scoped roles for pipelines (image-pull, deploy to a specific namespace) and rotate service account tokens or use federated identities.</p>
      </div>`},{question:"What are Network Policies and how do you use them to restrict Pod-to-Pod communication?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Network Policies  Network-level Access Control</strong></h3>
        <p>NetworkPolicy objects define allowed ingress/egress traffic for selected pods. They are implemented by the cluster CNI (e.g., Calico, Azure CNI).</p>
        <pre><code>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
  namespace: prod
spec:
  podSelector: {}        # selects all pods
  policyTypes:
  - Ingress
  - Egress
  ingress: []           # deny all ingress by default</code></pre>
        <p><strong>Usage:</strong> Start with default-deny and explicitly allow needed traffic (namespace-to-namespace or pod-to-pod).</p>
      </div>`},{question:"What kind of network policies have you implemented for security?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Practical NetworkPolicy Patterns I've Used</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Default-deny:</strong> Deny all traffic, then open only required ports/endpoints.</li>
          <li><strong>Namespace isolation:</strong> Allow traffic only from specific namespaces (eg. <code>frontend</code>  <code>backend</code>).</li>
          <li><strong>DB access control:</strong> Only allow queries from specific service accounts or backend pods to DB pods.</li>
          <li><strong>Egress restrictions:</strong> Block external internet access from sensitive workloads; allow via proxy instead.</li>
          <li><strong>Calico advanced policies:</strong> Use Calico network sets and globalNetworkPolicy for cross-namespace rules.</li>
        </ul>
        <p><strong>In Practice:</strong> I implemented default-deny + selective ingress for PCI-sensitive workloads and enforced policies via CI validation (policy-as-code).</p>
      </div>`},{question:"How do you integrate Azure AD authentication with AKS for RBAC?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure AD Integration with AKS (User Authentication)</strong></h3>
        <p>Integrating AKS with Azure AD centralizes user authentication (SSO) and lets you map Azure AD identities to Kubernetes RBAC.</p>
        <ol style="margin-left:1.2rem;">
          <li>Create an AKS cluster with AAD enabled or enable AAD on an existing cluster (<code>--enable-aad</code> / managed AAD).</li>
          <li>Grant Azure AD users/groups cluster roles using <code>kubectl create clusterrolebinding</code> or via Azure RBAC for Kubernetes.</li>
          <li>Use Azure CLI or kubectl with Azure AD tokens to authenticate; leverage Azure AD conditional access and MFA.</li>
        </ol>
        <pre><code># Example: bind an AAD group to cluster-admin (not recommended for prod)
az aks create ... --enable-aad

kubectl create clusterrolebinding aad-admin-binding \\
  --clusterrole=cluster-admin \\
  --user="AzureADGroupObjectId" </code></pre>
        <p><strong>Best Practice:</strong> Use Azure AD groups  map to minimally scoped Kubernetes Roles, avoid granting broad cluster-admin.</p>
        <p><strong>In Practice:</strong> I use managed AAD and Azure RBAC integration so admins are managed in Azure AD, and Kubernetes permissions are assigned via role bindings and Azure policies.</p>
      </div>`}]},{title:"11. Logging, Monitoring & Troubleshooting",questions:[{question:"What command is used to check Pod logs?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Checking Pod Logs</strong></h3>
        <p>Use <code>kubectl logs</code> to view container logs inside a Pod.</p>
        <pre><code># Logs of a running Pod
kubectl logs my-pod

# Logs of a specific container inside Pod
kubectl logs my-pod -c container-name

# Stream logs (real-time)
kubectl logs -f my-pod</code></pre>
        <p><strong>In Practice:</strong> I often use <code>-n namespace</code> and <code>--previous</code> to debug recently crashed containers.</p>
      </div>`},{question:"What are commands to troubleshoot Pod-related issues?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Troubleshooting Pod Issues</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>kubectl describe pod &lt;pod&gt;</code>  Inspect events, status, and reasons for failure.</li>
          <li><code>kubectl get pods -o wide</code>  Check node assignment and status.</li>
          <li><code>kubectl exec -it &lt;pod&gt; -- /bin/sh</code>  Debug inside container.</li>
          <li><code>kubectl get events --sort-by='.metadata.creationTimestamp'</code>  See recent events.</li>
        </ul>
        <p><strong>In Practice:</strong> I start with <code>kubectl describe</code> to identify scheduling or image errors, then exec for runtime debugging.</p>
      </div>`},{question:"How do you debug a Pod stuck in CrashLoopBackOff?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CrashLoopBackOff Debugging</strong></h3>
        <p>This happens when a container repeatedly crashes after startup.</p>
        <ol style="margin-left:1.2rem;">
          <li>Check logs using <code>kubectl logs --previous</code>.</li>
          <li>Describe Pod (<code>kubectl describe pod &lt;pod&gt;</code>) for reasons/events.</li>
          <li>Inspect startup scripts, env vars, config mounts for issues.</li>
          <li>Increase restart delay or probe thresholds if initialization takes time.</li>
          <li>Temporarily override command using <code>kubectl debug</code> or edit deployment.</li>
        </ol>
        <p><strong>In Practice:</strong> I usually find env misconfigurations or missing secrets behind CrashLoopBackOffs.</p>
      </div>`},{question:"How do you debug a Pod stuck in ImagePullBackOff?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ImagePullBackOff Debugging</strong></h3>
        <p>Occurs when Kubernetes cannot pull the container image from the registry.</p>
        <ul style="margin-left:1.2rem;">
          <li>Check <code>kubectl describe pod</code>  Look for Failed to pull image reason.</li>
          <li>Verify image name/tag correctness.</li>
          <li>Ensure registry credentials (imagePullSecrets) are correct.</li>
          <li>Check network connectivity to registry (firewall/proxy).</li>
          <li>For ACR  ensure AKS is linked with proper permissions (<code>az aks update --attach-acr</code>).</li>
        </ul>
        <p><strong>In Practice:</strong> Most issues stem from missing imagePullSecrets or typo in image tags.</p>
      </div>`},{question:"What is your approach when Pods are being Evicted due to node pressure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Pod Evictions (Node Pressure)</strong></h3>
        <p>Evictions occur when a node runs out of CPU, memory, or disk.</p>
        <ul style="margin-left:1.2rem;">
          <li>Check reason using <code>kubectl describe pod</code> (e.g., Evicted: node had memory pressure).</li>
          <li>Inspect node usage via <code>kubectl top nodes</code> or Azure Monitor.</li>
          <li>Scale cluster or add node pools.</li>
          <li>Set <strong>resource requests/limits</strong> to prevent overcommit.</li>
          <li>Use <strong>PodPriority</strong> to protect critical workloads.</li>
        </ul>
        <p><strong>In Practice:</strong> I enable cluster autoscaler and define QoS classes to avoid frequent evictions.</p>
      </div>`},{question:"How do you integrate Prometheus and Grafana with AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Prometheus & Grafana Integration</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Install using Helm charts or kube-prometheus-stack.</li>
          <li>Expose Prometheus via ServiceMonitor and scrape metrics.</li>
          <li>Connect Grafana to Prometheus as data source.</li>
          <li>Use prebuilt dashboards for node, pod, and namespace-level metrics.</li>
        </ol>
        <pre><code>helm install monitoring prometheus-community/kube-prometheus-stack -n monitoring</code></pre>
        <p><strong>In Practice:</strong> I use Grafana dashboards to monitor pod CPU/memory and set alerts integrated with Slack or Azure Monitor.</p>
      </div>`},{question:"How do you configure monitoring using Azure Monitor for Containers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Monitor for Containers</strong></h3>
        <p>Azure Monitor automatically collects metrics and logs from AKS when enabled during cluster creation or later via CLI/portal.</p>
        <ul style="margin-left:1.2rem;">
          <li>Enable using <code>az aks enable-addons --addons monitoring</code>.</li>
          <li>Stores logs in Log Analytics Workspace.</li>
          <li>Provides metrics: CPU, memory, node utilization, container restarts.</li>
          <li>Supports alerts and workbooks for visualization.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate it with Application Insights and enable container-level log collection.</p>
      </div>`},{question:"How do you check metrics and alerts for Pod performance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Pod Performance Metrics</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>kubectl top pod</code>  for real-time CPU/memory.</li>
          <li>Prometheus/Grafana dashboards  for trends and alerts.</li>
          <li>Azure Monitor Metrics  for cluster-wide visibility.</li>
          <li>Define alerts for high CPU/memory, restarts, or latency.</li>
        </ul>
        <p><strong>In Practice:</strong> I create Grafana alerts on sustained high CPU for autoscaling and integrate alerts via Teams or Slack.</p>
      </div>`},{question:"How do you analyze logs in a multi-namespace cluster?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Log Analysis Across Namespaces</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <code>kubectl logs -n &lt;namespace&gt;</code> for individual namespaces.</li>
          <li>Aggregate logs with tools like <strong>ELK Stack (ElasticSearch, Logstash, Kibana)</strong> or <strong>Fluent Bit</strong>.</li>
          <li>Azure Monitor / Log Analytics  Query across namespaces using KQL.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Fluent Bit  Log Analytics to centralize all pod logs and query by namespace, app, or severity.</p>
      </div>`},{question:"How do you investigate 503 or timeout errors from Kubernetes services?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Investigating 503 / Timeout Issues</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Check service endpoints (<code>kubectl get endpoints svc-name</code>).</li>
          <li>Ensure pods are healthy and passing readiness probes.</li>
          <li>Verify Ingress and DNS routing (Ingress logs, kube-proxy status).</li>
          <li>Look for network policy blocks or node-level connection drops.</li>
          <li>Check backend app logs for connection timeouts or thread pool exhaustion.</li>
        </ol>
        <p><strong>In Practice:</strong> Most 503s occur due to readiness probe failures or misconfigured Ingress path matching.</p>
      </div>`}]},{title:"12. Integration with Azure Services",questions:[{question:"How do you connect AKS with ACR (Azure Container Registry)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Connecting AKS with ACR</strong></h3>
        <p>To allow AKS to pull container images from Azure Container Registry securely, you link the cluster with ACR permissions.</p>
        <ol style="margin-left:1.2rem;">
          <li>Assign ACR Pull Role using CLI:</li>
        </ol>
        <pre><code>az aks update \\ 
  --name myAKSCluster \\ 
  --resource-group myResourceGroup \\ 
  --attach-acr myACRName</code></pre>
        <p>This grants AKS-managed identity the <strong>AcrPull</strong> role on the registry.</p>
        <p><strong>In Practice:</strong> I prefer using <code>--attach-acr</code> for simplicity, but for enterprise setups, I create dedicated identities with least privileges.</p>
      </div>`},{question:"How do you integrate AKS with Azure Key Vault for secret injection?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Integrating AKS with Azure Key Vault</strong></h3>
        <p>Use <strong>Secrets Store CSI Driver</strong> + <strong>Azure Key Vault Provider</strong> to mount Key Vault secrets directly into AKS pods.</p>
        <ol style="margin-left:1.2rem;">
          <li>Enable add-on or install via Helm:</li>
        </ol>
        <pre><code>az aks enable-addons \\
  --addons azure-keyvault-secrets-provider \\
  --name myAKSCluster \\
  --resource-group myRG</code></pre>
        <p>Then define a <strong>SecretProviderClass</strong> YAML to specify Key Vault and secrets to mount.</p>
        <pre><code>secretObjects:
- secretName: app-secrets
  type: Opaque
  data:
  - objectName: db-password
    key: db-password</code></pre>
        <p><strong>In Practice:</strong> I mount secrets as volumes or sync them as Kubernetes secrets depending on app design.</p>
      </div>`},{question:"How do you integrate Azure Monitor or Log Analytics for cluster health?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Monitoring AKS with Azure Monitor</strong></h3>
        <p>Azure Monitor for Containers provides deep visibility into AKS cluster performance and logs.</p>
        <ol style="margin-left:1.2rem;">
          <li>Enable monitoring during or after cluster creation:</li>
        </ol>
        <pre><code>az aks enable-addons \\
  --addons monitoring \\
  --name myAKSCluster \\
  --resource-group myRG</code></pre>
        <p>Logs and metrics flow into <strong>Log Analytics Workspace</strong>, which can be queried using KQL.</p>
        <p><strong>In Practice:</strong> I integrate it with custom alerts (CPU > 80%, Pod restarts, latency spikes) and dashboards for proactive ops.</p>
      </div>`},{question:"How do you configure Azure Policy for AKS compliance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Policy for AKS Compliance</strong></h3>
        <p>Azure Policy enforces governance across AKS  e.g., restricting privileged pods, enforcing TLS, or requiring approved images.</p>
        <ol style="margin-left:1.2rem;">
          <li>Enable the add-on:</li>
        </ol>
        <pre><code>az aks enable-addons \\
  --addons azure-policy \\
  --name myAKSCluster \\
  --resource-group myRG</code></pre>
        <p>Policies can be assigned at subscription or cluster scope via Azure Portal or Policy as Code (ARM/Bicep).</p>
        <p><strong>In Practice:</strong> I use built-in AKS policy initiatives and monitor non-compliant resources in Defender for Cloud.</p>
      </div>`},{question:"How did you integrate CI/CD pipelines with AKS deployments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CI/CD Integration with AKS</strong></h3>
        <p>Integrate AKS with Azure DevOps or GitHub Actions for automated build, test, and deploy workflows.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>CI Stage:</strong> Build & push Docker image to ACR.</li>
          <li><strong>CD Stage:</strong> Deploy manifests/Helm charts to AKS using service connection.</li>
        </ul>
        <pre><code># Example (Azure DevOps YAML)
- task: Kubernetes@1
  displayName: Deploy to AKS
  inputs:
    connectionType: 'Azure Resource Manager'
    azureSubscription: 'MySub'
    azureResourceGroup: 'myRG'
    kubernetesCluster: 'myAKSCluster'
    namespace: 'prod'
    command: 'apply'
    useConfigurationFile: true
    configuration: 'manifests/deployment.yaml'</code></pre>
        <p><strong>In Practice:</strong> I integrate Helm in pipelines for versioned, parameterized AKS deployments with rollback support.</p>
      </div>`}]},{title:"13. Real-World Scenarios",questions:[{question:"A critical production AKS cluster is having multiple issues  Pods in ImagePullBackOff, Evicted, users getting 503.  What troubleshooting steps would you follow?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Immediate Troubleshooting Playbook (Triage  Fix  Validate)</strong></h3>
        <p>When multiple symptoms appear (ImagePullBackOff, Evicted pods, and 503s) follow a structured triage:</p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Establish scope & impact:</strong> Which namespaces / services are affected? Check ingress/controller status and alerting channels.</li>
          <li><strong>Check cluster-wide health:</strong>
            <pre><code>kubectl get nodes
kubectl get pods --all-namespaces -o wide</code></pre>
            Determine node NotReady, many CrashLoopBackOffs, or resource pressure.
          </li>
          <li><strong>Investigate ImagePullBackOff:</strong>
            <pre><code>kubectl describe pod <pod> -n <ns>    # look for Failed to pull image
kubectl get secret -n <ns>                      # check imagePullSecrets</code></pre>
            Common causes: wrong image name/tag, missing imagePullSecrets, ACR auth missing, registry outage, or network egress blocked.
          </li>
          <li><strong>Investigate Evicted pods:</strong>
            <pre><code>kubectl describe pod <evicted-pod> -n <ns></code></pre>
            Check events for node pressure reasons (memory/disk). Inspect node DiskPressure / MemoryPressure via <code>kubectl describe node</code> and <code>kubectl top nodes</code>.
          </li>
          <li><strong>Investigate 503 / user-facing errors:</strong>
            <pre><code>kubectl get endpoints svc-name -n <ns>
kubectl describe ingress <ingress> -n <ns>
kubectl logs <ingress-controller-pod> -n ingress-nginx</code></pre>
            503s often due to no healthy endpoints (readiness failed), ingress misconfiguration, or upstream pods crashing.
          </li>
          <li><strong>Correlate timing & metrics:</strong> Use Prometheus / Azure Monitor to see spikes in CPU/memory, pod restarts, node scale events, or network issues around the incident time.</li>
          <li><strong>Fix order (quick wins first):</strong>
            <ul style="margin-left:1.2rem;">
              <li>If ImagePullBackOff due to auth  re-attach ACR or fix imagePullSecrets (temporary: manually docker login on node for testing).</li>
              <li>If eviction due to node pressure  cordon & drain problematic nodes, scale node pool or increase capacity.</li>
              <li>If readiness probes failing  check app logs, env/config, or missing secrets; fix and let readiness restore endpoints.</li>
            </ul>
          </li>
          <li><strong>Validate & close:</strong> Confirm endpoints healthy, ingress returns 200, monitor alerts for stabilization.</li>
          <li><strong>Post-mortem:</strong> Capture root causes, timeline, and follow-up actions (capacity, image pipeline, secrets, network).</li>
        </ol>
        <p><strong>Example quick commands:</strong></p>
        <pre><code>kubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,STATUS:.status.phase,RESTARTS:.status.containerStatuses[*].restartCount
kubectl describe pod <pod> -n <ns>
kubectl logs <pod> -c <container> -n <ns> --previous</code></pre>
      </div>`},{question:"A critical production AKS cluster is having multiple issues  Pods in ImagePullBackOff, Evicted, users getting 503.  How do you prevent this in the future?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Preventive Measures & Hardening</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Authentication & image pipeline:</strong> Use managed identities / <code>az aks update --attach-acr</code> or ensure imagePullSecrets are rotated and stored in Key Vault; implement image promotion pipeline to avoid bad tags in prod.</li>
          <li><strong>Resource management:</strong> Enforce requests & limits for all pods; use Quality of Service classes; set appropriate Horizontal Pod Autoscaler and Cluster Autoscaler configs.</li>
          <li><strong>Node sizing & capacity planning:</strong> Use node pools for workload isolation (system vs app vs GPU); sustain buffer capacity for spikes and have scale-up cooldowns tuned.</li>
          <li><strong>Readiness & liveness tuning:</strong> Configure realistic initialDelaySeconds/periodSeconds/failureThreshold to avoid premature evictions or routing to unhealthy pods.</li>
          <li><strong>Monitoring & alerting:</strong> Alerts for image pull failures, node disk/ memory pressure, high pod restart rate, and zero endpoints for services.</li>
          <li><strong>Immutability & image tagging:</strong> Use immutable tags (sha digest) for prod, avoid :latest; enforce CI gates that test image pullability and security scanning.</li>
          <li><strong>Network resilience:</strong> Ensure required egress to registries, use retry/backoff in apps, and implement circuit-breakers in ingress or service mesh.</li>
          <li><strong>Backups & runbooks:</strong> Automate etcd snapshots and keep runbooks for common failure modes (ImagePullBackOff, Evictions, 503s) with concrete commands and escalation paths.</li>
        </ol>
        <p><strong>In Practice:</strong> I combine policy-as-code (validate manifests), image scanning, proactive capacity scaling, and runbook drills to reduce recurrence.</p>
      </div>`},{question:"You havent used NodePort before  can you explain how it works?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>NodePort  Exposing Service on Node Ports</strong></h3>
        <p><strong>What it is:</strong> NodePort opens a static port (range 3000032767) on every cluster node and forwards traffic to the Service  Pod.</p>
        <p><strong>How it works:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Kubernetes allocates a port on each node. Traffic to NodeIP:NodePort is forwarded to the Service's backend pods via kube-proxy.</li>
          <li>Useful for simple access during development or when external load balancer is not available.</li>
        </ul>
        <p><strong>When to use:</strong> Temporary testing, bare-metal clusters without cloud LB, or for debugging when you want to hit a node directly.</p>
        <p><strong>Limitations:</strong> Not ideal for production  no built-in external LB, port management overhead, and potential security exposure. Prefer Ingress/LoadBalancer for production.</p>
      </div>`},{question:"What resources have you created in Kubernetes (Deployments, Services, Ingress, PVCs, ConfigMaps, Secrets, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Resources I Create & Manage</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Workloads:</strong> Deployments, StatefulSets, DaemonSets, CronJobs</li>
          <li><strong>Networking:</strong> Services (ClusterIP, NodePort, LoadBalancer), Ingress resources, Ingress Controllers (NGINX/Traefik), NetworkPolicies</li>
          <li><strong>Storage:</strong> PersistentVolumeClaims (PVC), StorageClasses, volumeClaimTemplates in StatefulSets</li>
          <li><strong>Configuration:</strong> ConfigMaps, Secrets (and Secrets Store CSI integrations)</li>
          <li><strong>Infra & ops:</strong> ServiceAccounts, RBAC Roles & RoleBindings, PodDisruptionBudgets, HorizontalPodAutoscalers</li>
          <li><strong>Observability:</strong> ServiceMonitors, Prometheus CRs, Fluent Bit/Fluentd DaemonSets</li>
        </ul>
        <p><strong>In Practice:</strong> I design manifests + Helm charts to manage these resources consistently across environments with parameterized values.yaml.</p>
      </div>`},{question:"You are asked to restrict traffic between namespaces  how do you design the policy?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Designing Namespace-Isolation NetworkPolicy</strong></h3>
        <p>Goal: default-deny cross-namespace communication, allow only explicit flows.</p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Start default-deny:</strong> Apply a default deny NetworkPolicy in each namespace.</li>
          <li><strong>Define required allow rules:</strong> Create namespace-scoped NetworkPolicies that allow ingress only from specific namespaces or label selectors (e.g., frontend  backend).</li>
          <li><strong>Use labels & service accounts:</strong> Allow by podSelector or by namespace + pod labels to be precise.</li>
          <li><strong>Test incrementally:</strong> Apply policies in staging first and run integration tests.</li>
          <li><strong>Guard with CI validation:</strong> Lint network policies and run conformance tests as part of CI to avoid breaking deployments.</li>
        </ol>
        <pre><code>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-frontend-to-backend
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend</code></pre>
        <p><strong>In Practice:</strong> I implement a policy matrix (who can talk to whom) and codify it in Git for reviews and audits.</p>
      </div>`},{question:"You need to deploy a backend API and a frontend app with secure communication  how do you design it in AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Backend + Frontend Design on AKS</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Architecture:</strong>
            <ul style="margin-left:1.2rem;">
              <li>Frontend deployed as Deployment + Service (ClusterIP) behind an Ingress for external traffic.</li>
              <li>Backend deployed as Deployment + Service (ClusterIP)  only reachable from frontend namespace or specific clients.</li>
            </ul>
          </li>
          <li><strong>Ingress & TLS:</strong> Use Ingress Controller (NGINX/AGIC) + cert-manager to provision TLS via Let's Encrypt or use Azure Front Door for WAF and global LB. Terminate TLS at Ingress and optionally re-encrypt to backend.</li>
          <li><strong>Authentication & Secrets:</strong> Store DB credentials, API keys in Azure Key Vault and mount via Secrets Store CSI Driver. Use managed identities for AKS to access Key Vault.</li>
          <li><strong>Network security:</strong> Enforce NetworkPolicies: allow ingress to backend only from frontend pods. Use private AKS + private ACR if required.</li>
          <li><strong>Service-to-service auth:</strong> Use mTLS with service mesh (Istio/Linkerd) or JWT tokens validated by backend. Prefer short-lived tokens and OIDC providers.</li>
          <li><strong>CI/CD & image security:</strong> Build images in CI, scan them (Snyk/Trivy), push to ACR, and deploy via Helm with image tags pinned to digests.</li>
          <li><strong>Observability & resilience:</strong> Add readiness/liveness probes, HPA, resource requests/limits, Prometheus metrics, centralized logging, and alerting for latency/5xx rates.</li>
        </ol>
        <p><strong>Example points of configuration:</strong></p>
        <pre><code># Ingress TLS (snippet)
tls:
- hosts:
  - app.example.com
  secretName: app-tls

# NetworkPolicy: allow only frontend namespace
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend</code></pre>
        <p><strong>In Practice:</strong> I deploy frontend + backend in separate namespaces, use Ingress for TLS, Key Vault for secrets, NetworkPolicies for isolation, and add automated CI gates and canary rollouts for safe deployment.</p>
      </div>`}]},{title:"14. Common Azure-Specific Scenarios",questions:[{question:"How do you deploy K8s on Azure AKS (basic flow)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS Deployment Flow (End-to-End)</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Create resource group:</strong> <code>az group create --name aks-rg --location eastus</code></li>
          <li><strong>Create AKS cluster:</strong> 
            <pre><code>az aks create   --resource-group aks-rg   --name myAKSCluster   --node-count 2   --enable-addons monitoring   --generate-ssh-keys</code></pre>
          </li>
          <li><strong>Get kubeconfig:</strong> <code>az aks get-credentials --resource-group aks-rg --name myAKSCluster</code></li>
          <li><strong>Verify connection:</strong> <code>kubectl get nodes</code></li>
          <li><strong>Deploy apps:</strong> Apply YAML manifests or Helm charts using <code>kubectl apply -f</code> or <code>helm install</code>.</li>
          <li><strong>Expose services:</strong> Use <code>Service type=LoadBalancer</code> or <code>Ingress</code> for external access.</li>
        </ol>
        <p><strong>In Practice:</strong> I automate this via Terraform or Azure DevOps pipeline for consistent and repeatable provisioning.</p>
      </div>`},{question:"Where do you scale App Service or VMs, and why do we specify default instance count?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Scaling in Azure  App Service vs VMs</strong></h3>
        <p><strong>App Service Scaling:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Scaling happens in the <strong>App Service Plan</strong>.</li>
          <li>Supports <strong>manual</strong> and <strong>auto-scaling</strong> based on metrics like CPU, memory, HTTP queue length, etc.</li>
          <li><strong>Default instance count</strong> ensures minimum availability  even when scale-down occurs.</li>
        </ul>
        <p><strong>VM Scale Sets (VMSS):</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>VMSS auto-scales based on rules or Azure Monitor metrics.</li>
          <li>Default instance count defines base capacity (to handle normal load).</li>
        </ul>
        <p><strong>In Practice:</strong> I keep 23 minimum instances in prod to ensure HA, then autoscale based on load.</p>
      </div>`},{question:"Suppose you select Service type = LoadBalancer  which Azure Load Balancer (Basic or Standard) is created in the background?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Load Balancer Type behind AKS Services</strong></h3>
        <p>By default, AKS (new clusters) uses the <strong>Standard Load Balancer</strong> when you expose a service as <code>type: LoadBalancer</code>.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Standard LB</strong>  Default for all AKS clusters (since 2020+). Supports HA, multiple backend pools, and secure IP-based access.</li>
          <li><strong>Basic LB</strong>  Deprecated for AKS; used only in older clusters or explicitly configured.</li>
        </ul>
        <p><strong>Verification:</strong></p>
        <pre><code>az network lb list --resource-group MC_*_aks-rg --output table</code></pre>
        <p><strong>In Practice:</strong> Always prefer Standard LB  it supports zones, private IPs, and better SLAs.</p>
      </div>`},{question:"How do you configure Azure Monitor with AKS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AKS Monitoring via Azure Monitor</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Enable monitoring during cluster creation:</strong>
            <pre><code>az aks create --enable-addons monitoring --workspace-resource-id &lt;LogAnalyticsWorkspaceId&gt;</code></pre>
          </li>
          <li><strong>Or enable later:</strong>
            <pre><code>az aks enable-addons -a monitoring -n myAKSCluster -g aks-rg</code></pre>
          </li>
          <li><strong>Azure Monitor collects:</strong>
            <ul style="margin-left:1.2rem;">
              <li>Container logs (stdout/stderr)</li>
              <li>Node metrics (CPU, memory, disk, network)</li>
              <li>Kubernetes events and health states</li>
            </ul>
          </li>
          <li><strong>Access:</strong> Go to <em>Azure Portal  AKS  Insights</em> to view cluster, node, and container health dashboards.</li>
          <li><strong>Custom alerts:</strong> Create alerts in Log Analytics using Kusto queries (e.g., pod restarts, high CPU, etc.).</li>
        </ol>
        <p><strong>In Practice:</strong> I pair Azure Monitor with Prometheus/Grafana for custom dashboards and fine-grained alerting.</p>
      </div>`},{question:"How do you recover from a partial deployment failure in AKS via DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recovery Strategy for Partial AKS Deployment Failures</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Detect failure early:</strong> CI/CD pipeline should fail fast  use exit codes and <code>kubectl rollout status</code> checks.</li>
          <li><strong>Rollback:</strong> Use Helm rollback or kubectl rollout undo:
            <pre><code>kubectl rollout undo deployment/my-app -n prod</code></pre>
          </li>
          <li><strong>Validate previous version:</strong> Check application health, logs, and endpoints post-rollback.</li>
          <li><strong>Fix pipeline issue:</strong> Identify YAML error, image issue, or resource conflict from pipeline logs.</li>
          <li><strong>Redeploy safely:</strong> Re-run pipeline with corrected manifests or Helm chart values.</li>
          <li><strong>Prevent future issues:</strong>
            <ul style="margin-left:1.2rem;">
              <li>Use <strong>helm --atomic</strong> (auto rollback on failure)</li>
              <li>Enable pipeline checkpoints and artifact versioning</li>
              <li>Apply changes via canary or blue-green strategy</li>
            </ul>
          </li>
        </ol>
        <p><strong>In Practice:</strong> My pipelines validate manifests (with kubeval/kube-linter) and use Helm atomic installs to self-recover safely.</p>
      </div>`},{question:"How do you load balance applications running in Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Load Balancing in Kubernetes (AKS)</strong></h3>
        <p><strong>Internal Load Balancing:</strong> Within cluster via kube-proxy  Service type <code>ClusterIP</code> balances traffic to Pods using round robin.</p>
        <p><strong>External Load Balancing:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Service type=LoadBalancer</strong>  Creates Azure Load Balancer (Standard) automatically.</li>
          <li><strong>Ingress Controller</strong>  Handles HTTP/HTTPS load balancing, routing based on host/path rules.</li>
        </ul>
        <p><strong>Best Practice:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Ingress</strong> for web apps (layer 7)</li>
          <li>Use <strong>Service type=LoadBalancer</strong> for TCP/UDP workloads</li>
          <li>For internal-only services, use <code>internal: true</code> annotation on LoadBalancer Service</li>
        </ul>
        <p><strong>In Practice:</strong> I typically use AGIC (Azure Application Gateway Ingress Controller) for secure HTTPS traffic and path-based routing.</p>
      </div>`},{question:"What is the difference between Azure Web Apps and AKS for container hosting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Web Apps vs AKS  Container Hosting Comparison</strong></h3>
        <table border="1" style="border-collapse:collapse; margin:1rem 0;">
          <tr><th>Feature</th><th>Azure Web App (for Containers)</th><th>Azure Kubernetes Service (AKS)</th></tr>
          <tr><td><strong>Use Case</strong></td><td>Single container or lightweight microservice hosting</td><td>Large-scale microservices, orchestrated workloads</td></tr>
          <tr><td><strong>Scaling</strong></td><td>Built-in auto-scale (App Service Plan)</td><td>Pod/Node auto-scaling via HPA/CA</td></tr>
          <tr><td><strong>Management</strong></td><td>Fully managed (PaaS)</td><td>User-managed control plane (CaaS)</td></tr>
          <tr><td><strong>Networking</strong></td><td>Basic VNet integration</td><td>Advanced  CNI, Network Policies, Ingress, Private endpoints</td></tr>
          <tr><td><strong>CI/CD</strong></td><td>Simple  GitHub or DevOps push triggers</td><td>Complex  Helm charts, pipelines, canary rollouts</td></tr>
          <tr><td><strong>Best For</strong></td><td>Simple web APIs, staging apps</td><td>Enterprise-grade distributed workloads</td></tr>
        </table>
        <p><strong>In Practice:</strong> I choose Web App for low-maintenance APIs; AKS for microservices or workloads needing network isolation, scaling, and control.</p>
      </div>`}]},{title:"15. Miscellaneous & Cross-Functional",questions:[{question:"What are Services in Kubernetes  explain all types and use cases.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Kubernetes Services  Overview & Use Cases</strong></h3>
        <p>Services in Kubernetes abstract Pod IPs and provide stable endpoints for communication. They enable networking and load balancing between components.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>ClusterIP (default):</strong> Exposes the Service on an internal cluster IP  used for internal communication between Pods.</li>
          <li><strong>NodePort:</strong> Exposes the Service on each nodes IP at a static port  accessible externally via <code>&lt;NodeIP&gt;:&lt;Port&gt;</code>.</li>
          <li><strong>LoadBalancer:</strong> Provisions an external cloud load balancer (e.g., Azure Load Balancer)  ideal for exposing apps to the internet.</li>
          <li><strong>ExternalName:</strong> Maps a Service to an external DNS name  used for legacy or hybrid connections.</li>
          <li><strong>Headless Service:</strong> Used for StatefulSets  provides direct Pod DNS entries without load balancing.</li>
        </ul>
        <p><strong>In Practice:</strong> I use ClusterIP for microservice-to-microservice communication, LoadBalancer for public endpoints, and Headless for databases or stateful apps.</p>
      </div>`},{question:"What is the difference between Ingress and Ingress Controller?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Ingress vs Ingress Controller</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Ingress:</strong> Kubernetes API object that defines HTTP/HTTPS routing rules (e.g., host, path-based routing).</li>
          <li><strong>Ingress Controller:</strong> Actual implementation that processes Ingress resources and routes traffic (e.g., NGINX, Azure Application Gateway Ingress Controller).</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>Ingress (rules)  Ingress Controller (executes rules using reverse proxy)</code></pre>
        <p><strong>In Practice:</strong> I commonly use Azure Application Gateway Ingress Controller (AGIC) for production-grade HTTPS routing and TLS termination.</p>
      </div>`},{question:"What is traffic flow from user to Pod using ingress rule?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Ingress Traffic Flow  End to End</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>User Request:</strong> User accesses <code>https://app.company.com</code>.</li>
          <li><strong>DNS Resolution:</strong> DNS points to external Load Balancer IP.</li>
          <li><strong>Ingress Controller:</strong> Load Balancer forwards traffic to Ingress Controller Pod (e.g., NGINX/AGIC).</li>
          <li><strong>Ingress Rule:</strong> Controller matches host/path (e.g., /api  backend-svc).</li>
          <li><strong>Service:</strong> The backend service routes request to one of its Pods (via kube-proxy).</li>
          <li><strong>Pod:</strong> Pod processes request and responds back through the same path in reverse.</li>
        </ol>
        <p><strong>In Practice:</strong> For TLS termination, AGIC or NGINX terminates SSL, and internal traffic continues over HTTP for performance.</p>
      </div>`},{question:"Have you worked on Azure Functions, Service Bus, or Logic Apps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Integration Services  Practical Experience</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure Functions:</strong> Used for event-driven automation  triggers from Event Grid, Service Bus, or HTTP endpoints.</li>
          <li><strong>Service Bus:</strong> Implemented message-based communication between microservices (queue/topic-based decoupling).</li>
          <li><strong>Logic Apps:</strong> Designed workflow orchestration for finance and operations  integrating with Outlook, Teams, and Azure SQL.</li>
        </ul>
        <p><strong>In Practice:</strong> Combined all three  Logic App triggers Azure Function  Function posts to Service Bus  AKS backend processes message asynchronously.</p>
      </div>`},{question:"What deployment method do you use in your CI/CD (YAML, Helm, kubectl, Terraform)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deployment Strategies in CI/CD</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>YAML Manifests:</strong> Used for simple or static configurations.</li>
          <li><strong>Helm Charts:</strong> Preferred for modular, templated deployments with parameterized values.</li>
          <li><strong>kubectl Apply:</strong> Used in early CI/CD stages or quick patches via scripts.</li>
          <li><strong>Terraform:</strong> Used for provisioning AKS infrastructure + base namespace setup before app deployment.</li>
        </ul>
        <p><strong>In Practice:</strong> My pipelines are multi-stage:
          <br>Stage 1  Terraform (infra)
          <br>Stage 2  Helm Deploy (apps)
          <br>Stage 3  Post-deployment tests + rollout validation.
        </p>
      </div>`},{question:"Have you worked on LLM or AI model deployments using Kubernetes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>LLM / AI Model Deployments on Kubernetes</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Deployed <strong>OpenAI-like models</strong> and <strong>HuggingFace Transformers</strong> in containerized form.</li>
          <li>Used <strong>GPU-enabled node pools</strong> in AKS with <code>nvidia.com/gpu</code> resource limits for inference workloads.</li>
          <li>Implemented <strong>Horizontal Pod Autoscaling</strong> based on custom metrics (QPS, GPU utilization).</li>
          <li>Configured <strong>model volume mounts</strong> for large datasets using Azure Files and Blob Fuse.</li>
          <li>Used <strong>Helm + KServe</strong> for managing multiple ML endpoints (vLLM, ONNX, PyTorch models).</li>
        </ul>
        <p><strong>In Practice:</strong> Ive deployed AI inference services behind Ingress (TLS + Auth)  scaling dynamically via HPA and caching using Redis to reduce model cold starts.</p>
      </div>`}]}];function Zw(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("kubernetes"),a=fm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-blue-600 to-indigo-600 shadow-glow",children:l.jsx(xs,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Kubernetes"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Container Orchestration Interview Questions"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Master Kubernetes with comprehensive questions on pods, services, deployments, storage, networking, and advanced orchestration concepts."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:fm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:(u==null?void 0:u.question)||"",b=typeof u=="object"&&u&&"answer"in u?u.answer:null,v=typeof u=="object"&&"answerHtml"in u?u.answerHtml:null;return l.jsx("div",{className:"border-l-2 border-blue-500/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-blue-500 font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),b||v?l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:v.replace(/\$\{/g,"$${")}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:b})})]}):null]})]})},f)})})})]},p))})]})]})}const oD=Object.freeze(Object.defineProperty({__proto__:null,default:Zw},Symbol.toStringTag,{value:"Module"})),vm=[{title:"1. Linux Fundamentals",questions:[{question:"What is Linux, and why is it preferred in DevOps environments?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Linux  Foundation of DevOps</strong></h3>
        <p><strong>Linux</strong> is an open-source operating system built around the Unix model. Its widely used in servers, containers, and cloud systems because of its flexibility and stability.</p>
        <p><strong>Why preferred in DevOps:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Open-source & cost-effective  no licensing overhead.</li>
          <li>Highly stable and secure for production workloads.</li>
          <li>Supports automation tools (Ansible, Jenkins, Terraform, Docker, etc.).</li>
          <li>Powerful CLI for scripting and system control.</li>
          <li>Lightweight and ideal for containerized environments (Docker/K8s).</li>
        </ul>
        <p><strong>In practice:</strong> Almost all CI/CD agents, app servers, and container nodes in my projects run on RHEL or Ubuntu Linux.</p>
      </div>`},{question:"What are the major Linux distributions youve worked with (RHEL, Ubuntu, CentOS, Oracle Linux)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Linux Distributions I've Worked With</strong></h3>
        <p><strong>Common distributions Ive used:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>RHEL (Red Hat Enterprise Linux):</strong> Used for production servers and enterprise workloads  stable, secure, and supported.</li>
          <li><strong>Ubuntu:</strong> Preferred for development, Jenkins build servers, and Docker hosts.</li>
          <li><strong>CentOS:</strong> Legacy systems and lab environments (RHEL-compatible).</li>
          <li><strong>Oracle Linux:</strong> Used in projects requiring Oracle DB integration with UEK kernel optimization.</li>
        </ul>
        <p><strong>Example:</strong> I currently manage RHEL 8.10 mail and monitoring servers, and Ubuntu VMs for CI/CD build agents.</p>
      </div>`},{question:"What are the basic Linux commands you use daily?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Common Linux Commands I Use Regularly</strong></h3>
        <ul style="margin-left:1.2rem; columns: 2;">
          <li><code>ls, cd, pwd</code>  navigation</li>
          <li><code>cat, less, tail -f</code>  view logs/configs</li>
          <li><code>df -h, du -sh</code>  check disk usage</li>
          <li><code>top, htop, free -m</code>  system resource monitoring</li>
          <li><code>ps aux | grep</code>  process lookup</li>
          <li><code>systemctl start|stop|status</code>  service control</li>
          <li><code>journalctl -xe</code>  check system logs</li>
          <li><code>vi, nano</code>  file editing</li>
          <li><code>scp, rsync, ssh</code>  file transfers & remote management</li>
          <li><code>chmod, chown</code>  permissions</li>
        </ul>
        <p><strong>In practice:</strong> I often use <code>tail -f /var/log/messages</code> or <code>journalctl</code> during troubleshooting.</p>
      </div>`},{question:"How do you check the current kernel version and OS details?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Checking Kernel & OS Information</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># Kernel version
uname -r

# Detailed kernel + architecture
uname -a

# OS release details
cat /etc/os-release

# RHEL-specific
cat /etc/redhat-release</code></pre>
        <p><strong>Example Output:</strong> <code>Linux 4.18.0-553.el8.x86_64</code> (RHEL 8.10).</p>
        <p><strong>In use:</strong> I verify OS version before applying patches or when validating package compatibility during upgrades.</p>
      </div>`},{question:"Whats the difference between absolute and relative paths?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Absolute vs Relative Paths</strong></h3>
        <p><strong>Absolute path:</strong> Starts from root (<code>/</code>) and specifies full directory path.</p>
        <pre><code>/var/log/nginx/access.log</code></pre>
        <p><strong>Relative path:</strong> Defined with respect to current working directory (<code>pwd</code>).</p>
        <pre><code>../nginx/access.log</code></pre>
        <p><strong>Difference:</strong> Absolute path always points to the same file regardless of current directory; relative path changes based on where you are.</p>
        <p><strong>In practice:</strong> I use absolute paths in scripts to avoid ambiguity during cron or CI/CD executions.</p>
      </div>`},{question:"How do you check the uptime and load average of a server?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Checking Uptime & Load Average</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code>uptime
top
cat /proc/loadavg</code></pre>
        <p><strong>Sample Output:</strong></p>
        <pre><code> 14:22:10 up 15 days,  3:10,  2 users,  load average: 0.45, 0.33, 0.27</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><strong>uptime:</strong> Shows system running time since last boot.</li>
          <li><strong>load average:</strong> Shows average system load over 1, 5, and 15 minutes.</li>
        </ul>
        <p><strong>In practice:</strong> I monitor load averages to identify CPU-bound processes or detect overloaded build agents.</p>
      </div>`},{question:"What are runlevels and how are they used in Linux?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Understanding Linux Runlevels</strong></h3>
        <p>Runlevels define the state or mode in which a Linux system operates (multi-user, graphical, rescue, etc.).</p>
        <p><strong>Traditional SysV Runlevels:</strong></p>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Runlevel</th><th>Mode</th></tr>
          <tr><td>0</td><td>Shutdown</td></tr>
          <tr><td>1</td><td>Single user mode (maintenance)</td></tr>
          <tr><td>3</td><td>Multi-user (no GUI)</td></tr>
          <tr><td>5</td><td>Multi-user with GUI</td></tr>
          <tr><td>6</td><td>Reboot</td></tr>
        </table>
        <p><strong>In systemd (modern distros):</strong> Targets replaced runlevels  e.g., <code>multi-user.target</code>  runlevel 3.</p>
        <p><strong>Commands:</strong></p>
        <pre><code>systemctl get-default
systemctl set-default multi-user.target</code></pre>
        <p><strong>In practice:</strong> I use runlevel targets for headless servers (CLI-only) and rescue targets for recovery operations.</p>
      </div>`}]},{title:"2. File System & Disk Management",questions:[{question:"How do you check disk space and usage (df -h, du -sh, etc.)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Checking Disk Space & Usage</strong></h3>
        <p><strong>Common commands:</strong></p>
        <pre><code># Check overall disk usage
df -h

# Check specific directory usage
du -sh /var/log

# Check top folders consuming space
du -sh * | sort -h</code></pre>
        <p><strong>Explanation:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><code>df -h</code>: Shows mounted filesystems and available space in human-readable format.</li>
          <li><code>du -sh</code>: Shows size of a specific directory or file.</li>
          <li><code>sort -h</code>: Sorts by human-readable size  useful for finding heavy folders.</li>
        </ul>
        <p><strong>In practice:</strong> I run these when disk alerts trigger on servers or CI/CD agents to locate log or dump directories consuming space.</p>
      </div>`},{question:"Is there any flag you use with the df command?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Useful df Command Flags</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>-h</code>  Human-readable output (GB/MB).</li>
          <li><code>-T</code>  Show filesystem type (e.g., ext4, xfs, nfs).</li>
          <li><code>-i</code>  Display inode usage (useful when file count is high).</li>
          <li><code>-x tmpfs</code>  Exclude certain filesystem types (like tmpfs).</li>
        </ul>
        <pre><code>df -hT         # Size + Type
df -hi         # Human + inode usage</code></pre>
        <p><strong>In practice:</strong> I use <code>df -hT</code> for capacity planning and <code>df -hi</code> when applications fail due to inode exhaustion (many small files).</p>
      </div>`},{question:"How do you find large files or directories consuming space?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Finding Large Files & Directories</strong></h3>
        <p><strong>Commands I use:</strong></p>
        <pre><code># Find top directories consuming space
du -ah / | sort -rh | head -20

# Find files larger than 500MB
find / -type f -size +500M -exec ls -lh {} ; | sort -k 5 -h</code></pre>
        <p><strong>Quick Tip:</strong> Run from the partition level (e.g., /var, /opt) instead of root for faster results.</p>
        <p><strong>In practice:</strong> I regularly use this to identify oversized logs in /var/log or large Docker images under /var/lib/docker.</p>
      </div>`},{question:"How do you mount a file system or volume?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Mounting a Filesystem</strong></h3>
        <p><strong>Steps:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Create a directory as a mount point:<br><code>mkdir /mnt/data</code></li>
          <li>Mount the device:<br><code>mount /dev/sdb1 /mnt/data</code></li>
          <li>Verify mount:<br><code>df -hT</code> or <code>mount | grep data</code></li>
        </ol>
        <p><strong>Unmount:</strong> <code>umount /mnt/data</code></p>
        <p><strong>In practice:</strong> I mount new disks during storage expansion or for backup targets before setting them as persistent mounts via <code>/etc/fstab</code>.</p>
      </div>`},{question:"How do you mount an external drive or file share (NFS/SMB)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Mounting Network File Shares</strong></h3>
        <p><strong>For NFS share:</strong></p>
        <pre><code>sudo mkdir /mnt/nfs
sudo mount -t nfs server:/data /mnt/nfs</code></pre>
        <p><strong>For SMB/CIFS share:</strong></p>
        <pre><code>sudo mkdir /mnt/smb
sudo mount -t cifs //server/share /mnt/smb -o username=user,password=pass,vers=3.0</code></pre>
        <p><strong>Persistent Mount (via /etc/fstab):</strong></p>
        <pre><code>server:/data  /mnt/nfs  nfs  defaults  0  0</code></pre>
        <p><strong>In practice:</strong> I use NFS mounts between build servers and artifact storage; for Windows shares, SMB with credentials or keytab-based auth.</p>
      </div>`},{question:"How do you extend a volume using LVM (Logical Volume Manager)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Extending Volume using LVM</strong></h3>
        <p><strong>Steps I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Check existing volume groups and logical volumes:<br><code>vgs</code> and <code>lvs</code></li>
          <li>Extend LV by size:<br><code>lvextend -L +5G /dev/mapper/vg_data-lv_app</code></li>
          <li>Resize filesystem:<br><code>resize2fs /dev/mapper/vg_data-lv_app</code></li>
          <li>Verify space:<br><code>df -h</code></li>
        </ol>
        <p><strong>Note:</strong> For XFS filesystems, use <code>xfs_growfs /mountpoint</code> instead of <code>resize2fs</code>.</p>
        <p><strong>In practice:</strong> I often use this method to extend log or app partitions dynamically when disk usage crosses 80% threshold.</p>
      </div>`},{question:"How do you verify a mount point is persistent after reboot?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Verifying Persistent Mounts</strong></h3>
        <p><strong>Check /etc/fstab:</strong></p>
        <pre><code>cat /etc/fstab</code></pre>
        <p>or test manually without reboot:</p>
        <pre><code>mount -a</code></pre>
        <p>If the mount works and no error appears  its properly configured for persistence.</p>
        <p><strong>In practice:</strong> I always validate <code>mount -a</code> after editing fstab to avoid boot-time issues due to syntax errors.</p>
      </div>`},{question:"Whats the difference between /etc/fstab and manual mount?",answerHtml:`<div class="answer-rich">
        <h3> <strong>/etc/fstab vs Manual Mount</strong></h3>
        <p><strong>/etc/fstab:</strong> File that defines persistent mounts  automatically mounts disks/shares at boot time.</p>
        <p><strong>Manual Mount:</strong> Temporary  mount lasts only until reboot.</p>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Aspect</th><th>/etc/fstab</th><th>manual mount</th></tr>
          <tr><td>Persistence</td><td>Permanent (auto after reboot)</td><td>Temporary</td></tr>
          <tr><td>Usage</td><td>System-level setup</td><td>Ad-hoc or troubleshooting</td></tr>
          <tr><td>Command</td><td>mount -a (auto)</td><td>mount /dev/sdb1 /data</td></tr>
        </table>
        <p><strong>In practice:</strong> I use manual mounts for quick testing, then add the verified entry in <code>/etc/fstab</code> once confirmed stable.</p>
      </div>`}]},{title:"3. Process, Memory & Performance Monitoring",questions:[{question:"How do you check CPU and memory utilization in Linux?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Checking CPU & Memory Utilization</strong></h3>
        <p><strong>Common commands:</strong></p>
        <pre><code># Real-time CPU & memory usage
top

# Summary of memory
free -h

# CPU usage over time
mpstat 1

# Memory & swap usage
vmstat -s | grep -E 'memory|swap'</code></pre>
        <p><strong>In practice:</strong> I use <code>top</code> for real-time troubleshooting and <code>vmstat</code> for historical trend during high-load analysis.</p>
      </div>`},{question:"What commands do you use for performance monitoring? (top, htop, vmstat, free, etc.)",answerHtml:`<div class="answer-rich">
        <h3> <strong>Performance Monitoring Commands</strong></h3>
        <p><strong>Tools I regularly use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>top:</strong> Live CPU, memory, and process overview.</li>
          <li><strong>htop:</strong> Interactive version of top with color UI and sorting.</li>
          <li><strong>vmstat:</strong> Virtual memory, swap, IO, and system stats.</li>
          <li><strong>iostat:</strong> Disk I/O performance by device.</li>
          <li><strong>free -h:</strong> Memory usage summary.</li>
          <li><strong>sar:</strong> Historical CPU, memory, and I/O usage reports.</li>
        </ul>
        <pre><code>iostat -xz 1   # Extended disk I/O every second
vmstat 2       # CPU & memory stats every 2s
sar -u 5 5     # Average CPU utilization sample</code></pre>
        <p><strong>In practice:</strong> I use <code>sar</code> logs from sysstat package to diagnose intermittent CPU spikes.</p>
      </div>`},{question:"What is the difference between top, htop, and iotop?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Difference Between top, htop & iotop</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Tool</th><th>Purpose</th><th>Highlights</th></tr>
          <tr><td><strong>top</strong></td><td>Default process monitor</td><td>CPU, memory, load avg  CLI-based</td></tr>
          <tr><td><strong>htop</strong></td><td>Enhanced version of top</td><td>Interactive UI, tree view, sorting, color-coded</td></tr>
          <tr><td><strong>iotop</strong></td><td>Monitors disk I/O per process</td><td>Shows read/write IOPS, throughput per PID</td></tr>
        </table>
        <p><strong>In practice:</strong> I run <code>htop</code> for quick diagnosis, and <code>iotop</code> when high I/O wait is observed in <code>top</code> output.</p>
      </div>`},{question:"How do you monitor running processes?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Monitoring Active Processes</strong></h3>
        <p><strong>Common commands:</strong></p>
        <pre><code>ps aux          # List all processes
ps -ef | grep nginx
pgrep -l nginx  # Find process by name
pstree -p       # Show parent-child hierarchy
top or htop     # Real-time view</code></pre>
        <p><strong>In practice:</strong> I use <code>ps aux --sort=-%mem | head</code> to quickly identify memory-heavy processes during alert triage.</p>
      </div>`},{question:"How do you identify which process is consuming the most memory or CPU?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Finding High CPU or Memory Processes</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># Sort by CPU
ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%cpu | head

# Sort by Memory
ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | head

# Real-time view
top -o %CPU</code></pre>
        <p><strong>In practice:</strong> During CPU alerts, I first check <code>top</code> for the process consuming >70%, then correlate PID with app logs to identify root cause (like infinite loops or heavy queries).</p>
      </div>`},{question:"How do you check which process is using a specific port? (netstat, ss, lsof)",answerHtml:`<div class="answer-rich">
        <h3> <strong>Check Process Using a Port</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># Using netstat
netstat -tulnp | grep 8080

# Using ss (modern replacement)
ss -tulnp | grep 8080

# Using lsof
lsof -i :8080</code></pre>
        <p><strong>In practice:</strong> I prefer <code>ss</code> since its faster and pre-installed on newer distros  handy when multiple apps compete for same port (e.g., Jenkins, Nginx).</p>
      </div>`},{question:"How do you troubleshoot high CPU or RAM usage?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Troubleshooting High CPU or RAM Usage</strong></h3>
        <p><strong>Step-by-step approach:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Check overall system load:</strong> <code>uptime</code>, <code>top</code>, <code>vmstat 2</code>.</li>
          <li><strong>Identify culprit process:</strong> <code>ps -eo pid,cmd,%cpu,%mem --sort=-%cpu | head</code>.</li>
          <li><strong>Inspect memory leaks:</strong> <code>pmap -x &lt;PID&gt;</code> or <code>smem -p</code>.</li>
          <li><strong>Analyze I/O wait:</strong> <code>iostat -xz 1</code>.</li>
          <li><strong>Check logs:</strong> Application or system logs under <code>/var/log</code>.</li>
          <li><strong>Restart or scale:</strong> If persistent, restart service or scale horizontally (VM/Container).</li>
        </ol>
        <p><strong>Example:</strong> Once found high CPU caused by an unindexed SQL query in a Java app  fixed via query optimization and JVM memory tuning.</p>
      </div>`},{question:"How do you check network connectivity between two servers? (ping, traceroute, nc, etc.)",answerHtml:`<div class="answer-rich">
        <h3> <strong>Testing Network Connectivity</strong></h3>
        <p><strong>Basic tools:</strong></p>
        <pre><code># Check reachability
ping &lt;IP or hostname&gt;

# Trace network hops
traceroute &lt;destination&gt;

# Check specific port connectivity
nc -zv &lt;IP&gt; 22
telnet &lt;IP&gt; 443

# DNS resolution
nslookup example.com</code></pre>
        <p><strong>In practice:</strong> I use <code>nc -zv</code> to test outbound ports (like 443 or 1433) during pipeline agent setup or when validating private endpoint connectivity in Azure.</p>
      </div>`}]},{title:"4. Services & System Management",questions:[{question:"What is systemctl, and how is it used?",answerHtml:`<div class="answer-rich">
        <h3> <strong>systemctl  Core Service Management Tool</strong></h3>
        <p><strong>systemctl</strong> is the command-line utility to control and manage services, units, and the overall system state in Linux systems that use <strong>systemd</strong>.</p>
        <p><strong>Common uses:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Start/stop/restart services.</li>
          <li>Enable or disable services at boot.</li>
          <li>Check system and service status.</li>
          <li>Manage system targets (runlevels).</li>
        </ul>
        <pre><code>systemctl status nginx
systemctl start nginx
systemctl stop nginx
systemctl restart nginx
systemctl enable nginx</code></pre>
        <p><strong>In practice:</strong> I use <code>systemctl</code> daily to manage Nginx, Docker, Jenkins, and custom app services on RHEL and Ubuntu servers.</p>
      </div>`},{question:"How do you start, stop, restart, enable, or disable a service?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Managing Services with systemctl</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># Start a service
sudo systemctl start nginx

# Stop a service
sudo systemctl stop nginx

# Restart a service
sudo systemctl restart nginx

# Enable service at boot
sudo systemctl enable nginx

# Disable service from auto-start
sudo systemctl disable nginx</code></pre>
        <p><strong>Verification:</strong></p>
        <pre><code>systemctl is-enabled nginx
systemctl status nginx</code></pre>
        <p><strong>In practice:</strong> Before restarting critical services like Docker or Jenkins, I always check dependencies and health using <code>systemctl list-dependencies</code> to avoid chain failures.</p>
      </div>`},{question:"How do you check if a service is enabled at boot?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Check Service Auto-start on Boot</strong></h3>
        <p><strong>Command:</strong></p>
        <pre><code>systemctl is-enabled docker</code></pre>
        <p><strong>Output:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><code>enabled</code>  service will start automatically at boot.</li>
          <li><code>disabled</code>  not auto-started.</li>
          <li><code>static</code>  required by another unit, not directly enabled.</li>
        </ul>
        <p><strong>Alternative:</strong></p>
        <pre><code>systemctl list-unit-files | grep enabled</code></pre>
        <p><strong>In practice:</strong> I verify service enablement after new deployments, especially for monitoring agents, Docker, or app daemons, to ensure they persist after reboot.</p>
      </div>`},{question:"How do you create your own custom service in Linux?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Creating a Custom Service</strong></h3>
        <p><strong>Steps:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Create a unit file under <code>/etc/systemd/system/</code>, e.g., <code>/etc/systemd/system/myapp.service</code>.</li>
          <li>Add the following content:</li>
        </ol>
        <pre><code>[Unit]
Description=My Custom Python App
After=network.target

[Service]
ExecStart=/usr/bin/python3 /opt/myapp/app.py
Restart=always
User=ec2-user
WorkingDirectory=/opt/myapp

[Install]
WantedBy=multi-user.target</code></pre>
        <ol start="3" style="margin-left:1.2rem;">
          <li>Reload systemd and enable service:</li>
        </ol>
        <pre><code>sudo systemctl daemon-reload
sudo systemctl enable myapp
sudo systemctl start myapp</code></pre>
        <p><strong>In practice:</strong> I create custom services for app daemons and background scripts (like log collectors or webhook listeners) so they auto-restart if they crash.</p>
      </div>`},{question:"How do you check system logs under /var/log?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Checking System Logs</strong></h3>
        <p><strong>Common log locations:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><code>/var/log/messages</code>  system and service logs (RHEL/CentOS).</li>
          <li><code>/var/log/syslog</code>  general logs (Ubuntu/Debian).</li>
          <li><code>/var/log/secure</code>  authentication logs.</li>
          <li><code>/var/log/dmesg</code>  kernel logs.</li>
          <li><code>/var/log/nginx/</code>, <code>/var/log/httpd/</code>  web server logs.</li>
        </ul>
        <p><strong>Useful commands:</strong></p>
        <pre><code>sudo tail -f /var/log/messages
sudo journalctl -xe
sudo grep "error" /var/log/syslog</code></pre>
        <p><strong>In practice:</strong> I use <code>journalctl -xe</code> to diagnose service start failures and <code>tail -f</code> when monitoring real-time logs during deployments.</p>
      </div>`},{question:"How do you troubleshoot boot or service startup failures using logs?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Troubleshooting Boot or Service Startup Failures</strong></h3>
        <p><strong>Approach I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Check the failing service status:</li>
          <pre><code>systemctl status myapp.service</code></pre>
          <li>Read detailed error logs:</li>
          <pre><code>journalctl -u myapp.service -xe</code></pre>
          <li>Verify dependencies and targets:</li>
          <pre><code>systemctl list-dependencies myapp.service</code></pre>
          <li>Inspect boot logs if issue occurs during startup:</li>
          <pre><code>dmesg | tail -20
journalctl -b</code></pre>
          <li>Fix issues like wrong ExecStart path, permission errors, or missing dependencies.</li>
        </ol>
        <p><strong>Example:</strong> Once a Python app service failed due to incorrect virtualenv path  fixed the <code>ExecStart</code> entry and reloaded daemon, service started fine.</p>
      </div>`}]},{title:"5. Users, Permissions & Security",questions:[{question:"How do you add or remove users in Linux?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Adding & Removing Users</strong></h3>
        <p><strong>Add new user:</strong></p>
        <pre><code>sudo useradd -m -s /bin/bash devuser
sudo passwd devuser</code></pre>
        <p><strong>Remove a user:</strong></p>
        <pre><code>sudo userdel devuser
sudo userdel -r devuser   # remove user and home directory</code></pre>
        <p><strong>In practice:</strong> I create dedicated system users for CI/CD or app services (e.g., <code>jenkins</code>, <code>nginx</code>) instead of running under root  improves security and audit clarity.</p>
      </div>`},{question:"How do you modify or reset a users password?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Modifying or Resetting Passwords</strong></h3>
        <p><strong>Reset user password:</strong></p>
        <pre><code>sudo passwd devuser</code></pre>
        <p><strong>Force password change on next login:</strong></p>
        <pre><code>sudo passwd -e devuser</code></pre>
        <p><strong>Lock or unlock user account:</strong></p>
        <pre><code>sudo usermod -L devuser    # lock account
sudo usermod -U devuser    # unlock</code></pre>
        <p><strong>In practice:</strong> I lock service accounts instead of deleting them to retain file ownership mappings and logs.</p>
      </div>`},{question:"How do you check users groups and permissions?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Checking User Groups & Permissions</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># Check user's primary and secondary groups
id devuser

# List all groups user belongs to
groups devuser

# List group memberships of all users
getent group</code></pre>
        <p><strong>File ownership & permissions:</strong></p>
        <pre><code>ls -l /home/devuser</code></pre>
        <p><strong>In practice:</strong> I often validate <code>id</code> and <code>groups</code> output before giving sudo or NFS share access to users.</p>
      </div>`},{question:"What is the difference between chmod, chown, and chgrp?",answerHtml:`<div class="answer-rich">
        <h3> <strong>chmod vs chown vs chgrp</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Command</th><th>Purpose</th><th>Example</th></tr>
          <tr><td><strong>chmod</strong></td><td>Change file or directory permissions</td><td><code>chmod 755 script.sh</code></td></tr>
          <tr><td><strong>chown</strong></td><td>Change file owner</td><td><code>chown devuser script.sh</code></td></tr>
          <tr><td><strong>chgrp</strong></td><td>Change group ownership</td><td><code>chgrp developers script.sh</code></td></tr>
        </table>
        <p><strong>In practice:</strong> I use <code>chown -R appuser:appgroup /opt/app</code> to ensure app directories have proper ownership before deployment.</p>
      </div>`},{question:"How do you set file and directory permissions recursively?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Recursive Permission Setting</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># Change permissions recursively
chmod -R 755 /opt/app

# Change ownership recursively
chown -R devuser:developers /opt/app</code></pre>
        <p><strong>Note:</strong> Use recursion carefully  changing ownership on system paths can break dependencies.</p>
        <p><strong>In practice:</strong> Before recursive changes, I test on specific subpaths (<code>/opt/app/logs</code>) and validate with <code>ls -ld</code>.</p>
      </div>`},{question:"What are SUID, SGID, and sticky bits?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Special Permission Bits</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>SUID (Set User ID):</strong> File runs with the permission of the file owner.<br><code>-rwsr-xr-x</code>  Example: <code>/usr/bin/passwd</code>.</li>
          <li><strong>SGID (Set Group ID):</strong> File executes with group privileges; new files in directory inherit group.<br><code>chmod g+s /shared</code>.</li>
          <li><strong>Sticky Bit:</strong> Only file owner can delete files inside a directory.<br><code>chmod +t /tmp</code>.</li>
        </ul>
        <p><strong>In practice:</strong> I ensure <code>/tmp</code> and shared folders have sticky bit set to prevent accidental deletion by other users.</p>
      </div>`},{question:"How do you secure SSH access on a Linux server?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Securing SSH Access</strong></h3>
        <p><strong>Steps I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Create a new user and assign sudo privileges (avoid root login).</li>
          <li>Generate SSH key pair (<code>ssh-keygen</code>) and copy public key to <code>~/.ssh/authorized_keys</code>.</li>
          <li>Restrict SSH to specific users or groups using <code>AllowUsers</code> or <code>AllowGroups</code> in <code>/etc/ssh/sshd_config</code>.</li>
          <li>Change default SSH port from 22 to custom (e.g., 2222).</li>
          <li>Disable password-based logins and root access.</li>
        </ol>
        <pre><code>sudo vi /etc/ssh/sshd_config
PermitRootLogin no
PasswordAuthentication no
Port 2222
AllowUsers devuser</code></pre>
        <p>Then restart SSH: <code>sudo systemctl restart sshd</code></p>
        <p><strong>In practice:</strong> I always test new SSH rules in a separate session to avoid locking myself out.</p>
      </div>`},{question:"How do you disable root login or password-based SSH authentication?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Disabling Root Login & Password SSH Access</strong></h3>
        <p><strong>Edit SSH configuration:</strong></p>
        <pre><code>sudo vi /etc/ssh/sshd_config</code></pre>
        <p>Set the following parameters:</p>
        <pre><code>PermitRootLogin no
PasswordAuthentication no</code></pre>
        <p>Then restart service:</p>
        <pre><code>sudo systemctl restart sshd</code></pre>
        <p><strong>In practice:</strong> I use key-based access via <code>~/.ssh/authorized_keys</code> for all admins and enforce MFA on jump servers for additional security.</p>
      </div>`},{question:"What are best practices to harden SSH security (port change, key pairs, fail2ban, etc.)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>SSH Hardening Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Disable root login and password authentication.</li>
          <li>Use key-based authentication (<code>ssh-keygen</code>).</li>
          <li>Change default SSH port (<code>Port 2222</code>).</li>
          <li>Restrict access using <code>AllowUsers</code> or <code>AllowGroups</code>.</li>
          <li>Enable <strong>Fail2Ban</strong> to block brute-force attempts.</li>
          <li>Use <strong>firewalld</strong> or <strong>iptables</strong> to allow only trusted IPs.</li>
          <li>Implement <strong>MFA</strong> (Google Authenticator or Azure AD SSH integration).</li>
        </ul>
        <p><strong>In practice:</strong> I combine SSH key auth with Fail2Ban and non-default port  reduces brute-force attempts by >90% on public-facing servers.</p>
      </div>`}]},{title:"6. Shell Scripting & Automation",questions:[{question:"What is a shell script, and how do you create one?",answerHtml:`<div class="answer-rich">
        <h3> <strong>What is a Shell Script?</strong></h3>
        <p>A <strong>shell script</strong> is a text file containing a sequence of Linux commands executed together. It automates repetitive tasks like deployments, backups, monitoring, or patching.</p>
        <p><strong>To create a shell script:</strong></p>
        <pre><code>#!/bin/bash
echo "System uptime:"
uptime</code></pre>
        <p><strong>Steps:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Create a file  <code>vi myscript.sh</code></li>
          <li>Add commands & shebang (<code>#!/bin/bash</code>)</li>
          <li>Make it executable  <code>chmod +x myscript.sh</code></li>
          <li>Run  <code>./myscript.sh</code></li>
        </ol>
        <p><strong>In practice:</strong> I use shell scripts for automating server patching, system checks, and Jenkins job triggers.</p>
      </div>`},{question:"How do you make a script executable and run it?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Making a Script Executable</strong></h3>
        <pre><code># Give execute permission
chmod +x backup.sh

# Run using relative path
./backup.sh

# Or specify shell explicitly
bash backup.sh</code></pre>
        <p><strong>In practice:</strong> I prefer <code>#!/bin/bash -e</code> at the top  ensures script stops on first error (useful in automation pipelines).</p>
      </div>`},{question:"What is the difference between > and >> in shell redirection?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Shell Output Redirection</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Operator</th><th>Description</th><th>Example</th></tr>
          <tr><td><code>&gt;</code></td><td>Overwrite output to file</td><td><code>echo "Hello" > file.txt</code></td></tr>
          <tr><td><code>&gt;&gt;</code></td><td>Append output to file</td><td><code>echo "World" >> file.txt</code></td></tr>
        </table>
        <p><strong>In practice:</strong> I use <code>&gt;&gt;</code> in log scripts to append daily job results, preventing overwrite.</p>
      </div>`},{question:"How do you use variables in shell scripts?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Using Variables in Shell Scripts</strong></h3>
        <pre><code>#!/bin/bash
NAME="Ritesh"
echo "Hello &dollar;NAME, welcome to DevOps!"</code></pre>
        <p><strong>Best practices:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use uppercase for global vars.</li>
          <li>Wrap variable in <code>&dollar;{VAR}</code> for clarity.</li>
          <li>Use <code>export VAR=value</code> to make it available to child processes.</li>
        </ul>
        <p><strong>In practice:</strong> I inject environment variables (e.g., build paths, versions) dynamically in CI/CD scripts.</p>
      </div>`},{question:"What are conditional statements and loops in bash?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Conditionals & Loops</strong></h3>
        <p><strong>If-else example:</strong></p>
        <pre><code>if [ -f /etc/passwd ]; then
  echo "File exists"
else
  echo "File not found"
fi</code></pre>
        <p><strong>For loop example:</strong></p>
        <pre><code>for i in {1..3}; do
  echo "Iteration &dollar;i"
done</code></pre>
        <p><strong>While loop example:</strong></p>
        <pre><code>count=1
while [ &dollar;count -le 3 ]; do
  echo "Count: &dollar;count"
  ((count++))
done</code></pre>
        <p><strong>In practice:</strong> I use loops for batch user creation, service restarts, or iterating through log files.</p>
      </div>`},{question:"How do you handle input arguments in shell scripts?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Handling Input Arguments</strong></h3>
        <p><strong>Example:</strong></p>
        <pre><code>#!/bin/bash
echo "Script name: &dollar;0"
echo "First argument: &dollar;1"
echo "Second argument: &dollar;2"</code></pre>
        <p><strong>With condition:</strong></p>
        <pre><code>if [ &dollar;# -lt 2 ]; then
  echo "Usage: &dollar;0 &lt;source&gt; &lt;destination&gt;"
  exit 1
fi</code></pre>
        <p><strong>In practice:</strong> I pass arguments like environment (dev/prod) or version tag into deployment scripts triggered by pipelines.</p>
      </div>`},{question:"How do you schedule tasks with cron jobs?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Scheduling Tasks with Cron</strong></h3>
        <p><strong>Syntax:</strong> <code>* * * * * command</code></p>
        <pre><code># Example: Run backup daily at 2 AM
0 2 * * * /opt/scripts/backup.sh &gt;&gt; /var/log/backup.log 2&gt;&1</code></pre>
        <p><strong>Command:</strong> <code>crontab -e</code> to edit users cron jobs.</p>
        <p><strong>In practice:</strong> I schedule log rotation, cleanup, and patch validation scripts using cron for lightweight automation.</p>
      </div>`},{question:"How do you list, add, and remove crontab entries?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Managing Crontab Entries</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># List all cron jobs
crontab -l

# Edit or add new cron jobs
crontab -e

# Remove all cron jobs for user
crontab -r</code></pre>
        <p><strong>System-wide cron jobs:</strong> Located under <code>/etc/cron.d</code> or <code>/etc/cron.daily</code>.</p>
        <p><strong>In practice:</strong> I maintain cron jobs via config management (Ansible templates) to keep consistency across environments.</p>
      </div>`},{question:"What is the difference between cron and systemd timers?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Cron vs systemd Timers</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Feature</th><th>cron</th><th>systemd timer</th></tr>
          <tr><td>Purpose</td><td>Simple scheduled jobs</td><td>Advanced scheduling with dependencies</td></tr>
          <tr><td>Logging</td><td>Minimal (maillog)</td><td>Integrated with <code>journalctl</code></td></tr>
          <tr><td>Management</td><td><code>crontab -e</code></td><td><code>systemctl list-timers</code></td></tr>
          <tr><td>Dependency control</td><td>No</td><td>Yes (start after network, etc.)</td></tr>
        </table>
        <p><strong>In practice:</strong> I prefer <strong>systemd timers</strong> for production scripts  easier logging, dependency handling, and monitoring.</p>
      </div>`},{question:"How do you automate patching using shell scripts or Ansible?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Automating Patching</strong></h3>
        <p><strong>Using Shell Script (RHEL example):</strong></p>
        <pre><code>#!/bin/bash
yum clean all
yum -y update
reboot</code></pre>
        <p><strong>Using Ansible Playbook:</strong></p>
        <pre><code>- hosts: all
  become: yes
  tasks:
    - name: Apply all updates
      yum:
        name: "*"
        state: latest
    - name: Reboot if required
      reboot:</code></pre>
        <p><strong>In practice:</strong> I prefer Ansible for patch automation  provides logs, rollback control, and can run on multiple servers parallelly.</p>
      </div>`},{question:"How do you log script output and send it via email or Slack?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Logging & Notifications</strong></h3>
        <p><strong>Redirect output to log:</strong></p>
        <pre><code>./deploy.sh &gt;&gt; /var/log/deploy.log 2&gt;&1</code></pre>
        <p><strong>Send log via email:</strong></p>
        <pre><code>mail -s "Backup Completed" admin@company.com &lt; /var/log/backup.log</code></pre>
        <p><strong>Send log to Slack (using webhook):</strong></p>
        <pre><code>curl -X POST -H 'Content-type: application/json' --data '{"text":" Backup completed successfully"}' https://hooks.slack.com/services/TOKEN/ID</code></pre>
        <p><strong>In practice:</strong> I integrate critical automation jobs with Slack webhooks  gives real-time alerts on job success or failure.</p>
      </div>`}]},{title:"7. PowerShell & Azure Automation",questions:[{question:"What is PowerShell, and where do you use it in Azure Automation?",answerHtml:`<div class="answer-rich">
        <h3> <strong>PowerShell  Automation & Scripting for Windows + Azure</strong></h3>
        <p>PowerShell is a task automation and configuration management framework  shell + scripting language  built on .NET. Modern PowerShell (PowerShell Core / pwsh) is cross-platform and used widely for Windows automation and Azure management via the <code>Az</code> modules.</p>
        <p><strong>Common Azure uses:</strong> Automating resource provisioning (VMs, networking), runbooks in <strong>Azure Automation</strong>, configuration tasks, retrieving/reporting inventory, and invoking remediation scripts from pipelines or Logic Apps.</p>
        <p><strong>In practice:</strong> I use PowerShell runbooks to automate patch windows, rotate secrets, and run scheduled infrastructure checks that call Azure REST/APIs or Az cmdlets.</p>
      </div>`},{question:"What is the difference between Bash and PowerShell scripting?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Bash vs PowerShell  Key Differences</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Data model:</strong> PowerShell uses .NET objects (rich typed objects)  easier manipulation of complex outputs. Bash works with plain text streams.</li>
          <li><strong>Cross-platform:</strong> Bash is native on Linux; PowerShell Core (pwsh) is cross-platform (Windows, Linux, macOS).</li>
          <li><strong>Cmdlets vs utilities:</strong> PowerShell provides cmdlets (Get-Item, Get-Process). Bash relies on Unix utilities (grep, awk, sed).</li>
          <li><strong>Scripting style:</strong> PowerShell has structured error handling (<code>try/catch</code>) and advanced piping of objects. Bash relies on exit codes and text parsing.</li>
        </ul>
        <p><strong>In practice:</strong> I use Bash for lightweight Linux automation in containers and PowerShell for Windows/Azure tasks where object output (Az module) simplifies scripting and reduces parsing errors.</p>
      </div>`},{question:"How do you schedule a PowerShell script in Windows or Azure?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Scheduling PowerShell Scripts</strong></h3>
        <p><strong>Windows (Task Scheduler):</strong></p>
        <pre><code># Create a scheduled task via GUI or using schtasks
schtasks /Create /SC DAILY /TN "DailyScript" /TR "C:\\scripts\\task.ps1" /ST 02:00</code></pre>
        <p><strong>Azure:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure Automation Runbook:</strong> Create a PowerShell runbook, publish it and link a schedule. Good for credentials stored in Automation or Key Vault.</li>
          <li><strong>Azure Functions (PowerShell):</strong> Use timer trigger for serverless scheduling.</li>
          <li><strong>Logic Apps:</strong> Use Recurrence trigger and call an Azure Automation runbook or webhook that executes the script.</li>
        </ul>
        <pre><code># Example: link schedule to runbook in portal or via Az CLI
az automation runbook create --resource-group rg --automation-account acct --name MyRunbook --type PowerShell</code></pre>
        <p><strong>In practice:</strong> I prefer Azure Automation for enterprise runbooks (credential handling, logging, retries); Functions for lightweight or event-driven tasks.</p>
      </div>`},{question:"How do you connect to Azure using Azure CLI on Linux?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Connecting to Azure from Linux (az cli)</strong></h3>
        <p><strong>Interactive login:</strong></p>
        <pre><code>az login
# If multiple subscriptions:
az account set --subscription "SUBSCRIPTION_ID_OR_NAME"</code></pre>
        <p><strong>Service principal (automation / CI):</strong></p>
        <pre><code>az login --service-principal -u "APP_ID" -p "CLIENT_SECRET" --tenant "TENANT_ID"
az account set --subscription "SUBSCRIPTION_ID"</code></pre>
        <p><strong>Device code (when interactive machine has no browser):</strong></p>
        <pre><code>az login --use-device-code</code></pre>
        <p><strong>In practice:</strong> CI/CD uses service principal credentials stored in pipeline secrets or use managed identities on runners/VMs to avoid secrets.</p>
      </div>`},{question:"In PowerShell, how would you write a script to retrieve the public IP automatically?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Retrieve Public IP in PowerShell</strong></h3>
        <p><strong>Option A  Call a public IP service (simple):</strong></p>
        <pre><code>&dollar;publicIp = (Invoke-RestMethod -Uri 'https://ifconfig.co/json').ip
Write-Output "Public IP: &dollar;publicIp"</code></pre>
        <p><strong>Option B  Using Azure Az module (fetch Azure Public IP resource):</strong></p>
        <pre><code>Install-Module -Name Az -Scope CurrentUser -Force
Connect-AzAccount
&dollar;pip = Get-AzPublicIpAddress -ResourceGroupName 'rg-name' -Name 'myPublicIP'
&dollar;ip = &dollar;pip.IpAddress
Write-Output "Azure Public IP: &dollar;ip"</code></pre>
        <p><strong>Notes:</strong> Use option B when you want the IP assigned to an Azure resource; option A is useful for VM's outbound public IP detection.</p>
      </div>`},{question:"How do you handle credentials securely in PowerShell scripts?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Secure Credential Handling</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Managed Identity:</strong> Use system-assigned or user-assigned managed identities for Azure VMs/Functions to authenticate without secrets: <code>Connect-AzAccount -Identity</code>.</li>
          <li><strong>Azure Key Vault:</strong> Store secrets/certificates in Key Vault and retrieve at runtime:</li>
        </ol>
        <pre><code># Example: Get secret from Key Vault
&dollar;secret = Get-AzKeyVaultSecret -VaultName 'kvName' -Name 'sp-secret'
&dollar;plain = &dollar;secret.SecretValueText</code></pre>
        <ol start="3" style="margin-left:1.2rem;">
          <li><strong>Azure Automation credential assets:</strong> Use built-in credential store for runbooks.</li>
          <li><strong>Avoid plaintext:</strong> Never hardcode secrets in scripts or repository. Use secure pipeline variables or Key Vault references in pipelines.</li>
        </ol>
        <p><strong>In practice:</strong> I deploy runbooks that use managed identities + Key Vault access policies to fetch secrets at runtime  eliminates secret sprawl and simplifies rotation.</p>
      </div>`},{question:"How do you integrate PowerShell scripts with Azure DevOps pipelines?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Integrating PowerShell with Azure DevOps</strong></h3>
        <p><strong>Inline or file-based PowerShell tasks in YAML:</strong></p>
        <pre><code>- task: PowerShell@2
  displayName: 'Run PowerShell script'
  inputs:
    targetType: 'filePath'      # or 'inline'
    filePath: 'scripts/deploy.ps1'
    pwsh: true
    azureSubscription: 'MyServiceConnection'  # when using azurePowerShell task</code></pre>
        <p><strong>Options:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use <code>AzurePowerShell@5</code> task to run Az cmdlets with a service connection.</li>
          <li>Store credentials/secrets in pipeline library or link Key Vault variable group and reference them as variables.</li>
          <li>Prefer managed identities on self-hosted agents / VMSS to avoid storing service principal secrets."""
        </ul>
        <p><strong>Example  Key Vault integration via pipeline variable:</strong></p>
        <pre><code>variables:
  - group: 'kv-dev-secrets'   # variable group linked to Key Vault

steps:
- task: PowerShell@2
  inputs:
    targetType: 'filePath'
    filePath: 'scripts/run.ps1'
    pwsh: true
  env:
    CLIENT_SECRET: &dollar;(my-secret-from-kv)</code></pre>
        <p><strong>In practice:</strong> I keep PowerShell scripts in repo, use service connections for Az auth, and pull sensitive values from Key Vault at runtime  ensures secure, auditable automation.</p>
      </div>`}]},{title:"8. Logs & Troubleshooting",questions:[{question:"How do you analyze system logs (/var/log/messages, /var/log/syslog, /var/log/secure)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Analyzing System Logs</strong></h3>
        <p><strong>Common log files:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><code>/var/log/messages</code>  General system and service logs (RHEL/CentOS).</li>
          <li><code>/var/log/syslog</code>  Equivalent system log for Ubuntu/Debian.</li>
          <li><code>/var/log/secure</code>  Authentication and SSH login activity.</li>
        </ul>
        <p><strong>Commands I use:</strong></p>
        <pre><code># View latest logs
sudo tail -n 50 /var/log/messages

# Filter specific service or keyword
sudo grep "sshd" /var/log/secure

# View logs in real-time
sudo tail -f /var/log/syslog</code></pre>
        <p><strong>In practice:</strong> I check these logs first whenever theres service failure, login issue, or system crash  helps trace authentication errors or startup issues quickly.</p>
      </div>`},{question:"How do you view and filter logs using grep, tail, awk, or less?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Viewing & Filtering Logs Efficiently</strong></h3>
        <p><strong>Common techniques:</strong></p>
        <pre><code># Show last 100 lines
tail -n 100 /var/log/messages

# Follow logs in real-time
tail -f /var/log/nginx/access.log

# Filter keyword
grep "ERROR" /var/log/app.log

# Filter multiple keywords
grep -E "ERROR|FAIL" /var/log/app.log

# Extract specific columns (awk)
awk '{print &dollar;1, &dollar;5, &dollar;9}' /var/log/httpd/access.log

# Paginated view
less /var/log/secure</code></pre>
        <p><strong>In practice:</strong> I often combine <code>grep</code> and <code>tail -f</code> to live-monitor logs for specific apps during deployments or debugging sessions.</p>
      </div>`},{question:"How do you check dmesg logs for kernel or hardware issues?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Checking Kernel & Hardware Logs with dmesg</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># View recent kernel messages
dmesg | tail -20

# Filter for disk, memory or network errors
dmesg | grep -iE "error|fail|disk|eth"

# Save complete output for analysis
dmesg &gt; /tmp/dmesg.log</code></pre>
        <p><strong>Use case:</strong> Identify hardware issues such as I/O errors, NIC failures, or kernel panics.</p>
        <p><strong>In practice:</strong> I check <code>dmesg</code> right after reboot or when VMs crash  it reveals kernel driver issues or disk mount failures not logged in syslog.</p>
      </div>`},{question:"How do you troubleshoot SSH connection issues?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Troubleshooting SSH Issues</strong></h3>
        <p><strong>Step-by-step approach:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Check connectivity:</strong> <code>ping &lt;server&gt;</code> or <code>nc -zv &lt;IP&gt; 22</code>.</li>
          <li><strong>Check SSH service:</strong> <code>systemctl status sshd</code> and restart if needed.</li>
          <li><strong>Inspect SSH logs:</strong></li>
          <pre><code>sudo tail -f /var/log/secure   # RHEL
sudo tail -f /var/log/auth.log # Ubuntu</code></pre>
          <li><strong>Verify permissions:</strong> Ensure <code>~/.ssh</code> is 700 and <code>authorized_keys</code> is 600.</li>
          <li><strong>Verbose mode:</strong> <code>ssh -vvv user@host</code> to debug authentication or key exchange issues.</li>
        </ol>
        <p><strong>In practice:</strong> 90% of SSH failures are due to key permission issues or firewall blocking port 22. I use verbose output + logs to pinpoint exactly where handshake fails.</p>
      </div>`},{question:"How do you identify failed login attempts or suspicious activity?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Detecting Failed Logins & Intrusion Attempts</strong></h3>
        <p><strong>Commands:</strong></p>
        <pre><code># Check failed logins (RHEL)
grep "Failed password" /var/log/secure | tail -10

# For Ubuntu/Debian
grep "Failed password" /var/log/auth.log

# Check successful logins
grep "Accepted password" /var/log/secure

# Count failed attempts per IP
grep "Failed password" /var/log/secure | awk '{print &dollar;(NF-3)}' | sort | uniq -c | sort -nr</code></pre>
        <p><strong>In practice:</strong> I use this analysis to detect brute-force attempts, then block offending IPs using <code>fail2ban</code> or firewalld rules.</p>
      </div>`},{question:"How do you create a log rotation policy for large log files?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Implementing Log Rotation</strong></h3>
        <p><strong>Logrotate configuration:</strong></p>
        <pre><code>/var/log/app.log {
    daily
    rotate 7
    compress
    delaycompress
    missingok
    notifempty
    create 0640 appuser adm
    postrotate
        systemctl reload app.service &gt; /dev/null 2&gt;&1 || true
    endscript
}</code></pre>
        <p><strong>Steps:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Create a file in <code>/etc/logrotate.d/app</code>.</li>
          <li>Customize rotation frequency (daily, weekly, size-based).</li>
          <li>Use <code>logrotate -f</code> to test configuration.</li>
        </ol>
        <p><strong>In practice:</strong> I configure log rotation for application and nginx logs to prevent disk filling  combine it with monitoring alerts when logs exceed threshold size.</p>
      </div>`}]},{title:"9. Python & Cross-Platform Automation (Optional)",questions:[{question:"Write a Python script to back up all files older than 30 days.",answerHtml:`<div class="answer-rich">
        <h3> <strong>Python Backup Script (Files older than 30 days)</strong></h3>
        <p>Below is a production-ready script that:</p>
        <ul style="margin-left:1.2rem;">
          <li>Finds files older than X days.</li>
          <li>Creates a timestamped compressed tarball preserving paths.</li>
          <li>Rotates old backups and logs operations.</li>
        </ul>
        <pre><code>#!/usr/bin/env python3
"""
backup_old_files.py
Backs up files older than N days from a source dir into a compressed tar.gz archive.
Usage: python3 backup_old_files.py --source /var/log --dest /backups --days 30
"""
import argparse
import logging
import os
import tarfile
import time
from pathlib import Path

# --- Basic logging setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[
        logging.FileHandler("/var/log/backup_old_files.log"),
        logging.StreamHandler()
    ]
)

def find_old_files(src: Path, days: int):
    cutoff = time.time() - (days * 86400)
    for root, dirs, files in os.walk(src):
        for f in files:
            p = Path(root) / f
            try:
                if p.stat().st_mtime < cutoff:
                    yield p
            except (FileNotFoundError, PermissionError):
                logging.warning("Cannot access %s", p)

def create_backup_archive(files, src: Path, dest: Path, days: int):
    dest.mkdir(parents=True, exist_ok=True)
    ts = time.strftime("%Y%m%d-%H%M%S")
    archive_name = dest / f"backup-{src.name}-{days}d-{ts}.tar.gz"
    logging.info("Creating archive: %s", archive_name)
    with tarfile.open(archive_name, "w:gz") as tar:
        for f in files:
            try:
                # store file with path relative to src root
                arcname = f.relative_to(src.parent)
                tar.add(f, arcname=arcname)
                logging.debug("Added %s as %s", f, arcname)
            except Exception as e:
                logging.exception("Failed to add %s to archive: %s", f, e)
    logging.info("Archive created: %s", archive_name)
    return archive_name

def rotate_backups(dest: Path, keep: int = 7):
    archives = sorted(dest.glob("backup-*.tar.gz"), key=os.path.getmtime, reverse=True)
    for old in archives[keep:]:
        try:
            logging.info("Removing old backup: %s", old)
            old.unlink()
        except Exception:
            logging.exception("Failed to remove backup: %s", old)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--source", required=True, type=Path, help="Source directory to scan")
    parser.add_argument("--dest", required=True, type=Path, help="Destination directory for backups")
    parser.add_argument("--days", type=int, default=30, help="Files older than DAYS will be backed up")
    parser.add_argument("--keep", type=int, default=7, help="Number of backups to keep")
    args = parser.parse_args()

    logging.info("Backup started: source=%s dest=%s days=%d", args.source, args.dest, args.days)
    files = list(find_old_files(args.source, args.days))
    if not files:
        logging.info("No files older than %d days found in %s", args.days, args.source)
        return

    archive = create_backup_archive(files, args.source, args.dest, args.days)
    rotate_backups(args.dest, keep=args.keep)
    logging.info("Backup finished successfully: %s", archive)

if __name__ == "__main__":
    main()</code></pre>
        <p><strong>Crontab example (run daily at 2 AM):</strong></p>
        <pre><code># Run backup script daily
0 2 * * * /usr/bin/python3 /opt/scripts/backup_old_files.py --source /var/log --dest /mnt/backups --days 30 --keep 14</code></pre>
        <p><strong>Notes / best practices:</strong> Run as a dedicated service account with minimal privileges, store backups on separate disk or network share, and test restore procedure regularly.</p>
      </div>`},{question:"How would you handle logging, retries, and email alerts in Python automation?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Logging, Retries & Alerts Pattern</strong></h3>
        <p><strong>Logging:</strong> Use the <code>logging</code> module with file + stdout handlers and rotate logs with <code>logging.handlers.RotatingFileHandler</code>. Include correlation IDs and timestamps.</p>
        <p><strong>Retries:</strong> Use a retry library like <code>tenacity</code> or implement exponential backoff. Example (tenacity):</p>
        <pre><code>from tenacity import retry, wait_exponential, stop_after_attempt

@retry(wait=wait_exponential(multiplier=1, min=2, max=60), stop=stop_after_attempt(5))
def flaky_call():
    # call external API or network service
    pass</code></pre>
        <p><strong>Email / Slack Alerts:</strong> - For email: use <code>smtplib</code> or integrate with SMTP relay (no plaintext creds in code  fetch from Key Vault).<br>
        - For Slack/Teams: post to an incoming webhook using <code>requests.post()</code>.</p>
        <pre><code># Simple Slack alert
import requests
def alert_slack(webhook, message):
    requests.post(webhook, json={"text": message}, timeout=10)</code></pre>
        <p><strong>Production pattern:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>All actions logged (INFO/ERROR); attach stack traces on exceptions.</li>
          <li>Critical failures trigger immediate alert (Slack/email) and push to monitoring (Prometheus or Application Insights).</li>
          <li>Retries for transient errors, with a final escalation if still failing.</li>
        </ul>
      </div>`},{question:"How can Python integrate with Azure CLI or Terraform for automation?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Integrating Python with Azure CLI & Terraform</strong></h3>
        <p><strong>Azure CLI:</strong> Use <code>subprocess</code> to call <code>az</code> or use Azure SDK for Python (<code>azure-*</code> packages) for richer control. Example (service principal auth + list VMs):</p>
        <pre><code>import subprocess, json
cmd = ["az", "login", "--service-principal", "-u", APP_ID, "-p", SECRET, "--tenant", TENANT]
subprocess.run(cmd, check=True)
res = subprocess.run(["az", "vm", "list", "-o", "json"], check=True, capture_output=True)
vms = json.loads(res.stdout)</code></pre>
        <p><strong>Terraform:</strong> Two approaches:</p>
        <ol style="margin-left:1.2rem;">
          <li>Invoke CLI via <code>subprocess</code> (<code>terraform init/plan/apply</code>), capture output and parse JSON plan (<code>terraform show -json</code>).</li>
          <li>Use Python libs like <code>python-terraform</code> for programmatic control (less common in pipelines).</li>
        </ol>
        <pre><code># Example: run terraform plan and read JSON
subprocess.run(["terraform", "init"], check=True)
subprocess.run(["terraform", "plan", "-out=tfplan"], check=True)
subprocess.run(["terraform", "show", "-json", "tfplan"], check=True, capture_output=True)</code></pre>
        <p><strong>Security & best practices:</strong> Use managed identities or service principals stored securely (Key Vault / pipeline secrets), avoid embedding credentials in scripts, and prefer SDKs where possible to avoid shell parsing issues.</p>
      </div>`},{question:"How would you schedule and monitor a Python automation job in Linux?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Scheduling & Monitoring Python Jobs</strong></h3>
        <p><strong>Scheduling options:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Cron:</strong> Simple and reliable for periodic scripts. (Use virtualenv wrapper and full paths.)</li>
          <li><strong>systemd timer:</strong> Better observability and dependency control; logs available in <code>journalctl</code>.</li>
          <li><strong>Airflow / Prefect:</strong> For complex DAGs and retries with UI-based monitoring.</li>
        </ul>
        <p><strong>Example  systemd service + timer:</strong></p>
        <pre><code># /etc/systemd/system/pyjob.service
[Unit]
Description=My Python Automation Job

[Service]
Type=oneshot
User=automation
WorkingDirectory=/opt/scripts
ExecStart=/usr/bin/python3 /opt/scripts/backup_old_files.py --source /var/log --dest /mnt/backups --days 30

# /etc/systemd/system/pyjob.timer
[Unit]
Description=Run Python backup daily

[Timer]
OnCalendar=daily
Persistent=true

[Install]
WantedBy=timers.target

# Enable timer
sudo systemctl daemon-reload
sudo systemctl enable --now pyjob.timer</code></pre>
        <p><strong>Monitoring:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Log to a file and ship logs to a central system (ELK / Log Analytics / Graylog).</li>
          <li>Expose health and metrics (Prometheus client) if long-running, or push metrics at job end.</li>
          <li>Use heartbeat services (healthchecks.io) to detect missed runs and send alerts.</li>
          <li>Use systemd/cron exit codes and notify on non-zero exit via webhook or pager.</li>
        </ul>
        <p><strong>In practice:</strong> I combine systemd timers for scheduling, AppInsights/Prometheus for metrics, and Slack/webhook alerts for failures  ensuring fast detection and automated retries where safe.</p>
      </div>`}]},{title:"10. Real-World Scenarios",questions:[{question:"Have you worked on Linux patching and user management?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Linux Patching & User Management  Real Experience</strong></h3>
        <p><strong>Yes  end-to-end:</strong> Ive automated patch windows, applied security updates, validated services, and managed user lifecycle in production environments.</p>
        <p><strong>Typical patch flow I use:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Announce maintenance window & take backups / snapshots.</li>
          <li>Run pre-checks: free disk, service status, replication health.</li>
          <li>Apply patches in batches (canary  remaining) using Ansible or Azure Automation.</li>
          <li>Reboot if required and run post-checks (service status, health endpoints).</li>
          <li>Rollback plan ready (snapshot restore or re-run previous config) and postmortem after window.</li>
        </ol>
        <p><strong>User management examples:</strong></p>
        <pre><code># Add user and set sudo
sudo useradd -m -s /bin/bash devuser
echo "devuser ALL=(ALL) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/devuser

# Lock user
sudo usermod -L olduser

# Remove and home
sudo userdel -r tempuser</code></pre>
        <p><strong>In practice:</strong> All user changes performed via Ansible to ensure idempotence, audit trail, and avoid ad-hoc manual changes that cause drift.</p>
      </div>`},{question:"How do you monitor performance metrics of 50+ servers simultaneously?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Scale Monitoring for 50+ Servers</strong></h3>
        <p><strong>Approach:</strong> Use a centralized telemetry stack (Prometheus + Grafana or Azure Monitor + Log Analytics) and lightweight collectors on each host.</p>
        <p><strong>Options Ive implemented:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Prometheus + node_exporter:</strong> Deploy node_exporter on each server; register targets in Prometheus (service discovery via Consul/SD or static job lists). Visualize in Grafana and set alerts in Alertmanager.</li>
          <li><strong>Azure Monitor (VM Insights / AMA):</strong> Install Azure Monitor Agent (AMA) to forward metrics & logs to Log Analytics workspace; use Workbooks and Alerts for notification.</li>
        </ul>
        <p><strong>Example Prometheus job:</strong></p>
        <pre><code>scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['10.0.1.10:9100','10.0.1.11:9100', '10.0.1.12:9100']</code></pre>
        <p><strong>Alerting pattern:</strong> Track P95 latency, CPU > 80% for 5m, disk usage > 85% and send alerts to PagerDuty/Teams/Slack. For 50+ servers, group alerts by service and severity to reduce noise.</p>
      </div>`},{question:"How do you extend volumes in Oracle Linux or RHEL production VMs?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Extending Volumes on Oracle Linux / RHEL (LVM & Non-LVM)</strong></h3>
        <p><strong>Common scenarios:</strong> (A) LVM-backed filesystem, (B) non-LVM (resize partition + filesystem), (C) cloud-managed disk resize (Azure/AWS).</p>
        <p><strong>Example  LVM (online expand ext4):</strong></p>
        <pre><code># 1. Add physical disk or extend underlying cloud disk
# 2. Create PV (if new disk)
sudo pvcreate /dev/sdb
sudo vgextend vg_data /dev/sdb

# 3. Extend LV
sudo lvextend -L +50G /dev/vg_data/lv_app

# 4. Grow filesystem (ext4)
sudo resize2fs /dev/vg_data/lv_app

# For XFS:
sudo xfs_growfs /mountpoint</code></pre>
        <p><strong>Example  Azure disk resize (VMSS/VM):</strong></p>
        <pre><code># In Azure portal or az cli increase disk size, then on VM:
sudo apt-get install cloud-guest-utils  # if needed
sudo growpart /dev/sda 1
sudo xfs_growfs /</code></pre>
        <p><strong>Verification:</strong> <code>lsblk</code>, <code>df -h</code>, and <code>lvs/vgs</code> to confirm space is available. I always test in staging and take snapshot before resizing in prod.</p>
      </div>`},{question:"How do you secure SSH for production servers (key-based, MFA)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>SSH Hardening for Production</strong></h3>
        <p><strong>Key measures I implement:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Key-based auth only:</strong> Disable <code>PasswordAuthentication</code> and ensure <code>PermitRootLogin no</code> in <code>/etc/ssh/sshd_config</code>.</li>
          <li><strong>Use dedicated jump/bastion:</strong> Hardened jump host with MFA and restricted ingress.</li>
          <li><strong>MFA:</strong> Integrate MFA via Azure AD SSH (Azure AD login for Linux) or via PAM modules (Google Authenticator / Duo) on bastions.</li>
          <li><strong>Restrict access:</strong> Use <code>AllowUsers</code> or firewall rules to limit source IPs.</li>
          <li><strong>Fail2Ban & monitoring:</strong> Block brute force and notify security team for repeated attempts.</li>
          <li><strong>Key lifecycle:</strong> Enforce key rotation and maintain keys in a central vault or certificate-based SSH (short-lived certs via Vault).</li>
        </ol>
        <p><strong>Example sshd changes:</strong></p>
        <pre><code>PermitRootLogin no
PasswordAuthentication no
ChallengeResponseAuthentication no
AllowUsers jump-admin devuser</code></pre>
        <p><strong>In practice:</strong> For production I route all admin access via a bastion that requires MFA; servers accept only certs or keys minted by a central CA (HashiCorp Vault/SSHD CA) to avoid long-lived key sprawl.</p>
      </div>`},{question:"How do you integrate system health scripts into Azure Monitor or Prometheus?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Integrating Health Scripts with Monitoring</strong></h3>
        <p><strong>Prometheus:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Expose custom metrics via an HTTP endpoint (Prometheus exposition format) and scrape them from Prometheus.</li>
          <li>Use a small exporter (Python/Go) or push metrics to Pushgateway for short-lived jobs.</li>
        </ul>
        <pre><code># Minimal /metrics endpoint (Flask example)
from prometheus_client import Gauge, start_http_server
g = Gauge('app_disk_free_bytes', 'Free disk bytes')
g.set(123456)
start_http_server(8000)</code></pre>
        <p><strong>Azure Monitor:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use Azure Monitor Agent (AMA) with custom logs or the Azure Monitor Metrics API to push custom metrics.</li>
          <li>Alternatively, send telemetry to Log Analytics via OMS/REST API from script and create alerts/workbooks.</li>
        </ul>
        <pre><code># Send custom log example (curl to Log Analytics Data Collector API)
# Build JSON payload and post with workspace key (best done via Az CLI or SDK)</code></pre>
        <p><strong>In practice:</strong> I deploy small exporters for disk, queue depth, or custom app checks and wire them into Prometheus + Alertmanager or send the same checks as custom logs to Log Analytics for Azure-native dashboards and alerts.</p>
      </div>`},{question:"How do you automate patching and reboot validation across multiple servers?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Automated Patching + Reboot Validation</strong></h3>
        <p><strong>Solution pattern I use:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Orchestration:</strong> Use Ansible playbooks or Azure Automation Hybrid Runbooks to orchestrate patching in controlled batches.</li>
          <li><strong>Pre-checks:</strong> Ensure cluster quorum, replication health, snapshot backups, and low load before patching.</li>
          <li><strong>Apply updates:</strong> Use package manager modules (<code>yum</code>/<code>apt</code>) via Ansible; reboot when package manager requires.</li>
          <li><strong>Post-reboot validation:</strong> Use health-check scripts that verify service status, application health endpoints, logins, and monitoring ingestion.</li>
          <li><strong>Rollback & reporting:</strong> If validation fails, trigger rollback or escalate; generate patch report and track in CMDB.</li>
        </ol>
        <p><strong>Ansible snippet (simplified):</strong></p>
        <pre><code>- hosts: canary
  become: yes
  tasks:
    - name: Apply updates
      yum:
        name: "*"
        state: latest
    - name: Reboot if required
      reboot:
        reboot_timeout: 600
    - name: Run health check
      uri:
        url: http://localhost/health
        return_content: no
      register: hc
    - name: Fail if health check failed
      fail:
        msg: "Health check failed"
      when: hc.status != 200</code></pre>
        <p><strong>In practice:</strong> I run patching in staged waves (canary  remaining), and post-reboot scripts report success to a central dashboard and Slack channel. If a node fails validation, its cordoned (for K8s) or removed from LB until fixed.</p>
      </div>`},{question:"How do you design a Linux automation framework combining Bash + Ansible + Azure CLI?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Design  Bash + Ansible + Azure CLI Automation Framework</strong></h3>
        <p><strong>Principles:</strong> Idempotence, modularity, secure secrets handling, and CI/CD integration.</p>
        <p><strong>High-level structure:</strong></p>
        <pre><code>repo/
 scripts/                # small bash helpers & wrappers (idempotent)
   check_disk.sh
 ansible/
   inventories/
   roles/
   playbooks/
 infra/
   azure/               # terraform/ARM for infra
 pipelines/
   azure-pipelines.yml
 README.md</code></pre>
        <p><strong>How components work together:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Provision infra:</strong> Terraform or ARM in <code>infra/azure</code> creates VMs, NSG, and managed identities.</li>
          <li><strong>Bootstrap VM:</strong> Use cloud-init or custom script (Bash) to install Python, Ansible, and AMA/node_exporter.</li>
          <li><strong>Configure via Ansible:</strong> Ansible runs playbooks using inventories created dynamically (host variables include managed identity or service principal info).</li>
          <li><strong>Azure CLI usage:</strong> Pipeline tasks use <code>az</code> for subscription context, resource creation, or pulling secrets/keys (via Key Vault).</li>
          <li><strong>CI/CD:</strong> Azure DevOps pipeline triggers Ansible runs on self-hosted runner or via Control-M; secrets injected from Key Vault variable groups.</li>
        </ol>
        <p><strong>Example pipeline steps:</strong></p>
        <pre><code>- script: |
    az login --service-principal -u &dollar;(SP_ID) -p &dollar;(SP_SECRET) --tenant &dollar;(TENANT)
    az account set --subscription &dollar;(SUB_ID)
  displayName: 'Azure Login'

- script: |
    ansible-playbook -i inventories/prod playbooks/site.yml --extra-vars "env=prod"
  displayName: 'Run Ansible Playbook'</code></pre>
        <p><strong>Best practices:</strong> - Keep bash scripts minimal (helpers). - Use Ansible for configuration & orchestration. - Use managed identities or Key Vault for secrets. - Test everything in staging and use canary deployments for production changes.</p>
      </div>`}]}];function Xw(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("linux"),a=vm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-green-500 to-emerald-500 shadow-glow",children:l.jsx(da,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Linux"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Linux Fundamentals, Shell Scripting & System Administration"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Master Linux commands, shell scripting, system management, and automation practices."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:vm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:(u==null?void 0:u.question)||"",b=typeof u=="object"&&u&&"answer"in u?u.answer:null,v=typeof u=="object"&&"answerHtml"in u?u.answerHtml:null;return l.jsx("div",{className:"border-l-2 border-green-500/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-green-500 font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),b||v?l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:v.replace(/\$\{/g,"$${")}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:b})})]}):null]})]})},f)})})})]},p))})]})]})}const nD=Object.freeze(Object.defineProperty({__proto__:null,default:Xw},Symbol.toStringTag,{value:"Module"})),ym=[{title:"1. Cloud Fundamentals",questions:[{question:"What is Cloud Computing in simple terms?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Understanding Cloud Computing</strong></h3>
        <p>Cloud computing means accessing IT resources like servers, storage, databases, or apps over the internet instead of running them on your local machine or data center.</p>
        <p><strong>Simple example:</strong> Instead of buying and maintaining your own server, you rent compute and storage from Azure or AWS and pay only for usage.</p>
        <p><strong>Key idea:</strong> It converts CapEx (hardware) into OpEx (pay-as-you-go service).</p>
      </div>`},{question:"What are the different cloud service models  IaaS, PaaS, SaaS?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Cloud Service Models</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>IaaS (Infrastructure as a Service):</strong> Provides virtualized infrastructure  VMs, networking, storage. Example: Azure VM, Azure Virtual Network.</li>
          <li><strong>PaaS (Platform as a Service):</strong> Provides platform for app development & deployment. Example: Azure App Service, Azure SQL DB, AKS.</li>
          <li><strong>SaaS (Software as a Service):</strong> End-user ready applications hosted by provider. Example: Office 365, Salesforce.</li>
        </ol>
        <p><strong>In practice:</strong> For DevOps work, I mostly deal with IaaS (VMs, VNets) and PaaS (App Services, AKS).</p>
      </div>`},{question:"What is the difference between Public Cloud, Private Cloud, and Hybrid Cloud?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Cloud Deployment Models</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Public Cloud:</strong> Shared infrastructure, managed by providers like Azure or AWS. Pay-as-you-go, best for scalability.</li>
          <li><strong>Private Cloud:</strong> Dedicated environment for one organization (on-prem or hosted). Offers more control and compliance.</li>
          <li><strong>Hybrid Cloud:</strong> Mix of public + private. Commonly used for DR or sensitive workloads.</li>
        </ol>
        <p><strong>Example:</strong> Ive used Hybrid Cloud setups where on-prem AD syncs with Azure AD via Azure AD Connect.</p>
      </div>`},{question:"What are the advantages of cloud computing?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Key Benefits of Cloud</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Scalability:</strong> Scale resources up/down easily.</li>
          <li><strong>High Availability:</strong> Redundancy across zones/regions.</li>
          <li><strong>Cost Efficiency:</strong> Pay only for what you use.</li>
          <li><strong>Security & Compliance:</strong> Built-in identity, encryption, RBAC.</li>
          <li><strong>Automation:</strong> Infrastructure as Code (Terraform, ARM, Bicep).</li>
        </ul>
        <p><strong>In real use:</strong> I often scale AKS clusters and VMs dynamically using auto-scaling to optimize cost.</p>
      </div>`},{question:"What is the difference between compute, network, and storage services in cloud?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Core Cloud Components</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Compute:</strong> Runs your workloads  VMs, App Services, Containers, Functions.</li>
          <li><strong>Network:</strong> Connects resources  VNets, Subnets, Load Balancers, Firewalls.</li>
          <li><strong>Storage:</strong> Persists data  Blob, Disk, File, Queue, Table.</li>
        </ol>
        <p><strong>Example:</strong> In a web app, I use Azure VM (compute), VNet + NSG (network), and Azure Disk or Blob Storage for persistence.</p>
      </div>`},{question:"What are all the Azure compute and network services youve worked on?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Hands-on Azure Services</strong></h3>
        <p><strong>Compute:</strong> Azure Virtual Machines, App Services, Azure Kubernetes Service (AKS), Function Apps, VM Scale Sets.</p>
        <p><strong>Networking:</strong> Virtual Networks, Subnets, NSGs, Application Gateway, Load Balancer, Private Endpoints, NAT Gateway, VPN Gateway, ExpressRoute.</p>
        <p><strong>Real use:</strong> In a production setup, I used AKS with Azure Application Gateway Ingress Controller (AGIC) and NSGs for fine-grained traffic control.</p>
      </div>`},{question:"What is a Function App, and when is it used?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Azure Function App (Serverless Compute)</strong></h3>
        <p>Function App is a serverless service that runs code in response to events (HTTP requests, timers, queue messages, etc.) without managing infrastructure.</p>
        <p><strong>When to use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>For lightweight automation or event-driven workflows.</li>
          <li>To replace cron jobs or backend triggers.</li>
          <li>To integrate alerts, storage, or API responses quickly.</li>
        </ul>
        <p><strong>Example:</strong> I used Function App to trigger a cleanup task daily based on Storage queue events in a monitoring pipeline.</p>
      </div>`},{question:"What is the purpose of a NAT Gateway or Internet Gateway?",answerHtml:`<div class="answer-rich">
        <h3> <strong>NAT Gateway / Internet Gateway</strong></h3>
        <p><strong>NAT Gateway:</strong> Allows private subnet resources (like VMs in private VNets) to access the internet securely for outbound traffic, while keeping them inaccessible from outside.</p>
        <p><strong>Internet Gateway (AWS term):</strong> Enables resources in public subnets to communicate with the internet directly (used in public subnets).</p>
        <p><strong>In Azure:</strong> NAT Gateway = Outbound internet for private subnets, Public IP or App Gateway = inbound access.</p>
        <p><strong>Practical use:</strong> In my setup, AKS nodes in private subnet used NAT Gateway for image pulls from ACR securely.</p>
      </div>`},{question:"What are the redundancy options in Azure for high availability?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Azure Redundancy & HA Options</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Availability Sets:</strong> Protect VMs from host-level failures.</li>
          <li><strong>Availability Zones:</strong> Isolate resources across physical datacenters in same region.</li>
          <li><strong>Geo-Redundant Storage (GRS):</strong> Replicates data to another region.</li>
          <li><strong>Load Balancers & Traffic Manager:</strong> Distribute workloads globally.</li>
        </ul>
        <p><strong>Real-world:</strong> I deployed production VMs in Availability Zones + used Azure Standard Load Balancer for failover.</p>
      </div>`},{question:"How do you ensure high availability and fault tolerance in the cloud?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Ensuring HA & Fault Tolerance</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Deploy across <strong>Availability Zones / Regions</strong>.</li>
          <li>Use <strong>Load Balancers</strong> for traffic distribution.</li>
          <li>Keep <strong>stateless apps</strong> where possible  use shared backend.</li>
          <li>Implement <strong>auto-scaling</strong> for compute and AKS nodes.</li>
          <li>Enable <strong>backup & replication</strong> (Azure Backup, GRS storage).</li>
        </ol>
        <p><strong>Example:</strong> My AKS setup used multi-zone node pools with Application Gateway and Azure Monitor alerts to auto-scale and maintain 99.9% uptime.</p>
      </div>`}]},{title:"2. CI/CD Concepts",questions:[{question:"What is Continuous Integration vs Continuous Deployment?",answerHtml:`<div class="answer-rich">
        <h3> <strong>CI vs CD  Core DevOps Flow</strong></h3>
        <p><strong>Continuous Integration (CI):</strong> Developers frequently merge code changes into a shared repo, and automated builds + tests run to validate the code.</p>
        <p><strong>Continuous Deployment (CD):</strong> Extends CI by automatically deploying tested builds to environments (Dev, QA, Prod).</p>
        <p><strong>Example:</strong> In Azure DevOps, my CI pipeline builds code and publishes an artifact, while the CD pipeline picks that artifact and deploys it to AKS or VM via release stages.</p>
        <p><strong>Goal:</strong> Faster releases, consistent quality, and reduced human error.</p>
      </div>`},{question:"What are Environment Variables in CI/CD pipelines?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Environment Variables in Pipelines</strong></h3>
        <p>Environment variables are dynamic values used across pipeline stages or jobs to control behavior without hardcoding values.</p>
        <p><strong>Types:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Pipeline-level variables (global)</li>
          <li>Stage/job-level variables (scoped)</li>
          <li>Secret variables (stored securely in Library or Key Vault)</li>
        </ul>
        <p><strong>Example (YAML):</strong></p>
        <pre><code>variables:
  environment: 'dev'
  app_name: 'my-api'
steps:
  - script: echo "Deploying to &dollar;(environment)"</code></pre>
        <p><strong>In practice:</strong> I maintain variable groups per environment and link them to pipeline stages to keep configurations centralized and secure.</p>
      </div>`},{question:"What is a Rollback and why is it important?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Rollback  Safety Mechanism</strong></h3>
        <p><strong>Rollback</strong> means reverting the system to a previous stable version when a new deployment fails or causes issues.</p>
        <p><strong>Why important:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Ensures uptime and service reliability.</li>
          <li>Minimizes impact of faulty releases.</li>
          <li>Critical for production-grade CI/CD pipelines.</li>
        </ul>
        <p><strong>Example:</strong> I use versioned artifacts in Azure DevOps  if a deployment fails, CD pipeline redeploys the last successful build automatically.</p>
      </div>`},{question:"What are Artifacts and what is an Artifact Repository?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Artifacts in CI/CD</strong></h3>
        <p><strong>Artifact:</strong> A versioned output (build result) that gets passed between pipeline stages  e.g., a .zip, .jar, Docker image, or Helm chart.</p>
        <p><strong>Artifact Repository:</strong> A centralized storage for artifacts  ensures versioning, traceability, and reusability.</p>
        <p><strong>Examples:</strong> Azure Artifacts, Nexus, JFrog Artifactory, or Azure Container Registry (for Docker images).</p>
        <p><strong>In practice:</strong> I use Azure DevOps to publish build outputs and push Docker images to ACR, which are then deployed by CD pipelines.</p>
      </div>`},{question:"Why do we use multiple stages in pipelines?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Multi-stage Pipelines  Structured Delivery</strong></h3>
        <p>Stages break down a pipeline into logical parts like Build  Test  Deploy  Validate, providing better control and visibility.</p>
        <p><strong>Benefits:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Isolation between environments (Dev/QA/Prod).</li>
          <li>Manual approvals between critical transitions.</li>
          <li>Parallel execution for faster delivery.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>stages:
  - stage: Build
  - stage: Deploy_Dev
  - stage: Deploy_Prod</code></pre>
        <p><strong>In real projects:</strong> My pipelines promote builds through Dev  QA  Prod, each with dedicated variable groups and approvals.</p>
      </div>`},{question:"How do you promote changes from lower to production environment safely?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Safe Promotion Strategy</strong></h3>
        <p><strong>Goal:</strong> Move tested builds from Dev to Prod without re-building or re-packaging.</p>
        <p><strong>Strategy I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Use <strong>artifacts</strong> from CI  never rebuild for prod.</li>
          <li>Deploy sequentially: Dev  QA  UAT  Prod.</li>
          <li>Enable <strong>manual approvals</strong> for critical environments.</li>
          <li>Use <strong>variable groups</strong> or Key Vault for environment-specific configs.</li>
          <li>Run <strong>smoke tests</strong> and health checks before marking deployment successful.</li>
        </ol>
        <p><strong>Example:</strong> In my Azure DevOps pipeline, only approved builds from UAT stage are promoted to Prod using same artifact version to ensure consistency.</p>
      </div>`},{question:"How do you handle secrets and configurations across pipeline environments?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Secure Secrets & Config Management</strong></h3>
        <p><strong>Best practices I follow:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Key Vault</strong> for secrets like passwords, tokens, and keys.</li>
          <li>Integrate Key Vault with variable groups in Azure DevOps pipelines.</li>
          <li>Mark secrets as hidden or locked so they dont appear in logs.</li>
          <li>Use environment variables to inject secrets dynamically at runtime.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>env:
  DB_PASS: &dollar;(dbPassword)  # fetched from Key Vault
steps:
  - script: echo "Deploying using secret safely"</code></pre>
        <p><strong>In practice:</strong> I always segregate secrets by environment and use RBAC to restrict access in prod pipelines.</p>
      </div>`},{question:"What is the difference between Self-hosted and Microsoft-hosted agents?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Agent Types in Azure Pipelines</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Aspect</th><th>Microsoft-hosted</th><th>Self-hosted</th></tr>
          <tr><td>Hosted by</td><td>Azure DevOps (ephemeral)</td><td>Your own server/VM</td></tr>
          <tr><td>Custom Tools</td><td>Limited (preinstalled set)</td><td>Full control over tools & versions</td></tr>
          <tr><td>Speed</td><td>Slower for large builds</td><td>Faster (persistent caching)</td></tr>
          <tr><td>Security</td><td>Runs in shared pool</td><td>Private network, isolated execution</td></tr>
          <tr><td>Cost</td><td>Free limited minutes</td><td>Runs on your infra (no minute limit)</td></tr>
        </table>
        <p><strong>In real projects:</strong> I use self-hosted agents for private VNet deployments and Microsoft-hosted for public code builds.</p>
      </div>`},{question:"Why do you prefer Self-hosted agents over Microsoft-hosted?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Why Self-hosted Agents</strong></h3>
        <p><strong>Reasons I prefer self-hosted agents:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Direct access to private VNets and internal services.</li>
          <li>Faster builds  agents reuse cached dependencies.</li>
          <li>Full control over installed tools, versions, and runtime.</li>
          <li>No dependency on Microsoft-hosted capacity or build queue delays.</li>
          <li>Can manage custom runners for secure environments.</li>
        </ul>
        <p><strong>Example:</strong> For my healthcare client, I deployed self-hosted Linux agents in a secured subnet to build Docker images and deploy directly to AKS without exposing any endpoint publicly.</p>
      </div>`}]},{title:"3. Deployment Strategies & Testing",questions:[{question:"What is Blue-Green Deployment strategy?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Blue-Green Deployment  Zero-Downtime Release</strong></h3>
        <p><strong>Blue-Green Deployment</strong> maintains two identical environments  one active (Blue) and one idle (Green). The new version is deployed to Green, tested, and then traffic is switched from Blue  Green instantly.</p>
        <p><strong>Benefits:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Zero downtime deployment.</li>
          <li>Instant rollback (just redirect traffic back).</li>
          <li>No impact on end-users during deployment.</li>
        </ul>
        <p><strong>Example:</strong> In Azure, I use two App Service slots (Production = Blue, Staging = Green). After validation, I swap slots to release instantly.</p>
        <pre><code>az webapp deployment slot swap --resource-group myrg --name myapp --slot staging --target-slot production</code></pre>
      </div>`},{question:"What is Canary Deployment?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Canary Deployment  Controlled Rollout</strong></h3>
        <p><strong>Canary Deployment</strong> releases the new version to a small percentage of users first (like 510%), monitors performance, and then gradually increases rollout if stable.</p>
        <p><strong>Benefits:</strong> Reduces risk  any issue affects only a subset of users.</p>
        <p><strong>Example:</strong> I implemented canary rollout in AKS using ingress rules  routing 10% of traffic to new pod version and gradually scaling it up via Helm values.</p>
        <pre><code>annotations:
  nginx.ingress.kubernetes.io/canary: "true"
  nginx.ingress.kubernetes.io/canary-weight: "10"</code></pre>
      </div>`},{question:"How do you ensure zero-downtime deployments?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Zero-Downtime Deployment Strategy</strong></h3>
        <p><strong>Steps I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Use <strong>load balancers</strong> or traffic managers to shift traffic gradually.</li>
          <li>Implement <strong>Blue-Green or Rolling deployments</strong> for AKS or App Services.</li>
          <li>Run <strong>health probes</strong> to ensure new version is live before switching traffic.</li>
          <li>Leverage <strong>readiness/liveness probes</strong> in Kubernetes for pod validation.</li>
          <li>Keep database migrations backward-compatible.</li>
        </ol>
        <p><strong>Example:</strong> My production AKS used RollingUpdate strategy  always kept 2 pods running while updating one, ensuring 100% uptime.</p>
      </div>`},{question:"What is Load Testing vs Stress Testing?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Load vs Stress Testing</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Aspect</th><th>Load Testing</th><th>Stress Testing</th></tr>
          <tr><td>Purpose</td><td>Check performance under expected traffic</td><td>Check system behavior under extreme load</td></tr>
          <tr><td>Goal</td><td>Measure response time & throughput</td><td>Find breaking point or failure threshold</td></tr>
          <tr><td>Outcome</td><td>Optimized resource usage</td><td>Improved stability & recovery</td></tr>
        </table>
        <p><strong>Example:</strong> I used JMeter for load testing API performance and stressed it using Locust to verify AKS auto-scaling triggers correctly under peak load.</p>
      </div>`},{question:"Name some testing tools you have used (JMeter, Postman, Selenium, etc.).",answerHtml:`<div class="answer-rich">
        <h3> <strong>Testing Tools I've Worked With</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Postman:</strong> API functional testing & environment automation.</li>
          <li><strong>JMeter:</strong> Load and stress testing for APIs & microservices.</li>
          <li><strong>Selenium:</strong> UI automation for web apps.</li>
          <li><strong>k6 / Locust:</strong> Cloud-native performance testing.</li>
          <li><strong>SonarQube:</strong> Code quality & static analysis integrated in CI pipeline.</li>
        </ul>
        <p><strong>In practice:</strong> I integrate Postman collections and JMeter scripts directly in Azure DevOps CI/CD for automated testing after deployment.</p>
      </div>`},{question:"What is SAST and DAST in security testing?",answerHtml:`<div class="answer-rich">
        <h3> <strong>SAST vs DAST</strong></h3>
        <p><strong>SAST (Static Application Security Testing):</strong> Analyzes source code or binaries without running the app  finds vulnerabilities early (e.g., SQL Injection, hardcoded secrets).</p>
        <p><strong>DAST (Dynamic Application Security Testing):</strong> Tests the running app externally  mimics real attack behavior.</p>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Aspect</th><th>SAST</th><th>DAST</th></tr>
          <tr><td>When</td><td>During development (CI phase)</td><td>After deployment (staging/prod)</td></tr>
          <tr><td>Input</td><td>Source code</td><td>Running application</td></tr>
          <tr><td>Tools</td><td>SonarQube, Checkov</td><td>OWASP ZAP, Burp Suite</td></tr>
        </table>
        <p><strong>Example:</strong> I run SonarQube (SAST) during CI for code checks and OWASP ZAP (DAST) post-deploy for endpoint vulnerabilities.</p>
      </div>`},{question:"How do you test and validate infrastructure post-deployment?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Infrastructure Validation After Deployment</strong></h3>
        <p><strong>Validation checks I perform:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Verify resources via Terraform output or Azure CLI (e.g., subnets, NSGs, IPs).</li>
          <li>Run <strong>smoke tests</strong> for connectivity (ping, curl, port checks).</li>
          <li>Validate <strong>monitoring & logging</strong> (Log Analytics, Grafana, Azure Monitor).</li>
          <li>Use <strong>automated scripts</strong> or Function Apps for post-deploy health checks.</li>
          <li>Cross-verify tags, policies, and RBAC roles applied correctly.</li>
        </ul>
        <p><strong>Example:</strong> After each deployment, I trigger a PowerShell script in pipeline to validate that all VMs are reporting to Log Analytics workspace.</p>
      </div>`},{question:"How do you troubleshoot if your web app is slow  list 5 checks youll perform.",answerHtml:`<div class="answer-rich">
        <h3> <strong>Troubleshooting a Slow Web App</strong></h3>
        <p><strong>Top 5 checks I perform:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Application Metrics:</strong> Check CPU, memory, and request latency in Azure Monitor or App Insights.</li>
          <li><strong>Database Performance:</strong> Look for slow queries or DTU utilization in SQL Insights.</li>
          <li><strong>Network Latency:</strong> Use NSG flow logs and Application Gateway metrics for slow hops.</li>
          <li><strong>Dependency Calls:</strong> Analyze API dependency map in Application Insights for slow downstream services.</li>
          <li><strong>Code & Logs:</strong> Review logs for exceptions or blocking calls; integrate with Log Analytics / Grafana.</li>
        </ol>
        <p><strong>Example:</strong> Once I reduced app latency by 40% by optimizing a backend API call identified via Application Insights dependency tracking.</p>
      </div>`}]},{title:"4. DevSecOps & Security",questions:[{question:"What is DevSecOps and why is it important?",answerHtml:`<div class="answer-rich">
        <h3> <strong>DevSecOps  Security Shift-Left</strong></h3>
        <p><strong>DevSecOps</strong> means integrating security practices into every phase of the DevOps lifecycle  from coding to deployment  instead of treating it as a separate process.</p>
        <p><strong>Why its important:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Detects vulnerabilities early (cheaper to fix).</li>
          <li>Ensures compliance & governance (e.g., HIPAA, ISO, GDPR).</li>
          <li>Protects infrastructure, code, and data from breaches.</li>
        </ul>
        <p><strong>In practice:</strong> I integrate tools like SonarQube (code scan), Trivy (image scan), and Checkov (IaC scan) directly into CI/CD pipelines to catch issues before production.</p>
      </div>`},{question:"How do you integrate security scanning in CI/CD pipelines?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Integrating Security Scans in CI/CD</strong></h3>
        <p><strong>Approach I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Code scanning (SAST):</strong> SonarQube integrated in build stage.</li>
          <li><strong>Dependency scanning:</strong> Use tools like Snyk or Trivy to check vulnerable libraries.</li>
          <li><strong>Container image scanning:</strong> Trivy runs after Docker build to check OS and package CVEs.</li>
          <li><strong>Infrastructure scanning:</strong> Checkov scans Terraform or ARM templates before apply.</li>
          <li><strong>DAST scanning:</strong> OWASP ZAP runs after deployment to test live endpoints.</li>
        </ol>
        <p><strong>Example (YAML snippet):</strong></p>
        <pre><code>- task: trivy@1
  inputs:
    command: 'image'
    imageName: '&dollar;(ACR_NAME).azurecr.io/app:&dollar;(Build.BuildId)'</code></pre>
        <p><strong>Outcome:</strong> Pipeline fails automatically if CVEs exceed critical threshold.</p>
      </div>`},{question:"What is the difference between SAST (Static) and DAST (Dynamic) analysis?",answerHtml:`<div class="answer-rich">
        <h3> <strong>SAST vs DAST</strong></h3>
        <p><strong>SAST (Static Application Security Testing):</strong> Analyzes code or binaries before execution  identifies insecure code patterns, hardcoded credentials, etc.</p>
        <p><strong>DAST (Dynamic Application Security Testing):</strong> Scans the running app externally to simulate attacks and detect runtime vulnerabilities.</p>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Aspect</th><th>SAST</th><th>DAST</th></tr>
          <tr><td>Stage</td><td>During Build (Pre-deployment)</td><td>During Runtime (Post-deployment)</td></tr>
          <tr><td>Focus</td><td>Source code weaknesses</td><td>Runtime vulnerabilities</td></tr>
          <tr><td>Tools</td><td>SonarQube, Checkov</td><td>OWASP ZAP, Burp Suite</td></tr>
        </table>
        <p><strong>In practice:</strong> I run SonarQube (SAST) in CI and OWASP ZAP (DAST) automatically after app deployment in staging.</p>
      </div>`},{question:"How do you secure private databases or storage accounts in Azure?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Securing Databases & Storage Accounts</strong></h3>
        <p><strong>Steps I follow for securing data services:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Disable public access completely.</li>
          <li>Use <strong>Private Endpoints</strong> or <strong>Service Endpoints</strong> to connect securely within VNet.</li>
          <li>Enable <strong>Firewall Rules</strong> to allow only trusted IPs or VNets.</li>
          <li>Use <strong>Azure RBAC</strong> and Managed Identities instead of credentials.</li>
          <li>Enable <strong>Encryption:</strong> at-rest (Storage Service Encryption) and in-transit (TLS/SSL).</li>
        </ul>
        <p><strong>Example:</strong> I connect AKS pods to Azure SQL via Private Endpoint  no public exposure, all traffic stays inside VNet.</p>
      </div>`},{question:"How do you give Reader access for Cosmos DB, and whats the role name?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Cosmos DB  Reader Access</strong></h3>
        <p>Access control in Cosmos DB is managed using Azure RBAC roles.</p>
        <p><strong>To give read-only access:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Go to <strong>Azure Portal  Cosmos DB  Access Control (IAM)</strong>.</li>
          <li>Click <strong>Add Role Assignment</strong>.</li>
          <li>Select role: <strong>Cosmos DB Account Reader Role</strong>.</li>
          <li>Assign it to the target user, group, or service principal.</li>
        </ol>
        <pre><code>az role assignment create \\
  --assignee user@domain.com \\
  --role "Cosmos DB Account Reader Role" \\
  --scope /subscriptions/{subId}/resourceGroups/{rg}/providers/Microsoft.DocumentDB/databaseAccounts/{account}</code></pre>
        <p><strong>In real use:</strong> I grant this role to automation service principals to read DB metadata without write permissions.</p>
      </div>`},{question:"How do you ensure DB access is private and not publicly accessible?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Private Database Access  No Public Exposure</strong></h3>
        <p><strong>Steps I implement:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Disable Allow public network access.</li>
          <li>Create <strong>Private Endpoint</strong> in same VNet/subnet as app.</li>
          <li>Restrict inbound rules using <strong>NSGs</strong>.</li>
          <li>Use <strong>Managed Identity</strong> for authentication  no secrets.</li>
          <li>Validate via <code>nslookup</code>  should resolve to <code>privatelink.database.windows.net</code>.</li>
        </ul>
        <p><strong>Example:</strong> My AKS and App Service connect to Azure SQL only via Private Endpoint  all internet ingress blocked at NSG level.</p>
      </div>`},{question:"What are the best practices for key management and network isolation?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Key Management & Network Isolation Best Practices</strong></h3>
        <p><strong>Key Management:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Store keys and secrets in <strong>Azure Key Vault</strong>.</li>
          <li>Enable <strong>Soft Delete</strong> and <strong>Purge Protection</strong> for Key Vault.</li>
          <li>Use <strong>Managed Identity</strong> to access Key Vault  no plaintext secrets.</li>
        </ul>
        <p><strong>Network Isolation:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Deploy workloads inside <strong>VNets</strong> and use <strong>NSGs</strong> to control traffic.</li>
          <li>Use <strong>Private Links</strong> for PaaS resources like Storage, DB, and Key Vault.</li>
          <li>Block all inbound 0.0.0.0/0 traffic unless explicitly required.</li>
        </ul>
        <p><strong>Example:</strong> I configured Key Vault with private endpoint + RBAC-only access; app retrieves secrets using its system-assigned identity over private VNet.</p>
      </div>`},{question:"What are the five pillars of the Azure Well-Architected Framework?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Azure Well-Architected Framework  5 Pillars</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Reliability:</strong> Design for high availability, redundancy, and disaster recovery.</li>
          <li><strong>Security:</strong> Implement least privilege, encryption, and network isolation.</li>
          <li><strong>Cost Optimization:</strong> Use budgets, auto-scaling, and right-sizing.</li>
          <li><strong>Operational Excellence:</strong> Automate, monitor, and define clear runbooks.</li>
          <li><strong>Performance Efficiency:</strong> Choose right SKUs, caching, and scaling strategies.</li>
        </ol>
        <p><strong>In practice:</strong> I apply these pillars as design checks before production rollout  especially around reliability, cost, and security validations.</p>
      </div>`}]},{title:"5. Architecture, Networking & Azure Setup",questions:[{question:"What is a Hub-and-Spoke Architecture and which services are required to build it?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Hub-and-Spoke  Centralized Network Topology</strong></h3>
        <p>Hub-and-Spoke is a recommended Azure network pattern where a central <strong>hub VNet</strong> hosts shared services (firewall, VPN/ExpressRoute, DNS, monitoring) and multiple <strong>spoke VNets</strong> host workloads. Spokes peer with hub and not with each other (traffic flows via hub).</p>
        <p><strong>Core services typically deployed in the hub:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Azure Firewall / NVA (centralized egress, filtering, logging)</li>
          <li>VPN Gateway or ExpressRoute Gateway (on-prem connectivity)</li>
          <li>Azure Bastion (secure RDP/SSH access)</li>
          <li>Private DNS / Azure DNS forwarding</li>
          <li>Shared monitoring & logging (Log Analytics, Sentinel)</li>
        </ul>
        <p><strong>Why use it:</strong> Centralised security, consistent egress control, scale, and easier governance across subscriptions/environments.</p>
      </div>`},{question:"When you set up a hub-spoke model, what key components must you deploy?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Key Components for Hub-Spoke Setup</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Hub VNet:</strong> Host shared services and gateways.</li>
          <li><strong>Spoke VNets:</strong> Each environment/app in its own spoke for isolation.</li>
          <li><strong>VNet Peering:</strong> Peer spokes to hub (use hub as central transit).</li>
          <li><strong>Azure Firewall / NVA:</strong> Centralized policy, NAT, logging to Log Analytics.</li>
          <li><strong>Gateway (VPN/ExpressRoute):</strong> On-prem connectivity.</li>
          <li><strong>Route Tables (UDR):</strong> Direct traffic to firewall/NVA for inspection.</li>
          <li><strong>Monitoring & DNS:</strong> Log Analytics workspace, private DNS zones with forwarding.</li>
        </ol>
        <p><strong>Implementation tip:</strong> Use automation (Terraform/ARM/Bicep) to enforce consistent naming, NSG rules, and UDRs across spokes.</p>
      </div>`},{question:"What is Azure CNI Overlay, and why do we use it?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Azure CNI Overlay  Container Networking Option</strong></h3>
        <p><strong>Azure CNI</strong> provides pod IPs that are routable in the VNet (each pod gets an IP from VNet). <strong>Azure CNI Overlay</strong> is an overlay-mode variant used to reduce VNet IP consumption by encapsulating pod traffic and allowing more flexible IP management.</p>
        <p><strong>Why use overlay:</strong> When your VNet IP space is constrained or you want to run large clusters without exhausting subnet IPs  overlay gives you an encapsulated network plane so pods do not require unique VNet IPs for every pod.</p>
        <p><strong>Practical note:</strong> Choose overlay when addressing is tight and you still need network policies and VNet connectivity  otherwise default Azure CNI (native) gives simpler routing and easier VNet integration.</p>
      </div>`},{question:"Why Azure CNI Overlay and not other networking models?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Choosing Azure CNI Overlay  Trade-offs</strong></h3>
        <p><strong>When overlay is preferred:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Subnet IP exhaustion is a real constraint (large number of pods).</li>
          <li>You still need network policies but cannot allocate huge VNet ranges.</li>
          <li>Want to limit allocation of VNet IPs per pod for cost/management reasons.</li>
        </ul>
        <p><strong>When not to use overlay:</strong> If you need full native IP visibility (easier troubleshooting) or simple VNet routing, native Azure CNI or kubenet can be a better fit.</p>
        <p><strong>In practice:</strong> I evaluate IP requirements, security controls, and operational complexity  choose overlay only if it solves IP-scarcity without adding too much troubleshooting overhead.</p>
      </div>`},{question:"What is an API Gateway, and how does it differ from Application Gateway?",answerHtml:`<div class="answer-rich">
        <h3> <strong>API Gateway vs Application Gateway</strong></h3>
        <p><strong>API Gateway (e.g., Azure API Management):</strong> Focused on API lifecycle  routing, versioning, request transformation, authentication, rate-limiting, usage plans, analytics and developer portal.</p>
        <p><strong>Application Gateway (Layer 7 load balancer + WAF):</strong> Provides HTTP/HTTPS load balancing, SSL termination, path-based routing, Web Application Firewall. Not focused on API management features like policies or developer portal.</p>
        <p><strong>When to use which:</strong> Use API Gateway when you need API policies, monetization, quotas, and developer experience. Use Application Gateway when you need L7 load balancing + WAF for web apps.</p>
      </div>`},{question:"What are Private Endpoints and how do they improve security?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Private Endpoints  Private Link Connectivity</strong></h3>
        <p>Private Endpoints (Azure Private Link) assign a private IP in your VNet to a PaaS resource (Storage, SQL, Cosmos DB, Key Vault). Traffic between your VNet and the service traverses the Azure backbone  no public internet.</p>
        <p><strong>Security benefits:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Eliminates public exposure of PaaS endpoints.</li>
          <li>Works with NSGs and private DNS (full traffic control).</li>
          <li>Better compliance posture (data plane stays inside VNet).</li>
        </ul>
        <p><strong>Example:</strong> I attach private endpoint for Azure SQL so only app subnets can reach DB; public network access is disabled.</p>
      </div>`},{question:"What is a NAT Gateway, and how does it differ from a Load Balancer?",answerHtml:`<div class="answer-rich">
        <h3> <strong>NAT Gateway vs Load Balancer</strong></h3>
        <p><strong>NAT Gateway:</strong> Provides outbound internet connectivity for resources in a private subnet using a set of public IPs  ideal for predictable, scalable SNAT and preserving source IP mapping.</p>
        <p><strong>Load Balancer (Standard):</strong> Distributes inbound (and optionally outbound) traffic across backend instances; used for high-availability and scaling of services.</p>
        <p><strong>Key difference:</strong> NAT Gateway is primarily for controlled outbound SNAT from private resources. Load Balancer is for distributing inbound traffic and health-probing backends (and can do outbound SNAT in some configurations, but not its principal purpose).</p>
        <p><strong>Practical use:</strong> I use NAT Gateway for clusters in private subnets to pull images and call external APIs; Application Gateway / Load Balancer handles incoming client traffic.</p>
      </div>`},{question:"What is the purpose of a Function App in Azure architecture?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Function App  Serverless / Event-driven Compute</strong></h3>
        <p>Function App hosts serverless functions to run small units of code in response to events (HTTP, queue, timer, Event Grid). Use when you need quick glue logic, light ETL, or event-driven automation without managing servers.</p>
        <p><strong>Common uses:</strong> Webhook handlers, scheduled jobs, automated remediation, message processing, or lightweight data transformations.</p>
        <p><strong>Example:</strong> I use Function Apps to process Service Bus messages, run nightly housekeeping, and trigger autoscaling decisions based on custom metrics.</p>
      </div>`},{question:"What message services have you used in Azure (Event Grid, Service Bus, etc.)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Azure Messaging Services  Real Use</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Event Grid:</strong> Pub/sub for lightweight event routing (serverless triggers, reactive integrations).</li>
          <li><strong>Service Bus:</strong> Enterprise messaging with queues/topics, FIFO, transactions  good for guaranteed delivery and complex workflows.</li>
          <li><strong>Event Hubs:</strong> Big data / telemetry ingestion (high-throughput streaming).</li>
          <li><strong>Storage Queues:</strong> Simple queueing for lightweight background jobs.</li>
        </ul>
        <p><strong>In practice:</strong> I used Event Grid to trigger Function Apps on blob creation, Service Bus topics for order processing across microservices, and Event Hubs for telemetry ingestion into Stream Analytics.</p>
      </div>`},{question:"How do you manage access and permissions across Azure subscriptions?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Access & Permission Management Across Subscriptions</strong></h3>
        <p><strong>Controls and patterns I use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure AD RBAC:</strong> Assign roles at subscription / resource-group / resource scope with least privilege.</li>
          <li><strong>Management Groups:</strong> Group subscriptions for policy and role inheritance.</li>
          <li><strong>Azure Policy & Blueprints:</strong> Enforce guardrails and deploy baseline resources across subscriptions.</li>
          <li><strong>Privileged Identity Management (PIM):</strong> Time-bound elevation for critical roles.</li>
          <li><strong>Cross-tenant / delegated ops:</strong> Azure Lighthouse for managing customer tenants securely.</li>
        </ul>
        <p><strong>Quick CLI example  assign role to a service principal at subscription scope:</strong></p>
        <pre><code>az role assignment create \\
  --assignee <appId-or-objectId> \\
  --role "Contributor" \\
  --scope /subscriptions/{subscriptionId}</code></pre>
        <p><strong>Operational tip:</strong> Keep platform/service principals in a central identity subscription and use managed identities for resource-to-resource access to avoid credential sprawl.</p>
      </div>`}]},{title:"6. Monitoring, Performance & Optimization",questions:[{question:"How do you troubleshoot performance issues in microservices?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Troubleshooting Microservices Performance</strong></h3>
        <p><strong>Approach I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Check application metrics:</strong> CPU, memory, thread count, request latency per service via Application Insights or Prometheus.</li>
          <li><strong>Trace inter-service calls:</strong> Use distributed tracing (Application Insights, Jaeger, or OpenTelemetry) to identify slow dependencies.</li>
          <li><strong>Review logs:</strong> Use centralized logging (Log Analytics / ELK) for exceptions or high-latency endpoints.</li>
          <li><strong>Database layer:</strong> Analyze query execution time, missing indexes, connection pool saturation.</li>
          <li><strong>Network latency:</strong> Inspect API Gateway or ingress rules, DNS resolution, or misconfigured load balancers.</li>
        </ol>
        <p><strong>Example:</strong> Once I isolated a bottleneck where one microservices API retries caused cascading delays  fixed using proper circuit breaker and retry policies in code.</p>
      </div>`},{question:"What are the key metrics to monitor for production health?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Production Health Monitoring Metrics</strong></h3>
        <p><strong>Key Metrics I always track:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Availability:</strong> Uptime, failed requests, service response codes.</li>
          <li><strong>Performance:</strong> Response time (P50, P95, P99), latency, throughput (req/sec).</li>
          <li><strong>Resource Utilization:</strong> CPU, memory, disk I/O, network I/O.</li>
          <li><strong>App Health:</strong> Dependency calls, queue backlog, error rates.</li>
          <li><strong>Business KPIs:</strong> Transaction rate, user sessions, conversion drop.</li>
        </ul>
        <p><strong>Tools:</strong> Azure Monitor, Application Insights, Grafana dashboards, and Prometheus exporters for deep visibility.</p>
      </div>`},{question:"How do you identify and fix bottlenecks in web applications?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Finding & Fixing Web App Bottlenecks</strong></h3>
        <p><strong>Step-by-step approach:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Measure end-to-end latency:</strong> Use Application Insights Performance blade or APM tools.</li>
          <li><strong>Analyze dependencies:</strong> Check DB queries, external API latency, or cache misses.</li>
          <li><strong>Profile code:</strong> Use Application Insights Profiler or VS Code profiling tools for method-level delays.</li>
          <li><strong>Enable caching:</strong> Add Redis or CDN to offload heavy static content.</li>
          <li><strong>Scale out:</strong> Introduce horizontal scaling or load balancing if CPU/memory saturation is consistent.</li>
        </ol>
        <p><strong>Example:</strong> Optimized an Azure Web App by enabling Redis caching + indexing key DB columns, reducing response time from 1.8s  400ms.</p>
      </div>`},{question:"How do you integrate monitoring and alerting for performance degradation?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Monitoring & Alert Integration</strong></h3>
        <p><strong>Typical setup I use:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Collect metrics:</strong> Application Insights, Azure Monitor, or Prometheus scrape targets.</li>
          <li><strong>Create Alerts:</strong> Define thresholds (e.g., CPU > 80%, latency > 2s, error rate > 5%).</li>
          <li><strong>Action Groups:</strong> Send alerts to Teams, email, webhook, or Logic App for automation.</li>
          <li><strong>Visual Dashboards:</strong> Use Grafana or Azure Dashboards for real-time visibility.</li>
          <li><strong>Integrate with CI/CD:</strong> Stop deployment if alert state is critical (health gate).</li>
        </ol>
        <p><strong>Example:</strong> I configured Azure Monitor alerts that call a Logic App webhook  triggers Function App  scales up AKS nodes dynamically on sustained CPU > 75%.</p>
      </div>`},{question:"Can we configure auto-scaling vertically in Azure?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Vertical Auto-scaling in Azure</strong></h3>
        <p>Vertical scaling = increasing resource capacity (CPU/RAM) of an existing instance.</p>
        <p><strong>Azure limitation:</strong> Azure doesnt support true automatic vertical scaling for VMs  resizing requires a restart.</p>
        <p><strong>However:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>For <strong>App Services:</strong> You can auto-scale vertically using scale-up rules in portal (manual trigger or automation runbook).</li>
          <li>For <strong>VMs:</strong> Use Azure Automation or Logic Apps to resize during off-peak hours based on metrics.</li>
        </ul>
        <p><strong>In practice:</strong> I prefer horizontal scaling (VMSS / AKS) since its faster and zero-downtime compared to vertical resize.</p>
      </div>`},{question:"How do you ensure cost optimization for resources?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Cost Optimization Strategies</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Right-sizing:</strong> Review usage metrics monthly; downsize underutilized VMs.</li>
          <li><strong>Auto-shutdown:</strong> Enable auto-shutdown for non-prod environments.</li>
          <li><strong>Reserved Instances:</strong> Buy RIs or Savings Plans for predictable workloads.</li>
          <li><strong>Spot VMs:</strong> Use for non-critical workloads or build agents.</li>
          <li><strong>Storage tiering:</strong> Move cold data to Cool or Archive tier.</li>
          <li><strong>Budgets & Alerts:</strong> Create budgets in Azure Cost Management with thresholds.</li>
        </ul>
        <p><strong>Example:</strong> Reduced cost by 30% by enabling auto-shutdown on Dev/Test VMs and moving logs to archive tier after 30 days.</p>
      </div>`},{question:"What tools do you use for performance testing (e.g., K6, JMeter)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Performance Testing Tools I Use</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>JMeter:</strong> For API load & stress testing (simulates concurrent users).</li>
          <li><strong>K6:</strong> Lightweight CLI-based load testing integrated with CI pipelines.</li>
          <li><strong>Locust:</strong> Python-based load generator for distributed testing.</li>
          <li><strong>Azure Load Testing:</strong> Native managed service to execute JMeter/K6 scripts at scale.</li>
        </ul>
        <p><strong>In practice:</strong> I integrate K6 tests in my pipeline post-deployment to validate that the service handles expected load before marking deployment successful.</p>
      </div>`},{question:"What is the difference between horizontal and vertical scaling?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Horizontal vs Vertical Scaling</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Aspect</th><th>Horizontal Scaling</th><th>Vertical Scaling</th></tr>
          <tr><td>Definition</td><td>Add more instances</td><td>Increase size of existing instance</td></tr>
          <tr><td>Downtime</td><td>No downtime (load balanced)</td><td>May require restart</td></tr>
          <tr><td>Cost</td><td>Pay per instance (scalable)</td><td>Limited by max VM SKU</td></tr>
          <tr><td>Examples</td><td>VM Scale Sets, AKS node pools</td><td>Changing VM size from B2s  D4s</td></tr>
        </table>
        <p><strong>In practice:</strong> I design production systems with horizontal scaling first for resilience, then plan vertical scaling only when performance tuning demands stronger compute nodes.</p>
      </div>`}]},{title:"7. Reliability & SRE Concepts",questions:[{question:"What is Site Reliability Engineering (SRE)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Understanding Site Reliability Engineering (SRE)</strong></h3>
        <p><strong>SRE</strong> is the discipline of applying software engineering practices to IT operations  with the goal of achieving high reliability, scalability, and automation in production systems.</p>
        <p><strong>Core idea:</strong> Treat operations like code  automate, measure, and continuously improve.</p>
        <p><strong>Key focus areas:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Service availability & uptime.</li>
          <li>Performance & latency optimization.</li>
          <li>Incident management & automated recovery.</li>
          <li>Error budget and release stability balance.</li>
        </ul>
        <p><strong>In practice:</strong> I maintain defined SLOs for all critical services and integrate them with Azure Monitor dashboards for proactive alerting and auto-remediation.</p>
      </div>`},{question:"What are the main principles of SRE (SLI, SLO, SLA)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>SRE Metrics  SLI, SLO, SLA</strong></h3>
        <p><strong>SLI (Service Level Indicator):</strong> Quantitative metric that measures reliability  e.g., uptime %, latency, error rate.</p>
        <p><strong>SLO (Service Level Objective):</strong> Target for SLIs  defines acceptable performance, e.g., 99.9% uptime, <200ms latency.</p>
        <p><strong>SLA (Service Level Agreement):</strong> Business contract with users/customers that defines penalties if SLOs are not met.</p>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Term</th><th>Purpose</th><th>Example</th></tr>
          <tr><td>SLI</td><td>What we measure</td><td>Request success rate = 99.95%</td></tr>
          <tr><td>SLO</td><td>What we aim for</td><td>Availability target = 99.9%</td></tr>
          <tr><td>SLA</td><td>What we promise</td><td>Compensation if uptime <99.9%</td></tr>
        </table>
        <p><strong>In practice:</strong> My Azure dashboards show SLIs from App Insights & Log Analytics  if we breach the SLO, the pipeline blocks further deployments until issue resolution.</p>
      </div>`},{question:"How do you handle incident response and postmortems?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Incident Response & Postmortems</strong></h3>
        <p><strong>Incident Response Process I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Detection:</strong> Alerts from Azure Monitor, Grafana, or Prometheus trigger incident ticket.</li>
          <li><strong>Classification:</strong> Assign severity (SEV1SEV3) and notify on-call team via Teams/PagerDuty.</li>
          <li><strong>Mitigation:</strong> Apply temporary fix or rollback to restore service quickly.</li>
          <li><strong>Root Cause Analysis (RCA):</strong> Identify exact failure point using logs, metrics, and traces.</li>
          <li><strong>Postmortem:</strong> Document cause, timeline, and action items to prevent recurrence.</li>
        </ol>
        <p><strong>Tools I use:</strong> Azure Monitor Alerts  Logic App for on-call notifications + Confluence templates for postmortem tracking.</p>
      </div>`},{question:"What tools do you use for monitoring reliability?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Reliability Monitoring Stack</strong></h3>
        <p><strong>Tools & Services I use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure Monitor:</strong> Centralized metrics & alerts for VMs, AKS, and App Services.</li>
          <li><strong>Application Insights:</strong> Telemetry, request traces, and availability tests.</li>
          <li><strong>Grafana + Prometheus:</strong> Custom dashboards and SLO tracking via SLIs.</li>
          <li><strong>Log Analytics:</strong> Unified query-based troubleshooting for logs across environments.</li>
          <li><strong>Azure Service Health:</strong> Tracks outages and maintenance at region/subscription level.</li>
        </ul>
        <p><strong>Example:</strong> I built a Grafana dashboard pulling Prometheus metrics to visualize request latency (SLI) vs target (SLO)  auto-alerts if breach probability >5%.</p>
      </div>`},{question:"How do you ensure high uptime and error budget tracking?",answerHtml:`<div class="answer-rich">
        <h3> <strong>High Uptime & Error Budget Management</strong></h3>
        <p><strong>Process:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Define SLOs:</strong> For example, 99.9% uptime = 43.2 min downtime/month.</li>
          <li><strong>Track SLIs continuously:</strong> Application Insights availability tests every 5 mins.</li>
          <li><strong>Maintain error budget:</strong> If consumed >50%, freeze feature deployments.</li>
          <li><strong>Auto-healing & redundancy:</strong> Use AKS self-healing, zone-redundant deployments, and auto-scaling.</li>
          <li><strong>Review monthly reports:</strong> RCA and optimization if SLOs repeatedly breached.</li>
        </ol>
        <p><strong>Example:</strong> In one setup, I automated error budget tracking using KQL queries in Log Analytics that alert when downtime exceeds allowed thresholds.</p>
      </div>`},{question:"How do you automate recovery or failover?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Automated Recovery & Failover</strong></h3>
        <p><strong>Mechanisms I use:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Health Probes & Load Balancers:</strong> Route traffic to healthy instances automatically.</li>
          <li><strong>Auto-Healing:</strong> Use AKS or VMSS with health extension  auto-restarts unhealthy nodes/pods.</li>
          <li><strong>Azure Traffic Manager / Front Door:</strong> Global DNS failover based on endpoint health.</li>
          <li><strong>Runbook Automation:</strong> Azure Automation or Logic Apps triggered on alert to restart services or redeploy pods.</li>
          <li><strong>DR Strategy:</strong> Geo-redundant resources + failover group for databases.</li>
        </ul>
        <p><strong>Example:</strong> Configured an Azure Automation Runbook that automatically reboots unhealthy VMs when CPU stuck >95% for 10 min, restoring service in <2 min without manual action.</p>
      </div>`},{question:"What is observability, and how is it different from monitoring?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Observability vs Monitoring</strong></h3>
        <p><strong>Monitoring:</strong> Collects and alerts on predefined metrics and logs  tells you <em>when</em> something is wrong.</p>
        <p><strong>Observability:</strong> Gives deep insight into <em>why</em> something is wrong  combines metrics, logs, and traces to understand system behavior holistically.</p>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Aspect</th><th>Monitoring</th><th>Observability</th></tr>
          <tr><td>Purpose</td><td>Detect known issues</td><td>Explore unknown issues</td></tr>
          <tr><td>Data Types</td><td>Metrics, basic logs</td><td>Metrics, structured logs, traces</td></tr>
          <tr><td>Focus</td><td>Reactive</td><td>Proactive / Diagnostic</td></tr>
        </table>
        <p><strong>In practice:</strong> I use Azure Monitor for metrics (monitoring) and Application Insights + OpenTelemetry tracing for observability  together, they shorten MTTR drastically.</p>
      </div>`}]},{title:"8. Cloud Tools, Commands & Troubleshooting",questions:[{question:"What command would you use to find files larger than 100MB in Linux?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Finding Large Files in Linux</strong></h3>
        <p>To find files larger than 100MB recursively from a directory, use the <code>find</code> command with <code>-size</code> flag:</p>
        <pre><code>find / -type f -size +100M -exec ls -lh {} ; | sort -k 5 -h</code></pre>
        <p><strong>Explanation:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><code>-type f</code>  search for files only.</li>
          <li><code>-size +100M</code>  files greater than 100MB.</li>
          <li><code>ls -lh</code>  show human-readable size & permissions.</li>
          <li><code>sort -k 5 -h</code>  sort output by file size.</li>
        </ul>
        <p><strong>In practice:</strong> I use this command during disk utilization alerts to identify logs or dump files causing high storage usage on build or application servers.</p>
      </div>`},{question:"What access level do you have in your Azure environment?",answerHtml:`<div class="answer-rich">
        <h3> <strong>My Azure Access Level</strong></h3>
        <p><strong>Role:</strong> Contributor-level access across project subscriptions, with limited Owner rights in non-production environments.</p>
        <p><strong>Practical permissions include:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Create/manage VNets, VMs, Storage Accounts, and AKS clusters.</li>
          <li>Deploy and manage resources via Terraform and Azure DevOps pipelines.</li>
          <li>Access to Azure Key Vault (read/secrets) and ACR for image pulls.</li>
          <li>View-only access on production resources (via Reader or custom role with restricted write permissions).</li>
        </ul>
        <p><strong>In practice:</strong> This segregation ensures production stability  I can troubleshoot and validate configs, but deployments happen only via pipelines with approvals.</p>
      </div>`},{question:"What are the resources youve created in your project?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Resources Ive Created & Managed</strong></h3>
        <p>In my current project, Ive provisioned complete infrastructure using Terraform and Azure DevOps pipelines. Major resources include:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Compute:</strong> Azure VMs, App Services, AKS Clusters.</li>
          <li><strong>Networking:</strong> VNets, Subnets, NSGs, NAT Gateway, Load Balancer, Application Gateway.</li>
          <li><strong>Storage:</strong> Blob Storage, File Shares, Managed Disks.</li>
          <li><strong>Security:</strong> Key Vault, Private Endpoints, RBAC roles.</li>
          <li><strong>Monitoring:</strong> Log Analytics, Azure Monitor, Alerts, Action Groups.</li>
          <li><strong>Automation:</strong> Function Apps, Logic Apps, and Azure Automation Accounts.</li>
        </ul>
        <p><strong>In real deployment:</strong> I used Terraform modules to create reusable infrastructure stacks  separate for dev, QA, and production environments.</p>
      </div>`},{question:"How do you update latest GitHub changes to previous branches?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Updating Older Branches with Latest Changes</strong></h3>
        <p>When I need to sync an older branch (e.g., <code>release/qa</code>) with latest main branch updates:</p>
        <ol style="margin-left:1.2rem;">
          <li>Checkout the older branch:<br><code>git checkout release/qa</code></li>
          <li>Fetch latest changes:<br><code>git fetch origin</code></li>
          <li>Merge or rebase from main:<br><code>git merge origin/main</code> or <code>git rebase origin/main</code></li>
          <li>Resolve conflicts if any, test locally, and push:<br><code>git push origin release/qa</code></li>
        </ol>
        <p><strong>In practice:</strong> I prefer <code>merge</code> for team sync branches (to preserve history) and <code>rebase</code> only for feature branches before PR submission.</p>
      </div>`},{question:"How do you handle technologies you havent worked on before?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Approach to New Technologies</strong></h3>
        <p><strong>My process:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Start with official documentation or Microsoft Learn tutorials.</li>
          <li>Set up a small lab / sandbox to get hands-on experience.</li>
          <li>Integrate the tool in a test CI/CD pipeline for real-time understanding.</li>
          <li>Read blogs and watch quickstart videos to understand best practices.</li>
          <li>Finally, apply it in non-prod first, then scale to production with automation.</li>
        </ol>
        <p><strong>Example:</strong> I learned Docker and AKS by building a local container lab and later deploying the same setup through Terraform + Azure DevOps in real projects.</p>
      </div>`},{question:"How do you approach tight deadlines and limited resources?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Handling Tight Deadlines Effectively</strong></h3>
        <p><strong>My approach:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Prioritize:</strong> Break tasks into must-have vs good-to-have items.</li>
          <li><strong>Automate repetitive steps:</strong> Use scripts or templates to save time.</li>
          <li><strong>Parallelize work:</strong> Delegate tasks to team or run multiple builds simultaneously.</li>
          <li><strong>Stay transparent:</strong> Keep stakeholders updated about progress or blockers.</li>
          <li><strong>Focus on quality first:</strong> Avoid shortcuts that cause rework later.</li>
        </ul>
        <p><strong>Example:</strong> During an AKS migration with a strict timeline, I automated provisioning with Terraform and CI/CD pipeline triggers, saving nearly 40% deployment time.</p>
      </div>`},{question:"Describe the most challenging technical issue youve solved.",answerHtml:`<div class="answer-rich">
        <h3> <strong>Most Challenging Issue  Real Scenario</strong></h3>
        <p><strong>Scenario:</strong> We had intermittent downtime in a microservices-based application hosted on AKS. The issue wasnt visible at pod level  everything showed healthy.</p>
        <p><strong>Diagnosis:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Used Application Insights to trace dependency latency  found spikes at ingress layer.</li>
          <li>Checked logs  identified that NAT Gateway SNAT port exhaustion was occurring under load.</li>
          <li>Analyzed VNet flows and found multiple pods sharing limited outbound ports.</li>
        </ul>
        <p><strong>Fix:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Implemented Azure NAT Gateway scaling + connection reuse policy.</li>
          <li>Added proper connection pooling at app level to avoid exhausting ephemeral ports.</li>
        </ul>
        <p><strong>Outcome:</strong> Reduced connection failures by 95% and improved response latency by 300ms. This fix became a reference design across multiple environments.</p>
      </div>`}]},{title:"9. Miscellaneous Scenarios & Behavioral Add-ons",questions:[{question:"How would you explain Cloud vs On-Premises to a non-technical person?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Explaining Cloud vs On-Premises (Simple Analogy)</strong></h3>
        <p><strong>On-Premises:</strong> Like owning a personal car  you buy it, maintain it, and pay even when not using it.</p>
        <p><strong>Cloud:</strong> Like using a taxi  pay only when you need it, maintenance and upgrades handled by provider.</p>
        <p><strong>In short:</strong> Cloud gives flexibility, scalability, and zero maintenance overhead  perfect for businesses that want to focus on growth, not infrastructure upkeep.</p>
      </div>`},{question:"How would you work with cross-functional teams under tight deadlines?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Collaboration Under Pressure</strong></h3>
        <p><strong>My approach:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Clearly define priorities and ownership upfront.</li>
          <li>Set up daily syncs or quick stand-ups with Dev, QA, Security, and Infra teams.</li>
          <li>Use shared dashboards (Azure Boards or Jira) to track blockers transparently.</li>
          <li>Automate handoff steps (CI/CD triggers, notifications) to reduce manual dependency.</li>
          <li>Keep escalation channels ready for critical blockers.</li>
        </ul>
        <p><strong>Example:</strong> During a release crunch, I aligned Dev + QA via daily 15-min syncs and used pipeline gates to auto-notify test completion  we met the release deadline 2 days early.</p>
      </div>`},{question:"How would you handle a technology shift in an ongoing project?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Managing a Technology Shift</strong></h3>
        <p><strong>Steps I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Assess business impact  identify what must change and what can stay.</li>
          <li>Plan phased adoption  run pilot in non-prod to validate stability.</li>
          <li>Upskill the team quickly with internal sessions or vendor training.</li>
          <li>Refactor CI/CD pipelines and IaC modules for new components.</li>
          <li>Keep rollback or hybrid path ready until new stack stabilizes.</li>
        </ol>
        <p><strong>Example:</strong> We migrated a legacy Jenkins setup to Azure DevOps YAML  I created reusable pipeline templates, trained the team, and completed transition in 4 sprints without downtime.</p>
      </div>`},{question:"What would be your deployment strategy to ensure production stability?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Stable Deployment Strategy</strong></h3>
        <p><strong>Key points I ensure:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Use <strong>staged pipelines</strong>  Dev  QA  UAT  Prod with approvals.</li>
          <li>Implement <strong>Blue-Green or Canary deployment</strong> for zero downtime.</li>
          <li><strong>Automate health checks</strong> post-deployment via scripts or Function Apps.</li>
          <li>Enable <strong>auto rollback</strong> on failed health probes or high error rate.</li>
          <li><strong>Monitor error budget</strong>  halt future releases if SLOs breached.</li>
        </ol>
        <p><strong>Example:</strong> I configured health probes and load balancer checks  failed rollout auto-triggered rollback using pipeline logic, saving a major outage.</p>
      </div>`},{question:"What are key DevOps tools youve used in the last two years?",answerHtml:`<div class="answer-rich">
        <h3> <strong>My DevOps Toolchain</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>CI/CD:</strong> Azure DevOps, GitHub Actions, Jenkins.</li>
          <li><strong>IaC:</strong> Terraform, ARM/Bicep.</li>
          <li><strong>Containers:</strong> Docker, Kubernetes (AKS).</li>
          <li><strong>Monitoring:</strong> Grafana, Prometheus, Azure Monitor, Application Insights.</li>
          <li><strong>Security:</strong> SonarQube, Trivy, Checkov, OWASP ZAP.</li>
          <li><strong>Version Control:</strong> Git, GitHub, Azure Repos.</li>
        </ul>
        <p><strong>In practice:</strong> I combine these in an end-to-end Azure DevOps workflow  Terraform builds infra, pipelines deploy app code, and monitoring triggers alerts for any anomaly.</p>
      </div>`},{question:"Describe a time when you had to work with limited resources but ensured delivery.",answerHtml:`<div class="answer-rich">
        <h3> <strong>Delivering with Limited Resources</strong></h3>
        <p><strong>Scenario:</strong> Our project had budget limits  no new licenses or cloud nodes could be added.</p>
        <p><strong>My solution:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Reused existing self-hosted agents to run parallel CI/CD jobs.</li>
          <li>Consolidated environments (merged QA + Staging) with controlled branching.</li>
          <li>Implemented Infrastructure-as-Code to rebuild resources quickly when needed.</li>
          <li>Automated non-critical tasks (backups, validations) via Function Apps.</li>
        </ul>
        <p><strong>Result:</strong> Delivered all planned releases on time while reducing monthly compute cost by 25%.</p>
      </div>`},{question:"Describe a critical production issue and how you resolved it end-to-end.",answerHtml:`<div class="answer-rich">
        <h3> <strong>Critical Production Issue  Real Example</strong></h3>
        <p><strong>Issue:</strong> Production API service started returning 5xx intermittently during high traffic  impacting customer orders.</p>
        <p><strong>Actions taken:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Checked App Insights  found thread pool exhaustion under load.</li>
          <li>Scaled out App Service instances via pipeline trigger.</li>
          <li>Added queue-based buffering between API and downstream DB calls.</li>
          <li>Configured alert rules and autoscale thresholds for CPU & latency.</li>
          <li>Performed postmortem to fine-tune thread pool and DB connection limits.</li>
        </ol>
        <p><strong>Outcome:</strong> Reduced error rate from 12%  &lt;0.5% within 30 minutes; added permanent alert-based auto-scaling rule for future prevention.</p>
      </div>`},{question:"Tell me about your project contributions and impact in your last role.",answerHtml:`<div class="answer-rich">
        <h3> <strong>My Contributions & Impact</strong></h3>
        <p><strong>Highlights from my last project:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Built complete Azure infrastructure using Terraform modules (VMs, AKS, VNets, NAT, Storage, Monitoring).</li>
          <li>Implemented end-to-end CI/CD pipelines in Azure DevOps integrating Terraform + Docker + ACR + AKS.</li>
          <li>Configured centralized monitoring with Grafana + Log Analytics + alerting to Teams.</li>
          <li>Set up Blue-Green deployments for API layer ensuring zero downtime releases.</li>
          <li>Enabled cost optimization through automation  30% resource savings.</li>
        </ul>
        <p><strong>Impact:</strong> Reduced manual deployment effort from 2 hours  15 minutes and improved release reliability across environments.</p>
      </div>`}]}];function eb(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("miscellaneous"),a=ym.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-amber-500 to-orange-500 shadow-glow",children:l.jsx(kv,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Miscellaneous Topics"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Cloud Fundamentals, IaC, CI/CD, DevSecOps & Architecture"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Explore various DevOps concepts including cloud fundamentals, deployment strategies, and more."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:ym.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:u.question,b=typeof u=="object"&&"answer"in u?u.answer:null,v=typeof u=="object"&&"answerHtml"in u?u.answerHtml:null;return l.jsx("div",{className:"border-l-2 border-amber-500/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-amber-500 font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),b||v?l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:v.replace(/\$\{/g,"$${")}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:b})})]}):null]})]})},f)})})})]},p))})]})]})}const sD=Object.freeze(Object.defineProperty({__proto__:null,default:eb},Symbol.toStringTag,{value:"Module"}));function tb(){const e=zo();return m.useEffect(()=>{console.error("404 Error: Page not found ",e.pathname)},[e.pathname]),l.jsx("div",{className:"flex min-h-screen items-center justify-center bg-gradient-to-br from-gray-100 via-gray-200 to-gray-300 dark:from-gray-900 dark:via-gray-950 dark:to-black text-gray-800 dark:text-gray-200 transition-colors duration-500",children:l.jsxs("div",{className:"text-center px-6",children:[l.jsx("div",{className:"flex justify-center mb-6",children:l.jsx("div",{className:"bg-red-100 dark:bg-red-900/30 p-4 rounded-full shadow-inner animate-bounce",children:l.jsx(eI,{className:"h-10 w-10 text-red-600 dark:text-red-400"})})}),l.jsx("h1",{className:"text-6xl font-extrabold tracking-tight mb-4 text-red-600 dark:text-red-400",children:"404"}),l.jsx("p",{className:"text-xl mb-6 text-gray-600 dark:text-gray-400",children:"Oops! The page you're looking for doesnt exist."}),l.jsx(ba,{to:"/",className:"inline-block px-6 py-3 bg-blue-600 hover:bg-blue-700 text-white font-semibold rounded-xl shadow-lg transition-transform transform hover:-translate-y-0.5",children:" Back to Home"}),l.jsxs("p",{className:"mt-6 text-sm opacity-70",children:[l.jsx("code",{children:e.pathname})," was not found on this server."]})]})})}const iD=Object.freeze(Object.defineProperty({__proto__:null,default:tb},Symbol.toStringTag,{value:"Module"})),wm=[{title:"1. Terraform  Fundamentals & Core Concepts",questions:[{question:"What is Terraform?",answerHtml:` <div class="answer-rich">
        <h3> <strong>Terraform Overview</strong></h3>
        <p>Terraform is an <strong>Infrastructure as Code (IaC)</strong> tool by HashiCorp used to provision, manage, and version cloud infrastructure declaratively.</p>
        <ul style="margin-left:1.2rem;">
          <li>Supports multiple clouds (Azure, AWS, GCP, VMware, etc.) using providers.</li>
          <li>Executes infra creation via a dependency graph and plans changes before applying.</li>
          <li>Ensures reproducibility & consistency  same code = same infra every time.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Terraform to manage multi-env infra (Dev, QA, Prod) from same codebase using workspaces and variables.</p>
      </div>`},{question:"What is a provider in Terraform?",answerHtml:` <div class="answer-rich">
        <h3> <strong>Providers  the Cloud API connectors</strong></h3>
        <p>Providers are plugins that let Terraform interact with specific cloud services (e.g., AzureRM, AWS, Google).</p>
        <pre><code>provider "azurerm" {
  features {}
}</code></pre>
        <p><strong>In Practice:</strong> Each provider manages authentication, resource definitions, and version pinning to ensure consistency across deployments.</p>
      </div>`},{question:"What is a resource block?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Resource Blocks  define actual infra</strong></h3>
        <p>A resource block represents a single piece of infrastructure (VM, VNet, S3 bucket, etc.) managed by Terraform.</p>
        <pre><code>resource "azurerm_resource_group" "rg" {
  name     = "prod-rg"
  location = "East US"
}</code></pre>
        <p>Each resource block maps to an API object in the provider. Terraform tracks its state for drift detection and lifecycle operations.</p>
      </div>`},{question:"What is a variable in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Input Variables  parameterize configurations</strong></h3>
        <p>Variables make Terraform code reusable and dynamic by accepting values at runtime or via tfvars files.</p>
        <pre><code>variable "location" {
  type    = string
  default = "East US"
}</code></pre>
        <p><strong>In Practice:</strong> I keep environment-specific values in separate tfvars files and inject them through pipelines.</p>
      </div>`},{question:"What is the use of output block and how to pass it in pipeline?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Output Blocks  expose values from Terraform</strong></h3>
        <p>Outputs are used to display or export values (like resource IDs, IPs) post-deployment. Useful in CI/CD to pass data between stages.</p>
        <pre><code>output "rg_name" {
  value = azurerm_resource_group.rg.name
}</code></pre>
        <p><strong>Pipeline Usage:</strong> Terraform CLI exposes outputs via <code>terraform output -json</code> which can be parsed and passed to next job or script.</p>
      </div>`},{question:"What is data block in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Data Blocks  fetch existing infra</strong></h3>
        <p>Data blocks query existing infrastructure instead of creating new resources.</p>
        <pre><code>data "azurerm_resource_group" "existing" {
  name = "shared-rg"
}</code></pre>
        <p><strong>In Practice:</strong> I use data sources to reference shared infra (like existing VNets, subnets, key vaults) to avoid duplication.</p>
      </div>`},{question:"How to write code with module and for_each?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Modules + for_each  scalable infra pattern</strong></h3>
        <p>Modules package reusable Terraform code. <code>for_each</code> helps deploy multiple instances dynamically.</p>
        <pre><code>module "vm" {
  source   = "./modules/vm"
  for_each = var.vm_list
  name     = each.key
  size     = each.value.size
}</code></pre>
        <p><strong>In Practice:</strong> I maintain a shared modules repo and use for_each to deploy consistent infra for multiple environments.</p>
      </div>`},{question:"What is terraform show?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform show  inspect current state</strong></h3>
        <p>Displays details of the latest Terraform state or plan in a readable format.</p>
        <pre><code>terraform show</code></pre>
        <p>Useful for debugging outputs, resource attributes, or reviewing infra without logging into cloud portals.</p>
      </div>`},{question:"What is terraform fmt?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform fmt  auto-format Terraform code</strong></h3>
        <p>Automatically formats <code>.tf</code> files according to Terraform's canonical style guide.</p>
        <pre><code>terraform fmt -recursive</code></pre>
        <p><strong>In Practice:</strong> I integrate fmt checks into pipelines to enforce consistent code formatting and PR hygiene.</p>
      </div>`},{question:"What is terraform function?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Functions  logic within HCL</strong></h3>
        <p>Terraform provides built-in functions to transform data or compute values dynamically (e.g., <code>concat()</code>, <code>lookup()</code>, <code>join()</code>).</p>
        <pre><code>tags = merge(var.common_tags, { env = var.env })</code></pre>
        <p><strong>In Practice:</strong> Functions reduce code repetition and make variable-driven configs more intelligent.</p>
      </div>`}]},{title:"2. Terraform  Workspaces, Execution & State Management",questions:[{question:"What is terraform workspace and why we use it?",answerHtml:` <div class="answer-rich">
        <h3> <strong>Terraform Workspaces  manage multiple environments</strong></h3>
        <p>Workspaces allow maintaining separate state files for different environments (e.g., dev, qa, prod) using the same configuration.</p>
        <pre><code>terraform workspace new dev
terraform workspace select dev</code></pre>
        <p><strong>In Practice:</strong> I use workspaces to isolate state per environment, so changes in one dont impact others.</p>
      </div> `},{question:"What is the use of terraform slug?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Slug  unique identifier concept</strong></h3>
        <p>Terraform doesnt have a direct command called slug  its often used to denote unique resource naming conventions (like environment + app + region).</p>
        <p><strong>In Practice:</strong> I implement slugs using <code>locals</code> to create predictable resource names.</p>
        <pre><code>locals {
  slug = "&dollar;{var.env}-&dollar;{var.app}-&dollar;{var.region}"
}</code></pre>
      </div>`},{question:"What is terraform init?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform init  initialize project</strong></h3>
        <p>Initializes Terraform configuration directory  downloads providers, initializes backend, and prepares workspace for execution.</p>
        <pre><code>terraform init</code></pre>
        <p><strong>In Practice:</strong> Always run after adding new providers or changing backend configurations.</p>
      </div>`},{question:"What is the difference between terraform plan and terraform apply?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Plan vs Apply  dry-run and execution</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>terraform plan:</strong> Shows proposed changes without executing them.</li>
          <li><strong>terraform apply:</strong> Executes the actual infra changes.</li>
        </ul>
        <pre><code>terraform plan -out=tfplan
terraform apply tfplan</code></pre>
        <p><strong>In Practice:</strong> I always review plan output in PR before apply step in pipeline.</p>
      </div>`},{question:"What is the purpose of terraform import?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform import  bring existing infra under management</strong></h3>
        <p>Imports existing cloud resources into Terraform state so they can be managed as code.</p>
        <pre><code>terraform import azurerm_resource_group.rg /subscriptions/.../resourceGroups/my-rg</code></pre>
        <p><strong>In Practice:</strong> I use it during migration of manual infra to Terraform-managed infra.</p>
      </div>`},{question:"What is the use of dynamic block?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Dynamic Blocks  generate nested blocks dynamically</strong></h3>
        <p>Used when nested configuration needs to repeat based on input variables (like NSG rules, tags).</p>
        <pre><code>dynamic "security_rule" {
  for_each = var.rules
  content {
    name        = each.key
    description = each.value.description
    priority    = each.value.priority
    direction   = each.value.direction
  }
}</code></pre>
        <p><strong>In Practice:</strong> Makes HCL clean and scalable when working with repetitive configurations.</p>
      </div>`},{question:"How do you secure your Terraform setup?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Securing Terraform  Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use remote backend with encryption (Azure Blob / S3 + KMS).</li>
          <li>Never hardcode credentials  use env vars or service principals.</li>
          <li>Restrict access to state file.</li>
          <li>Rotate secrets & use Terraform Cloud/Enterprise for RBAC.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate Terraform with Key Vault / Secrets Manager for sensitive variables.</p>
      </div>`},{question:"What is drift condition in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Drift  when real infra diverges from state</strong></h3>
        <p>Drift occurs when infrastructure is modified outside of Terraform (e.g., manual portal changes).</p>
        <p>Detected using:</p>
        <pre><code>terraform plan</code></pre>
        <p><strong>In Practice:</strong> Regularly run plans in CI/CD to detect drift early.</p>
      </div>`},{question:"How do you handle state management with multiple people?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Collaborative State Management</strong></h3>
        <p>Store state remotely using a shared backend (e.g., Azure Blob, S3, Terraform Cloud) with locking enabled.</p>
        <pre><code>backend "azurerm" {
  resource_group_name  = "tfstate-rg"
  storage_account_name = "tfstateacc"
  container_name       = "tfstate"
  key                  = "infra.tfstate"
}</code></pre>
        <p><strong>In Practice:</strong> Enables team collaboration, ensures locking, and prevents concurrent apply issues.</p>
      </div>`},{question:"What is Terraform Core?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Core  the engine</strong></h3>
        <p>Terraform Core is the binary that reads configuration, builds dependency graph, compares state, and executes CRUD operations via providers.</p>
        <p><strong>In Practice:</strong> Core is provider-agnostic and handles the entire lifecycle orchestration.</p>
      </div>`}]},{title:"3. Terraform  Configuration Language, State & Variables",questions:[{question:"What is Terraform Configuration Language?",answerHtml:`<div class="answer-rich">
        <h3> <strong>HCL  HashiCorp Configuration Language</strong></h3>
        <p>Terraform uses <strong>HCL (HashiCorp Configuration Language)</strong>  a declarative language optimized for describing infrastructure.</p>
        <ul style="margin-left:1.2rem;">
          <li>Readable and supports JSON as alternate syntax.</li>
          <li>Combines variables, resources, outputs, locals, and expressions.</li>
          <li>Fully supports loops (<code>for_each</code>, <code>count</code>) and conditionals.</li>
        </ul>
        <p><strong>In Practice:</strong> I structure Terraform code with <code>main.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code> for clarity.</p>
      </div>`},{question:"What are Terraform Providers?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Providers  the API bridges</strong></h3>
        <p>Providers act as plugins to communicate with specific platforms or services (AzureRM, AWS, Google, Kubernetes, etc.).</p>
        <pre><code>provider "azurerm" {
  features {}
  subscription_id = var.sub_id
}</code></pre>
        <p><strong>In Practice:</strong> I pin provider versions to avoid breaking updates across environments.</p>
      </div>`},{question:"What are Terraform Modules?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Modules  reusable infra packages</strong></h3>
        <p>Modules organize Terraform code into reusable components.</p>
        <pre><code>module "network" {
  source = "./modules/vnet"
  vnet_name = "prod-vnet"
}</code></pre>
        <p><strong>In Practice:</strong> I use versioned Git modules for shared infra patterns across projects.</p>
      </div>`},{question:"What is Terraform State?",answerHtml:`<div class="answer-rich">
        <h3> <strong>State  Terraforms source of truth</strong></h3>
        <p>The state file (<code>terraform.tfstate</code>) tracks real resource mappings and metadata.</p>
        <ul style="margin-left:1.2rem;">
          <li>Used by Terraform to detect drift and plan changes.</li>
          <li>Can be stored locally or remotely for collaboration.</li>
        </ul>
        <p><strong>In Practice:</strong> I always use remote backends with locking (Azure Blob / S3 + DynamoDB).</p>
      </div>`},{question:"What is the difference between Terraform Core and Provider?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Core vs Provider</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Core:</strong> The engine that reads configs, builds dependency graph, and applies changes.</li>
          <li><strong>Provider:</strong> The plugin that implements CRUD operations for specific platforms.</li>
        </ul>
        <p><strong>In Practice:</strong> Core is provider-agnostic; providers act like drivers for specific clouds.</p>
      </div>`},{question:"What is a .tf file?",answerHtml:`<div class="answer-rich">
        <h3> <strong>.tf Files  configuration units</strong></h3>
        <p><code>.tf</code> files define resources, variables, outputs, and data sources written in HCL.</p>
        <p><strong>In Practice:</strong> I split code into <code>main.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code>, and <code>provider.tf</code> for modularity.</p>
      </div>`},{question:"What is a .tfstate file?",answerHtml:`<div class="answer-rich">
        <h3> <strong>.tfstate  the actual infra snapshot</strong></h3>
        <p>JSON file containing mappings between Terraform config and deployed resources.</p>
        <pre><code>terraform show terraform.tfstate</code></pre>
        <p><strong>In Practice:</strong> I never commit tfstate to Git  always store it remotely and encrypted.</p>
      </div>`},{question:"What are input variables in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Input Variables  dynamic parameters</strong></h3>
        <p>Used to make Terraform configurations flexible and reusable.</p>
        <pre><code>variable "location" {
  type    = string
  default = "East US"
}</code></pre>
        <p>Values are passed via CLI, environment variables, or tfvars files.</p>
      </div>`},{question:"What are output variables in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Output Variables  share deployment results</strong></h3>
        <p>Outputs expose information from Terraform for use in other modules or pipelines.</p>
        <pre><code>output "rg_name" {
  value = azurerm_resource_group.rg.name
}</code></pre>
        <p><strong>In Practice:</strong> I use outputs to pass resource IDs and endpoints to CI/CD jobs.</p>
      </div>`},{question:"What is the use of backend in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Backend  where Terraform stores state</strong></h3>
        <p>Defines how and where Terraform stores its state (local or remote).</p>
        <pre><code>terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstateacc"
    container_name       = "tfstate"
    key                  = "prod.tfstate"
  }
}</code></pre>
        <p><strong>In Practice:</strong> Remote backends enable locking, collaboration, and disaster recovery.</p>
      </div>`}]},{title:"4. Terraform  Resources, Meta-Arguments, Loops & Provisioners",questions:[{question:"What is a resource in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Resource  actual infra components</strong></h3>
        <p>Resources represent the real cloud components (VMs, VNets, Buckets) that Terraform manages via providers.</p>
        <pre><code>resource "azurerm_virtual_network" "vnet" {
  name                = "prod-vnet"
  address_space       = ["10.0.0.0/16"]
  location            = var.location
  resource_group_name = var.rg_name
}</code></pre>
        <p><strong>In Practice:</strong> Each resource maps to an API call that Terraform tracks in the state for lifecycle operations.</p>
      </div>`},{question:"What are meta-arguments in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Meta-Arguments  control how resources behave</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>depends_on</strong>  define explicit dependency.</li>
          <li><strong>count</strong> / <strong>for_each</strong>  loop resources.</li>
          <li><strong>provider</strong>  use a specific provider config.</li>
          <li><strong>lifecycle</strong>  customize creation/deletion rules.</li>
        </ul>
        <p><strong>In Practice:</strong> I use these to control infra flow and avoid race conditions in resource creation.</p>
      </div>`},{question:"What is the lifecycle block in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Lifecycle Block  fine-tune resource behavior</strong></h3>
        <p>Defines how Terraform should handle create/update/delete actions.</p>
        <pre><code>lifecycle {
  prevent_destroy = true
  ignore_changes  = [tags]
}</code></pre>
        <p><strong>In Practice:</strong> I enable <code>prevent_destroy</code> for production databases or core networking components.</p>
      </div>`},{question:"What is the purpose of depends_on in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>depends_on  enforce resource ordering</strong></h3>
        <p>Explicitly declares dependency between resources to ensure Terraform creates them in the correct order.</p>
        <pre><code>resource "azurerm_subnet" "subnet" {
  depends_on = [azurerm_virtual_network.vnet]
}</code></pre>
        <p><strong>In Practice:</strong> Useful when Terraform cant automatically infer dependency (e.g., role assignments or custom scripts).</p>
      </div>`},{question:"What is the function of count and for_each in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>count & for_each  loops for resource creation</strong></h3>
        <p><strong>count:</strong> Used for simple numeric iterations.</p>
        <pre><code>resource "azurerm_storage_account" "sa" {
  count = 3
  name  = "st&dollar;{count.index}"
}</code></pre>
        <p><strong>for_each:</strong> Used for map/object-based dynamic deployments.</p>
        <pre><code>for_each = var.vm_config
name     = each.key
size     = each.value.size</code></pre>
        <p><strong>In Practice:</strong> I prefer <code>for_each</code> for environment-based resource provisioning.</p>
      </div>`},{question:"What is the use of locals in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Locals  reusable computed values</strong></h3>
        <p>Locals store intermediate values or computed expressions to avoid repetition.</p>
        <pre><code>locals {
  common_tags = {
    env  = var.env
    team = "DevOps"
  }
}</code></pre>
        <p><strong>In Practice:</strong> I use locals for consistent tagging and resource naming conventions.</p>
      </div>`},{question:"What are provisioners in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Provisioners  execute scripts post-deployment</strong></h3>
        <p>Provisioners run commands or scripts on resources during creation or destruction.</p>
        <pre><code>provisioner "remote-exec" {
  inline = ["sudo apt update", "sudo apt install nginx -y"]
}</code></pre>
        <p><strong>In Practice:</strong> I only use provisioners for last-mile setup when configuration management tools arent available.</p>
      </div>`},{question:"What is the use of data block in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Data Blocks  read existing resources</strong></h3>
        <p>Used to fetch and reference existing infrastructure details.</p>
        <pre><code>data "azurerm_resource_group" "shared" {
  name = "core-rg"
}</code></pre>
        <p><strong>In Practice:</strong> I use data blocks to reference shared VNet, subnets, or Key Vaults without creating duplicates.</p>
      </div>`},{question:"What is the use of Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform  automate infra lifecycle</strong></h3>
        <p>Terraform simplifies infrastructure provisioning, scaling, and management using code.</p>
        <ul style="margin-left:1.2rem;">
          <li>Ensures consistency across environments.</li>
          <li>Automates infra creation using CI/CD.</li>
          <li>Supports multi-cloud deployments.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Terraform in pipelines to auto-provision environments on pull request merges.</p>
      </div>`},{question:"What are Terraform Meta Arguments overall purpose?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Meta Arguments  execution control switches</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>count / for_each</code>  Loop over resources.</li>
          <li><code>depends_on</code>  Explicit dependency order.</li>
          <li><code>provider</code>  Multi-provider management.</li>
          <li><code>lifecycle</code>  Control update/destroy behavior.</li>
        </ul>
        <p><strong>In Practice:</strong> These give full control over how Terraform executes and manages dependencies.</p>
      </div>`}]},{title:"5. Terraform  Installation & Setup",questions:[{question:"How to install Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Installing Terraform  CLI setup</strong></h3>
        <p>Terraform can be installed using OS package managers or by downloading the binary from HashiCorp releases.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Windows:</strong> Download ZIP  extract  add to PATH.</li>
          <li><strong>macOS:</strong> <code>brew tap hashicorp/tap &amp;&amp; brew install hashicorp/tap/terraform</code></li>
          <li><strong>Linux:</strong> <code>wget https://releases.hashicorp.com/terraform/&lt;version&gt;/terraform_&lt;version&gt;_linux_amd64.zip</code></li>
        </ul>
        <p><strong>In Practice:</strong> I use Terraform via CI/CD runners (Azure DevOps, GitHub Actions) using pre-installed Terraform tasks or container images.</p>
      </div>`},{question:"What is the latest version of Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Versioning</strong></h3>
        <p>The latest stable version can be checked on <a href="https://developer.hashicorp.com/terraform/downloads" target="_blank">Terraform Downloads</a>.</p>
        <p>As of 2025, its typically around <strong>v1.9.x</strong> (updated frequently).</p>
        <p><strong>In Practice:</strong> Always pin provider and Terraform versions in code to avoid breaking changes.</p>
      </div>`},{question:"Which command is used to check Terraform version?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Check Terraform version</strong></h3>
        <p>Use the following CLI command to verify installed Terraform version:</p>
        <pre><code>terraform version</code></pre>
        <p>Shows Terraform binary and provider plugin versions.</p>
        <p><strong>In Practice:</strong> I include version check in CI pipeline pre-steps to ensure environment consistency.</p>
      </div>`},{question:"How do you upgrade Terraform to the latest version?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Upgrading Terraform</strong></h3>
        <p>Upgrade steps depend on OS:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Windows:</strong> Replace old binary in PATH with the new one.</li>
          <li><strong>macOS/Linux:</strong> <code>brew upgrade hashicorp/tap/terraform</code> or re-download binary.</li>
        </ul>
        <p>Post upgrade, run:</p>
        <pre><code>terraform -v</code></pre>
        <p><strong>In Practice:</strong> Always test upgrade in non-prod first and check provider compatibility using <code>terraform init -upgrade</code>.</p>
      </div>`},{question:"Which file is created after running terraform init?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform init  generates .terraform directory</strong></h3>
        <p>Running <code>terraform init</code> creates a <strong>.terraform</strong> directory that stores provider plugins and backend metadata.</p>
        <ul style="margin-left:1.2rem;">
          <li>Downloads providers & modules.</li>
          <li>Initializes backend connection.</li>
          <li>Prepares working directory for execution.</li>
        </ul>
        <pre><code>terraform init</code></pre>
        <p><strong>In Practice:</strong> I commit a <code>.terraform.lock.hcl</code> file to lock provider versions.</p>
      </div>`},{question:"What is the terraform block?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform {} block  core configuration</strong></h3>
        <p>The <code>terraform</code> block defines backend, required providers, and Terraform version constraints.</p>
        <pre><code>terraform {
  required_version = ">= 1.8.0"
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.100"
    }
  }
}</code></pre>
        <p><strong>In Practice:</strong> I always define version constraints to avoid unexpected provider updates.</p>
      </div>`},{question:"What is the syntax of provider block?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Provider Block  authentication & setup</strong></h3>
        <p>Defines connection details and authentication method for the target platform.</p>
        <pre><code>provider "azurerm" {
  features {}
  subscription_id = var.subscription_id
  tenant_id       = var.tenant_id
  client_id       = var.client_id
  client_secret   = var.client_secret
}</code></pre>
        <p><strong>In Practice:</strong> I prefer using environment variables or service principals instead of hardcoded secrets.</p>
      </div>`}]},{title:"6. Terraform  Workflow (Init, Plan, Apply, Destroy, etc.)",questions:[{question:"What is the use of terraform init?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform init  initialize working directory</strong></h3>
        <p>Initializes a Terraform working directory, downloads provider plugins, and configures backend for state storage.</p>
        <pre><code>terraform init</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Downloads provider binaries.</li>
          <li>Initializes backend (like AzureRM, S3, etc.).</li>
          <li>Prepares module dependencies.</li>
        </ul>
        <p><strong>In Practice:</strong> I always run this first in CI pipelines to ensure backend & providers are properly synced.</p>
      </div>`},{question:"What is the use of terraform init -upgrade?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform init -upgrade  refresh provider versions</strong></h3>
        <p>Forces Terraform to check the registry for newer versions of providers or modules defined in configuration.</p>
        <pre><code>terraform init -upgrade</code></pre>
        <p><strong>Use Case:</strong> When upgrading to new provider releases or changing required_version in terraform block.</p>
        <p><strong>In Practice:</strong> I use it in staging before production rollout to validate provider compatibility.</p>
      </div>`},{question:"What is the use of terraform plan?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform plan  preview infrastructure changes</strong></h3>
        <p>Generates an execution plan showing what Terraform will create, update, or destroy.</p>
        <pre><code>terraform plan</code></pre>
        <p><strong>Output:</strong> Additions (+), Changes (~), Deletions (-)</p>
        <p><strong>In Practice:</strong> I review plan output in PRs to avoid accidental resource deletion or misconfigurations.</p>
      </div>`},{question:"What is the use of terraform plan -out=<fileName>?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform plan -out=plan.tfplan  save plan for later apply</strong></h3>
        <p>Stores the execution plan in a binary file that can be safely used later for apply step.</p>
        <pre><code>terraform plan -out=tfplan</code></pre>
        <p><strong>In Practice:</strong> I use this in pipelines to separate planning and apply stages (CI/CD safety measure).</p>
      </div>`},{question:"What is the use of terraform apply?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform apply  execute changes</strong></h3>
        <p>Applies the planned changes to reach desired state defined in .tf files.</p>
        <pre><code>terraform apply</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Creates, modifies, or deletes resources as per plan.</li>
          <li>Asks for manual confirmation unless <code>-auto-approve</code> is used.</li>
        </ul>
        <p><strong>In Practice:</strong> I run this with <code>-auto-approve</code> only in automated pipelines.</p>
      </div>`},{question:"What is the use of terraform apply <filename>?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform apply tfplan  apply from saved plan</strong></h3>
        <p>Applies a previously generated plan file created using <code>terraform plan -out</code>.</p>
        <pre><code>terraform apply tfplan</code></pre>
        <p><strong>In Practice:</strong> Ensures that only the reviewed plan is applied  avoids runtime configuration drift in CI/CD.</p>
      </div>`},{question:"What is the use of terraform destroy?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform destroy  delete all managed infrastructure</strong></h3>
        <p>Destroys all resources defined in Terraform configuration files.</p>
        <pre><code>terraform destroy</code></pre>
        <p><strong>In Practice:</strong> Used in non-prod environments or cleanup stages after temporary infra testing.</p>
      </div>`},{question:"What is the use of terraform validate?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform validate  syntax and logic check</strong></h3>
        <p>Validates Terraform configuration files for syntax errors and structural correctness before running plan.</p>
        <pre><code>terraform validate</code></pre>
        <p><strong>In Practice:</strong> I run this in pre-commit hook or as a pipeline step before plan stage.</p>
      </div>`},{question:"What is the use of terraform fmt?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform fmt  auto-format configuration files</strong></h3>
        <p>Automatically formats all .tf files to follow HashiCorp style conventions.</p>
        <pre><code>terraform fmt -recursive</code></pre>
        <p><strong>In Practice:</strong> Keeps code consistent across teams and avoids merge conflicts due to spacing differences.</p>
      </div>`},{question:"What is the use of terraform show?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform show  view current state or plan</strong></h3>
        <p>Displays the details of the current Terraform state or a saved plan in human-readable or JSON format.</p>
        <pre><code>terraform show
terraform show -json > state.json</code></pre>
        <p><strong>In Practice:</strong> Useful for auditing current infra or debugging state data.</p>
      </div>`},{question:"What is the use of terraform output?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform output  view defined outputs</strong></h3>
        <p>Displays values from output blocks of your configuration.</p>
        <pre><code>terraform output
terraform output resource_id</code></pre>
        <p><strong>In Practice:</strong> I use outputs in pipelines to pass dynamic data (like IPs, IDs) to next steps.</p>
      </div>`},{question:"What is the use of terraform taint?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform taint  mark resource for recreation</strong></h3>
        <p>Marks a resource for forced recreation on next apply.</p>
        <pre><code>terraform taint azurerm_virtual_machine.vm1</code></pre>
        <p><strong>In Practice:</strong> Used to rebuild faulty or misconfigured resources without deleting the entire infra.</p>
      </div>`},{question:"What is the use of terraform untaint?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform untaint  remove taint mark</strong></h3>
        <p>Removes taint from a resource so that it wont be recreated in the next apply.</p>
        <pre><code>terraform untaint azurerm_virtual_machine.vm1</code></pre>
        <p><strong>In Practice:</strong> I use it to reverse accidental tainting in shared environments.</p>
      </div>`},{question:"What is the use of terraform import?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform import  bring existing infra under Terraform management</strong></h3>
        <p>Imports existing resources from cloud into Terraform state without re-creating them.</p>
        <pre><code>terraform import azurerm_resource_group.rg /subscriptions/{id}/resourceGroups/myRG</code></pre>
        <p><strong>In Practice:</strong> I use it while onboarding legacy infrastructure into IaC pipelines.</p>
      </div>`},{question:"What is the use of terraform graph?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform graph  visualize resource dependencies</strong></h3>
        <p>Generates a visual representation of resource dependency graph.</p>
        <pre><code>terraform graph | dot -Tpng > graph.png</code></pre>
        <p><strong>In Practice:</strong> Helpful in understanding complex module relationships in large projects.</p>
      </div>`},{question:"What is the use of terraform state?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform state  manage state file entries</strong></h3>
        <p>Used to inspect, list, move, or remove resources in the Terraform state.</p>
        <pre><code>terraform state list
terraform state show azurerm_resource_group.rg</code></pre>
        <p><strong>In Practice:</strong> I use it for debugging or manually syncing state when resources are renamed.</p>
      </div>`},{question:"What is the use of terraform workspace?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform workspace  manage multiple environment states</strong></h3>
        <p>Allows creation of isolated state files for different environments (dev, test, prod).</p>
        <pre><code>terraform workspace new dev
terraform workspace select prod</code></pre>
        <p><strong>In Practice:</strong> I use workspaces for small setups; for large orgs, prefer remote backend separation.</p>
      </div>`}]},{title:"7. Terraform  Variables, tfvars & Type Handling",questions:[{question:"What is variable.tf and terraform.tfvars? Difference?",answerHtml:`<div class="answer-rich">
        <h3> <strong>variable.tf vs terraform.tfvars</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>variable.tf:</strong> Defines variable declarations (name, type, description, default).</li>
          <li><strong>terraform.tfvars:</strong> Holds actual values for declared variables.</li>
        </ul>
        <pre><code>// variable.tf
variable "region" { type = string }

// terraform.tfvars
region = "eastus"</code></pre>
        <p><strong>In Practice:</strong> I keep variable definitions in <code>variables.tf</code> and environment-specific values in <code>*.tfvars</code> files.</p>
      </div>`},{question:"What is difference between variable.tf and variables.tf?",answerHtml:`<div class="answer-rich">
        <h3> <strong>variable.tf vs variables.tf  naming convention</strong></h3>
        <p>No functional difference. Terraform loads all <code>*.tf</code> files in the same directory. Naming is purely for organization.</p>
        <p><strong>In Practice:</strong> I use <code>variables.tf</code> for clarity since it usually holds multiple variable definitions.</p>
      </div>`},{question:"How to save sensitive data in state file?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Securing sensitive data in state</strong></h3>
        <p>Terraform state may still contain sensitive data in plaintext. To protect it:</p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>remote backend</strong> (Azure Blob, S3, Terraform Cloud).</li>
          <li>Enable <strong>encryption at rest</strong> (Storage-level encryption).</li>
          <li>Avoid exposing sensitive outputs.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Azure Blob backend with SSE and limited RBAC access to secure state.</p>
      </div>`},{question:"If you mark a variable as sensitive, how will you see it in terraform state?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Sensitive variables in state</strong></h3>
        <p>Even if a variable is marked <code>sensitive = true</code>, Terraform still stores its value in the state file (for idempotency).</p>
        <p>It only hides the value from CLI and plan output.</p>
        <p><strong>In Practice:</strong> To view, use <code>terraform state show</code> carefully or secure backend access strictly.</p>
      </div>`},{question:"How do you provide default value to a variable?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Setting default values</strong></h3>
        <pre><code>variable "region" {
  type    = string
  default = "eastus"
}</code></pre>
        <p><strong>In Practice:</strong> I use defaults for non-sensitive, common values to reduce pipeline input complexity.</p>
      </div>`},{question:"What are type constraints in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Type constraints  enforce input types</strong></h3>
        <p>Used to restrict input variable types (string, list, map, object, bool, number).</p>
        <pre><code>variable "tags" {
  type = map(string)
}</code></pre>
        <p><strong>In Practice:</strong> Helps catch input errors early and enforce input contracts in modules.</p>
      </div>`},{question:"Difference between list and map?",answerHtml:`<div class="answer-rich">
        <h3> <strong>list vs map</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>List:</strong> Ordered collection (indexed numerically).</li>
          <li><strong>Map:</strong> Key-value pairs (unordered).</li>
        </ul>
        <pre><code>list = ["dev", "test"]
map  = { env = "dev", region = "eastus" }</code></pre>
        <p><strong>In Practice:</strong> Lists for ordered items (like subnets), maps for tagging or configurations.</p>
      </div>`},{question:"What are the data types in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform data types</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>string</li>
          <li>number</li>
          <li>bool</li>
          <li>list(type)</li>
          <li>map(type)</li>
          <li>object({})</li>
          <li>tuple([types])</li>
        </ul>
        <p><strong>In Practice:</strong> I use objects for structured variables (e.g., VM config with name, size, tags).</p>
      </div>`},{question:"If you dont want to hardcode IP in main.tf, how will you handle that?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Avoid hardcoding by using variables or data sources</strong></h3>
        <p>Options:</p>
        <ul style="margin-left:1.2rem;">
          <li>Declare variable  define in <code>terraform.tfvars</code>.</li>
          <li>Fetch dynamically via <code>data</code> block (e.g., existing network).</li>
        </ul>
        <pre><code>variable "vm_ip" {}
// terraform.tfvars
vm_ip = "10.0.0.5"</code></pre>
        <p><strong>In Practice:</strong> I prefer to fetch IPs dynamically using data sources or remote state outputs.</p>
      </div>`},{question:"How do you manage secrets or sensitive variables in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Managing sensitive variables securely</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>environment variables</strong> (TF_VAR_ prefix).</li>
          <li>Integrate with <strong>Azure Key Vault / AWS Secrets Manager</strong>.</li>
          <li>Mark variables <code>sensitive = true</code>.</li>
          <li>Avoid committing .tfvars with secrets to git.</li>
        </ul>
        <p><strong>In Practice:</strong> I inject secrets into CI pipelines via secured variable groups linked to Vault.</p>
      </div>`},{question:"What are variable types in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Variable types</strong></h3>
        <p>Terraform supports primitive, complex, and structural types:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Primitive:</strong> string, number, bool</li>
          <li><strong>Complex:</strong> list, map, set</li>
          <li><strong>Structural:</strong> object, tuple</li>
        </ul>
      </div>`},{question:"What is the precedence of variable values?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Variable precedence order (lowest  highest)</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Default value in variable declaration</li>
          <li>terraform.tfvars or *.auto.tfvars</li>
          <li>Environment variables (TF_VAR_)</li>
          <li>CLI flag (<code>-var</code>)</li>
          <li>Terraform Cloud / workspace variables</li>
        </ol>
        <p><strong>In Practice:</strong> I prefer using tfvars for env configs and CLI vars for overrides in CI/CD.</p>
      </div>`},{question:"How to declare variables in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Declaring variables</strong></h3>
        <pre><code>variable "region" {
  description = "Azure region"
  type        = string
  default     = "eastus"
}</code></pre>
        <p><strong>In Practice:</strong> Always include descriptions for clarity in large projects.</p>
      </div>`},{question:"What is the default value in variable declaration?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Default value</strong></h3>
        <p>When no value is provided, Terraform uses the <code>default</code> value from the variable block.</p>
        <pre><code>variable "env" {
  default = "dev"
}</code></pre>
        <p><strong>In Practice:</strong> Useful for simplifying non-prod deployments.</p>
      </div>`},{question:"What is the difference between var.name and local.name?",answerHtml:`<div class="answer-rich">
        <h3> <strong>var.name vs local.name</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>var.name:</strong> References an input variable.</li>
          <li><strong>local.name:</strong> Refers to a computed or derived local value inside locals block.</li>
        </ul>
        <pre><code>locals {
  full_name = "&dollar;{var.env}-app"
}</code></pre>
        <p><strong>In Practice:</strong> I use locals for computed naming conventions or reusing expressions.</p>
      </div>`},{question:"What does sensitive = true mean?",answerHtml:`<div class="answer-rich">
        <h3> <strong>sensitive = true  hide output or input</strong></h3>
        <p>Prevents Terraform from displaying variable or output value in CLI logs or plan output.</p>
        <pre><code>variable "password" {
  type      = string
  sensitive = true
}</code></pre>
        <p><strong>In Practice:</strong> Protects secrets in CI logs but does not encrypt state.</p>
      </div>`},{question:"How to define environment variables for Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Defining environment variables</strong></h3>
        <p>Prefix variable names with <code>TF_VAR_</code> so Terraform can auto-load them.</p>
        <pre><code>export TF_VAR_region=eastus
terraform apply</code></pre>
        <p><strong>In Practice:</strong> Useful for injecting values securely in pipelines.</p>
      </div>`},{question:"What are the different ways to assign values to variables?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Ways to assign variable values</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Default in variable block</li>
          <li>terraform.tfvars or *.auto.tfvars</li>
          <li>Command line: <code>-var</code> or <code>-var-file</code></li>
          <li>Environment variables (TF_VAR_)</li>
          <li>Terraform Cloud workspace variables</li>
        </ol>
        <p><strong>In Practice:</strong> I use tfvars for static configs, CLI vars for dynamic pipeline overrides.</p>
      </div>`},{question:"What is the difference between input variables and output values?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Input vs Output</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Input variables:</strong> Take values from user/environment into Terraform.</li>
          <li><strong>Output values:</strong> Expose data from Terraform to external systems or modules.</li>
        </ul>
        <p><strong>In Practice:</strong> Inputs = incoming configs, Outputs = results shared to pipeline or modules.</p>
      </div>`},{question:"How to pass variables from CLI?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Passing variables from CLI</strong></h3>
        <p>Use <code>-var</code> flag or <code>-var-file</code> option.</p>
        <pre><code>terraform apply -var="region=eastus"
terraform apply -var-file="prod.tfvars"</code></pre>
        <p><strong>In Practice:</strong> I use <code>-var-file</code> for environment segregation and automation consistency.</p>
      </div>`}]},{title:"8. Terraform  State File & Backend",questions:[{question:"What is state file and where do we store it?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform State File</strong></h3>
        <p>The state file (<code>terraform.tfstate</code>) tracks the mapping between Terraform resources and real-world infrastructure.</p>
        <p><strong>Storage:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Local backend:</strong> Stored in your working directory.</li>
          <li><strong>Remote backend:</strong> Stored in remote storage (S3, Azure Blob, Terraform Cloud, etc.)</li>
        </ul>
        <p><strong>In Practice:</strong> Always store state remotely to enable collaboration and prevent data loss.</p>
      </div>`},{question:"When will the state file be created?",answerHtml:`<div class="answer-rich">
        <h3> <strong>State creation timing</strong></h3>
        <p>Terraform creates the state file after the first successful <code>terraform apply</code>.</p>
        <p><strong>In Practice:</strong> Before apply, plan just simulates changes  no state file exists until resources are actually provisioned.</p>
      </div>`},{question:"After which command is the state file created?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Command that creates the state file</strong></h3>
        <p>After <code>terraform apply</code>  once resources are deployed successfully.</p>
        <p><strong>Note:</strong> The <code>terraform plan</code> command only reads configuration; it doesnt modify or create state.</p>
      </div>`},{question:"How do you encrypt the state file?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Encrypting the state file</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Local backend:</strong> Use OS-level encryption (BitLocker, FileVault).</li>
          <li><strong>Cloud backend:</strong> Use backend encryption options:
            <ul>
              <li><strong>S3:</strong> Enable SSE (Server-Side Encryption) or KMS.</li>
              <li><strong>Azure Blob:</strong> Enable Storage Service Encryption.</li>
              <li><strong>Terraform Cloud:</strong> Encrypted by default.</li>
            </ul>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I rely on Azure Blob + SSE with role-based access to protect tfstate.</p>
      </div>`},{question:"What is statefile management?",answerHtml:`<div class="answer-rich">
        <h3> <strong>State file management</strong></h3>
        <p>It means maintaining, securing, and organizing the Terraform state file to ensure consistency and collaboration.</p>
        <ul style="margin-left:1.2rem;">
          <li>Using remote backend</li>
          <li>Versioning and backups</li>
          <li>Locking mechanism</li>
          <li>Controlled access (RBAC)</li>
        </ul>
        <p><strong>In Practice:</strong> I use versioned blob containers to roll back accidental state corruption.</p>
      </div>`},{question:"What is state locking in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>State locking</strong></h3>
        <p>Prevents concurrent modifications to state file during Terraform operations.</p>
        <p><strong>Example:</strong> Two people running <code>terraform apply</code> at the same time  lock ensures only one runs.</p>
        <p><strong>In Practice:</strong> Remote backends like S3 + DynamoDB or Terraform Cloud provide automatic locking.</p>
      </div>`},{question:"What is a lock file and why is it created?",answerHtml:`<div class="answer-rich">
        <h3> <strong>.terraform.lock.hcl  Dependency Lock File</strong></h3>
        <p>This file locks provider versions to ensure consistent builds across environments.</p>
        <ul style="margin-left:1.2rem;">
          <li>Auto-created after <code>terraform init</code>.</li>
          <li>Contains provider version & checksum details.</li>
        </ul>
        <p><strong>In Practice:</strong> Commit it to version control so team members use the same provider versions.</p>
      </div>`},{question:"What are the different backends youve used?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Common Terraform Backends</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Local</li>
          <li>Azure Blob Storage</li>
          <li>AWS S3 + DynamoDB (for locking)</li>
          <li>GCS (Google Cloud Storage)</li>
          <li>Terraform Cloud / Enterprise</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer remote backends with state locking and versioning enabled for team setups.</p>
      </div>`},{question:"What happens if local tfstate is deleted accidentally?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Accidental state deletion</strong></h3>
        <p>If deleted and not stored remotely  Terraform loses track of all infrastructure.</p>
        <p><strong>In Practice:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>If resources exist in cloud  Terraform will try to recreate them (duplicate issue).</li>
          <li>Recovery possible only if backup or remote backend exists.</li>
        </ul>
        <p><strong>Best Practice:</strong> Always use remote backend with versioning.</p>
      </div>`},{question:"How do you recover a lost local state file?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Recovering lost state file</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Recover from <strong>VCS / backup</strong> if committed.</li>
          <li>Use <code>terraform import</code> to re-import resources manually.</li>
          <li>If using cloud backend  restore from <strong>version history</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> I once used Azure Blobs version restore to recover accidentally deleted tfstate.</p>
      </div>`},{question:"How do you manage multiple state files in team environments?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Managing multiple state files</strong></h3>
        <p>By using <strong>workspaces</strong> or <strong>separate backend configurations</strong> per environment.</p>
        <ul style="margin-left:1.2rem;">
          <li>dev, test, prod each with its own state file.</li>
          <li>Backend key parameter differentiates state locations.</li>
        </ul>
        <pre><code>key = "terraform/state/prod.tfstate"</code></pre>
        <p><strong>In Practice:</strong> Helps isolate environments and avoid cross-deployment impact.</p>
      </div>`},{question:"What if 150 people use the same backend  how do you identify changes?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Tracking changes in shared backend</strong></h3>
        <p>Use remote backend that provides:</p>
        <ul style="margin-left:1.2rem;">
          <li>Audit logging (e.g., Terraform Cloud, S3 Access Logs).</li>
          <li>State versioning for change diffs.</li>
          <li>Access control (who can <code>apply</code> vs <code>plan</code>).</li>
        </ul>
        <p><strong>In Practice:</strong> Terraform Cloud records who applied each run and the exact diff  perfect for large teams.</p>
      </div>`}]},{title:"9. Terraform  Provisioners & Lifecycle",questions:[{question:"What is a provisioner in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Provisioner  Post-deployment automation</strong></h3>
        <p>Provisioners let you execute scripts or commands on resources after they're created or destroyed  often used for bootstrapping or configuration.</p>
        <p><strong>Example:</strong> Installing packages or configuring an app after VM deployment.</p>
        <pre><code>provisioner "remote-exec" {
  inline = ["sudo apt update", "sudo apt install nginx -y"]
}</code></pre>
        <p><strong>In Practice:</strong> I use provisioners only when config management tools (like Ansible) are not available.</p>
      </div>`},{question:"What is file provisioner?",answerHtml:`<div class="answer-rich">
        <h3> <strong>File Provisioner  Transfer files to VM</strong></h3>
        <p>Used to copy files or directories from local machine to the target VM.</p>
        <pre><code>provisioner "file" {
  source      = "install.ps1"
  destination = "C:/temp/install.ps1"
}</code></pre>
        <p><strong>In Practice:</strong> Handy for sending scripts or configs during bootstrap of Windows/Linux VMs.</p>
      </div>`},{question:"What is a null resource?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Null Resource  Generic execution placeholder</strong></h3>
        <p>It doesnt create cloud resources but allows you to trigger scripts or commands conditionally.</p>
        <pre><code>resource "null_resource" "postconfig" {
  triggers = { build_id = var.build_id }
  provisioner "local-exec" {
    command = "echo Deploying build &dollar;{var.build_id}"
  }
}</code></pre>
        <p><strong>In Practice:</strong> I use null_resource for CI/CD hooks or tagging existing infra.</p>
      </div>`},{question:"What is a lifecycle block in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Lifecycle block  Resource behavior control</strong></h3>
        <p>Defines how Terraform should treat a resource during plan/apply operations.</p>
        <pre><code>lifecycle {
  prevent_destroy = true
  ignore_changes  = [tags]
}</code></pre>
        <p><strong>In Practice:</strong> I use it to prevent accidental deletion of critical infra like prod VMs or databases.</p>
      </div>`},{question:"What arguments can be used in lifecycle block?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Lifecycle arguments</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>create_before_destroy</strong>  create new before deleting old</li>
          <li><strong>prevent_destroy</strong>  block accidental delete</li>
          <li><strong>ignore_changes</strong>  skip drift on specified attributes</li>
        </ul>
        <p><strong>In Practice:</strong> ignore_changes is useful when an external process modifies tags or IPs.</p>
      </div>`},{question:"Which provisioner is used to join a VM to a domain using PowerShell?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Remote-exec with PowerShell</strong></h3>
        <p>Use the <strong>remote-exec</strong> provisioner with a PowerShell command to join domain post VM creation.</p>
        <pre><code>provisioner "remote-exec" {
  inline = ["Add-Computer -DomainName corp.local -Credential (Get-Credential) -Restart"]
}</code></pre>
        <p><strong>In Practice:</strong> Common in hybrid infra where AD join is part of post-deploy automation.</p>
      </div>`},{question:"What happens if you manually install MS Office and re-run terraform apply?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Manual changes vs Terraform apply</strong></h3>
        <p>Terraform manages only defined resources. Manual installations (like MS Office) arent tracked  so <code>terraform apply</code> wont affect them unless managed explicitly.</p>
        <p><strong>In Practice:</strong> Manual drift detection possible via <code>terraform plan</code>  shows differences if state and infra diverge.</p>
      </div>`},{question:"What is a provisioner block  how is it used with VM deployment?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Provisioner block in VM resource</strong></h3>
        <p>Attached inside a resource (like <code>azurerm_linux_virtual_machine</code>) to run post-deployment commands.</p>
        <pre><code>resource "azurerm_linux_virtual_machine" "vm" {
  name     = "webvm01"
  ...
  provisioner "remote-exec" {
    inline = ["sudo apt install nginx -y"]
  }
}</code></pre>
        <p><strong>In Practice:</strong> Used for quick bootstrap or post-provision setup.</p>
      </div>`},{question:"What is the use of remote-exec?",answerHtml:`<div class="answer-rich">
        <h3> <strong>remote-exec  Execute commands on remote VM</strong></h3>
        <p>Runs shell or PowerShell commands on a VM after creation via SSH or WinRM.</p>
        <pre><code>provisioner "remote-exec" {
  inline = ["sudo apt update", "sudo apt install nginx -y"]
}</code></pre>
        <p><strong>In Practice:</strong> Useful for provisioning scripts or service restarts post-deployment.</p>
      </div>`},{question:"What is the use of local-exec?",answerHtml:`<div class="answer-rich">
        <h3> <strong>local-exec  Execute command locally</strong></h3>
        <p>Runs commands on the local machine (where Terraform runs), not on the remote target.</p>
        <pre><code>provisioner "local-exec" {
  command = "echo Deployment complete &gt;&gt; deploy.log"
}</code></pre>
        <p><strong>In Practice:</strong> I use it for triggering external pipelines, sending notifications, or executing scripts locally after apply.</p>
      </div>`},{question:"When to use provisioners?",answerHtml:`<div class="answer-rich">
        <h3> <strong>When to use provisioners</strong></h3>
        <p>Use them only when configuration cant be handled by cloud-init, userdata, or external tools.</p>
        <ul style="margin-left:1.2rem;">
          <li>Bootstrap setup (e.g., install agent or software)</li>
          <li>Post-deploy orchestration</li>
          <li>Triggering CI/CD hooks</li>
        </ul>
        <p><strong>In Practice:</strong> Keep provisioners minimal  use Ansible or DSC for heavy configuration.</p>
      </div>`},{question:"What are the drawbacks of using provisioners?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Drawbacks of provisioners</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Not idempotent  re-runs can cause errors.</li>
          <li>Dependency on SSH/WinRM connectivity.</li>
          <li>Difficult to debug failures.</li>
          <li>Can break declarative model of Terraform.</li>
        </ul>
        <p><strong>In Practice:</strong> Use provisioners only as a last resort, not for core infrastructure logic.</p>
      </div>`},{question:"What is a null resource and how is it used?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Null resource usage</strong></h3>
        <p>Acts as a trigger for local or remote actions without managing real infrastructure.</p>
        <pre><code>resource "null_resource" "deploy" {
  provisioner "local-exec" {
    command = "az vm start myVM"
  }
}</code></pre>
        <p><strong>In Practice:</strong> Great for running custom commands, calling APIs, or integrating with CI/CD.</p>
      </div>`},{question:"How do you automate post-deployment configuration with provisioners?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Automating post-deployment steps</strong></h3>
        <p>Attach <strong>remote-exec</strong> or <strong>file</strong> provisioners to VM resource blocks to handle configuration after deployment.</p>
        <pre><code>provisioner "remote-exec" {
  inline = ["sudo systemctl enable nginx", "sudo systemctl start nginx"]
}</code></pre>
        <p><strong>In Practice:</strong> I integrate post-provision steps (agents, monitoring, tagging) using remote-exec inside VM blocks.</p>
      </div>`}]},{title:"10. Terraform  Modules (Child, Root, Registry, Reuse) & Patterns",questions:[{question:"What is a module in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Module  Logical grouping of resources</strong></h3>
        <p>A <strong>module</strong> is a container for multiple resources that are used together. It helps structure, reuse, and organize Terraform configurations.</p>
        <p><strong>In Practice:</strong> Every Terraform configuration is a module  even the root one.</p>
      </div>`},{question:"What is the difference between child module and root module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Root vs Child Modules</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Root module:</strong> The main Terraform configuration directory where you run commands (contains main.tf, variables.tf, outputs.tf).</li>
          <li><strong>Child module:</strong> A reusable submodule called from the root or another module.</li>
        </ul>
        <p><strong>In Practice:</strong> I use root modules for environment orchestration (dev/prod) and child modules for reusable infra units (VNet, VM, Storage).</p>
      </div>`},{question:"What is a pattern module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Pattern module  Standardized reusable module</strong></h3>
        <p>A pattern module encapsulates best practices and pre-approved configurations for a specific resource type, promoting consistency across environments.</p>
        <p><strong>Example:</strong> A networking pattern module with VNet, subnets, and NSGs bundled.</p>
        <p><strong>In Practice:</strong> Pattern modules reduce rework and ensure compliance in large orgs.</p>
      </div>`},{question:"Difference between pattern module and root-child module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Pattern vs Root-Child Modules</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Root-child:</strong> Structural hierarchy for one deployment.</li>
          <li><strong>Pattern module:</strong> Organization-level reusable module template (generic + versioned).</li>
        </ul>
        <p><strong>In Practice:</strong> I build pattern modules once, then call them in root modules per environment.</p>
      </div>`},{question:"What is resource and pattern module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Resource module vs Pattern module</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Resource module:</strong> Manages a single resource type (e.g., storage, VM).</li>
          <li><strong>Pattern module:</strong> Combines multiple resource modules into a design pattern (e.g., full app environment).</li>
        </ul>
        <p><strong>In Practice:</strong> Resource modules = building blocks, pattern modules = solutions.</p>
      </div>`},{question:"Can you explain your module structure?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Typical Terraform Module Structure</strong></h3>
        <pre><code>modules/
  network/
     main.tf
     variables.tf
     outputs.tf
  vm/
     main.tf
     variables.tf
     outputs.tf
envs/
  dev/
     main.tf
     terraform.tfvars
  prod/
      main.tf
      terraform.tfvars</code></pre>
        <p><strong>In Practice:</strong> Keeps code DRY (Dont Repeat Yourself) and enables modular CI/CD pipelines.</p>
      </div>`},{question:"How did you upgrade your Terraform template recently?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Upgrading Terraform templates</strong></h3>
        <p>I used <strong>terraform 0.13upgrade</strong> and <strong>terraform validate</strong> to refactor old syntax, followed by provider version updates and module version tagging.</p>
        <p><strong>Steps:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Update provider constraints in <code>terraform</code> block.</li>
          <li>Run <code>terraform init -upgrade</code>.</li>
          <li>Test in non-prod workspace.</li>
          <li>Commit and tag module version.</li>
        </ol>
        <p><strong>In Practice:</strong> Always test upgrades using a sandbox environment before production rollout.</p>
      </div>`},{question:"What kind of modular approach do you follow to provision resources?",answerHtml:`<div class="answer-rich">
        <h3> <strong>My Modular Strategy</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Reusable resource-level modules (VNet, VM, NSG).</li>
          <li>Pattern modules for common stacks (WebApp + DB).</li>
          <li>Root modules per environment (dev, test, prod).</li>
        </ul>
        <p><strong>In Practice:</strong> This structure simplifies maintenance, versioning, and parallel team development.</p>
      </div>`},{question:"What is a root module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Root module  Entry point</strong></h3>
        <p>The root module is the main working directory where Terraform commands (<code>init, plan, apply</code>) are executed.</p>
        <p><strong>In Practice:</strong> It orchestrates multiple child modules and manages environment-specific configs.</p>
      </div>`},{question:"What are child modules?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Child modules  Reusable components</strong></h3>
        <p>Reusable Terraform modules called from root modules or other modules using the <code>module</code> block.</p>
        <p><strong>In Practice:</strong> Each infra unit (network, compute, storage) is a separate child module for isolation and reuse.</p>
      </div>`},{question:"How to call a module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Calling a module</strong></h3>
        <pre><code>module "network" {
  source = "../modules/network"
  vnet_name = var.vnet_name
  address_space = var.vnet_cidr
}</code></pre>
        <p><strong>In Practice:</strong> Keep source path relative in local modules, and versioned for registry modules.</p>
      </div>`},{question:"What is the use of module block?",answerHtml:`<div class="answer-rich">
        <h3> <strong>module {} block  Calling reusable code</strong></h3>
        <p>The <code>module</code> block is used to reference a child or registry module with input variables.</p>
        <p>It ensures consistent provisioning and reusability.</p>
      </div>`},{question:"What arguments are required in a module block?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Module block arguments</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>source:</strong> Path or registry address of module.</li>
          <li><strong>version:</strong> (optional) specific version for module.</li>
          <li><strong>Input variables:</strong> key-value pairs for module parameters.</li>
        </ul>
      </div>`},{question:"How to pass variables from root to child module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Passing variables from root  child</strong></h3>
        <p>Root module passes values via <code>module</code> block using key-value pairs matching childs <code>variable</code> definitions.</p>
        <pre><code>module "vm" {
  source  = "../modules/vm"
  vm_name = var.vm_name
}</code></pre>
        <p><strong>In Practice:</strong> Keeps configuration DRY and easy to modify per environment.</p>
      </div>`},{question:"Can a module call another module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Nested module calls</strong></h3>
        <p>Yes. A module can call another module to form a layered or composable design.</p>
        <p><strong>In Practice:</strong> I use nested modules in pattern modules  e.g., a web-app pattern calls VM, NSG, and storage modules internally.</p>
      </div>`},{question:"What is module source in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Module source  Where module code is fetched from</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Local path (<code>../modules/vm</code>)</li>
          <li>GitHub repo (<code>git::https://github.com/org/module.git</code>)</li>
          <li>Terraform Registry (<code>terraform-aws-modules/vpc/aws</code>)</li>
        </ul>
        <p><strong>In Practice:</strong> Always version control remote sources for consistent builds.</p>
      </div>`},{question:"How to use modules from Terraform Registry?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Registry modules</strong></h3>
        <pre><code>module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "5.0.0"
  name    = "myvpc"
  cidr    = "10.0.0.0/16"
}</code></pre>
        <p><strong>In Practice:</strong> I prefer verified registry modules to reduce boilerplate and speed up deployments.</p>
      </div>`},{question:"How to organize modules in a Terraform project?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Organizing modules effectively</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>modules/</strong> folder for reusable components</li>
          <li><strong>envs/</strong> folder for environment-specific root configs</li>
          <li>Version each module for controlled updates</li>
        </ul>
        <p><strong>In Practice:</strong> This separation allows parallel development & simplified CI/CD pipelines.</p>
      </div>`},{question:"How do you upgrade Terraform templates?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Upgrading Terraform templates safely</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Backup state and version files.</li>
          <li>Upgrade Terraform binary.</li>
          <li>Run <code>terraform init -upgrade</code>.</li>
          <li>Validate and plan changes.</li>
          <li>Fix deprecated syntax and test in non-prod.</li>
        </ol>
        <p><strong>In Practice:</strong> I maintain module version tags (v1.0, v1.1) to isolate new template versions.</p>
      </div>`}]},{title:"11. Terraform  Dependencies & Logic",questions:[{question:"What are implicit and explicit dependencies?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Implicit vs Explicit Dependencies</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Implicit Dependency:</strong> Automatically inferred when one resource references another. Example:<br/>
          <code>subnet_id = azurerm_subnet.main.id</code>  Terraform knows subnet must be created before the VM.</li>
          <li><strong>Explicit Dependency:</strong> Manually defined using <code>depends_on</code> when references are indirect or non-existent.</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer implicit dependencies wherever possible to reduce graph complexity, and use explicit only for edge cases (like provisioners or module ordering).</p>
      </div>`},{question:"Difference between depends_on and implicit dependency?",answerHtml:`<div class="answer-rich">
        <h3> <strong>depends_on vs Implicit Dependency</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>depends_on:</strong> Manually enforces creation/destruction order even if theres no direct attribute reference.</li>
          <li><strong>Implicit Dependency:</strong> Automatically inferred through attribute references.</li>
        </ul>
        <p><strong>In Practice:</strong> I use <code>depends_on</code> in modules when dependencies cross boundaries (e.g., NSG depends on VNet but not referenced directly).</p>
      </div>`},{question:"What is the move block in Terraform? Does it affect infrastructure?",answerHtml:`<div class="answer-rich">
        <h3> <strong>move block  State-only refactor</strong></h3>
        <p>The <strong>move block</strong> (introduced in Terraform 1.1+) is used to rename or relocate resources/modules in state without re-creating infrastructure.</p>
        <pre><code>moved {
  from = aws_instance.old_name
  to   = aws_instance.new_name
}</code></pre>
        <p><strong>Important:</strong> It only updates the state file mapping  actual infra remains unchanged.</p>
        <p><strong>In Practice:</strong> I use this during module refactoring or resource renaming to prevent accidental re-creation.</p>
      </div>`},{question:"What is count and for_each? Which one will you use for creating 10 storage accounts?",answerHtml:`<div class="answer-rich">
        <h3> <strong>count vs for_each</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>count:</strong> Creates multiple identical resources using index-based iteration.</li>
          <li><strong>for_each:</strong> Creates resources using key-value mapping (map or set)  more readable and predictable for unique resources.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>resource "azurerm_storage_account" "sa" {
  for_each = toset(["dev", "test", "prod"])
  name     = "storage&dollar;{each.key}"
}</code></pre>
        <p><strong>In Practice:</strong> For 10 storage accounts with unique names, I prefer <code>for_each</code>  easier tracking and no index shift risk.</p>
      </div>`},{question:"Why is delete operation tricky when using count?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Count Deletion Issue</strong></h3>
        <p>With <code>count</code>, Terraform tracks resources by index. If you remove an element from the middle, all subsequent indices shift  causing unwanted deletions and re-creations.</p>
        <p><strong>Example:</strong> Deleting element 2 of 5 may cause 35 to be destroyed and re-created.</p>
        <p><strong>In Practice:</strong> I use <code>for_each</code> when each resource needs stable identity (e.g., named VMs or Storage Accounts).</p>
      </div>`},{question:"What is lifecycle block and how does it handle dependencies?",answerHtml:`<div class="answer-rich">
        <h3> <strong>lifecycle block  Behavior control</strong></h3>
        <p><strong>Lifecycle block</strong> defines how Terraform manages create/update/delete of a resource and handles dependent behavior.</p>
        <pre><code>lifecycle {
  prevent_destroy = true
  ignore_changes  = [tags]
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><strong>create_before_destroy:</strong> Ensures new resource is created before destroying old one.</li>
          <li><strong>ignore_changes:</strong> Avoids unnecessary updates when dependent attributes change.</li>
        </ul>
        <p><strong>In Practice:</strong> I use this to avoid downtime for VMs or production network resources during dependent changes.</p>
      </div>`},{question:"How do you manage inter-resource dependencies in a module?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Managing Dependencies Inside Modules</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>output  input chaining</strong> between modules (network output  VM input).</li>
          <li>Use <code>depends_on</code> only when implicit references dont exist (e.g., tagging sequence).</li>
          <li>Use <code>lifecycle.create_before_destroy</code> for resource replacements to maintain order.</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain dependency flow at module level to keep root module clean and predictable.</p>
      </div>`},{question:"What is a provider in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Provider  The Cloud Plugin</strong></h3>
        <p>A <strong>provider</strong> in Terraform acts as a bridge between Terraform and the target platform (like Azure, AWS, GCP, GitHub, Kubernetes, etc.).</p>
        <ul style="margin-left:1.2rem;">
          <li>Each provider exposes resources and data sources specific to that platform.</li>
          <li>Terraform uses providers to authenticate, create, update, and destroy infrastructure.</li>
          <li>Providers are downloaded automatically from the <strong>Terraform Registry</strong> or private registries.</li>
        </ul>
        <p><strong>Example:</strong> AzureRM, AWS, Kubernetes, Helm, GitHub are common providers.</p>
        <p><strong>In Practice:</strong> In an AKS project, I used <code>azurerm</code> for infra provisioning and <code>kubernetes</code> provider for post-deployment configuration (ConfigMaps, Secrets, etc.).</p>
      </div>`},{question:"How to configure a provider in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provider Configuration</strong></h3>
        <p>Provider configuration is defined using the <code>provider</code> block inside your Terraform code.</p>
        <pre><code>provider "azurerm" {
  features {}
  subscription_id = var.subscription_id
  tenant_id       = var.tenant_id
  client_id       = var.client_id
  client_secret   = var.client_secret
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><strong>features {}</strong> is mandatory for AzureRM provider (acts as initialization block).</li>
          <li>Credentials can be passed via variables, environment variables, or managed identities.</li>
        </ul>
        <p><strong>In Practice:</strong> We use Service Principal credentials from Azure Key Vault in DevOps pipelines to initialize the AzureRM provider securely.</p>
      </div>`},{question:"What is the use of version argument in provider block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provider Version Argument</strong></h3>
        <p>The <code>version</code> argument ensures Terraform uses a specific version of the provider  preventing breaking changes from future releases.</p>
        <pre><code>provider "azurerm" {
  features {}
  version = "=3.89.0"
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Version locking guarantees consistent behavior across environments.</li>
          <li>You can specify exact, minimum, or range versions like <code>>= 3.0, < 4.0</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> We fixed AzureRM version to <code>3.89.0</code> after a breaking change in the 4.x preview release broke Terraform apply in production.</p>
      </div>`},{question:"How to restrict provider version?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Restricting Provider Versions</strong></h3>
        <p>Provider versions are restricted using the <code>required_providers</code> block inside <code>terraform</code> configuration.</p>
        <pre><code>terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.89.0"
    }
  }
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><code>~></code> allows patch-level updates (e.g., 3.89.x but not 3.90).</li>
          <li>Helps ensure reproducible builds and safe upgrades.</li>
        </ul>
        <p><strong>In Practice:</strong> This version restriction prevented developers from unintentionally upgrading provider versions during <code>terraform init -upgrade</code>.</p>
      </div>`},{question:"What is required_providers block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>required_providers  Declaration of Dependencies</strong></h3>
        <p>The <code>required_providers</code> block defines which providers Terraform needs and where to download them from.</p>
        <pre><code>terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.15.0"
    }
  }
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Ensures consistent provider versions across developer and CI/CD environments.</li>
          <li>Also supports custom provider sources (e.g., private registries).</li>
        </ul>
        <p><strong>In Practice:</strong> Our Terraform repo used AzureRM + Helm providers to deploy AKS workloads with consistent provider versions via <code>required_providers</code>.</p>
      </div>`},{question:"What is alias in provider block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provider Aliasing  Multi-Account Config</strong></h3>
        <p>The <code>alias</code> keyword allows multiple configurations of the same provider (for example, deploying resources in multiple subscriptions or regions).</p>
        <pre><code>provider "azurerm" {
  features {}
  subscription_id = var.sub1
  alias           = "primary"
}

provider "azurerm" {
  features {}
  subscription_id = var.sub2
  alias           = "secondary"
}

resource "azurerm_resource_group" "rg1" {
  name     = "rg-primary"
  location = "eastus"
  provider = azurerm.primary
}</code></pre>
        <p><strong>In Practice:</strong> We used alias to deploy networking in shared subscription and compute resources in app subscription.</p>
      </div>`},{question:"How to use multiple providers in one configuration?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-Provider Configuration</strong></h3>
        <p>You can use multiple providers (e.g., Azure + Kubernetes + Helm) in a single Terraform setup.</p>
        <pre><code>provider "azurerm" {
  features {}
}

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>AzureRM provider creates AKS cluster.</li>
          <li>Kubernetes provider uses cluster credentials to deploy workloads.</li>
        </ul>
        <p><strong>In Practice:</strong> After creating AKS via AzureRM, we used Kubernetes provider to deploy Helm charts automatically from the same Terraform pipeline.</p>
      </div>`},{question:"What is provider inheritance?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provider Inheritance</strong></h3>
        <p>Child modules automatically inherit the provider configuration from the parent module unless overridden.</p>
        <pre><code>module "network" {
  source = "./modules/network"
  # inherits default azurerm provider
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>If a module needs a different provider, you can explicitly pass it using the <code>providers</code> argument:</li>
          <pre><code>module "compute" {
  source    = "./modules/compute"
  providers = {
    azurerm = azurerm.secondary
  }
}</code></pre>
        </ul>
        <p><strong>In Practice:</strong> Networking module inherited main provider, but logging module used aliased provider pointing to a different subscription.</p>
      </div>`},{question:"How to authenticate Terraform with cloud providers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provider Authentication Methods</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Environment variables:</strong> Set credentials via env vars like <code>ARM_CLIENT_ID</code>, <code>ARM_CLIENT_SECRET</code>, etc.</li>
          <li><strong>Service Principal (Azure):</strong> Pass credentials in provider block or via pipeline variables.</li>
          <li><strong>Managed Identity:</strong> Use system-assigned or user-assigned identity for Terraform runner VM or pipeline agent.</li>
          <li><strong>Azure CLI login:</strong> If logged in via <code>az login</code>, provider automatically picks credentials.</li>
        </ul>
        <p><strong>In Practice:</strong> In DevOps pipelines, we use Azure service connection (SPN) and pass credentials as environment variables for non-interactive authentication.</p>
      </div>`},{question:"What is AzureRM provider and how is it configured?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AzureRM Provider Configuration</strong></h3>
        <p><strong>AzureRM</strong> provider is used to manage Azure resources using Terraform.</p>
        <pre><code>terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.90.0"
    }
  }
}

provider "azurerm" {
  features {}
  subscription_id = var.subscription_id
  tenant_id       = var.tenant_id
  client_id       = var.client_id
  client_secret   = var.client_secret
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><strong>features {}</strong> is mandatory.</li>
          <li>Supports multiple auth methods: SPN, Azure CLI, Managed Identity.</li>
          <li>Used to deploy Azure resources like RG, VNet, AKS, Key Vault, etc.</li>
        </ul>
        <p><strong>In Practice:</strong> We configured AzureRM provider with service principal via Key Vault and deployed infra via Azure DevOps YAML pipelines.</p>
      </div>`}]},{title:"12. Terraform  Basics & Core Concepts",questions:[{question:"What is Terraform and why do we use it in infrastructure provisioning?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform  Infrastructure as Code tool</strong></h3>
        <p><strong>Terraform</strong> is an open-source IaC tool by HashiCorp that automates the provisioning and management of cloud, on-prem, and hybrid infrastructure using declarative configuration files.</p>
        <p><strong>Why we use it:</strong> To ensure consistency, repeatability, and version control across infrastructure deployments.</p>
        <p><strong>In Practice:</strong> I use Terraform for provisioning complete cloud stacks (network, VMs, AKS, storage) with automated drift detection and rollback safety.</p>
      </div>`},{question:"What are Terraforms main features?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Key Features of Terraform</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Declarative language (HCL)</li>
          <li>State management</li>
          <li>Execution plan & change preview</li>
          <li>Multi-cloud orchestration</li>
          <li>Dependency graph management</li>
          <li>Module reusability</li>
          <li>Immutable infrastructure principle</li>
        </ul>
        <p><strong>In Practice:</strong> These features let me manage infra at scale with versioned, auditable changes.</p>
      </div>`},{question:"What is Infrastructure as Code (IaC)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Infrastructure as Code (IaC)</strong></h3>
        <p>IaC is a method of managing and provisioning infrastructure using code instead of manual processes.</p>
        <p><strong>Terraform implements IaC</strong> through HCL configurations that define desired infrastructure state.</p>
        <p><strong>In Practice:</strong> IaC helps me automate infra creation via CI/CD pipelines and maintain consistency across environments.</p>
      </div>`},{question:"What are the benefits of Terraform in DevOps?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Benefits of Terraform in DevOps</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Faster environment provisioning</li>
          <li>Version-controlled infrastructure</li>
          <li>Reusable modular code</li>
          <li>Cross-cloud automation (Azure, AWS, GCP)</li>
          <li>Integrates easily with CI/CD pipelines</li>
        </ul>
        <p><strong>In Practice:</strong> Using Terraform in my DevOps workflows reduces deployment time and improves infra reliability.</p>
      </div>`},{question:"How does Terraform differ from Ansible, Puppet, or Chef?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform vs Configuration Management Tools</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Terraform:</strong> Focuses on provisioning and managing infrastructure (IaC).</li>
          <li><strong>Ansible/Puppet/Chef:</strong> Focus on configuration management inside provisioned servers.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Terraform to build infra (VMs, networks) and Ansible for post-provisioning configuration (install software, patching).</p>
      </div>`},{question:"What is Terraform Core and what are Providers?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Core & Providers</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Terraform Core:</strong> The engine that parses configs, builds dependency graphs, creates plans, and applies infrastructure changes.</li>
          <li><strong>Providers:</strong> Plugins that let Terraform interact with APIs (e.g., AzureRM, AWS, Kubernetes).</li>
        </ul>
        <p><strong>In Practice:</strong> Core drives orchestration; providers handle communication with actual infra services.</p>
      </div>`},{question:"What are .tf and .tfstate files?",answerHtml:`<div class="answer-rich">
        <h3> <strong>.tf vs .tfstate</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>.tf:</strong> Configuration files written in HCL that define desired infra.</li>
          <li><strong>.tfstate:</strong> JSON file that records the current state of deployed resources.</li>
        </ul>
        <p><strong>In Practice:</strong> I store state files in remote backends (Azure Storage, S3) for team collaboration and state locking.</p>
      </div>`},{question:"When does the state file get created (init/plan/apply)?",answerHtml:`<div class="answer-rich">
  <h3> <strong>State File Creation</strong></h3>
  <p>The state file is created <strong>after the first successful terraform apply</strong>.</p>
  <p><strong>init</strong>  initializes backend & providers.<br/>
  <strong>plan</strong>  simulates execution (doesnt write state).<br/>
  <strong>apply</strong>  actually creates/updates resources and writes state.</p>
  </div>`},{question:"What are the different stages in Terraform deployment?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Deployment Stages</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li><strong>Write:</strong> Define resources in .tf files.</li>
          <li><strong>Init:</strong> Initialize provider plugins and backend.</li>
          <li><strong>Plan:</strong> Preview the execution plan.</li>
          <li><strong>Apply:</strong> Execute and create/update resources.</li>
          <li><strong>Destroy:</strong> Tear down infrastructure.</li>
        </ol>
        <p><strong>In Practice:</strong> I integrate these stages in pipelines for automated infra promotion (dev  prod).</p>
      </div>`},{question:"What is the lifecycle of a Terraform deployment?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Deployment Lifecycle</strong></h3>
        <p>Lifecycle defines the flow from defining infrastructure to managing its updates and deletion.</p>
        <p><strong>Steps:</strong> Write  Init  Plan  Apply  Monitor  Destroy.</p>
        <p><strong>In Practice:</strong> I maintain versioned templates and use workspaces for environment-specific lifecycle management.</p>
      </div>`},{question:"What is the Terraform workflow  init, plan, apply, destroy?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform Workflow Commands</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>init:</strong> Initializes backend and providers.</li>
          <li><strong>plan:</strong> Creates an execution plan (what will change).</li>
          <li><strong>apply:</strong> Applies the plan to build/update infra.</li>
          <li><strong>destroy:</strong> Destroys managed infra.</li>
        </ul>
        <p><strong>In Practice:</strong> I always review the plan before applying to avoid accidental changes.</p>
      </div>`},{question:"What is terraform plan and apply command?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform plan & apply</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>plan:</strong> Shows the execution plan  what will be created, modified, or destroyed.</li>
          <li><strong>apply:</strong> Executes the plan and applies the actual changes.</li>
        </ul>
        <p><strong>In Practice:</strong> I store plan output using <code>terraform plan -out=planfile</code> and apply it with <code>terraform apply planfile</code> for predictable deployments.</p>
      </div>`},{question:"What is terraform fmt and why is it important?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform fmt  Code formatting</strong></h3>
        <p><strong>terraform fmt</strong> formats Terraform files to canonical style for consistency and readability.</p>
        <p><strong>In Practice:</strong> I use it in pre-commit hooks in CI/CD so that all team members maintain consistent Terraform code style.</p>
      </div>`},{question:"What is terraform validate used for?",answerHtml:`<div class="answer-rich">
        <h3> <strong>terraform validate  Syntax & semantic check</strong></h3>
        <p><strong>terraform validate</strong> checks if the configuration is syntactically correct and internally consistent, without contacting providers.</p>
        <p><strong>In Practice:</strong> I run it as an early gate in CI to catch typos or missing variable references before planning.</p>
      </div>`},{question:"What is the advantage of running terraform plan before apply?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Why run plan before apply</strong></h3>
        <p><strong>terraform plan</strong> lets you preview the exact changes Terraform will make before execution.</p>
        <ul style="margin-left:1.2rem;">
          <li>Detect unwanted resource deletions or modifications</li>
          <li>Review and approve changes safely</li>
          <li>Generate reproducible plan files for apply</li>
        </ul>
        <p><strong>In Practice:</strong> I always use <code>terraform plan -out</code> in CI/CD to ensure human validation before apply.</p>
      </div>`}]},{title:"13. Terraform  State Management & Backend",questions:[{question:"What is Terraform state file and why is it required?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform State File (terraform.tfstate)</strong></h3>
        <p>The <strong>state file</strong> is a JSON document that tracks the current real-world infrastructure Terraform manages.</p>
        <p><strong>Why its needed:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Maps Terraform resources to real cloud resources.</li>
          <li>Stores metadata and dependencies.</li>
          <li>Enables Terraform to detect drift and perform incremental updates.</li>
        </ul>
        <p><strong>In Practice:</strong> Without state, Terraform cannot know what already exists, leading to potential re-creation of resources.</p>
      </div>`},{question:"What is a backend in Terraform?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Backend in Terraform</strong></h3>
        <p>A <strong>backend</strong> defines where Terraform stores its state file and how operations like locking are handled.</p>
        <p><strong>Example:</strong> Local backend stores state on disk, while remote backends (e.g., AzureRM, S3) store it centrally for team access.</p>
      </div>`},{question:"What is the difference between local and remote backend?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Local vs Remote Backend</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Local backend:</strong> State stored locally on machine ("terraform.tfstate").</li>
          <li><strong>Remote backend:</strong> State stored remotely (e.g., Azure Blob, AWS S3, Terraform Cloud).</li>
        </ul>
        <p><strong>In Practice:</strong> I use remote backend with state locking (via Azure Storage or DynamoDB) for team-based infra projects.</p>
      </div>`},{question:"What are Terraform backends, and why are they important?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Importance of Backends</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Enable team collaboration via shared remote state.</li>
          <li>Provide state locking to avoid concurrent conflicts.</li>
          <li>Improve security with encryption and versioning.</li>
        </ul>
        <p><strong>In Practice:</strong> Remote backends also ensure centralized state recovery and drift detection.</p>
      </div>`},{question:"How do you configure a remote backend in Azure?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Remote Backend Configuration  Azure</strong></h3>
        <pre><code>
terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfsatestorage"
    container_name       = "tfstate"
    key                  = "infra.tfstate"
  }
}
        </code></pre>
        <p><strong>In Practice:</strong> I enable <code>blob versioning</code> and <code>Soft Delete</code> on the container to prevent accidental loss.</p>
      </div>`},{question:"How do you store Terraform state securely?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Secure Storage of State File</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use remote backend (AzureRM/S3) with encryption-at-rest.</li>
          <li>Enable storage account access control (RBAC/Policies).</li>
          <li>Store secrets separately in Key Vault instead of plain text.</li>
          <li>Enable state versioning for rollback capability.</li>
        </ul>
      </div>`},{question:"How do you manage Terraform state file locking?",answerHtml:`<div class="answer-rich">
        <h3> <strong>State File Locking</strong></h3>
        <p>State locking prevents multiple users from modifying the same state simultaneously.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure:</strong> Native locking via blob lease.</li>
          <li><strong>AWS:</strong> Locking with DynamoDB table.</li>
          <li><strong>Terraform Cloud:</strong> Automatically managed locking.</li>
        </ul>
      </div>`},{question:"What is state locking and why is it important?",answerHtml:`<div class="answer-rich">
        <h3> <strong>State Locking Importance</strong></h3>
        <p>Locks prevent race conditions where two applies could overwrite each others changes, leading to inconsistent infra.</p>
        <p><strong>In Practice:</strong> Always ensure backend supports locking before enabling team collaboration.</p>
      </div>`},{question:"How do you handle Terraform state file corruption?",answerHtml:`<div class="answer-rich">
        <h3> <strong>State File Corruption Recovery</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Restore previous version (from backend versioning).</li>
          <li>Run <code>terraform refresh</code> to rebuild state from real infra.</li>
          <li>Manually import missing resources with <code>terraform import</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> Always enable state file versioning and soft delete for safe recovery.</p>
      </div>`},{question:"What happens if Terraform apply fails midway?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Apply Failure Handling</strong></h3>
        <p>If apply fails midway, Terraform updates the state with only successfully created resources.</p>
        <p>Next run of <code>terraform apply</code> continues from that partial state.</p>
        <p><strong>In Practice:</strong> I validate the plan before applying to reduce mid-run failures.</p>
      </div>`},{question:"How do you resolve state file lock issues?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Resolving Lock Issues</strong></h3>
        <p>If a process crashes leaving the lock active, use:</p>
        <pre><code>terraform force-unlock &lt;LOCK_ID&gt;</code></pre>
        <p><strong>Note:</strong> Use cautiously  ensure no one else is applying simultaneously.</p>
      </div>`},{question:"What happens if state file is deleted? How do you recover it?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Recovering Deleted State File</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Restore from backend versioning or backup.</li>
          <li>If unavailable, re-import existing infra with <code>terraform import</code>.</li>
          <li>Manually recreate state structure if needed (last resort).</li>
        </ul>
        <p><strong>In Practice:</strong> I always enable blob versioning + daily backup for remote state containers.</p>
      </div>`},{question:"You have 1000 resource groups but lost state file  how will you rebuild infra?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Rebuilding Lost State at Scale</strong></h3>
        <p>Use <code>terraform import</code> in automation:</p>
        <pre><code>
for rg in &dollar;(az group list --query "[].name" -o tsv); do
  terraform import azurerm_resource_group.&dollar;rg /subscriptions/&lt;sub&gt;/resourceGroups/&dollar;rg
done
        </code></pre>
        <p><strong>In Practice:</strong> I script imports for large infra recovery when state is lost.</p>
      </div>`},{question:"What is terraform refresh command and what does it do?",answerHtml:` <div class="answer-rich">
        <h3> <strong>terraform refresh</strong></h3>
        <p><strong>terraform refresh</strong> updates the state file to match the actual infrastructure.</p>
        <p><strong>Use Case:</strong> Detect manual changes made outside Terraform and bring the state back in sync.</p>
      </div>`},{question:"How will you determine a zero drift condition?",answerHtml:` <div class="answer-rich">
        <h3> <strong>Zero Drift Verification</strong></h3>
        <p>Run:</p>
        <pre><code>terraform plan</code></pre>
        <p>If the plan shows <strong>No changes. Infrastructure is up-to-date</strong>  zero drift condition confirmed.</p>
      </div>`},{question:"What is drift detection in Terraform?",answerHtml:` <div class="answer-rich">
        <h3> <strong>Drift Detection</strong></h3>
        <p><strong>Drift</strong> occurs when actual infrastructure deviates from Terraform state (e.g., manual change).</p>
        <p><strong>Detection:</strong> Using <code>terraform plan</code> or <code>terraform refresh</code>.</p>
        <p><strong>In Practice:</strong> I schedule drift detection checks in CI/CD pipelines to detect manual modifications early.</p>
      </div>`}]},{title:"14. Infrastructure as Code (IaC) & Automation",questions:[{question:"What is Infrastructure as Code (IaC)?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Infrastructure as Code (IaC)  Concept</strong></h3>
        <p>IaC means defining and provisioning infrastructure using code instead of manual configurations. It brings DevOps principles (version control, CI/CD, automation) into infrastructure management.</p>
        <p><strong>Example:</strong> Using Terraform scripts to create VNets, VMs, and Storage instead of clicking in Azure Portal.</p>
        <p><strong>Benefits:</strong> Consistency, repeatability, scalability, rollback capability, and full infrastructure version control.</p>
        <p><strong>In practice:</strong> I manage infra via Terraform pipelines integrated with Azure DevOps to ensure automated, auditable deployments.</p>
      </div>`},{question:"What is the difference between Terraform and ARM templates?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform vs ARM Templates</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Aspect</th><th>Terraform</th><th>ARM Template</th></tr>
          <tr><td>Language</td><td>HCL (HashiCorp Configuration Language)</td><td>JSON</td></tr>
          <tr><td>Provider Support</td><td>Multi-cloud (Azure, AWS, GCP)</td><td>Azure only</td></tr>
          <tr><td>State Management</td><td>Maintains a state file (.tfstate)</td><td>No explicit state file</td></tr>
          <tr><td>Reusability</td><td>Supports modules and remote backends</td><td>Less modular, hard to reuse</td></tr>
          <tr><td>Learning Curve</td><td>Simple, declarative syntax</td><td>Complex JSON structure</td></tr>
        </table>
        <p><strong>In real projects:</strong> I prefer Terraform because its cloud-agnostic, integrates with CI/CD, and offers versioned state management.</p>
      </div>`},{question:"What is Infrastructure Drift, and how do you manage it?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Infrastructure Drift & Management</strong></h3>
        <p><strong>Drift</strong> happens when someone changes infrastructure manually (outside Terraform), so the actual cloud resources no longer match your IaC code.</p>
        <p><strong>Detection:</strong> Run <code>terraform plan</code> or <code>terraform refresh</code> to compare real resources vs state.</p>
        <p><strong>Management strategy:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Restrict manual changes via RBAC & policies.</li>
          <li>Use CI/CD pipelines for all infra updates.</li>
          <li>Run periodic drift detection jobs in pipelines.</li>
        </ul>
        <p><strong>Example:</strong> I set up a nightly pipeline that runs <code>terraform plan -detailed-exitcode</code> to detect any drift automatically.</p>
      </div>`},{question:"How do you handle state management in IaC tools?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Terraform State Management</strong></h3>
        <p>Terraform maintains a <strong>state file (.tfstate)</strong> that maps resources defined in code to actual cloud resources.</p>
        <p><strong>Best practices:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>remote backend</strong> (like Azure Storage, S3) for shared team access.</li>
          <li>Enable <strong>state locking</strong> (using Azure Blob or DynamoDB) to prevent concurrent modifications.</li>
          <li>Never commit <code>.tfstate</code> in Git; it may contain sensitive data.</li>
        </ul>
        <pre><code>backend "azurerm" {
  resource_group_name  = "riteshrg"
  storage_account_name = "riteshstorageacc"
  container_name       = "riteshcontainer"
  key                  = "main.tfstate"
}</code></pre>
        <p><strong>In practice:</strong> I store all Terraform states in a secured Azure Storage backend with RBAC-controlled access.</p>
      </div>`},{question:"How will you pass custom scripts to VMs during creation?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Custom Script Execution on VM</strong></h3>
        <p>You can use Terraform <strong>custom_data</strong> or <strong>provisioners</strong> to execute shell/PowerShell scripts during VM provisioning.</p>
        <p><strong>Common options:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>custom_data:</strong> Base64-encoded script runs at first boot via cloud-init (Linux).</li>
          <li><strong>Custom Script Extension:</strong> Executes scripts post-deployment for configuration.</li>
        </ul>
        <pre><code>resource "azurerm_virtual_machine_extension" "init" {
  name                 = "vm-init"
  virtual_machine_id   = azurerm_linux_virtual_machine.myvm.id
  publisher            = "Microsoft.Azure.Extensions"
  type                 = "CustomScript"
  type_handler_version = "2.1"
  settings = <<SETTINGS
    {
      "script": "sudo apt update && sudo apt install nginx -y"
    }
  SETTINGS
}</code></pre>
        <p><strong>Use case:</strong> I use it to bootstrap monitoring agents or app dependencies post-VM deployment.</p>
      </div>`},{question:"What is the difference between local-exec and remote-exec provisioners?",answerHtml:`<div class="answer-rich">
        <h3> <strong>local-exec vs remote-exec</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;">
          <tr><th>Provisioner</th><th>Runs Where</th><th>Use Case</th></tr>
          <tr><td>local-exec</td><td>On the machine running Terraform</td><td>To trigger local scripts, CLI commands, or API calls post-deploy (e.g., send Slack message, trigger pipeline).</td></tr>
          <tr><td>remote-exec</td><td>Inside the target resource (e.g., VM)</td><td>To execute configuration commands directly on the resource (e.g., install packages).</td></tr>
        </table>
        <p><strong>In real use:</strong> I use <code>remote-exec</code> for in-VM setup and <code>local-exec</code> for notifying external systems like monitoring or CI/CD pipelines.</p>
      </div>`},{question:"Explain modular approach in Terraform  how child and parent modules interact.",answerHtml:`<div class="answer-rich">
        <h3> <strong>Modular Terraform Architecture</strong></h3>
        <p>Modules help organize Terraform code by grouping reusable resources together. The <strong>root (parent) module</strong> calls <strong>child modules</strong> and passes inputs/outputs.</p>
        <p><strong>Structure example:</strong></p>
        <pre><code>modules/
  vnet/
     main.tf
  vm/
     main.tf
main.tf (root)
</code></pre>
        <p><strong>Parent to Child Interaction:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li>Child exposes <code>variables.tf</code> for input parameters.</li>
          <li>Parent passes those values during module call.</li>
          <li>Child outputs are returned to parent using <code>output.tf</code>.</li>
        </ol>
        <pre><code>module "vm" {
  source     = "./modules/vm"
  vnet_name  = module.vnet.name
  subnet_id  = module.vnet.subnet_id
}</code></pre>
        <p><strong>In practice:</strong> I maintain separate modules per resource type (network, compute, storage) and reuse them across environments (dev, QA, prod).</p>
      </div>`},{question:"What is the strategy to update IaC for a new feature without impacting production?",answerHtml:`<div class="answer-rich">
        <h3> <strong>Safe IaC Update Strategy</strong></h3>
        <p><strong>Goal:</strong> Introduce new infra features while keeping production stable.</p>
        <p><strong>Best practices I follow:</strong></p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Branch-based workflow:</strong> Create feature branch  test infra in non-prod.</li>
          <li><strong>Use workspaces:</strong> Separate state files for dev / qa / prod.</li>
          <li><strong>Run terraform plan:</strong> Always validate impact before apply.</li>
          <li><strong>Use targeted apply:</strong> <code>terraform apply -target=module.new_component</code> for isolated rollout.</li>
          <li><strong>Implement approvals:</strong> Include manual approval step in Azure DevOps pipeline for prod changes.</li>
        </ol>
        <p><strong>Example:</strong> When I added a new subnet for AKS, I first tested in staging workspace, validated routing, and then merged to production branch after approval.</p>
      </div>`}]},{title:"15. Terraform Providers & Authentication",questions:[{question:"What are Terraform providers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Providers  Bridge to External Systems</strong></h3>
        <p><strong>Providers</strong> are plugins in Terraform that enable interaction with APIs of various platforms  cloud, SaaS, or infrastructure tools.</p>
        <ul style="margin-left:1.2rem;">
          <li>They define <strong>resources</strong> (what you can create) and <strong>data sources</strong> (what you can read).</li>
          <li>Examples: <code>azurerm</code> (Azure), <code>aws</code> (AWS), <code>google</code> (GCP), <code>kubernetes</code> (K8s).</li>
        </ul>
        <pre><code>provider "azurerm" {
  features {}
}</code></pre>
        <p><strong>In Practice:</strong> I use multiple providers to provision both Azure infrastructure and Kubernetes workloads from a single Terraform project.</p>
      </div>`},{question:"How does Terraform provider authentication work?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provider Authentication  Secure API Access</strong></h3>
        <p>Terraform authenticates with cloud providers via environment variables, CLI sessions, or service principals.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>AzureRM:</strong> Auth via <code>az login</code> or a Service Principal (App ID + Secret).</li>
          <li><strong>AWS:</strong> Auth via <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>.</li>
          <li><strong>GCP:</strong> Auth via <code>GOOGLE_CREDENTIALS</code> (JSON key file).</li>
        </ul>
        <pre><code>provider "azurerm" {
  features {}
  subscription_id = var.sub_id
  tenant_id       = var.tenant_id
  client_id       = var.client_id
  client_secret   = var.client_secret
}</code></pre>
        <p><strong>Best Practice:</strong> Use environment variables or secret managers (Key Vault, Vault, etc.)  never hardcode credentials.</p>
      </div>`},{question:"How do you use AzureRM provider?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AzureRM Provider  Manage Azure Resources</strong></h3>
        <p><strong>AzureRM</strong> is Terraforms official provider for Azure.</p>
        <pre><code>terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.100.0"
    }
  }
}

provider "azurerm" {
  features {}
}</code></pre>
        <p><strong>Authentication Methods:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Azure CLI (<code>az login</code>)</li>
          <li>Service Principal (client_id / client_secret)</li>
          <li>Managed Identity (for pipelines or AKS)</li>
        </ul>
        <p><strong>In Practice:</strong> I use Managed Identity in DevOps pipelines to authenticate Terraform automatically without storing secrets.</p>
      </div>`},{question:"Can Terraform handle multiple providers?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-Provider Support in Terraform</strong></h3>
        <p>Yes  Terraform can use multiple providers simultaneously (e.g., Azure + AWS + Kubernetes).</p>
        <pre><code>provider "azurerm" {
  alias  = "azure"
  features {}
}

provider "aws" {
  alias  = "aws"
  region = "us-east-1"
}</code></pre>
        <p>You can then reference provider aliases in resources:</p>
        <pre><code>resource "azurerm_resource_group" "rg" {
  provider = azurerm.azure
  name     = "rg-demo"
  location = "East US"
}</code></pre>
        <p><strong>In Practice:</strong> I use this approach for hybrid deployments  e.g., deploying Azure network + AWS backup.</p>
      </div>`},{question:"What are provider version constraints?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provider Version Constraints  Stability Control</strong></h3>
        <p>Version constraints lock provider versions to ensure consistent builds across environments.</p>
        <pre><code>terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.100.0"
    }
  }
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><code>~> 3.100.0</code>  allows minor updates but not breaking changes.</li>
          <li><code>= 3.90.0</code>  exact version only.</li>
          <li><code>>= 3.80.0</code>  any version above minimum.</li>
        </ul>
        <p><strong>In Practice:</strong> I pin versions to avoid unexpected behavior when HashiCorp releases new provider updates.</p>
      </div>`},{question:"What is the difference between provider and provisioner?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provider vs Provisioner  Core Difference</strong></h3>
        <table border="1" style="border-collapse:collapse; width:100%;">
          <tr><th>Aspect</th><th>Provider</th><th>Provisioner</th></tr>
          <tr><td>Purpose</td><td>Manages cloud/infrastructure resources via APIs</td><td>Executes scripts or configuration inside created resources</td></tr>
          <tr><td>Scope</td><td>High-level (Azure, AWS, etc.)</td><td>Low-level (e.g., file copy, remote-exec)</td></tr>
          <tr><td>Example</td><td><code>azurerm</code>, <code>aws</code>, <code>kubernetes</code></td><td><code>local-exec</code>, <code>remote-exec</code></td></tr>
        </table>
        <p><strong>In Practice:</strong> I rarely use provisioners  prefer automation via configuration management (Ansible, cloud-init).</p>
      </div>`},{question:"How do you manage provider versioning?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Provider Versions</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Specify provider version constraints in <code>required_providers</code>.</li>
          <li>Lock versions using <code>.terraform.lock.hcl</code> (auto-generated on init).</li>
          <li>Update manually with:
            <pre><code>terraform init -upgrade</code></pre>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I commit <code>.terraform.lock.hcl</code> to source control for consistent builds across Dev, QA, and Prod.</p>
      </div>`},{question:"How do you authenticate Terraform with Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Authenticating Terraform with Azure</strong></h3>
        <p>Terraform can authenticate to Azure in multiple ways:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure CLI:</strong> Use local login session.<br><code>az login</code></li>
          <li><strong>Service Principal:</strong> Recommended for CI/CD.<br>
            <pre><code>export ARM_CLIENT_ID=xxxx
export ARM_CLIENT_SECRET=xxxx
export ARM_TENANT_ID=xxxx
export ARM_SUBSCRIPTION_ID=xxxx</code></pre>
          </li>
          <li><strong>Managed Identity:</strong> For Azure DevOps agents or AKS.
            <pre><code>provider "azurerm" {
  use_msi = true
  features {}
}</code></pre>
          </li>
        </ul>
        <p><strong>Best Practice:</strong> Use Managed Identity or Key Vault integration  never store credentials in code or state files.</p>
      </div>`}]},{title:"16. Terraform Variables, Locals & Outputs",questions:[{question:"What are input variables and output variables?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Input & Output Variables  Parameterization in Terraform</strong></h3>
        <p><strong>Input Variables:</strong> Used to pass dynamic values into Terraform configurations (like parameters).</p>
        <p><strong>Output Variables:</strong> Used to display or export information after resource creation (e.g., IP, IDs).</p>
        <pre><code># Input variable
variable "location" {
  type    = string
  default = "eastus"
}

# Output variable
output "vm_ip" {
  value = azurerm_public_ip.myip.ip_address
}</code></pre>
        <p><strong>In Practice:</strong> Inputs make modules reusable; outputs help pass values between modules or to CI/CD pipelines.</p>
      </div>`},{question:"What is the difference between locals and variables?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Locals vs Variables  Simplifying Code Logic</strong></h3>
        <p><strong>Variables:</strong> Take user or external input.</p>
        <p><strong>Locals:</strong> Used for computed or derived values inside Terraform code  cannot be overridden externally.</p>
        <pre><code>variable "env" {
  default = "prod"
}

locals {
  rg_name = "rg-&dollar;{var.env}"
}</code></pre>
        <p><strong>In Practice:</strong> I use <code>locals</code> to simplify expressions and avoid repeating long interpolations.</p>
      </div>`},{question:"What are Terraform variable types (string, map, object, list)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Variable Types</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>string</strong>  single value (<code>"eastus"</code>)</li>
          <li><strong>number</strong>  numeric (<code>3</code>)</li>
          <li><strong>bool</strong>  true/false</li>
          <li><strong>list</strong>  ordered list of values (<code>["dev","qa","prod"]</code>)</li>
          <li><strong>map</strong>  key-value pairs (<code>{ env="prod", region="eastus" }</code>)</li>
          <li><strong>object</strong>  complex structured data</li>
        </ul>
        <pre><code>variable "tags" {
  type = map(string)
  default = {
    owner = "team-devops"
    env   = "prod"
  }
}</code></pre>
        <p><strong>In Practice:</strong> I use maps and objects to define structured configs like tags, network ranges, or security rules.</p>
      </div>`},{question:"What is the use of a variable group?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Variable Groups  Centralized Variable Management</strong></h3>
        <p>Variable Groups are used in CI/CD pipelines (like Azure DevOps) to store and reuse Terraform variables across multiple stages or pipelines.</p>
        <ul style="margin-left:1.2rem;">
          <li>Securely manage shared variables.</li>
          <li>Centralized update instead of editing multiple pipelines.</li>
        </ul>
        <p><strong>In Practice:</strong> I keep environment-wide variables (subscription ID, location, resource prefix) in variable groups for reusability and consistency.</p>
      </div>`},{question:"How do you handle sensitive variables in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Sensitive Variables Securely</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Mark variable as <code>sensitive = true</code> in Terraform.</li>
          <li>Use environment variables or secret managers (Azure Key Vault, Vault, GitHub Secrets).</li>
          <li>Never commit sensitive data (client secrets, passwords) to Git.</li>
        </ul>
        <pre><code>variable "client_secret" {
  type      = string
  sensitive = true
}</code></pre>
        <p><strong>In Practice:</strong> I integrate Terraform with Azure Key Vault to dynamically pull secrets during runtime.</p>
      </div>`},{question:"How do you secure sensitive outputs in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Sensitive Outputs</strong></h3>
        <p>Mark outputs as <code>sensitive = true</code> to prevent them from being displayed in logs or CLI output.</p>
        <pre><code>output "admin_password" {
  value     = azurerm_linux_virtual_machine.vm.admin_password
  sensitive = true
}</code></pre>
        <p><strong>In Practice:</strong> I use this for outputs like passwords, keys, and connection strings  keeps them hidden in DevOps logs.</p>
      </div>`},{question:"What is the use of terraform output block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Output Block  Exposing Key Information</strong></h3>
        <p>Used to display or pass computed values (like IPs or IDs) after resource creation.</p>
        <pre><code>output "public_ip" {
  value = azurerm_public_ip.myip.ip_address
}</code></pre>
        <p><strong>Use Cases:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Debugging and validation after apply.</li>
          <li>Sharing outputs with other modules or pipelines.</li>
        </ul>
        <p><strong>In Practice:</strong> I use output blocks to feed values into downstream deployment stages (like AKS or App Services).</p>
      </div>`},{question:"Write the steps of output block in Terraform.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Steps to Create an Output Block</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Identify the resource attribute to output (e.g., IP, name).</li>
          <li>Create an <code>output</code> block in your Terraform file.</li>
          <li>Assign value using resource reference.</li>
          <li>Run <code>terraform apply</code> to view output values.</li>
        </ol>
        <pre><code>output "rg_name" {
  value = azurerm_resource_group.main.name
}</code></pre>
        <p><strong>In Practice:</strong> I often pipe outputs to scripts or use them for chaining module deployments.</p>
      </div>`},{question:"How do you provide default values to variables?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Default Variable Values</strong></h3>
        <p>Set default values in the <code>variable</code> block. If not overridden, Terraform uses the default.</p>
        <pre><code>variable "location" {
  type    = string
  default = "eastus"
}</code></pre>
        <p><strong>In Practice:</strong> I assign defaults for non-sensitive, environment-wide values like region or SKU.</p>
      </div>`},{question:"How do you pass variables from CLI or tfvars file?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Passing Variables  Multiple Options</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>CLI:</strong> <code>terraform apply -var="location=eastus"</code></li>
          <li><strong>TFVARS file:</strong>
            <pre><code># terraform.tfvars
location = "eastus"
vm_count = 2</code></pre>
            Run: <code>terraform apply -var-file="terraform.tfvars"</code>
          </li>
          <li><strong>Environment Variables:</strong> <code>export TF_VAR_location=eastus</code></li>
        </ul>
        <p><strong>In Practice:</strong> I prefer <code>.tfvars</code> files for environment-based configuration management.</p>
      </div>`},{question:"What are local values and how are they used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Local Values  Simplify Repetitive Logic</strong></h3>
        <p><code>locals</code> define intermediate values to make code cleaner and avoid duplication.</p>
        <pre><code>locals {
  prefix = "&dollar;{var.env}-&dollar;{var.app}"
}

resource "azurerm_resource_group" "rg" {
  name     = "&dollar;{local.prefix}-rg"
  location = var.location
}</code></pre>
        <p><strong>In Practice:</strong> I use locals to standardize naming conventions and reduce code repetition.</p>
      </div>`},{question:"Difference between data source and local block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Data Source vs Local Block</strong></h3>
        <table border="1" style="border-collapse:collapse; width:100%;">
          <tr><th>Aspect</th><th>Data Source</th><th>Local Block</th></tr>
          <tr><td>Purpose</td><td>Fetch existing resource info from provider</td><td>Compute or store intermediate values</td></tr>
          <tr><td>Source</td><td>External (API / cloud resource)</td><td>Internal (Terraform logic)</td></tr>
          <tr><td>Example</td><td><code>data "azurerm_resource_group" "rg" { name = "prod-rg" }</code></td><td><code>locals { rg_name = "prod-rg" }</code></td></tr>
        </table>
        <p><strong>In Practice:</strong> I use <code>data</code> blocks for referencing pre-existing infra and <code>locals</code> for computed names or tags.</p>
      </div>`}]},{title:"17. Terraform - Modules & Reusability",questions:[{question:"What are modules in Terraform and why are they important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Modules  Reusable Infrastructure Blocks</strong></h3>
        <p><strong>Definition:</strong> A Terraform <code>module</code> is a container for multiple resources used together. Every Terraform configuration has at least one root module, and you can create child modules for reusability.</p>
        <p><strong>Importance:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Encapsulates reusable infrastructure logic.</li>
          <li>Improves code maintainability and DRY (Dont Repeat Yourself).</li>
          <li>Allows version control and standardization across environments.</li>
        </ul>
        <pre><code>module "network" {
  source = "./modules/network"
  vnet_name = "vnet-prod"
  location  = var.location
}</code></pre>
        <p><strong>In Practice:</strong> I modularize common components like VNets, AKS clusters, and storage accounts for use across multiple projects.</p>
      </div>`},{question:"What is the difference between Terraform modules and resources?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Modules vs Resources  Abstraction Levels</strong></h3>
        <table border="1" style="border-collapse:collapse; width:100%;">
          <tr><th>Aspect</th><th>Resources</th><th>Modules</th></tr>
          <tr><td>Purpose</td><td>Define individual infrastructure components.</td><td>Group related resources for reuse and organization.</td></tr>
          <tr><td>Granularity</td><td>Fine-grained (e.g., VM, Storage Account)</td><td>High-level abstraction (e.g., Full network stack)</td></tr>
          <tr><td>Reusability</td><td>Limited</td><td>Highly reusable across environments</td></tr>
          <tr><td>Example</td><td><code>resource "azurerm_vnet"</code></td><td><code>module "vnet_module"</code></td></tr>
        </table>
        <p><strong>In Practice:</strong> Modules are like templates that call multiple resources under one logical block.</p>
      </div>`},{question:"How do you create a reusable module?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Creating a Reusable Module</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Create a folder (e.g., <code>modules/vnet</code>).</li>
          <li>Add <code>main.tf</code>, <code>variables.tf</code>, and <code>outputs.tf</code> files.</li>
          <li>Define resources inside <code>main.tf</code>.</li>
          <li>Use <code>variable</code> blocks for configurable values.</li>
          <li>Expose key attributes using <code>output</code> blocks.</li>
          <li>Call the module in root config:</li>
        </ol>
        <pre><code>module "vnet" {
  source = "./modules/vnet"
  vnet_name = "vnet-prod"
  location  = var.location
}</code></pre>
        <p><strong>In Practice:</strong> I keep all reusable modules (network, compute, AKS, storage) in a common repo for team-wide use.</p>
      </div>`},{question:"What is a child module and how do you call it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Child Modules  Nested Reusable Blocks</strong></h3>
        <p>A <strong>child module</strong> is any module called from another module (the root module or another child module).</p>
        <p>They are defined using the <code>module</code> block inside another configuration.</p>
        <pre><code># Root module (main.tf)
module "vnet" {
  source = "./modules/vnet"
  location = var.location
}</code></pre>
        <p><strong>In Practice:</strong> I use child modules to separate application layers  for example, networking, AKS, and database modules within one environment.</p>
      </div>`},{question:"What are shared modules and how do you manage them?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Shared Modules  Centralized Reusability</strong></h3>
        <p><strong>Definition:</strong> Shared modules are reusable modules stored in a shared repository or Terraform Registry, used across teams or environments.</p>
        <p><strong>Best Practices:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Store in a centralized Git repo (e.g., <code>git::https://.../terraform-modules.git//network</code>).</li>
          <li>Use semantic versioning for stability (<code>ref=v1.2.0</code>).</li>
          <li>Keep consistent structure: <code>main.tf</code>, <code>variables.tf</code>, <code>outputs.tf</code>.</li>
        </ul>
        <pre><code>module "network" {
  source = "git::https://github.com/org/terraform-modules.git//vnet?ref=v1.0.0"
}</code></pre>
        <p><strong>In Practice:</strong> My teams maintain a shared repo of Terraform modules for Azure networking, compute, and storage.</p>
      </div>`},{question:"What are parent and child modules in a 3-tier app?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Parent & Child Modules in a 3-Tier Architecture</strong></h3>
        <p>In a 3-tier setup (Web, App, DB):</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Parent module:</strong> The root configuration that orchestrates the overall deployment.</li>
          <li><strong>Child modules:</strong> Individual module blocks for each layer (web tier, app tier, db tier).</li>
        </ul>
        <pre><code># parent/main.tf
module "web_tier" {
  source = "../modules/web"
}

module "app_tier" {
  source = "../modules/app"
}

module "db_tier" {
  source = "../modules/db"
}</code></pre>
        <p><strong>In Practice:</strong> I structure 3-tier environments using one parent (root) that calls tier-specific child modules for modular deployment.</p>
      </div>`},{question:"What are Terraform best practices for module versioning?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Best Practices for Module Versioning</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>semantic versioning</strong> (<code>v1.0.0</code>, <code>v1.1.0</code>, <code>v2.0.0</code>).</li>
          <li>Pin module versions explicitly using <code>?ref=</code> in source URL or <code>version</code> in registry.</li>
          <li>Tag releases in Git to control stability.</li>
          <li>Test module updates in staging before production rollout.</li>
        </ul>
        <pre><code>module "aks" {
  source  = "git::https://github.com/org/terraform-modules.git//aks?ref=v2.1.0"
}</code></pre>
        <p><strong>In Practice:</strong> I always lock module versions to avoid breaking changes from upstream updates.</p>
      </div>`},{question:"How do you manage Terraform modules in version control?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Modules in Version Control (Git)</strong></h3>
        <p><strong>Approach:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Keep modules in a separate <strong>Terraform Modules Repo</strong> (e.g., <code>terraform-modules</code>).</li>
          <li>Each module resides in its own folder (network, compute, AKS).</li>
          <li>Use Git tags and branches for version control and stability.</li>
          <li>Implement CI checks for <code>terraform fmt</code>, <code>validate</code>, and <code>plan</code>.</li>
        </ul>
        <pre><code>source = "git::https://github.com/org/terraform-modules.git//network?ref=v1.2.0"</code></pre>
        <p><strong>In Practice:</strong> My teams manage modules via Git repos with tagging, branching, and automated testing to ensure module integrity.</p>
      </div>`}]},{title:"18. Terraform - Lifecycle & Dependencies",questions:[{question:"What is a lifecycle block and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Lifecycle Block  Resource Behavior Control</strong></h3>
        <p>The <code>lifecycle</code> block allows you to control how Terraform manages the creation, update, and destruction of resources.</p>
        <p><strong>Used for:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Controlling resource replacement logic.</li>
          <li>Preserving critical resources from deletion.</li>
          <li>Ignoring specific attribute changes to prevent unnecessary recreation.</li>
        </ul>
        <pre><code>resource "azurerm_storage_account" "example" {
  name = "examplestorage"
  lifecycle {
    prevent_destroy = true
  }
}</code></pre>
        <p><strong>In Practice:</strong> I use lifecycle blocks to avoid accidental deletion of production resources like storage accounts or VNETs.</p>
      </div>`},{question:"What are lifecycle arguments like create_before_destroy, prevent_destroy, ignore_changes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Lifecycle Arguments Explained</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>create_before_destroy:</strong> Ensures new resource is created before old one is destroyed (useful for zero downtime).</li>
          <li><strong>prevent_destroy:</strong> Prevents Terraform from deleting a resource (adds safety for prod resources).</li>
          <li><strong>ignore_changes:</strong> Ignores specific attributes during <code>terraform apply</code> (useful when certain values change outside Terraform).</li>
        </ul>
        <pre><code>lifecycle {
  create_before_destroy = true
  prevent_destroy        = false
  ignore_changes         = [tags, metadata]
}</code></pre>
        <p><strong>In Practice:</strong> I use <code>create_before_destroy</code> for AKS node pools and <code>ignore_changes</code> for fields auto-updated by Azure.</p>
      </div>`},{question:"Where did you use lifecycle in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Practical Lifecycle Usage</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>AKS clusters:</strong> Used <code>create_before_destroy</code> to avoid downtime during upgrades.</li>
          <li><strong>Storage Accounts:</strong> Used <code>prevent_destroy</code> to avoid accidental deletion of data.</li>
          <li><strong>VM NICs:</strong> Used <code>ignore_changes</code> for IP assignments managed outside Terraform.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>lifecycle {
  prevent_destroy = true
  ignore_changes  = [tags]
}</code></pre>
        <p><strong>In Practice:</strong> This ensures Terraform operations are safe, predictable, and production-friendly.</p>
      </div>`},{question:"What is the use of depends_on?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>depends_on  Enforcing Explicit Dependencies</strong></h3>
        <p>The <code>depends_on</code> argument ensures Terraform creates or destroys resources in a specific order when automatic detection fails.</p>
        <pre><code>resource "azurerm_network_interface" "example" {
  depends_on = [azurerm_virtual_network.vnet]
}</code></pre>
        <p><strong>In Practice:</strong> I use <code>depends_on</code> when resource references arent direct (e.g., output dependencies or module boundaries).</p>
      </div>`},{question:"How do you handle resource dependencies in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Resource Dependencies</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Terraform automatically infers dependencies via references (<code>resource_a.id</code>).</li>
          <li>When implicit dependency isnt clear, use <code>depends_on</code>.</li>
          <li>For module-level dependencies, use module outputs and variables for chaining.</li>
        </ul>
        <pre><code>resource "azurerm_network_interface" "nic" {
  subnet_id = azurerm_subnet.subnet.id
}</code></pre>
        <p><strong>In Practice:</strong> I rely mostly on implicit references but add <code>depends_on</code> for complex setups like AKS cluster waiting for network provisioning.</p>
      </div>`},{question:"If there are interdependent resources (VM, NSG, subnet), how will you serialize deployment?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Serializing Interdependent Resource Deployments</strong></h3>
        <p>Terraform handles dependencies automatically when outputs are referenced properly.</p>
        <pre><code>resource "azurerm_network_security_group" "nsg" {}
resource "azurerm_subnet" "subnet" {
  network_security_group_id = azurerm_network_security_group.nsg.id
}
resource "azurerm_network_interface" "nic" {
  subnet_id = azurerm_subnet.subnet.id
}</code></pre>
        <p><strong>In Practice:</strong> By chaining IDs between resources, Terraform automatically deploys them in the correct order  NSG  Subnet  NIC  VM.</p>
      </div>`},{question:"What happens when we rename a Terraform resource?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resource Rename  Terraform Perception</strong></h3>
        <p>Terraform treats a renamed resource as a <strong>new resource</strong> and plans to destroy the old one and create a new one.</p>
        <p><strong>To prevent unwanted recreation:</strong> Use <code>terraform state mv</code> to rename in state.</p>
        <pre><code>terraform state mv azurerm_storage_account.old azurerm_storage_account.new</code></pre>
        <p><strong>In Practice:</strong> I always migrate state before renaming to avoid data loss or downtime.</p>
      </div>`},{question:"What happens if a resource name changes in map?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resource Name Change in Map Variables</strong></h3>
        <p>If youre using <code>for_each</code> with a map, changing the key name means Terraform will:</p>
        <ul style="margin-left:1.2rem;">
          <li>Destroy the old resource (old key removed).</li>
          <li>Create a new resource (new key added).</li>
        </ul>
        <p><strong>In Practice:</strong> Always manage map keys carefully  key change = resource recreation.</p>
      </div>`},{question:"What if an item in input map is deleted  how will Terraform behave?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deleted Item in Input Map  Resource Destruction</strong></h3>
        <p>If an item in a <code>for_each</code> map is removed, Terraform destroys the corresponding resource.</p>
        <pre><code>for_each = {
  vm1 = "eastus"
  # vm2 removed
}</code></pre>
        <p>Terraform plan output will show a <strong>destroy</strong> action for the removed key.</p>
        <p><strong>In Practice:</strong> I use <code>lifecycle.prevent_destroy</code> temporarily when removing keys to ensure safe manual cleanup.</p>
      </div>`}]},{title:"19. Terraform - Count, for_each & Dynamic Blocks",questions:[{question:"What is the difference between count and for_each?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>count vs for_each  Core Difference</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>count</strong>  used for simple repetition based on index (0,1,2...).</li>
          <li><strong>for_each</strong>  used for mapping unique resources using keys (map or set).</li>
        </ul>
        <pre><code># count example
resource "azurerm_resource_group" "rg" {
  count = 3
  name  = "rg-&dollar;{count.index}"
}

# for_each example
resource "azurerm_resource_group" "rg" {
  for_each = toset(["dev","qa","prod"])
  name     = "rg-&dollar;{each.key}"
}</code></pre>
        <p><strong>In Practice:</strong> I use <code>count</code> for numeric loops and <code>for_each</code> for object-based, named resources.</p>
      </div>`},{question:"When will you use count and when for_each?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Choosing between count and for_each</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>count</strong>  when resources are identical and order-based (like 3 subnets).</li>
          <li>Use <strong>for_each</strong>  when each item has a unique identity (map or object data).</li>
        </ul>
        <p><strong>In Practice:</strong> I prefer <code>for_each</code> for maintainability and clear references (e.g., <code>each.key</code> = "prod").</p>
      </div>`},{question:"for_each ko directly list ke saath kyu nahi use kar sakte?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>for_each with list  Why not allowed directly?</strong></h3>
        <p>Terraform doesnt allow <code>for_each</code> with plain lists because list items dont have unique keys.</p>
        <p>To use it, convert list  set:</p>
        <pre><code>for_each = toset(["dev", "qa", "prod"])</code></pre>
        <p><strong>In Practice:</strong> I always use <code>toset()</code> to ensure uniqueness and predictable referencing.</p>
      </div>`},{question:"for_each me map use karte waqt each.key aur each.value kya represent karte hain?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>each.key vs each.value in for_each map</strong></h3>
        <pre><code>for_each = {
  dev  = "eastus"
  prod = "westus"
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><strong>each.key</strong>  "dev" / "prod"</li>
          <li><strong>each.value</strong>  "eastus" / "westus"</li>
        </ul>
        <p><strong>In Practice:</strong> Helpful when defining environment-specific configurations dynamically.</p>
      </div>`},{question:"toset() ka kya role hota hai jab list ko for_each me convert karte ho?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>toset() role  converting list for for_each</strong></h3>
        <p><code>toset()</code> converts a list into a set of unique values so <code>for_each</code> can use it.</p>
        <pre><code>for_each = toset(["eastus", "westus"])</code></pre>
        <p><strong>In Practice:</strong> Without <code>toset()</code>, Terraform throws a type error for <code>for_each</code> using list.</p>
      </div>`},{question:"Kya for_each ke andar duplicates allowed hain?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>No Duplicates Allowed in for_each</strong></h3>
        <p><code>for_each</code> requires unique keys  duplicates cause a plan error.</p>
        <pre><code>for_each = toset(["eastus", "eastus"]) #  Error</code></pre>
        <p><strong>In Practice:</strong> Always ensure uniqueness using <code>toset()</code> or <code>distinct()</code>.</p>
      </div>`},{question:"Agar tum ek for_each ke resource ka index chahte ho to kya karoge?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Index Access for for_each Resources</strong></h3>
        <p>Theres no direct <code>index</code> like count, but you can emulate it:</p>
        <pre><code>locals {
  regions = tolist(toset(["eastus", "westus"]))
}

for_each = toset(local.regions)
region_index = index(local.regions, each.key)</code></pre>
        <p><strong>In Practice:</strong> I use <code>index()</code> function for generating order-based logic with <code>for_each</code>.</p>
      </div>`},{question:"Kya tum for_each ke sath dynamic blocks use kar sakte ho? Agar haan, kaise?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Yes  for_each with Dynamic Blocks</strong></h3>
        <p>You can use <code>dynamic</code> blocks inside resources managed by <code>for_each</code> to generate nested configurations.</p>
        <pre><code>resource "azurerm_network_security_group" "nsg" {
  for_each = var.rules

  dynamic "security_rule" {
    for_each = each.value
    content {
      name = security_rule.key
      priority = security_rule.value.priority
    }
  }
}</code></pre>
        <p><strong>In Practice:</strong> Useful for defining multiple NSG rules dynamically from variable maps.</p>
      </div>`},{question:"for_each resource ko dusre for_each resource se refer kaise karte ho?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cross-Referencing for_each Resources</strong></h3>
        <p>Use matching keys between resources.</p>
        <pre><code>resource "azurerm_subnet" "subnet" {
  for_each = var.subnets
}

resource "azurerm_network_interface" "nic" {
  for_each = var.subnets
  subnet_id = azurerm_subnet.subnet[each.key].id
}</code></pre>
        <p><strong>In Practice:</strong> This ensures 1:1 mapping between dependent for_each resources.</p>
      </div>`},{question:"for_each me condition lagana chahte ho (e.g. kuch resources hi create ho)  kaise karoge?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Conditional for_each  Filtering Resources</strong></h3>
        <pre><code>for_each = {
  for k, v in var.subnets : k => v
  if v.enabled == true
}</code></pre>
        <p><strong>In Practice:</strong> I use conditional expressions to deploy only enabled resources dynamically.</p>
      </div>`},{question:"Ek for_each loop me tum locals ka use kaise kar sakte ho?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using Locals with for_each</strong></h3>
        <p>Use <code>locals</code> to pre-process data before looping.</p>
        <pre><code>locals {
  envs = { dev = "eastus", prod = "westus" }
}

for_each = local.envs
location = each.value</code></pre>
        <p><strong>In Practice:</strong> Locals simplify loops by cleaning variable logic.</p>
      </div>`},{question:"Multi-level nesting (RG  VNet  Subnet) for_each kaise handle karoge using map of objects?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Nested for_each  Multi-level Infra</strong></h3>
        <pre><code>variable "vnet_config" {
  type = map(object({
    address_space = list(string)
    subnets       = map(object({ cidr = string }))
  }))
}

resource "azurerm_virtual_network" "vnet" {
  for_each = var.vnet_config
  name     = each.key

  resource "azurerm_subnet" "subnet" {
    for_each = each.value.subnets
    name     = each.key
    address_prefixes = [each.value.cidr]
  }
}</code></pre>
        <p><strong>In Practice:</strong> Ideal for hierarchical deployment like RG  VNet  Subnets using maps of objects.</p>
      </div>`},{question:"What are dynamic blocks and how are they used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Dynamic Blocks  Reusable Nested Configs</strong></h3>
        <p>Dynamic blocks are used to generate nested arguments dynamically within a resource.</p>
        <pre><code>dynamic "security_rule" {
  for_each = var.rules
  content {
    name     = security_rule.key
    priority = security_rule.value.priority
  }
}</code></pre>
        <p><strong>In Practice:</strong> I use dynamic blocks for NSG rules, tags, or multiple sub-resources inside modules.</p>
      </div>`},{question:"What is Terraforms behavior when for_each resource renamed?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Renaming for_each Resource  Recreation Behavior</strong></h3>
        <p>If key names change in <code>for_each</code> map, Terraform destroys the old resource and creates a new one.</p>
        <p><strong>To prevent recreation:</strong></p>
        <pre><code>terraform state mv "module.old" "module.new"</code></pre>
        <p><strong>In Practice:</strong> I use <code>terraform state mv</code> when refactoring module or key names to preserve existing infrastructure.</p>
      </div>`}]},{title:"20. Terraform - Provisioners & Null Resources",questions:[{question:"What is a provisioner block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Provisioner Block  Definition</strong></h3>
        <p>
          A <code>provisioner</code> block in Terraform allows you to run scripts or commands 
          on a local or remote machine during the resource creation or destruction phase.
          It acts as a bridge between infrastructure provisioning and configuration tasks.
        </p>
        <h4> Syntax Example:</h4>
        <pre><code>resource "azurerm_virtual_machine" "vm" {
  name = "myvm"
  
  provisioner "remote-exec" {
    inline = [
      "sudo apt update",
      "sudo apt install -y nginx"
    ]
  }
}</code></pre>
        <h4> Key Concepts:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Provisioners are executed only <strong>after resource creation</strong>.</li>
          <li>They are considered a <strong>last resort</strong>  use configuration management tools (like Ansible) instead if possible.</li>
          <li>They can run during <strong>create</strong> or <strong>destroy</strong> phases.</li>
        </ul>
        <p><strong>In Practice:</strong> I use provisioners for post-deployment setup (e.g., bootstrapping, agent installation, custom config).</p>
      </div>`},{question:"What is the difference between local-exec and remote-exec?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>local-exec vs remote-exec</strong></h3>
        <table class="comparison-table">
          <tr><th>Aspect</th><th>local-exec</th><th>remote-exec</th></tr>
          <tr><td>Where it runs</td><td>On your local machine (where Terraform runs)</td><td>Inside the remote VM or target system</td></tr>
          <tr><td>Connection</td><td>No SSH needed</td><td>Requires SSH/WinRM connection</td></tr>
          <tr><td>Typical Use</td><td>Trigger pipelines, scripts, API calls</td><td>Configure server, install packages</td></tr>
          <tr><td>Example</td><td><code>local-exec { command = "echo done" }</code></td><td><code>remote-exec { inline = ["sudo apt update"] }</code></td></tr>
        </table>
        <p><strong>In Practice:</strong> I use <code>local-exec</code> to trigger Ansible or shell scripts post-apply, and <code>remote-exec</code> to configure a VM once its up.</p>
      </div>`},{question:"What are use cases of provisioners?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Use Cases for Provisioners</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Bootstrapping servers with agents (Datadog, Azure Monitor, CloudWatch).</li>
          <li>Triggering <strong>external automation</strong> (Ansible, Jenkins jobs, scripts).</li>
          <li>Running <strong>final configuration commands</strong> not supported natively in Terraform.</li>
          <li>Registering systems with external services (DNS, CMDB, etc.).</li>
          <li>Destroy-time cleanup (via <code>when = destroy</code>).</li>
        </ul>
        <pre><code>provisioner "local-exec" {
  when    = destroy
  command = "echo 'Cleaning up...'"
}</code></pre>
        <p><strong>In Practice:</strong> I mostly avoid heavy logic in provisioners and keep them for short, idempotent setup tasks.</p>
      </div>`},{question:"What is a null_resource in Terraform and when do you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>null_resource  A Dummy Resource for Logic Execution</strong></h3>
        <p>
          <code>null_resource</code> doesnt create any real infrastructure; it acts as a placeholder 
          to trigger provisioners or scripts based on conditions, dependencies, or file changes.
        </p>
        <pre><code>resource "null_resource" "setup" {
  triggers = {
    build_id = var.build_id
  }

  provisioner "local-exec" {
    command = "echo 'Running post-build setup for &dollar;{self.triggers.build_id}'"
  }
}</code></pre>
        <h4> Use Cases:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Triggering external scripts or tools during pipeline runs.</li>
          <li>Running local/remote configuration after infra is created.</li>
          <li>Re-running provisioners when <code>triggers</code> change.</li>
          <li>Glue logic between Terraform and CI/CD workflows.</li>
        </ul>
        <p><strong>In Practice:</strong> I use <code>null_resource</code> to run scripts when versioned inputs or templates change  without touching infra.</p>
      </div>`},{question:"What happens if you install software manually and rerun Terraform apply?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Behavior  Manual Changes Outside Terraform</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Terraform <strong>does not detect manual changes</strong> inside the VM (like installed packages).</li>
          <li>State file only tracks what Terraform created, not OS-level configuration.</li>
          <li>If provisioners ran before, Terraform <strong>wont re-run them automatically</strong> unless resource is recreated or triggers change.</li>
        </ul>
        <h4> Best Practice:</h4>
        <p>Provisioners should always be <strong>idempotent</strong> (safe to run multiple times).</p>
        <pre><code>provisioner "remote-exec" {
  inline = [
    "which nginx || sudo apt install -y nginx"
  ]
}</code></pre>
        <p><strong>In Practice:</strong> I avoid relying on provisioners for system config  instead, I use Ansible or Packer for image pre-bake. Terraform should focus only on infra state.</p>
      </div>`}]},{title:"21. Terraform - Data Sources",questions:[{question:"What is a data source and how is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Data Source  Definition</strong></h3>
        <p>
          A <code>data</code> source in Terraform is used to <strong>fetch or reference existing resources</strong> 
          that are <em>not managed</em> by your current Terraform configuration.
        </p>
        <p>
          Instead of creating new infrastructure, data sources allow Terraform to read information 
          (like IDs, names, or secrets) from existing infrastructure.
        </p>
        <h4> Example:</h4>
        <pre><code>data "azurerm_resource_group" "rg" {
  name = "prod-rg"
}

resource "azurerm_storage_account" "sa" {
  name                     = "tfstorage001"
  resource_group_name      = data.azurerm_resource_group.rg.name
  location                 = data.azurerm_resource_group.rg.location
}</code></pre>
        <p><strong>In Practice:</strong> I use data sources to pull details of existing Azure resources  like VNets, Key Vaults, or Subnets  
        instead of hardcoding them.</p>
      </div>`},{question:"What is the difference between data block and locals block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Data Block vs Locals Block</strong></h3>
        <table class="comparison-table">
          <tr><th>Aspect</th><th>Data Block</th><th>Locals Block</th></tr>
          <tr><td>Purpose</td><td>Fetches data from existing infrastructure (read from provider)</td><td>Stores computed or reusable values within Terraform</td></tr>
          <tr><td>Data Source</td><td>External (Azure, AWS, etc.)</td><td>Internal (Terraform expressions)</td></tr>
          <tr><td>Example</td><td><code>data "azurerm_resource_group" "rg"</code></td><td><code>locals { rg_name = "prod-rg" }</code></td></tr>
          <tr><td>Dynamic?</td><td>Yes, based on external infra</td><td>Static, derived from code logic</td></tr>
        </table>
        <p><strong>In Practice:</strong> I use <code>locals</code> for transformations and <code>data</code> for real infra lookups  both together make code modular and DRY.</p>
      </div>`},{question:"Do data sources become part of state file?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Data Sources & State File</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Yes , data sources are recorded in the state file  but only their <strong>fetched values</strong>, not their configuration.</li>
          <li>Terraform caches data lookups in state for consistency and performance.</li>
          <li>If the external resource changes, you must run <code>terraform refresh</code> or <code>terraform apply</code> to sync it.</li>
        </ul>
        <pre><code># To refresh data source values
terraform refresh</code></pre>
        <p><strong>In Practice:</strong> This behavior ensures Terraform doesnt re-fetch data every time unless explicitly refreshed.</p>
      </div>`},{question:"What is the purpose of terraform data lookup?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Purpose of Data Lookup</strong></h3>
        <p>
          Terraform data lookup allows modules and configurations to <strong>reference existing infrastructure</strong> 
          without needing to recreate or hardcode values.
        </p>
        <h4> Use Cases:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Fetch existing Resource Group or VNet details.</li>
          <li>Read Key Vault secrets or Storage Account keys.</li>
          <li>Retrieve Azure AD object IDs, subscription info, etc.</li>
        </ul>
        <p><strong>In Practice:</strong> Using data lookups keeps Terraform code flexible across multiple environments (e.g., dev, test, prod) without hardcoding names or IDs.</p>
      </div>`},{question:"How do you use Key Vault data source in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Key Vault Data Source</strong></h3>
        <p>
          You can use <code>azurerm_key_vault</code> and <code>azurerm_key_vault_secret</code> data sources to fetch secrets securely 
          without exposing them in code.
        </p>
        <pre><code>data "azurerm_key_vault" "kv" {
  name                = "my-keyvault"
  resource_group_name = "rg-secure"
}

data "azurerm_key_vault_secret" "db_password" {
  name         = "db-pass"
  key_vault_id = data.azurerm_key_vault.kv.id
}

resource "azurerm_app_service" "app" {
  name     = "secure-app"
  app_settings = {
    "DB_PASSWORD" = data.azurerm_key_vault_secret.db_password.value
  }
}</code></pre>
        <p><strong>In Practice:</strong> I integrate Key Vault lookups in Terraform for credentials, storage keys, and SSL certs  ensuring zero secret exposure in plain text.</p>
      </div>`}]},{title:"22. Terraform - Workspaces & Multi-Environment Management",questions:[{question:"What is Terraform workspace and why do we use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Workspaces  Logical Environment Separation</strong></h3>
        <p>
          A <strong>workspace</strong> is an isolated state environment within a single Terraform configuration.  
          It lets you use the <strong>same code</strong> for multiple environments like <code>dev</code>, <code>uat</code>, and <code>prod</code>  each having its own state file.
        </p>
        <pre><code># create and switch workspace
terraform workspace new dev
terraform workspace select prod</code></pre>
        <p>
          Each workspace maintains its <strong>own terraform.tfstate</strong>, stored under the backend (local or remote).  
          This ensures isolation  so changes in dev dont impact prod.
        </p>
        <p><strong>In Practice:</strong> I use workspaces for small/mid projects or when state isolation is enough, without duplicating code folders.</p>
      </div>`},{question:"How do you manage multiple environments (dev, uat, prod)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-Environment Management  Best Practices</strong></h3>
        <p>There are two main strategies for managing multiple environments:</p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Workspaces:</strong> Use one set of Terraform files and separate states using workspaces (<code>terraform workspace new dev</code>).</li>
          <li><strong>Folder-based structure:</strong> Create separate folders like <code>/environments/dev</code>, <code>/environments/prod</code> with their own backend configs.</li>
        </ol>
        <pre><code>terraform/
 main.tf
 variables.tf
 environments/
     dev/
     prod/</code></pre>
        <p><strong>In Practice:</strong> I prefer folders for enterprise-grade setups  easier integration with pipelines and variable overrides via tfvars.</p>
      </div>`},{question:"What is the difference between workspaces and folders?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Workspaces vs Folders</strong></h3>
        <table class="comparison-table">
          <tr><th>Aspect</th><th>Workspaces</th><th>Folders</th></tr>
          <tr><td>State Separation</td><td>Single codebase, multiple states</td><td>Separate code + separate states</td></tr>
          <tr><td>Code Reuse</td><td>High</td><td>Medium (some duplication)</td></tr>
          <tr><td>Backend</td><td>Same backend, multiple states</td><td>Independent backends per env</td></tr>
          <tr><td>Best For</td><td>Small to medium infra</td><td>Large scale production setups</td></tr>
        </table>
        <p><strong>In Practice:</strong> Workspaces = lightweight separation,  
        Folders = enterprise-style governance with stronger isolation.</p>
      </div>`},{question:"How do you handle multi-subscription deployments using Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-Subscription Deployments in Terraform (AzureRM)</strong></h3>
        <p>For Azure, you can authenticate to multiple subscriptions by configuring multiple <code>provider</code> blocks with different <code>alias</code> names.</p>
        <pre><code>provider "azurerm" {
  features {}
  subscription_id = "1111-aaaa"
  alias = "prod"
}

provider "azurerm" {
  features {}
  subscription_id = "2222-bbbb"
  alias = "dev"
}

resource "azurerm_resource_group" "rg_prod" {
  provider = azurerm.prod
  name     = "rg-prod"
  location = "eastus"
}</code></pre>
        <p><strong>In Practice:</strong> I often combine provider aliases + workspace or environment variable logic 
        to dynamically select the subscription based on deployment target.</p>
      </div>`},{question:"How do you manage environment isolation in pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environment Isolation in Pipelines (Azure DevOps / GitHub Actions)</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>separate state files</strong> per environment (keyed by workspace or environment name).</li>
          <li>Store <strong>environment-specific variables</strong> in separate variable groups or YAML variable templates.</li>
          <li>Lock state backend using remote locking (Azure Blob / Terraform Cloud).</li>
          <li>Use <code>TF_WORKSPACE</code> or <code>terraform workspace select</code> step dynamically in pipeline.</li>
        </ul>
        <pre><code>- script: terraform workspace select &dollar;(env) || terraform workspace new &dollar;(env)
  displayName: "Select Terraform Workspace"</code></pre>
        <p><strong>In Practice:</strong> This keeps pipeline execution isolated and prevents cross-environment overwrites.</p>
      </div>`},{question:"What is the purpose of terraform workspace command?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>terraform workspace command  Environment Management CLI</strong></h3>
        <p>Its used to create, switch, list, and delete workspaces.</p>
        <pre><code># Common commands
terraform workspace list
terraform workspace new dev
terraform workspace select prod
terraform workspace show
terraform workspace delete test</code></pre>
        <p><strong>In Practice:</strong> I script workspace creation and selection inside CI/CD to auto-isolate Terraform states for every environment or branch deployment.</p>
      </div>`}]},{title:"23. Terraform - Integration with Azure DevOps",questions:[{question:"How do you integrate Terraform with Azure DevOps pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform + Azure DevOps Integration (End-to-End Flow)</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Store Terraform code in an <strong>Azure Repos Git repository</strong>.</li>
          <li>Create a <strong>Service Connection</strong> in ADO (type: Azure Resource Manager  Service Principal).</li>
          <li>In pipeline YAML, add Terraform tasks for <code>init</code>, <code>plan</code>, <code>apply</code>.</li>
          <li>Configure remote backend in <strong>Azure Blob Storage</strong> for state management.</li>
          <li>Use ADO <strong>Variable Groups</strong> and <strong>Key Vault linkage</strong> for secure credentials.</li>
        </ol>
        <pre><code>- task: TerraformCLI@1
  inputs:
    command: 'init'
    backendType: 'azurerm'</code></pre>
        <p><strong>In Practice:</strong> I create a 3-stage YAML pipeline  init, plan, and apply  triggered via PR validation and approvals.</p>
      </div>`},{question:"How do you use Terraform code in CI/CD pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>CI/CD Flow for Terraform in ADO</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>CI Stage:</strong> Validate syntax using <code>terraform fmt</code> and <code>terraform validate</code>.</li>
          <li><strong>CD Stage:</strong> Run <code>terraform plan</code>  approval  <code>terraform apply</code>.</li>
          <li>Use artifact publishing to share plan files between stages.</li>
        </ul>
        <pre><code>- stage: Plan
  jobs:
  - script: terraform plan -out=tfplan
    displayName: "Terraform Plan"
- stage: Apply
  dependsOn: Plan
  approval: required
  jobs:
  - script: terraform apply tfplan</code></pre>
        <p><strong>In Practice:</strong> I enforce approval gates before <code>apply</code> in prod for governance and safety.</p>
      </div>`},{question:"What is the difference between classic and YAML pipeline for Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Classic vs YAML Pipelines  Key Differences</strong></h3>
        <table class="comparison-table">
          <tr><th>Aspect</th><th>Classic Pipeline</th><th>YAML Pipeline</th></tr>
          <tr><td>Definition</td><td>UI-based (visual designer)</td><td>Code-based (.yaml in repo)</td></tr>
          <tr><td>Version Control</td><td>Not versioned</td><td>Versioned with code</td></tr>
          <tr><td>Reusability</td><td>Limited</td><td>Reusable templates & conditions</td></tr>
          <tr><td>Governance</td><td>Less control</td><td>Full control via YAML logic</td></tr>
        </table>
        <p><strong>In Practice:</strong> I always use YAML  easier to maintain, review, and re-run across branches.</p>
      </div>`},{question:"How do you fetch secrets from Azure Key Vault into pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Fetching Secrets from Key Vault in ADO</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Create a Key Vault and store secrets (like client_id, client_secret).</li>
          <li>In ADO, create a <strong>Variable Group</strong> linked to Key Vault.</li>
          <li>Grant ADO service principal <strong>Get & List</strong> access on Key Vault.</li>
          <li>Reference secrets in YAML pipeline using <code>&dollar;(secretName)</code>.</li>
        </ol>
        <pre><code>variables:
- group: kv-linked-vars

steps:
- script: echo "Client ID: &dollar;(azure-client-id)"</code></pre>
        <p><strong>In Practice:</strong> This ensures no secrets are hard-coded  everything pulled securely from vault.</p>
      </div>`},{question:"How do you secure Terraform pipeline execution in Azure DevOps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Pipeline Security Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>Azure Key Vault</strong> for secrets (never store plain text vars).</li>
          <li>Enable <strong>RBAC</strong> on Service Connection  least privilege access.</li>
          <li>Use <strong>branch policies + approvals</strong> for prod applies.</li>
          <li>Restrict pipeline execution using <strong>environment approvals</strong>.</li>
          <li>Store state in <strong>secured Azure Blob</strong> with container-level SAS restrictions.</li>
        </ul>
        <p><strong>In Practice:</strong> I enable RBAC and Key Vault for full compliance & audit trail.</p>
      </div>`},{question:"How do you handle Terraform state in Azure Blob backend via pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Remote State via Azure Blob Backend</strong></h3>
        <p>Define backend configuration in <code>backend.tf</code>:</p>
        <pre><code>terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstatestorage"
    container_name       = "tfstate"
    key                  = "prod.terraform.tfstate"
  }
}</code></pre>
        <p><strong>In Pipeline:</strong> Make sure Service Principal has Storage Blob Contributor access.  
        Terraform automatically locks state during apply.</p>
        <p><strong>In Practice:</strong> For multi-envs, I dynamically change <code>key</code> based on workspace or branch name.</p>
      </div>`},{question:"How do you handle multi-stage pipelines (init, plan, apply)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Multi-Stage Terraform Pipeline</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Stage 1  Init:</strong> Initialize backend and providers.</li>
          <li><strong>Stage 2  Plan:</strong> Generate plan file and publish as artifact.</li>
          <li><strong>Stage 3  Apply:</strong> Manual approval  apply using plan file.</li>
        </ul>
        <pre><code>- stage: Plan
  jobs:
  - script: terraform plan -out=tfplan
- stage: Apply
  dependsOn: Plan
  approvals: required
  jobs:
  - script: terraform apply tfplan</code></pre>
        <p><strong>In Practice:</strong> Keeps deployments auditable and controlled, especially for prod.</p>
      </div>`},{question:"Which branching strategy do you follow for Terraform code?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Branching Strategy (Git Flow)</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Main:</strong> Stable production-ready code.</li>
          <li><strong>Develop:</strong> Active integration branch.</li>
          <li><strong>Feature/*:</strong> New infra or resource changes.</li>
          <li><strong>Hotfix/*:</strong> Urgent fixes to prod infra.</li>
        </ul>
        <p>PR  Validation  Plan  Manual Approval  Apply to main.</p>
        <p><strong>In Practice:</strong> I automate validation (terraform fmt/validate) on PR creation.</p>
      </div>`},{question:"How to create ADO global variables in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ADO Global Variables Integration</strong></h3>
        <p>You can pass ADO pipeline variables into Terraform as environment variables:</p>
        <pre><code>- script: |
    terraform plan -var "env=&dollar;(Environment)" -var "location=&dollar;(Location)"</code></pre>
        <p>Or define global variable groups in ADO Library and reference them:</p>
        <pre><code>variables:
- group: global-terraform-vars</code></pre>
        <p><strong>In Practice:</strong> I use YAML variable templates per environment for consistency.</p>
      </div>`},{question:"How to reinitialize Terraform when backend config changes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reinitializing Terraform Backend</strong></h3>
        <p>Whenever backend configuration (like storage account name or container) changes, run:</p>
        <pre><code>terraform init -reconfigure</code></pre>
        <p>This forces Terraform to reinitialize the backend without deleting local state.  
        Optionally, you can migrate existing state using <code>-migrate-state</code>.</p>
      </div>`},{question:"What happens when plan shows 1 to add, 1 to delete  do you apply or not?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Interpreting 1 to add, 1 to delete in Terraform Plan</strong></h3>
        <p>This means a <strong>resource replacement</strong> will occur  often due to changes in immutable attributes (e.g., VM size, subnet association).</p>
        <p><strong>In Practice:</strong> I always verify in <code>plan</code> output whats being destroyed and recreated before applying  to avoid downtime.</p>
        <p>For critical resources, I use <code>lifecycle.prevent_destroy = true</code> for safety.</p>
      </div>`},{question:"How do you handle errors during pipeline execution?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Error Handling in Terraform Pipelines</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <code>set -e</code> in bash to fail pipeline on first error.</li>
          <li>Review logs from Terraform output and <strong>Azure Activity Logs</strong>.</li>
          <li>Use <strong>terraform show</strong> or <strong>terraform state list</strong> to inspect partial state.</li>
          <li>If failure occurs post-apply, run <code>terraform refresh</code> to sync state.</li>
        </ul>
        <p><strong>In Practice:</strong> I also use conditional pipeline steps for retries on transient Azure API failures.</p>
      </div>`},{question:"What happens if two people run the same pipeline simultaneously?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Concurrent Pipeline Execution</strong></h3>
        <p>Terraform locks the state file during <code>apply</code>  only one process can modify it.</p>
        <p>If another run starts while one is in progress, it will fail with:</p>
        <pre><code>Error: Error acquiring the state lock</code></pre>
        <p><strong>In Practice:</strong> I enable remote locking (Azure Blob or Terraform Cloud) and restrict concurrent approvals for safety.</p>
      </div>`},{question:"If pipeline error occurs, whats your troubleshooting process?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Pipeline Troubleshooting Approach</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Check <strong>ADO job logs</strong>  identify failed step.</li>
          <li>Check <strong>Terraform output</strong> for syntax or dependency errors.</li>
          <li>Verify <strong>Service Principal permissions</strong> (especially if deployment permissions failed).</li>
          <li>Run <code>terraform plan</code> locally to reproduce.</li>
          <li>Reinit backend with <code>terraform init -reconfigure</code> if state mismatch occurs.</li>
          <li>For Azure issues  check <strong>Activity Logs</strong> or <strong>Resource Locks</strong>.</li>
        </ol>
        <p><strong>In Practice:</strong> I keep <code>TF_LOG=DEBUG</code> enabled temporarily for deeper trace.</p>
      </div>`}]},{title:"24. Terraform - Security, RBAC & Compliance",questions:[{question:"What are Terraform best practices for security?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Security Best Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Store state remotely</strong>  use secured backend (Azure Blob / S3) with encryption at rest & access control.</li>
          <li><strong>Never hardcode secrets</strong>  use Key Vault or environment variables instead.</li>
          <li><strong>Use least privilege</strong> for Service Principals and IAM roles.</li>
          <li>Enable <strong>RBAC</strong> and use Terraform-managed policies instead of manual assignments.</li>
          <li>Regularly <strong>validate and scan</strong> IaC code with tools like <code>Checkov</code> or <code>TFSec</code>.</li>
          <li>Keep <strong>provider versions pinned</strong> to avoid unintended behavior from updates.</li>
          <li>Use <strong>terraform fmt</strong> and <strong>validate</strong> in pipelines to detect syntax & structure issues early.</li>
        </ul>
        <p><strong>In Practice:</strong> I follow the principle of least privilege + no secrets in code and run Checkov scans before every apply.</p>
      </div>`},{question:"How do you secure Terraform state file?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Terraform State File</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Always use a <strong>remote backend</strong> (Azure Blob / S3 / Terraform Cloud)  never store local state in repo.</li>
          <li>Enable <strong>encryption at rest</strong> (Azure Storage encryption or SSE-KMS).</li>
          <li>Restrict state file access with <strong>RBAC & container-level permissions</strong>.</li>
          <li>Enable <strong>state locking</strong> (Terraform automatically handles via blob lease).</li>
          <li>Do not expose state via <code>terraform output</code> for sensitive values (mark as <code>sensitive = true</code>).</li>
          <li>Backup state file automatically using blob versioning or Terraform Cloud workspace history.</li>
        </ul>
        <pre><code>terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstatestorage"
    container_name       = "tfstate"
    key                  = "prod.tfstate"
  }
}</code></pre>
        <p><strong>In Practice:</strong> My state backend is isolated in a separate subscription with strict access controls.</p>
      </div>`},{question:"How have you implemented RBAC in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Implementing RBAC via Terraform</strong></h3>
        <p>Terraform can assign roles to users, groups, or service principals directly using AzureRM role assignment resources.</p>
        <pre><code>resource "azurerm_role_assignment" "sp_rbac" {
  principal_id   = var.sp_id
  role_definition_name = "Contributor"
  scope          = azurerm_resource_group.main.id
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Define <strong>role assignments</strong> declaratively to ensure consistency across environments.</li>
          <li>Use <strong>custom roles</strong> for fine-grained permissions if Contributor is too broad.</li>
          <li>Integrate Terraform with <strong>Azure AD groups</strong> for identity-based access control.</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain separate SPs per environment (Dev/Test/Prod)  each with specific RBAC scopes.</p>
      </div>`},{question:"How do you manage secrets securely in Terraform (Vault, Key Vault, Env vars)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secret Management in Terraform</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Store all sensitive values in <strong>Azure Key Vault</strong> (not .tfvars files).</li>
          <li>Use <code>data.azurerm_key_vault_secret</code> to fetch secrets dynamically:</li>
        </ul>
        <pre><code>data "azurerm_key_vault_secret" "sp_secret" {
  name         = "client-secret"
  key_vault_id = data.azurerm_key_vault.main.id
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Pass environment variables securely via CI/CD (ADO pipeline vars  env vars).</li>
          <li>Mark outputs as <code>sensitive = true</code> to avoid exposing them in logs.</li>
          <li>For large enterprises, integrate HashiCorp Vault with Terraform provider for dynamic secret retrieval.</li>
        </ul>
        <p><strong>In Practice:</strong> I use a combination of Key Vault for infra secrets + environment variables for transient credentials.</p>
      </div>`},{question:"How to restrict provider access using Service Principals?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Restricting Terraform Provider Access</strong></h3>
        <p>Terraform authenticates to Azure via a Service Principal  ensure it has minimal privileges:</p>
        <pre><code>provider "azurerm" {
  features {}
  client_id       = var.client_id
  client_secret   = var.client_secret
  tenant_id       = var.tenant_id
  subscription_id = var.subscription_id
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Assign only required <strong>RBAC roles</strong> (e.g., Reader, Network Contributor, not Owner).</li>
          <li>Use <strong>separate SPs</strong> per environment to isolate access scopes.</li>
          <li>Enable <strong>conditional access</strong> or MFA for privileged roles.</li>
          <li>Rotate SP credentials regularly and store them in Key Vault.</li>
        </ul>
        <p><strong>In Practice:</strong> My SPs are created per environment, scoped to RG/subscription, with no cross-env access.</p>
      </div>`},{question:"How to ensure compliance and drift monitoring using tools like Checkov/TFSec?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Compliance & Drift Management</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Checkov / TFSec:</strong> Static code analysis tools to catch security misconfigurations before apply.</li>
          <li>Integrate Checkov in pipeline (pre-plan stage):</li>
        </ul>
        <pre><code>- script: |
    pip install checkov
    checkov -d . --quiet</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Detects missing encryption, public access, open security groups, etc.</li>
          <li>Use <code>terraform plan</code> regularly to detect <strong>drift</strong>  mismatch between state and actual infra.</li>
          <li>For automation, integrate <strong>Terraform Cloud drift detection</strong> or schedule plan runs in CI/CD.</li>
        </ul>
        <p><strong>In Practice:</strong> My pipelines fail automatically if Checkov reports any high severity security issues.</p>
      </div>`}]},{title:"25. Terraform - Advanced / Enterprise / Cloud",questions:[{question:"What is Terraform Cloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Cloud Overview</strong></h3>
        <p><strong>Terraform Cloud</strong> is HashiCorps managed SaaS platform that provides a secure, collaborative environment for teams to run Terraform workflows.</p>
        <ul style="margin-left:1.2rem;">
          <li>Manages <strong>remote execution</strong> and <strong>state files</strong> centrally.</li>
          <li>Supports <strong>VCS integrations</strong> (GitHub, Azure DevOps, GitLab, Bitbucket).</li>
          <li>Includes <strong>workspaces</strong>, <strong>RBAC</strong>, <strong>drift detection</strong>, and <strong>Sentinel policy enforcement</strong>.</li>
          <li>Provides <strong>Terraform Cloud Agents</strong> for private network execution.</li>
        </ul>
        <p><strong>In Practice:</strong> We used Terraform Cloud to centralize all infra deployments, removing the need for local state files or manual applies.</p>
      </div>`},{question:"What are the benefits of Terraform Cloud over CLI?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Cloud vs CLI</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <thead>
            <tr><th style="border-bottom:1px solid #ccc;">Feature</th><th style="border-bottom:1px solid #ccc;">CLI</th><th style="border-bottom:1px solid #ccc;">Terraform Cloud</th></tr>
          </thead>
          <tbody>
            <tr><td>State Management</td><td>Local or remote manually configured</td><td>Managed & encrypted automatically</td></tr>
            <tr><td>Collaboration</td><td>Single user</td><td>Team-based runs, RBAC, VCS integration</td></tr>
            <tr><td>Policy Enforcement</td><td>Manual</td><td>Sentinel Policies</td></tr>
            <tr><td>Execution</td><td>Local</td><td>Remote with controlled access & approvals</td></tr>
            <tr><td>Audit & History</td><td>No built-in logs</td><td>Full audit logs and run history</td></tr>
          </tbody>
        </table>
        <p><strong>In Practice:</strong> Using Terraform Cloud removed the need for managing storage accounts and lock files manually.</p>
      </div>`},{question:"What is Sentinel Policy in Terraform Cloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sentinel Policy Framework</strong></h3>
        <p><strong>Sentinel</strong> is HashiCorps Policy-as-Code framework that allows enforcing organizational compliance within Terraform Cloud/Enterprise.</p>
        <ul style="margin-left:1.2rem;">
          <li>Used to validate <strong>plans before apply</strong>  e.g., block public S3 buckets or restrict instance sizes.</li>
          <li>Supports <strong>enforcement levels</strong>: <code>advisory</code>, <code>soft-mandatory</code>, <code>hard-mandatory</code>.</li>
          <li>Written in <strong>Sentinel language</strong> and can access Terraform plan/state data.</li>
        </ul>
        <pre><code>import "tfplan/v2" as tfplan
deny if tfplan.resource_changes["aws_s3_bucket"].change.after.acl == "public-read"</code></pre>
        <p><strong>In Practice:</strong> We used Sentinel to enforce naming conventions and prevent deployment in non-approved regions.</p>
      </div>`},{question:"How does Terraform handle concurrent operations in team environments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Concurrency</strong></h3>
        <p>Terraform uses <strong>state locking</strong> to prevent multiple users or pipelines from modifying infrastructure simultaneously.</p>
        <ul style="margin-left:1.2rem;">
          <li>When a <code>terraform apply</code> starts, the backend (e.g. S3, Azure Blob, or Terraform Cloud) <strong>locks the state file</strong>.</li>
          <li>Other runs wait or fail until the lock is released.</li>
          <li>Terraform Cloud handles this automatically with <strong>remote execution queues</strong>.</li>
        </ul>
        <p><strong>In Practice:</strong> Our pipelines never conflict  Terraform Cloud ensures serialized execution per workspace.</p>
      </div>`},{question:"How do you recover state after accidental deletion in a team setup?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>State Recovery Process</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use backend <strong>versioning</strong> (Azure Blob / S3 / Terraform Cloud) to restore previous versions of state.</li>
          <li>Terraform Cloud automatically keeps <strong>state snapshots</strong> for each run.</li>
          <li>If lost locally, you can use <code>terraform refresh</code> or <code>terraform import</code> to rebuild missing state entries.</li>
          <li>In extreme cases, re-import resources manually using <code>terraform import</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive recovered state by restoring a blob snapshot and re-linking it to the backend config.</p>
      </div>`},{question:"How do you handle DR scenario using Terraform for primary & secondary environments?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Disaster Recovery with Terraform</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Maintain separate <strong>workspaces or state files</strong> for primary & DR environments.</li>
          <li>Use same modules, but parameterize regions & naming via <code>tfvars</code>.</li>
          <li>Deploy DR infra (e.g., secondary VMs, storage, network) in standby mode.</li>
          <li>Sync data (e.g., database replication, blob geo-redundancy) outside Terraform scope.</li>
          <li>In failover, switch DNS / load balancer endpoints to DR region.</li>
        </ul>
        <pre><code>terraform workspace select dr
terraform apply -var-file="dr.tfvars"</code></pre>
        <p><strong>In Practice:</strong> We manage DR infra with separate state + pipeline trigger, keeping both regions consistent.</p>
      </div>`},{question:"How do you implement cross-account provisioning?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cross-Account / Cross-Subscription Provisioning</strong></h3>
        <p>Terraform can manage resources across multiple accounts/subscriptions using multiple provider blocks:</p>
        <pre><code>provider "azurerm" {
  alias           = "prod"
  subscription_id = var.prod_sub_id
  ...
}

provider "azurerm" {
  alias           = "dr"
  subscription_id = var.dr_sub_id
  ...
}

resource "azurerm_resource_group" "dr_rg" {
  name     = "dr-rg"
  provider = azurerm.dr
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Use provider aliases and specify per-resource provider context.</li>
          <li>Store credentials (SP IDs/secrets) securely via Key Vault or Terraform Cloud variables.</li>
        </ul>
        <p><strong>In Practice:</strong> We manage shared networking in one subscription and workloads in another using provider aliasing.</p>
      </div>`},{question:"What happens when an S3 bucket is manually modified (policy drift)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Drift Detection Scenario</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Manual changes (e.g., public ACL added) cause <strong>drift</strong> between state and real infra.</li>
          <li>Terraform detects drift during <code>terraform plan</code>  it will show changes to revert bucket policy.</li>
          <li>If you reapply, Terraform <strong>overwrites manual changes</strong> to match configuration.</li>
          <li>Use <code>ignore_changes</code> in lifecycle if certain attributes are managed externally.</li>
        </ul>
        <p><strong>In Practice:</strong> We schedule nightly <code>terraform plan -detailed-exitcode</code> runs to auto-detect drift.</p>
      </div>`},{question:"How to handle infrastructure drift detection automatically?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automatic Drift Detection</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Terraform Cloud automatically runs <strong>drift detection checks</strong> for each workspace periodically.</li>
          <li>In self-managed setup  schedule <code>terraform plan</code> in CI/CD (daily/weekly) to detect mismatches.</li>
          <li>Use <strong>terraform plan -detailed-exitcode</strong>  exit code <code>2</code> means drift found.</li>
          <li>Integrate with Slack or email notifications for alerting when drift detected.</li>
          <li>Combine with <strong>Checkov/TFSec</strong> to validate compliance continuously.</li>
        </ul>
        <p><strong>In Practice:</strong> Ive implemented automated drift checks via ADO nightly pipeline  posting plan diffs to Teams channel.</p>
      </div>`}]},{title:"26. Terraform - Troubleshooting & Real-World Scenarios",questions:[{question:"What happens if apply fails midway?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Partial Apply Failure</strong></h3>
        <p>If <code>terraform apply</code> fails midway, Terraform may have already created or modified some resources successfully.</p>
        <ul style="margin-left:1.2rem;">
          <li>The <strong>state file</strong> is still updated for resources successfully created before failure.</li>
          <li>Subsequent runs detect partially created resources and try to continue from that point.</li>
          <li>If the failure was due to dependency error or invalid config, fix the code and re-run <code>terraform apply</code>.</li>
          <li>If a resource was created incorrectly, use <code>terraform destroy -target</code> or <code>terraform taint</code> to recreate.</li>
        </ul>
        <p><strong>In Practice:</strong> I had a situation where half of the subnets were deployed; I re-ran apply after fixing NSG rule dependencies  Terraform reconciled automatically.</p>
      </div>`},{question:"How do you rollback infrastructure changes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rollback Strategy</strong></h3>
        <p>Terraform doesnt have a direct rollback command  rollback is achieved through version control and previous state restore.</p>
        <ul style="margin-left:1.2rem;">
          <li>Keep your Terraform code versioned in Git.</li>
          <li>Revert to previous Git commit and run <code>terraform apply</code>  Terraform reconciles state to the earlier version.</li>
          <li>Alternatively, restore previous <strong>state file version</strong> from backend (Azure Blob / S3 / Terraform Cloud).</li>
          <li>In Terraform Cloud, rollback is as simple as selecting a previous run and re-applying it.</li>
        </ul>
        <p><strong>In Practice:</strong> For a production rollback, we restored the previous blob snapshot and re-ran apply with known stable tag.</p>
      </div>`},{question:"What is Terraform taint and when to use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>terraform taint</strong></h3>
        <p><code>terraform taint</code> marks a resource for recreation during the next <code>apply</code>.</p>
        <ul style="margin-left:1.2rem;">
          <li>Used when a resource is unhealthy or manually corrupted.</li>
          <li>Example: A VM deployed successfully but failed post-config  you taint it to force re-creation.</li>
          <li>Command: <code>terraform taint azurerm_virtual_machine.appvm</code></li>
          <li>From Terraform v0.15+, use <code>terraform apply -replace</code> instead (recommended).</li>
        </ul>
        <p><strong>In Practice:</strong> We tainted an App Gateway when SSL cert binding failed due to expired cert file.</p>
      </div>`},{question:"Whats the difference between terraform destroy and terraform apply -destroy?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Destroy vs Apply -destroy</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>terraform destroy</code>  Explicit command that destroys all managed resources in current workspace.</li>
          <li><code>terraform apply -destroy</code>  Equivalent to <code>destroy</code> but runs through the same plan  approval  apply workflow.</li>
          <li>Useful in CI/CD pipelines to review destruction plan before execution.</li>
          <li>Both commands update the state file to reflect deleted resources.</li>
        </ul>
        <p><strong>In Practice:</strong> We use <code>apply -destroy</code> in sandbox teardown pipelines to ensure approval before destruction.</p>
      </div>`},{question:"How do you handle resource drift due to manual changes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Resource Drift Handling</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Run <code>terraform plan</code>  it shows any differences between current config/state and actual cloud infra.</li>
          <li>To fix drift, either:</li>
          <ul>
            <li>Re-apply Terraform code (overwrites manual changes), or</li>
            <li>Use <code>terraform refresh</code> to sync new values into state (if manual change should persist).</li>
          </ul>
          <li>Optionally use <code>ignore_changes</code> lifecycle argument for attributes managed outside Terraform.</li>
        </ul>
        <p><strong>In Practice:</strong> A team member modified NSG rules manually  Terraform plan caught drift and reverted them to code-defined rules.</p>
      </div>`},{question:"How do you debug Terraform apply errors?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Debugging Techniques</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Enable verbose logging with <code>TF_LOG</code> env variable:</li>
          <pre><code>export TF_LOG=DEBUG
terraform apply</code></pre>
          <li>Redirect logs to a file: <code>TF_LOG_PATH=./tf.log</code></li>
          <li>Validate configs: <code>terraform validate</code> and <code>terraform fmt -check</code></li>
          <li>Use <code>terraform plan -out=planfile</code> to verify before apply.</li>
          <li>For provider issues, re-init with <code>terraform init -upgrade</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> I traced an intermittent ACR auth error via TF_LOG output showing expired SP token  fixed by rotating credentials.</p>
      </div>`},{question:"How do you troubleshoot VM configuration mismatch (A4  A5)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VM SKU Mismatch Troubleshooting</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Check <strong>plan output</strong>  Terraform will show size change (A4  A5).</li>
          <li>If Azure rejects due to quota or unsupported region, error appears during apply.</li>
          <li>To fix: update <code>vm_size</code> and re-apply after ensuring region supports SKU.</li>
          <li>If VM already running, Terraform will recreate it (destroy + create) unless you use <code>ignore_changes</code> on vm_size.</li>
        </ul>
        <p><strong>In Practice:</strong> During DR testing, A4 SKU deprecated  changed to A5 and re-deployed after checking quota in Azure Portal.</p>
      </div>`},{question:"How do you import existing resources (Azure, AWS VPC, etc.)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Import</strong></h3>
        <p>Use <code>terraform import</code> to bring existing infra under Terraform management without recreating it.</p>
        <ul style="margin-left:1.2rem;">
          <li>Create resource block in .tf file that matches existing infra.</li>
          <li>Run import command:</li>
          <pre><code>terraform import azurerm_resource_group.prod_rg /subscriptions/xxxx/resourceGroups/prod-rg</code></pre>
          <li>After import, run <code>terraform plan</code> to ensure code matches actual configuration.</li>
        </ul>
        <p><strong>In Practice:</strong> We imported existing VNets and Key Vaults into Terraform to standardize across all environments.</p>
      </div>`},{question:"How to handle manual updates done outside Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Manual Changes Outside Terraform</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Run <code>terraform plan</code> to detect drift.</li>
          <li>Decide whether to revert (apply Terraform) or accept (update state).</li>
          <li>Use <code>terraform refresh</code> or <code>terraform import</code> to sync correct values.</li>
          <li>To prevent this: enforce RBAC so only Terraform pipelines have modify rights.</li>
        </ul>
        <p><strong>In Practice:</strong> One engineer manually added tags  Terraform detected drift, and we updated locals block to include new tags permanently.</p>
      </div>`},{question:"How to re-sync Terraform with Prod state after changes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Re-Sync Terraform with Production State</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Run <code>terraform refresh</code>  updates state to match actual infra.</li>
          <li>If new resources exist, import them using <code>terraform import</code>.</li>
          <li>If removed resources exist in state, run <code>terraform state rm</code> to clean orphaned entries.</li>
          <li>Finally, re-run <code>terraform plan</code> to verify all in sync.</li>
        </ul>
        <p><strong>In Practice:</strong> After a hotfix deployment outside Terraform, we re-synced via import & refresh to align prod infra with Terraform state.</p>
      </div>`}]},{title:"27. Terraform  State Management",questions:[{question:"What is terraform.tfstate?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>terraform.tfstate  The Source of Truth</strong></h3>
        <p><code>terraform.tfstate</code> is a JSON file that stores the actual state of your deployed infrastructure  it maps your Terraform configuration to real cloud resources.</p>
        <ul style="margin-left:1.2rem;">
          <li>Tracks every resources ID, metadata, dependencies, and current configuration.</li>
          <li>Used by Terraform to determine what needs to be created, updated, or destroyed during the next <code>plan/apply</code>.</li>
        </ul>
        <p><strong>Example:</strong> The file contains resource entries like <code>azurerm_resource_group.rg</code> with its ID and properties.</p>
        <p><strong>In Practice:</strong> Never edit <code>tfstate</code> manually  instead, use <code>terraform state mv</code>, <code>rm</code>, or <code>import</code> commands.</p>
      </div>`},{question:"What is terraform.tfstate.backup?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>terraform.tfstate.backup  Safety Copy</strong></h3>
        <p>Terraform automatically creates a backup file (<code>terraform.tfstate.backup</code>) before modifying the main state file.</p>
        <ul style="margin-left:1.2rem;">
          <li>Used to recover in case the main <code>tfstate</code> becomes corrupted or overwritten.</li>
          <li>It ensures rollback capability after failed <code>apply</code> or <code>plan</code>.</li>
        </ul>
        <p><strong>In Practice:</strong> We store both <code>tfstate</code> and <code>tfstate.backup</code> in Azure Blob (remote backend) with versioning enabled for quick recovery.</p>
      </div>`},{question:"What is the use of state file?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Purpose of Terraform State File</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Maintains mapping between Terraform configuration and real resources.</li>
          <li>Detects drift (changes outside Terraform).</li>
          <li>Improves performance by caching resource attributes locally.</li>
          <li>Enables dependency resolution during resource creation.</li>
        </ul>
        <p><strong>In Practice:</strong> During <code>terraform plan</code>, Terraform compares the state file with actual cloud infra to determine changes.</p>
      </div>`},{question:"What is the difference between local and remote state?",answerHtml:`
      <div class="answer-rich">
        <h3> vs  <strong>Local vs Remote State</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;width:100%;">
          <thead><tr><th>Aspect</th><th>Local State</th><th>Remote State</th></tr></thead>
          <tbody>
            <tr><td>Storage</td><td>Stored on local machine as <code>terraform.tfstate</code></td><td>Stored in cloud storage (Azure Blob, S3, Terraform Cloud)</td></tr>
            <tr><td>Collaboration</td><td>Not suitable for teams</td><td>Ideal for shared environments</td></tr>
            <tr><td>Security</td><td>Less secure, local exposure risk</td><td>Can use encryption, RBAC, and versioning</td></tr>
            <tr><td>Locking</td><td>No locking</td><td>Supports locking to avoid parallel edits</td></tr>
          </tbody>
        </table>
        <p><strong>In Practice:</strong> Local state is fine for POC, but in production, always use remote backend (like Azure Blob or Terraform Cloud).</p>
      </div>`},{question:"What are the benefits of remote state?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Benefits of Remote State</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Centralized storage</strong>  accessible by all team members.</li>
          <li> <strong>State locking</strong>  prevents concurrent apply operations.</li>
          <li> <strong>Versioning & backup</strong>  automatic snapshot history for rollback.</li>
          <li> <strong>Encryption</strong>  protects sensitive data (passwords, keys).</li>
          <li> <strong>Integration</strong>  works well with CI/CD and pipelines.</li>
        </ul>
        <p><strong>In Practice:</strong> Azure DevOps pipelines use remote backend in Azure Storage with locking enabled via Azure Blob leases.</p>
      </div>`},{question:"What is a backend in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Backend  State Management Engine</strong></h3>
        <p>A <strong>backend</strong> defines where and how Terraform stores its state file.</p>
        <ul style="margin-left:1.2rem;">
          <li>Determines state storage location (local or remote).</li>
          <li>Handles locking, encryption, and consistency.</li>
          <li>Examples: <code>local</code>, <code>azurerm</code>, <code>s3</code>, <code>gcs</code>, <code>remote</code> (Terraform Cloud).</li>
        </ul>
        <p><strong>In Practice:</strong> We used <code>azurerm</code> backend to store tfstate in an Azure Blob container with SAS or SPN auth.</p>
      </div>`},{question:"What is the syntax of backend block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Backend Block Syntax</strong></h3>
        <pre><code>terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstate1234"
    container_name       = "tfstate"
    key                  = "prod.terraform.tfstate"
  }
}</code></pre>
        <p><strong>Notes:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>This block must be initialized using <code>terraform init</code>.</li>
          <li>Backend settings are not stored in the state  they are part of the config only.</li>
        </ul>
        <p><strong>In Practice:</strong> Different workspaces (dev, uat, prod) use different state keys in the same storage account.</p>
      </div>`},{question:"What are supported backends in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Supported Terraform Backends</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Local</strong>  stores state locally.</li>
          <li><strong>Remote</strong>  Terraform Cloud or Enterprise.</li>
          <li><strong>azurerm</strong>  Azure Blob Storage backend.</li>
          <li><strong>s3</strong>  AWS S3 backend with DynamoDB for locking.</li>
          <li><strong>gcs</strong>  Google Cloud Storage backend.</li>
          <li><strong>consul</strong>  HashiCorp Consul backend for HA setups.</li>
          <li><strong>http</strong> / <strong>etcd</strong>  custom or API-based storage.</li>
        </ul>
        <p><strong>In Practice:</strong> For Azure-based orgs, <code>azurerm</code> is the most used backend for state centralization.</p>
      </div>`},{question:"How to configure remote backend?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Remote Backend Configuration (Azure Example)</strong></h3>
        <pre><code>terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstateglobal"
    container_name       = "tfstate"
    key                  = "dev.terraform.tfstate"
  }
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li>Initialize with <code>terraform init -backend-config="key=prod.tfstate"</code>.</li>
          <li>Requires storage account, blob container, and access credentials.</li>
          <li>Locking supported via Azure blob leases.</li>
        </ul>
        <p><strong>In Practice:</strong> In our CI/CD, backend config values are injected dynamically from DevOps variable groups.</p>
      </div>`},{question:"What is state locking?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform State Locking</strong></h3>
        <p>State locking prevents concurrent operations (like two applies) from modifying the same state at the same time.</p>
        <ul style="margin-left:1.2rem;">
          <li>Implemented differently by each backend:
            <ul>
              <li><strong>azurerm:</strong> Uses Blob Storage leases.</li>
              <li><strong>s3:</strong> Uses DynamoDB table.</li>
              <li><strong>Terraform Cloud:</strong> Handles automatically.</li>
            </ul>
          </li>
          <li>If another user tries to apply, Terraform shows a state lock message.</li>
        </ul>
        <p><strong>In Practice:</strong> We rely on locking to prevent DevOps pipeline and local runs from corrupting the shared state.</p>
      </div>`},{question:"What is state file encryption?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform State Encryption</strong></h3>
        <p>Since state file can contain sensitive information (like secrets, passwords, connection strings), encryption is crucial.</p>
        <ul style="margin-left:1.2rem;">
          <li>Azure Blob backend supports <strong>encryption-at-rest</strong> (via SSE).</li>
          <li>S3 supports <strong>KMS encryption</strong>.</li>
          <li>Terraform Cloud encrypts state automatically.</li>
          <li>Additional layer: Use Vault or Key Vault to store sensitive variables instead of state.</li>
        </ul>
        <p><strong>In Practice:</strong> We enforce encryption policies via Azure Policy on all Terraform storage accounts.</p>
      </div>`},{question:"How do you recover if local tfstate is deleted?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recovering Deleted State</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>If using remote backend  state is safe in cloud (Azure Blob, etc.).</li>
          <li>If local backend  restore from <code>terraform.tfstate.backup</code>.</li>
          <li>If both lost  use <code>terraform import</code> to recreate state manually.</li>
        </ul>
        <pre><code>terraform import azurerm_resource_group.rg /subscriptions/.../resourceGroups/rg1</code></pre>
        <p><strong>In Practice:</strong> We enabled blob versioning so any deleted or corrupted tfstate can be restored instantly.</p>
      </div>`},{question:"How do you handle state file in a team setup?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Team State Management Practices</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>remote backend</strong> (Azure Blob, Terraform Cloud)  single source of truth.</li>
          <li>Enable <strong>locking</strong> to avoid conflicts.</li>
          <li>Enable <strong>versioning</strong> for rollback.</li>
          <li>Restrict write access  only CI/CD pipelines should apply changes.</li>
          <li>Store backend configuration securely (not in code).</li>
        </ul>
        <p><strong>In Practice:</strong> We disabled direct <code>terraform apply</code> locally  all state changes go through controlled DevOps pipelines using remote backend.</p>
      </div>`}]},{title:"28. Terraform  Backend (Azure, S3, etc.)",questions:[{question:"What is the Azure backend in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AzureRM Backend  Centralized State Management on Azure</strong></h3>
        <p>The <strong>azurerm backend</strong> in Terraform stores your state file (<code>terraform.tfstate</code>) securely inside an <strong>Azure Storage Account</strong> container.</p>
        <ul style="margin-left:1.2rem;">
          <li>Enables collaboration  multiple team members can share the same state remotely.</li>
          <li>Supports <strong>state locking</strong> via Azure Blob leases (prevents parallel applies).</li>
          <li>Provides <strong>encryption-at-rest</strong> and integration with Azure AD / Service Principals.</li>
        </ul>
        <pre><code>terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstateglobal"
    container_name       = "tfstate"
    key                  = "prod.terraform.tfstate"
  }
}</code></pre>
        <p><strong>In Practice:</strong> This backend is the standard in Azure environments  especially when integrating Terraform with Azure DevOps pipelines.</p>
      </div>`},{question:"What arguments are required for Azure backend?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Required Arguments for AzureRM Backend</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>resource_group_name</strong>  Name of the Resource Group containing the Storage Account.</li>
          <li><strong>storage_account_name</strong>  The Storage Account where state will be stored.</li>
          <li><strong>container_name</strong>  The Blob container to hold the state file.</li>
          <li><strong>key</strong>  Path and name of the state file (acts like a unique key per environment).</li>
        </ul>
        <p><strong>Optional Arguments:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>subscription_id</strong>  if deploying across subscriptions.</li>
          <li><strong>access_key</strong>  manual authentication (not recommended, use SPN or Managed Identity).</li>
          <li><strong>sas_token</strong>  for temporary access.</li>
          <li><strong>use_msi = true</strong>  to use Managed Identity in pipelines.</li>
        </ul>
        <p><strong>In Practice:</strong> In Azure DevOps, we pass these arguments dynamically using <code>-backend-config</code> to keep backend flexible per environment.</p>
      </div>`},{question:"How to configure S3 backend in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>S3 Backend  For AWS-based Remote State Storage</strong></h3>
        <p>The <strong>S3 backend</strong> stores Terraform state in an AWS S3 bucket and optionally uses DynamoDB for state locking.</p>
        <pre><code>terraform {
  backend "s3" {
    bucket         = "terraform-state-bucket"
    key            = "prod/network/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-locks"
    encrypt        = true
  }
}</code></pre>
        <ul style="margin-left:1.2rem;">
          <li><strong>bucket</strong>  S3 bucket name where state is stored.</li>
          <li><strong>key</strong>  Path of the tfstate file inside the bucket.</li>
          <li><strong>region</strong>  AWS region of the bucket.</li>
          <li><strong>dynamodb_table</strong>  Optional  used for state locking.</li>
          <li><strong>encrypt</strong>  Enables SSE (Server-Side Encryption) for security.</li>
        </ul>
        <p><strong>In Practice:</strong> In AWS-based projects, this setup is used with IAM roles or access keys provided via environment variables in CI/CD.</p>
      </div>`},{question:"What is the difference between S3 and Azure backend?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Comparison: S3 vs AzureRM Backend</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;width:100%;">
          <thead><tr><th>Aspect</th><th>AzureRM Backend</th><th>S3 Backend</th></tr></thead>
          <tbody>
            <tr><td>Storage Service</td><td>Azure Blob Storage</td><td>AWS S3 Bucket</td></tr>
            <tr><td>Locking Mechanism</td><td>Blob Lease</td><td>DynamoDB Table</td></tr>
            <tr><td>Encryption</td><td>Azure SSE + RBAC</td><td>S3 SSE + KMS</td></tr>
            <tr><td>Authentication</td><td>Service Principal / MSI / SAS</td><td>Access Keys / IAM Role</td></tr>
            <tr><td>Config Simplicity</td><td>Few parameters (4 required)</td><td>More granular config (region, table, encrypt)</td></tr>
            <tr><td>Common Usage</td><td>Azure DevOps pipelines</td><td>AWS CodePipeline, Jenkins, GitHub Actions</td></tr>
          </tbody>
        </table>
        <p><strong>In Practice:</strong> Both are functionally identical in Terraform behavior  only their authentication and locking mechanisms differ.</p>
      </div>`},{question:"What is the use of key argument in backend block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>key  Unique Identifier for State File</strong></h3>
        <p>The <code>key</code> argument defines the path and name of the state file inside your backend storage.</p>
        <ul style="margin-left:1.2rem;">
          <li>It acts as a namespace for your state within the storage.</li>
          <li>Used to separate environments (e.g. dev.tfstate, prod.tfstate).</li>
          <li>Supports folder-like hierarchy in cloud storage.</li>
        </ul>
        <pre><code>key = "env/dev/app1/terraform.tfstate"</code></pre>
        <p><strong>In Practice:</strong> We dynamically assign key values using variables  e.g. <code>key = "&dollar;{var.env}/&dollar;{var.module}.tfstate"</code> to separate per-environment states.</p>
      </div>`},{question:"What is the role of access_key and secret_key in backend configuration?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Authentication Parameters  access_key & secret_key</strong></h3>
        <p>These are used to authenticate Terraform with the backend service (Azure or AWS) when no other identity mechanism is configured.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>access_key</strong>  Identifies the user or application.</li>
          <li><strong>secret_key</strong>  Secret credential used to verify access.</li>
          <li>Can be passed in <code>backend-config</code> or environment variables (more secure).</li>
        </ul>
        <pre><code>terraform init -backend-config="access_key=XXXX" -backend-config="secret_key=YYYY"</code></pre>
        <p><strong>Best Practice:</strong> Avoid hardcoding credentials in code. Use:</p>
        <ul style="margin-left:1.2rem;">
          <li>Azure: Managed Identity / Service Principal + Azure CLI login.</li>
          <li>AWS: IAM Role / Environment Variables.</li>
        </ul>
        <p><strong>In Practice:</strong> In CI/CD pipelines, credentials are injected at runtime via secure DevOps variable groups or secrets store.</p>
      </div>`}]},{title:"29. Terraform  Lifecycle, Dependencies & Meta-Arguments",questions:[{question:"What is lifecycle block in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Lifecycle Block  Controlling Resource Behavior</strong></h3>
        <p>The <code>lifecycle</code> block in Terraform is a <strong>meta-argument</strong> that lets you control how Terraform creates, updates, or destroys a resource.</p>
        <p><strong>Purpose:</strong> To customize the default create  update  destroy flow of Terraform.</p>
        <pre><code>resource "azurerm_virtual_machine" "vm" {
  name = "app-vm"
  lifecycle {
    create_before_destroy = true
    prevent_destroy       = false
    ignore_changes        = [tags]
  }
}</code></pre>
        <p><strong>Common Use Cases:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Maintain zero downtime during updates (create_before_destroy).</li>
          <li>Protect critical infra from accidental deletion (prevent_destroy).</li>
          <li>Ignore certain changes like tags or OS updates (ignore_changes).</li>
        </ul>
      </div>`},{question:"What is create_before_destroy?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>create_before_destroy  Ensures Zero Downtime</strong></h3>
        <p>This lifecycle argument ensures that a new resource is created <strong>before</strong> Terraform destroys the old one.</p>
        <pre><code>lifecycle {
  create_before_destroy = true
}</code></pre>
        <p><strong>Use Case:</strong> When updating load balancers, VMs, or NICs that should not go down during replacement.</p>
        <p><strong>Note:</strong> It requires Terraform to be able to create duplicates temporarily. If the resource name or unique constraint prevents duplication, it will fail.</p>
      </div>`},{question:"What is prevent_destroy?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>prevent_destroy  Protect Critical Resources</strong></h3>
        <p>When set to true, Terraform will <strong>refuse to destroy</strong> the resource even if it is removed from code or replaced.</p>
        <pre><code>lifecycle {
  prevent_destroy = true
}</code></pre>
        <p><strong>Use Case:</strong> Protect production resources like databases, storage accounts, or state buckets from accidental deletion.</p>
        <p><strong>In Practice:</strong> During CI/CD or shared infra, we use this for any data-bearing resource (SQL, S3, Storage Account).</p>
        <p><strong>Terraform Behavior:</strong> Shows error: <em>Resource is marked prevent_destroy = true</em> during apply.</p>
      </div>`},{question:"What is ignore_changes?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>ignore_changes  Avoid Unnecessary Drift Updates</strong></h3>
        <p>Used to tell Terraform to <strong>ignore modifications</strong> made to specific attributes outside Terraform.</p>
        <pre><code>lifecycle {
  ignore_changes = [ tags, "os_disk" ]
}</code></pre>
        <p><strong>Use Case:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Ignore OS updates or metadata that change dynamically.</li>
          <li>Prevent Terraform from replacing VMs when tags are modified manually.</li>
        </ul>
        <p><strong>In Practice:</strong> Often used in DevOps pipelines to allow cloud automation tools (like Azure Policy or AWS Config) to modify tags or settings without breaking Terraform state alignment.</p>
      </div>`},{question:"What is the purpose of depends_on?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>depends_on  Explicit Dependency Management</strong></h3>
        <p>Terraform usually determines dependencies <strong>implicitly</strong> through references, but if not, we can force dependency using <code>depends_on</code>.</p>
        <pre><code>resource "azurerm_network_interface" "nic" {
  name = "vm-nic"
}

resource "azurerm_virtual_machine" "vm" {
  name = "app-vm"
  depends_on = [azurerm_network_interface.nic]
}</code></pre>
        <p><strong>Purpose:</strong> To ensure a resource is created only after another is ready, even if no attribute reference exists.</p>
        <p><strong>Use Case:</strong> Custom dependencies, null_resources, provisioners, or module-level sequencing.</p>
      </div>`},{question:"What is count and for_each?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>count & for_each  Resource Iteration Meta-Arguments</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>count</strong>  Used to create multiple resources based on an integer value.</li>
          <li><strong>for_each</strong>  Used to create resources based on a map or set of strings, providing fine-grained control.</li>
        </ul>
        <pre><code># Using count
resource "azurerm_resource_group" "rg" {
  count = 3
  name  = "rg-&dollar;{count.index}"
}

# Using for_each
resource "azurerm_resource_group" "rg" {
  for_each = toset(["dev", "qa", "prod"])
  name     = "rg-&dollar;{each.key}"
}</code></pre>
        <p><strong>Key Difference:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Use <strong>count</strong> when resources are identical and index-based.</li>
          <li>Use <strong>for_each</strong> when resources are distinct and map-based (for stable addressing).</li>
        </ul>
      </div>`},{question:"What is a dynamic block?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Dynamic Block  Generate Nested Blocks Programmatically</strong></h3>
        <p>Dynamic blocks are used to create multiple nested configuration blocks dynamically, based on loops or variable data.</p>
        <pre><code>resource "azurerm_network_security_group" "nsg" {
  name = "app-nsg"

  dynamic "security_rule" {
    for_each = var.rules
    content {
      name                       = each.key
      priority                   = each.value.priority
      direction                  = each.value.direction
      access                     = each.value.access
      protocol                   = each.value.protocol
    }
  }
}</code></pre>
        <p><strong>Use Case:</strong> Useful when nested attributes (like security rules, tags, users) need to be created dynamically from a list/map.</p>
      </div>`},{question:"What is a conditional expression?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Conditional Expression  Ternary Logic in Terraform</strong></h3>
        <p>Terraform supports <strong>ternary conditional logic</strong> to assign values dynamically.</p>
        <pre><code>name = var.env == "prod" ? "app-prod" : "app-dev"</code></pre>
        <p><strong>Use Case:</strong> Enable/disable certain resources, choose sizes, or set tags based on environment.</p>
        <p><strong>Example:</strong></p>
        <pre><code>count = var.enable_vm ? 1 : 0</code></pre>
        <p>This conditionally creates the resource only if <code>enable_vm</code> is true.</p>
      </div>`},{question:"What is implicit vs explicit dependency?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Implicit vs Explicit Dependencies</strong></h3>
        <table border="1" cellpadding="6" style="border-collapse:collapse;width:100%;">
          <thead><tr><th>Type</th><th>Description</th><th>Example</th></tr></thead>
          <tbody>
            <tr>
              <td>Implicit</td>
              <td>Automatically inferred by Terraform when one resource references another.</td>
              <td><code>subnet_id = azurerm_subnet.main.id</code></td>
            </tr>
            <tr>
              <td>Explicit</td>
              <td>Manually declared using <code>depends_on</code> when no attribute reference exists.</td>
              <td><code>depends_on = [azurerm_network_security_group.nsg]</code></td>
            </tr>
          </tbody>
        </table>
        <p><strong>In Practice:</strong> Prefer implicit dependencies  explicit should be last resort when required for orchestration.</p>
      </div>`},{question:"What is the move block and what does it do?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>move Block  Safely Renaming or Refactoring Resources</strong></h3>
        <p>The <strong>move block</strong> (introduced in Terraform 1.6+) is used to instruct Terraform to move state from one resource address to another  typically during refactor or renaming.</p>
        <pre><code>moved {
  from = azurerm_resource_group.old_rg
  to   = azurerm_resource_group.new_rg
}</code></pre>
        <p><strong>Use Case:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>Renaming resource labels safely without re-creation.</li>
          <li>Moving state between modules or resource types.</li>
          <li>Refactoring modules or flattening nested structures.</li>
        </ul>
        <p><strong>In Practice:</strong> Prevents accidental destroy/create cycle during refactoring. Used with <code>terraform plan</code> to validate safe migration.</p>
      </div>`}]},{title:"30. Terraform  Functions & Expressions",questions:[{question:"What are functions in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Functions in Terraform</strong></h3>
        <p>Functions in Terraform are <strong>built-in helpers</strong> that allow you to transform, compute, or extract data dynamically inside configuration files.</p>
        <p><strong>Purpose:</strong> To make configurations dynamic instead of hardcoding values.</p>
        <pre><code>resource "azurerm_storage_account" "sa" {
  name = lower(join("", ["App", var.env, "Storage"]))
}</code></pre>
        <p><strong>Explanation:</strong> Here <code>lower()</code> and <code>join()</code> functions are used to build a lowercase dynamic name based on variables.</p>
        <p><strong>Key Point:</strong> Terraform doesnt allow user-defined functions  only built-in ones.</p>
      </div>`},{question:"What are built-in functions in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Built-in Functions  Terraforms Power Tools</strong></h3>
        <p>Terraform includes 100+ built-in functions categorized into:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>String functions:</strong> lower(), upper(), replace(), substr(), join(), split()</li>
          <li><strong>Collection functions:</strong> length(), element(), contains(), lookup(), keys(), values()</li>
          <li><strong>File functions:</strong> file(), templatefile(), filebase64()</li>
          <li><strong>Numeric functions:</strong> min(), max(), abs(), floor(), ceil()</li>
          <li><strong>Date/time functions:</strong> timestamp(), formatdate()</li>
          <li><strong>Encoding functions:</strong> jsonencode(), base64encode()</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>tags = {
  environment = upper(var.env)
  created_at  = timestamp()
}</code></pre>
        <p>These make infrastructure code dynamic, consistent, and reusable.</p>
      </div>`},{question:"What is the use of lookup function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>lookup()  Safe Map Access</strong></h3>
        <p>The <code>lookup()</code> function retrieves a value from a map safely  with an optional default value if key is missing.</p>
        <pre><code>lookup(map, key, default)</code></pre>
        <p><strong>Example:</strong></p>
        <pre><code>lookup(var.vm_sizes, var.env, "Standard_B2s")</code></pre>
        <p>If <code>var.env</code> key exists in <code>var.vm_sizes</code>, it returns that; otherwise it returns the default Standard_B2s.</p>
        <p><strong>Use Case:</strong> Handling environment-based configurations safely without errors.</p>
      </div>`},{question:"What is the use of join function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>join()  Combine List into a String</strong></h3>
        <p><code>join()</code> merges list elements into a single string separated by a delimiter.</p>
        <pre><code>join(delimiter, list)</code></pre>
        <p><strong>Example:</strong></p>
        <pre><code>join("-", ["app", var.env, "eastus"])  "app-dev-eastus"</code></pre>
        <p><strong>Use Case:</strong> Useful for naming conventions, resource IDs, or labels.</p>
      </div>`},{question:"What is the use of split function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>split()  Break String into List</strong></h3>
        <p>The <code>split()</code> function splits a string into a list based on a given separator.</p>
        <pre><code>split(delimiter, string)</code></pre>
        <p><strong>Example:</strong></p>
        <pre><code>split(",", "dev,qa,prod")  ["dev", "qa", "prod"]</code></pre>
        <p><strong>Use Case:</strong> Parse comma-separated environment lists or tags.</p>
      </div>`},{question:"What is the use of length function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>length()  Count Elements</strong></h3>
        <p>The <code>length()</code> function returns number of elements in a list, map, or number of characters in a string.</p>
        <pre><code>length(var.subnets)  3</code></pre>
        <p><strong>Use Case:</strong> For validations, dynamic loop counts, or naming logic.</p>
        <p><strong>Example:</strong></p>
        <pre><code>count = length(var.subnets)</code></pre>
      </div>`},{question:"What is the use of element function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>element()  Fetch Element by Index</strong></h3>
        <p>The <code>element()</code> function returns the item at a specific index in a list.</p>
        <pre><code>element(list, index)</code></pre>
        <p><strong>Example:</strong></p>
        <pre><code>element(["dev", "qa", "prod"], 1)  "qa"</code></pre>
        <p><strong>Use Case:</strong> Access specific subnet or environment value from a list dynamically.</p>
      </div>`},{question:"What is the use of contains function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>contains()  Check Membership</strong></h3>
        <p>The <code>contains()</code> function checks if a list or set contains a given value and returns a boolean.</p>
        <pre><code>contains(["dev", "qa", "prod"], "qa")  true</code></pre>
        <p><strong>Use Case:</strong> Conditional logic based on environment or feature inclusion.</p>
        <p><strong>Example:</strong></p>
        <pre><code>count = contains(var.envs, "prod") ? 2 : 1</code></pre>
      </div>`},{question:"What is the use of file function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>file()  Read File Content</strong></h3>
        <p>The <code>file()</code> function reads raw text from a local file and returns its content as a string.</p>
        <pre><code>file("userdata.sh")</code></pre>
        <p><strong>Use Case:</strong> To pass script files or JSON templates into resources like VM extensions, cloud-init, etc.</p>
        <p><strong>Example:</strong></p>
        <pre><code>custom_data = file("&dollar;{path.module}/init.sh")</code></pre>
      </div>`},{question:"What is the use of templatefile function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>templatefile()  Parameterized File Rendering</strong></h3>
        <p>The <code>templatefile()</code> function allows you to load an external template file and replace variables dynamically.</p>
        <pre><code>templatefile(path, vars)</code></pre>
        <p><strong>Example:</strong></p>
        <pre><code>templatefile("&dollar;{path.module}/userdata.tpl", {
  env  = var.env
  name = var.app_name
})</code></pre>
        <p><strong>Use Case:</strong> Ideal for dynamic bash scripts, ARM templates, or YAML manifests where placeholders need runtime replacement.</p>
        <p><strong>Difference:</strong> <code>file()</code> reads plain text, <code>templatefile()</code> substitutes variables.</p>
      </div>`},{question:"What is a conditional expression in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Conditional Expression  Dynamic Logic Control</strong></h3>
        <p>Conditional expressions are used to dynamically select a value based on a condition (similar to ternary operator).</p>
        <pre><code>condition ? true_value : false_value</code></pre>
        <p><strong>Example:</strong></p>
        <pre><code>vm_size = var.env == "prod" ? "Standard_D2s_v3" : "Standard_B1s"</code></pre>
        <p><strong>Use Case:</strong> Enable/disable resources, choose configuration or tag values based on environment.</p>
        <p><strong>Advanced Example:</strong></p>
        <pre><code>count = contains(["prod","uat"], var.env) ? 1 : 0</code></pre>
      </div>`}]},{title:"31. Terraform  Debugging, Logs & Error Handling",questions:[{question:"How to enable debug logs in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Enable Debug Logs in Terraform</strong></h3>
        <p>Terraform debugging is controlled via environment variables. You can enable detailed logs by setting <code>TF_LOG</code> before running a command.</p>
        <pre><code>export TF_LOG=DEBUG
terraform apply</code></pre>
        <p><strong>Log Levels:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li><strong>TRACE</strong>  Most detailed (internal function traces)</li>
          <li><strong>DEBUG</strong>  Detailed troubleshooting info</li>
          <li><strong>INFO</strong>  General operational information</li>
          <li><strong>WARN</strong>  Warnings that dont stop execution</li>
          <li><strong>ERROR</strong>  Critical errors only</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>export TF_LOG=TRACE
terraform plan</code></pre>
        <p>This prints detailed logs directly to the console.</p>
      </div>`},{question:"What is TF_LOG?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>TF_LOG  Logging Level Controller</strong></h3>
        <p><code>TF_LOG</code> is an environment variable used to control the verbosity of Terraform logs.</p>
        <pre><code>export TF_LOG=DEBUG</code></pre>
        <p><strong>Available Levels:</strong></p>
        <ul>
          <li>TRACE</li>
          <li>DEBUG</li>
          <li>INFO</li>
          <li>WARN</li>
          <li>ERROR</li>
        </ul>
        <p><strong>Use Case:</strong> Quickly enable on-screen logs for troubleshooting without modifying configuration files.</p>
      </div>`},{question:"What is TF_LOG_PATH?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>TF_LOG_PATH  Save Logs to File</strong></h3>
        <p><code>TF_LOG_PATH</code> defines where the Terraform debug log should be written.</p>
        <pre><code>export TF_LOG=DEBUG
export TF_LOG_PATH="terraform-debug.log"
terraform apply</code></pre>
        <p>This creates a detailed log file for review instead of cluttering your console.</p>
        <p><strong>Best Practice:</strong> Always use TF_LOG_PATH during CI/CD runs for post-mortem debugging.</p>
      </div>`},{question:"How to troubleshoot Terraform issues?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Troubleshooting Steps</strong></h3>
        <ol style="margin-left:1.2rem;">
          <li>Run <code>terraform validate</code> to check syntax and provider errors.</li>
          <li>Use <code>terraform plan -out=tfplan</code> to see proposed changes safely.</li>
          <li>Enable logs with <code>TF_LOG=DEBUG</code> to inspect provider operations.</li>
          <li>Inspect <code>.terraform</code> directory for provider plugins or lock file mismatches.</li>
          <li>Check <code>terraform state list</code> for missing resources or drift.</li>
          <li>Reinitialize with <code>terraform init -reconfigure</code> if backend or provider changed.</li>
        </ol>
        <p><strong>Pro Tip:</strong> If you face state corruption, take a backup of <code>terraform.tfstate</code> before manual edits.</p>
      </div>`},{question:"How do you handle validation or dependency errors?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Validation & Dependency Error Handling</strong></h3>
        <p><strong>1 Validation Errors</strong>  occur when syntax, variable types, or missing attributes are detected.</p>
        <p><strong>Fix:</strong></p>
        <pre><code>terraform validate
terraform fmt</code></pre>
        <p> Correct syntax and type mismatches.</p>
        <p><strong>2 Dependency Errors</strong>  happen when resources rely on others that dont exist or are misordered.</p>
        <p><strong>Fix:</strong></p>
        <ul>
          <li>Use <code>depends_on</code> to explicitly define dependencies.</li>
          <li>Check module outputs/inputs mismatches.</li>
          <li>Re-run <code>terraform graph</code> to visualize dependencies.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>resource "azurerm_network_interface" "nic" {
  depends_on = [azurerm_virtual_network.vnet]
}</code></pre>
      </div>`},{question:"How do you perform dry-run debugging in CI/CD?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Dry-run (No Changes) Debugging in CI/CD</strong></h3>
        <p>In pipelines, you can use <code>terraform plan</code> to simulate an apply without changing resources.</p>
        <pre><code>terraform plan -detailed-exitcode</code></pre>
        <p><strong>Exit Codes:</strong></p>
        <ul>
          <li>0  No changes</li>
          <li>1  Error</li>
          <li>2  Changes present</li>
        </ul>
        <p><strong>Use in CI/CD:</strong></p>
        <pre><code>terraform init
terraform validate
terraform plan -detailed-exitcode -out=tfplan || EXIT_CODE=&dollar;?
if [ &dollar;EXIT_CODE -eq 1 ]; then
  echo "Error detected!"
  exit 1
elif [ &dollar;EXIT_CODE -eq 2 ]; then
  echo "Changes detected!"
else
  echo "No changes!"
fi</code></pre>
        <p><strong>Pro Tip:</strong> Combine with <code>TF_LOG_PATH</code> for full pipeline logs to debug silently failing plans.</p>
      </div>`}]},{title:"32. Terraform  Cloud & Enterprise",questions:[{question:"What is Terraform Cloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Cloud  Hosted Terraform Platform</strong></h3>
        <p><strong>Terraform Cloud</strong> is a SaaS (managed) version of Terraform provided by HashiCorp. It centralizes Terraform operations like plan, apply, state management, and policy enforcement.</p>
        <p><strong>Key Features:</strong></p>
        <ul>
          <li> Remote & secure <strong>state storage</strong> (no need for backend setup)</li>
          <li> Team-based access control and audit logs</li>
          <li> Remote <strong>runs & applies</strong> in a controlled environment</li>
          <li> Sensitive variable storage with encryption</li>
          <li> Integration with VCS (GitHub, GitLab, Azure Repos, Bitbucket)</li>
          <li> Policy enforcement using <strong>Sentinel</strong></li>
        </ul>
        <p><strong>Example Use Case:</strong> Instead of running Terraform locally, you connect your GitHub repo  push code  Terraform Cloud automatically triggers <code>plan</code> and <code>apply</code>.</p>
      </div>`},{question:"What is Terraform Enterprise?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Enterprise  Self-Hosted Terraform Cloud</strong></h3>
        <p><strong>Terraform Enterprise (TFE)</strong> is the <em>self-hosted</em> commercial version of Terraform Cloud for organizations needing full control, compliance, and internal security.</p>
        <p><strong>Key Features:</strong></p>
        <ul>
          <li> Installed in your private datacenter or VPC</li>
          <li> Enhanced security & compliance (SSO, audit logs, private networking)</li>
          <li> Private module & policy registries</li>
          <li> Full integration with corporate identity (Azure AD, LDAP, Okta)</li>
          <li> Advanced policy control using Sentinel and custom workflows</li>
        </ul>
        <p><strong>When to Use:</strong> Enterprise-grade companies with strict data residency and compliance requirements (like BFSI, healthcare, or govt).</p>
      </div>`},{question:"What are workspaces in Terraform Cloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Cloud Workspaces</strong></h3>
        <p>A <strong>workspace</strong> in Terraform Cloud represents one isolated instance of a Terraform configuration and its state.</p>
        <p>Each workspace has:</p>
        <ul>
          <li> Its own <strong>state file</strong></li>
          <li> Its own <strong>variables</strong> (env vars, TF vars)</li>
          <li> Independent <strong>execution runs</strong></li>
        </ul>
        <p><strong>Example:</strong> You can create separate workspaces for <code>dev</code>, <code>uat</code>, <code>prod</code>  all using the same Terraform code.</p>
        <pre><code>terraform workspace new dev
terraform workspace select prod</code></pre>
        <p>In Terraform Cloud UI, each workspace is linked to a VCS branch or directory.</p>
      </div>`},{question:"What is Sentinel policy in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Sentinel  Policy as Code Framework</strong></h3>
        <p><strong>Sentinel</strong> is HashiCorps <em>policy-as-code</em> framework integrated into Terraform Cloud/Enterprise.</p>
        <p>It allows you to enforce organizational rules automatically during <code>plan</code> or <code>apply</code> stages.</p>
        <p><strong>Example Use Cases:</strong></p>
        <ul>
          <li>Prevent creation of public S3 buckets</li>
          <li>Ensure all resources have tags</li>
          <li>Disallow costly VM SKUs in non-prod environments</li>
        </ul>
        <pre><code>main = rule { all resources.tags contains "Environment" }</code></pre>
        <p><strong>Sentinel Enforcement Levels:</strong></p>
        <ul>
          <li><strong>Advisory</strong>  Warns, but allows apply</li>
          <li><strong>Soft Mandatory</strong>  Allows override</li>
          <li><strong>Hard Mandatory</strong>  Blocks apply</li>
        </ul>
      </div>`},{question:"What is the difference between Terraform Cloud and CLI?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Cloud vs CLI Comparison</strong></h3>
        <table>
          <thead><tr><th>Feature</th><th>Terraform CLI (Local)</th><th>Terraform Cloud</th></tr></thead>
          <tbody>
            <tr><td>Execution</td><td>Runs on local machine</td><td>Runs remotely on HashiCorp servers</td></tr>
            <tr><td>State Management</td><td>Local or custom backend</td><td>Automatically managed & locked</td></tr>
            <tr><td>Team Collaboration</td><td>Manual sharing</td><td>Role-based access control</td></tr>
            <tr><td>Policy Enforcement</td><td>Manual checks</td><td>Built-in Sentinel policies</td></tr>
            <tr><td>VCS Integration</td><td>Manual triggers</td><td>Automatic plan/apply on git push</td></tr>
            <tr><td>Cost</td><td>Free (open-source)</td><td>Free tier + paid plans</td></tr>
          </tbody>
        </table>
      </div>`},{question:"What is the use of VCS integration in Terraform Cloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VCS Integration in Terraform Cloud</strong></h3>
        <p><strong>Version Control System (VCS) Integration</strong> connects Terraform Cloud with your Git repository (GitHub, GitLab, Azure Repos, Bitbucket, etc.).</p>
        <p><strong>Purpose:</strong></p>
        <ul>
          <li>Automatically trigger <code>terraform plan</code> when code is pushed</li>
          <li>Enable pull-request based infrastructure review</li>
          <li>Ensure auditability and collaboration (who changed what, when)</li>
        </ul>
        <p><strong>Example Flow:</strong></p>
        <ol>
          <li>Push change to <code>main.tf</code> in GitHub</li>
          <li>Terraform Cloud detects change via webhook</li>
          <li>Triggers <code>plan</code>  stores results in workspace</li>
          <li>Admin reviews and approves  <code>apply</code> executes remotely</li>
        </ol>
        <p><strong>Pro Tip:</strong> Combine VCS integration with Sentinel policies to enforce compliance before deployment.</p>
      </div>`}]},{title:"33. Terraform  Registry",questions:[{question:"What is Terraform Registry?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Registry  Central Module & Provider Hub</strong></h3>
        <p>The <strong>Terraform Registry</strong> is a public or private repository where Terraform <em>modules and providers</em> are published and shared.</p>
        <p><strong>Types:</strong></p>
        <ul>
          <li> <strong>Public Registry</strong>  Managed by HashiCorp (<a href="https://registry.terraform.io">registry.terraform.io</a>), contains verified modules for AWS, Azure, GCP, etc.</li>
          <li> <strong>Private Registry</strong>  Available in Terraform Cloud/Enterprise for internal organizational modules.</li>
        </ul>
        <p><strong>Purpose:</strong> Enables standardization, reuse, and consistency in infrastructure automation across teams.</p>
        <p><strong>Example:</strong></p>
        <pre><code>module "vnet" {
  source  = "Azure/network/azurerm"
  version = "3.5.0"
  address_space = ["10.0.0.0/16"]
}</code></pre>
        <p><strong>In Practice:</strong> Teams avoid reinventing the wheel by consuming tested modules from Registry, ensuring compliance and quality.</p>
      </div>`},{question:"How to publish a module to Registry?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Publishing Modules to Terraform Registry</strong></h3>
        <p>Modules can be published to the public or private registry depending on your use case.</p>
        <p><strong> Requirements for Public Registry:</strong></p>
        <ul>
          <li>Must be a public GitHub repo.</li>
          <li>Repo name format: <code>terraform-&lt;PROVIDER&gt;-&lt;NAME&gt;</code> (e.g., <code>terraform-azurerm-vnet</code>).</li>
          <li>Root directory must contain a valid <code>main.tf</code>, <code>variables.tf</code>, and <code>outputs.tf</code>.</li>
          <li>Include <code>README.md</code> with usage examples and input/output descriptions.</li>
        </ul>
        <p><strong>Steps:</strong></p>
        <ol>
          <li>Push your module repo to GitHub (public or private).</li>
          <li>Go to <a href="https://registry.terraform.io">Terraform Registry</a>  Sign in with GitHub.</li>
          <li>Click <strong>Publish Module</strong>  select repo.</li>
          <li>Registry automatically detects provider, version, and documentation.</li>
        </ol>
        <p><strong>For Private Registry:</strong> In Terraform Cloud  <code>Modules  Publish  From VCS  Select Org Repo</code>.</p>
        <p><strong>Pro Tip:</strong> Tag your module releases (e.g., <code>v1.0.0</code>) for versioning and stability.</p>
      </div>`},{question:"How to use modules from Terraform Registry?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using Modules from Terraform Registry</strong></h3>
        <p>Modules from the registry can be easily integrated using the <code>source</code> and <code>version</code> attributes.</p>
        <pre><code>module "storage" {
  source   = "Azure/storage-account/azurerm"
  version  = "2.8.0"
  resource_group_name = "rg-demo"
  account_tier        = "Standard"
}</code></pre>
        <p><strong>Usage Guidelines:</strong></p>
        <ul>
          <li>Always specify a <code>version</code> to avoid breaking changes.</li>
          <li>Check modules documentation for required/optional inputs.</li>
          <li>Use <code>terraform get</code> to fetch modules and dependencies.</li>
          <li>For private registries, ensure Terraform Cloud/Enterprise authentication is configured.</li>
        </ul>
        <p><strong>Pro Tip:</strong> Prefer using verified modules from trusted vendors (e.g., HashiCorp, Microsoft, AWS).</p>
      </div>`},{question:"How do you version modules and maintain registry hygiene?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Versioning & Hygiene in Terraform Registry</strong></h3>
        <p>Versioning modules ensures stability, compatibility, and rollback capability.</p>
        <p><strong>Best Practices:</strong></p>
        <ul>
          <li>Use <strong>Semantic Versioning (SemVer)</strong>: <code>vMAJOR.MINOR.PATCH</code> (e.g., <code>v1.2.3</code>).</li>
          <li><strong>MAJOR</strong>  Breaking changes</li>
          <li><strong>MINOR</strong>  Backward-compatible enhancements</li>
          <li><strong>PATCH</strong>  Bug fixes</li>
          <li>Maintain a clean <code>CHANGELOG.md</code> documenting updates.</li>
          <li>Remove deprecated outputs, variables, and unused files regularly.</li>
          <li>For private registries  enforce module review before publish.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code>module "network" {
  source  = "git::https://github.com/org/terraform-azurerm-network.git?ref=v2.1.0"
}</code></pre>
        <p><strong>Pro Tip:</strong> Automate publishing using CI/CD pipelines (e.g., GitHub Actions  tag  push  auto publish to Registry).</p>
      </div>`}]},{title:"34. Terraform  Security & Best Practices",questions:[{question:"What are Terraform best practices?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Best Practices</strong></h3>
        <p>Following best practices ensures maintainability, scalability, and security of your Infrastructure-as-Code.</p>
        <ul>
          <li> <strong>Use Modular Structure:</strong> Split configurations into reusable modules (<code>network</code>, <code>vm</code>, <code>storage</code>).</li>
          <li> <strong>Never commit secrets</strong> (keys, passwords, tokens) to code or Git.</li>
          <li> <strong>Use Remote State Backend</strong> with locking (e.g., S3 + DynamoDB or Azure Blob + State Locking).</li>
          <li> <strong>Version control</strong> all Terraform code (Git).</li>
          <li> <strong>Pin versions</strong> of providers and modules (avoid breaking changes).</li>
          <li> <strong>Use <code>terraform validate</code>, <code>fmt</code>, <code>plan</code></strong> before apply.</li>
          <li> <strong>Use workspaces</strong> for environment separation (dev/test/prod).</li>
          <li> <strong>Integrate CI/CD</strong> for testing and automated deployments.</li>
          <li> <strong>Enable policy-as-code</strong> with Sentinel/OPA for compliance.</li>
        </ul>
        <p><strong>Example Folder Layout:</strong></p>
        <pre><code> modules/
    network/
    compute/
    storage/
 environments/
    dev/
    prod/
    staging/
 main.tf</code></pre>
      </div>`},{question:"How to manage secrets securely in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Secrets Management in Terraform</strong></h3>
        <p>Terraform should never directly store secrets in code or state file. Use one of the following approaches:</p>
        <ul>
          <li> <strong>Environment Variables:</strong> Inject secrets dynamically (<code>TF_VAR_password</code>).</li>
          <li> <strong>HashiCorp Vault:</strong> Use Vault provider to fetch secrets securely at runtime.</li>
          <li> <strong>Cloud Key Vaults:</strong> Use <strong>Azure Key Vault</strong>, <strong>AWS Secrets Manager</strong>, or <strong>GCP Secret Manager</strong>.</li>
          <li> <strong>Terraform Cloud Variables:</strong> Mark sensitive variables as <code>sensitive=true</code>.</li>
          <li> <strong>Never output secrets</strong> in <code>outputs.tf</code> (use <code>sensitive = true</code> flag).</li>
        </ul>
        <pre><code>variable "db_password" {
  type      = string
  sensitive = true
}</code></pre>
        <p><strong>Pro Tip:</strong> Enable state encryption (AES-256) and store secrets outside tfvars.</p>
      </div>`},{question:"What is Vault provider and when do you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Vault Provider in Terraform</strong></h3>
        <p>The <strong>Vault provider</strong> integrates Terraform with <strong>HashiCorp Vault</strong> to dynamically read secrets or credentials.</p>
        <p><strong>Use Case:</strong> Securely inject secrets like DB passwords, API tokens, certificates during apply  without storing them in code.</p>
        <p><strong>Example:</strong></p>
        <pre><code>provider "vault" {
  address = "https://vault.example.com"
  token   = var.vault_token
}

data "vault_generic_secret" "db" {
  path = "secret/data/db-creds"
}

resource "azurerm_sql_server" "db" {
  administrator_login          = "admin"
  administrator_login_password = data.vault_generic_secret.db.data["password"]
}</code></pre>
        <p><strong>When to Use:</strong> In enterprises where Vault is centralized secret store and compliance requires rotation and audit.</p>
      </div>`},{question:"How to use environment variables securely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using Environment Variables Securely</strong></h3>
        <ul>
          <li>Use <code>TF_VAR_&lt;variable&gt;</code> convention for passing values.</li>
          <li>Store them in CI/CD secret managers (e.g., GitHub Secrets, Azure DevOps Library).</li>
          <li>Do not echo or log them in scripts or Terraform output.</li>
          <li>Mark variables <code>sensitive = true</code> to hide values in CLI and state file.</li>
        </ul>
        <p><strong>Example:</strong></p>
        <pre><code># Bash
export TF_VAR_client_secret="super-secret-key"</code></pre>
        <p><strong>In Terraform:</strong></p>
        <pre><code>variable "client_secret" {
  type      = string
  sensitive = true
}</code></pre>
      </div>`},{question:"How to manage Terraform state securely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Securing Terraform State File</strong></h3>
        <p><strong>tfstate</strong> contains sensitive data (like passwords, tokens, IDs). Best practices:</p>
        <ul>
          <li> Store state in a <strong>remote backend</strong> (Azure Blob, S3, GCS)  never local.</li>
          <li> Enable <strong>state encryption at rest</strong> (e.g., AES-256).</li>
          <li> Enable <strong>state locking</strong> (DynamoDB/Azure Storage).</li>
          <li> Use role-based access control (RBAC) to limit who can read/write state.</li>
          <li> Avoid storing tfstate in public version control.</li>
          <li> Use Terraform Cloud for managed and encrypted state with team permissions.</li>
        </ul>
        <p><strong>Example (Azure Backend):</strong></p>
        <pre><code>terraform {
  backend "azurerm" {
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstateprod"
    container_name       = "tfstate"
    key                  = "prod.tfstate"
  }
}</code></pre>
      </div>`},{question:"What is least privilege principle in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Least Privilege Principle (LPP)</strong></h3>
        <p>Grant Terraform only the minimal permissions necessary to perform its tasks.</p>
        <ul>
          <li> Create a dedicated <strong>service principal</strong> or IAM role for Terraform.</li>
          <li> Avoid giving Terraform <code>Owner</code> or <code>Admin</code> roles unnecessarily.</li>
          <li> Limit Terraforms role to create/update infrastructure only.</li>
          <li> Rotate credentials periodically.</li>
        </ul>
        <p><strong>Example (Azure):</strong></p>
        <pre><code>az ad sp create-for-rbac --name "terraform" \\
  --role "Contributor" --scopes /subscriptions/&lt;sub-id&gt;</code></pre>
      </div>`},{question:"How to enforce policies for secure IaC?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Enforcing Security Policies (Policy as Code)</strong></h3>
        <p>Terraform allows policy enforcement using <strong>Sentinel</strong> (Terraform Cloud) or <strong>OPA (Open Policy Agent)</strong>.</p>
        <ul>
          <li> Define rules like  No public S3 buckets, All resources must have tags, etc.</li>
          <li> Policies are evaluated before apply  to block non-compliant infrastructure.</li>
        </ul>
        <p><strong>Example (Sentinel):</strong></p>
        <pre><code>import "tfplan"
main = rule {
  all tfplan.resources.aws_s3_bucket as bucket {
    bucket.applied.private == true
  }
}</code></pre>
        <p><strong>Pro Tip:</strong> Integrate policies in CI/CD to ensure every commit passes security checks before apply.</p>
      </div>`}]},{title:"35. Terraform  Azure",questions:[{question:"How to use Terraform with Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using Terraform with Azure</strong></h3>
        <p>Terraform automates Azure infrastructure creation using the <strong>AzureRM provider</strong>.</p>
        <p> Steps to use Terraform with Azure:</p>
        <ol>
          <li> Install Terraform and Azure CLI</li>
          <li> Login to Azure using <code>az login</code></li>
          <li> Configure <code>provider "azurerm"</code> block</li>
          <li> Define resources (e.g., resource groups, VMs, VNets)</li>
          <li> Initialize, plan, and apply</li>
        </ol>
        <pre><code>terraform init
terraform plan
terraform apply</code></pre>
        <p><strong>Pro Tip:</strong> Use a remote backend like Azure Blob Storage for storing state securely.</p>
      </div>`},{question:"What is AzureRM provider?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AzureRM Provider</strong></h3>
        <p>The <strong>AzureRM provider</strong> is the official Terraform provider used to interact with Azure resources.</p>
        <p>It defines how Terraform connects to Azure APIs to manage resources like RGs, VNets, VMs, etc.</p>
        <pre><code>provider "azurerm" {
  features {}
  subscription_id = var.subscription_id
  tenant_id       = var.tenant_id
  client_id       = var.client_id
  client_secret   = var.client_secret
}</code></pre>
        <p><strong>Key point:</strong> The <code>features {}</code> block is mandatory (even if empty).</p>
      </div>`},{question:"How to create Resource Group using Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Creating Azure Resource Group</strong></h3>
        <pre><code>resource "azurerm_resource_group" "rg" {
  name     = "rg-demo"
  location = "East US"
  tags = {
    environment = "dev"
  }
}</code></pre>
        <p> This creates a new resource group <strong>rg-demo</strong> in <em>East US</em>.</p>
        <p><strong>Command Flow:</strong></p>
        <pre><code>terraform init
terraform plan
terraform apply</code></pre>
      </div>`},{question:"How to authenticate to Azure in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Authentication Methods</strong></h3>
        <ul>
          <li> <strong>Azure CLI Authentication:</strong> Run <code>az login</code>  Terraform picks up credentials automatically.</li>
          <li> <strong>Service Principal Authentication:</strong> Provide <code>client_id</code>, <code>client_secret</code>, <code>tenant_id</code>, and <code>subscription_id</code>.</li>
          <li> <strong>Managed Identity:</strong> When running Terraform from Azure VM or DevOps agent with a system-assigned identity.</li>
        </ul>
        <pre><code>provider "azurerm" {
  features {}
  use_msi = true
}</code></pre>
        <p><strong>Best Practice:</strong> Always use a Service Principal or Managed Identity  avoid using personal credentials.</p>
      </div>`},{question:"How to use service principal with Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using Azure Service Principal for Terraform</strong></h3>
        <p>1 Create a Service Principal:</p>
        <pre><code>az ad sp create-for-rbac --name "terraform" --role="Contributor" \\
  --scopes="/subscriptions/&lt;sub-id&gt;"</code></pre>
        <p>2 Capture the output:</p>
        <pre><code>{
  "appId": "xxxx",
  "password": "xxxx",
  "tenant": "xxxx"
}</code></pre>
        <p>3 Use it in provider block:</p>
        <pre><code>provider "azurerm" {
  features {}
  subscription_id = var.subscription_id
  client_id       = var.client_id      # appId
  client_secret   = var.client_secret  # password
  tenant_id       = var.tenant_id
}</code></pre>
        <p><strong>Pro Tip:</strong> Store these credentials in Azure Key Vault or CI/CD secret variables  never in code.</p>
      </div>`},{question:"How to deploy VM using Terraform in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Deploying Azure VM with Terraform</strong></h3>
        <pre><code>resource "azurerm_resource_group" "rg" {
  name     = "rg-vm"
  location = "East US"
}

resource "azurerm_virtual_network" "vnet" {
  name                = "vnet-demo"
  address_space       = ["10.0.0.0/16"]
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
}

resource "azurerm_subnet" "subnet" {
  name                 = "subnet1"
  resource_group_name  = azurerm_resource_group.rg.name
  virtual_network_name = azurerm_virtual_network.vnet.name
  address_prefixes     = ["10.0.1.0/24"]
}

resource "azurerm_network_interface" "nic" {
  name                = "nic1"
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
  ip_configuration {
    name                          = "internal"
    subnet_id                     = azurerm_subnet.subnet.id
    private_ip_address_allocation = "Dynamic"
  }
}

resource "azurerm_linux_virtual_machine" "vm" {
  name                  = "vm-demo"
  resource_group_name   = azurerm_resource_group.rg.name
  location              = azurerm_resource_group.rg.location
  size                  = "Standard_B1s"
  admin_username        = "azureuser"
  admin_password        = var.vm_password
  network_interface_ids = [azurerm_network_interface.nic.id]
  os_disk {
    caching              = "ReadWrite"
    storage_account_type = "Standard_LRS"
  }
  source_image_reference {
    publisher = "Canonical"
    offer     = "UbuntuServer"
    sku       = "18.04-LTS"
    version   = "latest"
  }
}</code></pre>
        <p><strong>Tip:</strong> Always parameterize location, size, and credentials using variables.</p>
      </div>`},{question:"How to connect on-prem to Azure via Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Connecting On-Prem to Azure using Terraform</strong></h3>
        <p>You can configure hybrid connectivity using Terraform to automate VPN or ExpressRoute setup.</p>
        <p><strong>Example: Site-to-Site VPN (On-prem  Azure)</strong></p>
        <ol>
          <li>Create a <strong>Virtual Network</strong> in Azure.</li>
          <li>Create a <strong>Virtual Network Gateway</strong>.</li>
          <li>Create a <strong>Local Network Gateway</strong> (representing on-prem network).</li>
          <li>Create a <strong>Connection</strong> between both gateways.</li>
        </ol>
        <pre><code>resource "azurerm_local_network_gateway" "onprem" {
  name                = "onprem-gateway"
  resource_group_name = azurerm_resource_group.rg.name
  location            = azurerm_resource_group.rg.location
  gateway_address     = "203.0.113.1"
  address_space       = ["10.10.0.0/16"]
}

resource "azurerm_virtual_network_gateway_connection" "vpn" {
  name                = "vpn-connection"
  location            = azurerm_resource_group.rg.location
  resource_group_name = azurerm_resource_group.rg.name
  type                = "IPsec"
  virtual_network_gateway_id = azurerm_virtual_network_gateway.vng.id
  local_network_gateway_id   = azurerm_local_network_gateway.onprem.id
  shared_key          = "YourSharedKey"
}</code></pre>
        <p><strong>Use Case:</strong> For hybrid deployments (e.g., connecting datacenter to Azure VNet).</p>
      </div>`},{question:"How to use Key Vault integration with Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Key Vault Integration</strong></h3>
        <p>Terraform can read secrets directly from Azure Key Vault using the <strong>azurerm_key_vault_secret</strong> data source.</p>
        <p><strong>Example:</strong></p>
        <pre><code>data "azurerm_key_vault" "kv" {
  name                = "my-keyvault"
  resource_group_name = "rg-secure"
}

data "azurerm_key_vault_secret" "db_pwd" {
  name         = "db-password"
  key_vault_id = data.azurerm_key_vault.kv.id
}

resource "azurerm_mssql_server" "sql" {
  name                         = "sql-prod"
  resource_group_name          = "rg-secure"
  location                     = "East US"
  administrator_login          = "adminuser"
  administrator_login_password = data.azurerm_key_vault_secret.db_pwd.value
}</code></pre>
        <p><strong>Benefit:</strong> Secrets never appear in tfvars or state file  they are fetched dynamically at runtime.</p>
      </div>`}]},{title:"36. Terraform  Miscellaneous / Advanced Topics",questions:[{question:"What is the difference between Terraform and ARM template?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform vs ARM Templates</strong></h3>
        <table>
          <tr><th>Aspect</th><th>Terraform</th><th>ARM Template</th></tr>
          <tr><td>Language</td><td>HCL (HashiCorp Configuration Language)</td><td>JSON</td></tr>
          <tr><td>Multi-Cloud</td><td> Yes (Azure, AWS, GCP)</td><td> Azure-only</td></tr>
          <tr><td>Reusability</td><td>Modules for reuse</td><td>Linked templates</td></tr>
          <tr><td>State Management</td><td>Maintains <code>terraform.tfstate</code></td><td>No state file</td></tr>
          <tr><td>Ease of Use</td><td>Declarative + Modular</td><td>JSON-heavy, verbose</td></tr>
        </table>
        <p><strong>Summary:</strong> Terraform is cross-platform and modular; ARM is Azure-native but limited in flexibility.</p>
      </div>`},{question:"What is drift in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Drift in Terraform</strong></h3>
        <p><strong>Drift</strong> occurs when the actual cloud infrastructure changes outside of Terraforms control.</p>
        <p> For example: Someone deletes or modifies an Azure VM manually from the portal.</p>
        <p><strong>Detection:</strong> Run <code>terraform plan</code>  it shows differences between real infra & state file.</p>
        <p><strong>Prevention:</strong> Use RBAC, state locking, and restrict manual portal access.</p>
      </div>`},{question:"What is Terraform refresh?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Refresh</strong></h3>
        <p><code>terraform refresh</code> updates the <strong>state file</strong> with the latest values from the cloud.</p>
        <pre><code>terraform refresh</code></pre>
        <p>It doesnt change infrastructure  it only syncs state to match reality.</p>
        <p><strong>Note:</strong> In newer versions (1.1+), refresh happens automatically during <code>plan</code> and <code>apply</code>.</p>
      </div>`},{question:"What is import in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Import</strong></h3>
        <p>Used to bring existing resources (created manually or by another team) under Terraform management.</p>
        <pre><code>terraform import azurerm_resource_group.rg /subscriptions/&lt;sub_id&gt;/resourceGroups/myRG</code></pre>
        <p><strong>Steps:</strong></p>
        <ol>
          <li>1 Create Terraform config for that resource.</li>
          <li>2 Run <code>terraform import</code> with resource ID.</li>
          <li>3 Terraform adds it to the state file without creating it again.</li>
        </ol>
        <p><strong>Use Case:</strong> Migrating existing Azure infra into IaC control.</p>
      </div>`},{question:"What is the difference between apply and plan?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>terraform plan vs terraform apply</strong></h3>
        <table>
          <tr><th>Command</th><th>Purpose</th></tr>
          <tr><td><code>terraform plan</code></td><td>Shows what changes Terraform will make (preview only)</td></tr>
          <tr><td><code>terraform apply</code></td><td>Executes the changes and applies them to real infra</td></tr>
        </table>
        <p><strong>Pro Tip:</strong> Use <code>terraform apply planfile.tfplan</code> in CI/CD for controlled deployments.</p>
      </div>`},{question:"What is the use of target in terraform apply?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Using -target flag</strong></h3>
        <p>Used to apply or destroy a specific resource or module only.</p>
        <pre><code>terraform apply -target=azurerm_resource_group.rg</code></pre>
        <p><strong>Use Case:</strong> Debugging or testing specific resources without full infra apply.</p>
        <p><strong> Caution:</strong> Avoid using frequently  it can cause dependency mismatch in state.</p>
      </div>`},{question:"How to write reusable Terraform code?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Reusable Terraform Code</strong></h3>
        <p> Use <strong>Modules</strong> to group related resources and reuse them across environments.</p>
        <pre><code>module "network" {
  source = "./modules/network"
  vnet_name = var.vnet_name
}</code></pre>
        <p> Parameterize using <strong>variables</strong>, <strong>locals</strong>, and <strong>outputs</strong>.</p>
        <p> Maintain module versions in source control or private registry.</p>
      </div>`},{question:"What is Terraform linting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Linting</strong></h3>
        <p>Terraform linting ensures your code follows syntax & style best practices.</p>
        <ul>
          <li> Use <code>terraform fmt</code>  auto-formats code.</li>
          <li> Use <code>terraform validate</code>  checks for syntax errors.</li>
          <li> Use <strong>tflint</strong>  checks code quality & provider-specific best practices.</li>
        </ul>
        <p><strong>Benefit:</strong> Prevents syntax, naming, and unused variable issues before deployment.</p>
      </div>`},{question:"What are common Terraform errors you faced?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Common Terraform Errors</strong></h3>
        <ul>
          <li> <strong>State Lock Error</strong>  occurs when another operation is holding the state.</li>
          <li> <strong>Provider Authentication Error</strong>  wrong credentials or expired SPN.</li>
          <li> <strong>Backend Init Error</strong>  missing or incorrect backend block.</li>
          <li> <strong>Dependency Cycle</strong>  circular depends_on references.</li>
          <li> <strong>Plan mismatch</strong>  drift or manual infra changes.</li>
        </ul>
        <p><strong>Debugging Tips:</strong> Enable logs  <code>TF_LOG=DEBUG</code> or <code>TF_LOG_PATH</code>.</p>
      </div>`},{question:"How do you handle module version upgrades safely?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Handling Module Upgrades Safely</strong></h3>
        <p>Steps:</p>
        <ol>
          <li>1 Pin module version in your configuration:
            <pre><code>source  = "git::https://repo//modules/network.git"
version = "1.0.2"</code></pre></li>
          <li>2 Test new version in lower environment (dev / sandbox).</li>
          <li>3 Use <code>terraform plan</code> to preview impact.</li>
          <li>4 Apply incrementally after validation.</li>
        </ol>
        <p><strong>Best Practice:</strong> Always tag module releases (v1.0.0, v1.0.1) and never use latest.</p>
      </div>`}]},{title:"37 . Scripting & Tools (Automation, Bash, PowerShell, Linters)",questions:[{question:"Have you worked on Bash or PowerShell scripting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Bash & PowerShell  Practical Scripting Experience</strong></h3>
        <p>Yes  I use Bash for Linux-based CI agents, automation, and glue scripts; PowerShell for Windows hosts and Azure-specific automation (Az modules).</p>
        <p><strong>Common tasks I script:</strong></p>
        <ul style="margin-left:1.2rem;">
          <li>CI pipeline steps (init  plan  upload artifacts  notify)</li>
          <li>Secrets fetch & inject (Key Vault / Vault) into runtime env</li>
          <li>Automated rollbacks, health checks & remediation (restart service, reclaim disk)</li>
          <li>Release packaging and module publish (tag  build  push)</li>
        </ul>
        <h4>Examples</h4>
        <pre><code># Bash: basic terraform pipeline step
set -e
export TF_IN_AUTOMATION=true
terraform init -input=false
terraform fmt -check
terraform validate
terraform plan -out=tfplan

# PowerShell: fetching secret from Key Vault (Az module)
&dollar;secret = Get-AzKeyVaultSecret -VaultName "kv-prod" -Name "dbPassword"
Write-Output "Secret length: &dollar;(&dollar;secret.SecretValueText.Length)"</code></pre>
        <p><strong>In Practice:</strong> I wrap CLI calls with retries and health checks (3 tries + exponential backoff) and log to central file for post-mortem.</p>
      </div>`},{question:"How will you ensure password is removed from repo after it was exposed?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Removing Exposed Passwords from Git History</strong></h3>
        <p>Immediate steps:</p>
        <ol style="margin-left:1.2rem;">
          <li>Revoke & rotate the exposed credential immediately (rotate secrets in Key Vault/Secrets Manager).</li>
          <li>Remove the secret from repo and local working copies: <code>git rm --cached &lt;file&gt;</code> + commit.</li>
          <li>Rewrite Git history to purge the secret using BFG or git filter-repo (preferred):</li>
        </ol>
        <pre><code># Using git filter-repo (recommended)
git clone --mirror git@github.com:org/repo.git
cd repo.git
git filter-repo --invert-paths --path path/to/secret.txt

# Or replace a secret string
git filter-repo --replace-text replacements.txt

# Push cleaned repo (force)
git push --force --all
git push --force --tags</code></pre>
        <p><strong>Notify:</strong> Inform team, rotate any leaked credentials, update CI secrets, and run a repo scan (TruffleHog/TruffleHog3) to confirm no other leaks.</p>
        <p><strong>In Practice:</strong> I always rotate the secret first, then scrub history, then enforce pre-commit hooks + secret scanners to avoid re-introduction.</p>
      </div>`},{question:"What is TFLint?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>TFLint  Terraform Linter & Best-Practice Checker</strong></h3>
        <p>TFLint is a pluggable linter for Terraform that checks provider-specific best practices, detects deprecated attributes, and enforces coding standards.</p>
        <p><strong>Benefits:</strong> catches issues that terraform validate misses (unused vars, deprecated fields, region mismatches, potential security issues).</p>
        <h4>Usage</h4>
        <pre><code># Install & run
brew install tflint       # or choco / apt
tflint --init             # initialize provider rules
tflint                    # run checks</code></pre>
        <p><strong>In Practice:</strong> I run tflint in CI (fail on error, warn on lower-severity rules) and keep custom rules for org policies.</p>
      </div>`},{question:"What is a linter in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Linter  Purpose</strong></h3>
        <p>Linters (terraform fmt, tflint, tfsec integration) analyze Terraform code for formatting, syntax, provider-specific issues, security smells, and style consistency.</p>
        <ul style="margin-left:1.2rem;">
          <li><code>terraform fmt</code>  enforces HCL formatting.</li>
          <li><code>terraform validate</code>  validates config semantics and provider schemas.</li>
          <li><code>tflint</code>  deeper linting & provider best practices.</li>
        </ul>
        <p><strong>In Practice:</strong> Enforce fmt & tflint in pre-commit + CI to maintain consistent code and reduce human-review noise.</p>
      </div>`},{question:"What is Checkov and how do you integrate it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Checkov  IaC Security Static Analysis</strong></h3>
        <p>Checkov scans Terraform (and other IaC) for security misconfigurations (open S3 buckets, unencrypted disks, weak NSG rules) using a rich rule set.</p>
        <h4>Integration</h4>
        <pre><code># Install
pip install checkov

# Run locally or in CI
checkov -d . --framework terraform

# CI (example step)
- script: |
    pip install checkov
    checkov -d . --quiet --output json > checkov-report.json
  displayName: "Run Checkov"</code></pre>
        <p><strong>Best Practice:</strong> Fail pipeline on "high" or "critical" findings; keep a policy baseline and suppress only with documented exceptions.</p>
        <p><strong>In Practice:</strong> I gate "apply" in prod unless Checkov passes and findings are triaged.</p>
      </div>`},{question:"What is TruffleHog and where do you use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>TruffleHog  Secret Scanning Tool</strong></h3>
        <p>TruffleHog scans git history and repos for high-entropy strings and known secret patterns (API keys, tokens).</p>
        <h4>Use Cases</h4>
        <ul style="margin-left:1.2rem;">
          <li>Scan new repos during initial onboarding</li>
          <li>Nightly scans to detect accidental check-ins</li>
          <li>Pre-merge hook scans for PRs</li>
        </ul>
        <pre><code># Simple run
trufflehog git https://github.com/org/repo

# GitHub Action: runs on PRs or pushes</code></pre>
        <p><strong>In Practice:</strong> We run TruffleHog during PR checks; if it flags a secret, CI fails and reviewer is alerted to rotate & scrub.</p>
      </div>`},{question:"How will you integrate SonarQube using scripting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>SonarQube Integration  Static Code Analysis</strong></h3>
        <p>SonarQube inspects code quality and security for languages (including HCL via plugins). Integration steps:</p>
        <ol style="margin-left:1.2rem;">
          <li>Install & configure SonarQube server (or use SonarCloud).</li>
          <li>Add sonar scanner to CI agent.</li>
          <li>Run scanner in pipeline and publish report to Sonar server.</li>
        </ol>
        <pre><code># Bash: SonarCloud/Server run example
export SONAR_TOKEN=...
sonar-scanner   -Dsonar.projectKey=myproj   -Dsonar.sources=.   -Dsonar.host.url=https://sonar.example.com   -Dsonar.login=&dollar;SONAR_TOKEN</code></pre>
        <p><strong>In Practice:</strong> I script sonar-scanner invocation in CI after unit tests and before merge  break build on new critical/blocker issues.</p>
      </div>`},{question:"How do you automate Terraform and security checks in pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Terraform + Security in CI/CD</strong></h3>
        <p>Typical multi-stage pipeline pattern I implement (YAML / GitHub Actions):</p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Pre-checks</strong>: terraform fmt, terraform validate, tflint</li>
          <li><strong>Security scan</strong>: Checkov, tfsec, TruffleHog</li>
          <li><strong>Plan</strong>: terraform init  terraform plan -out=tfplan</li>
          <li><strong>Store plan</strong> as artifact for apply stage</li>
          <li><strong>Approval gate</strong> for prod</li>
          <li><strong>Apply</strong>: terraform apply tfplan (run with limited SPN)</li>
          <li><strong>Post-check</strong>: run drift detection or run Checkov again on resulting state</li>
        </ol>
        <pre><code># Simplified YAML step sequence (GitHub Actions)
jobs:
  validate:
    steps:
      - run: terraform fmt -check
      - run: tflint
      - run: checkov -d .
  plan:
    needs: validate
    steps:
      - run: terraform init
      - run: terraform plan -out=tfplan
  apply:
    needs: plan
    if: github.ref == 'refs/heads/main'
    steps:
      - run: terraform apply tfplan</code></pre>
        <p><strong>In Practice:</strong> I fail fast on lint/security; only allow apply when plan artifact + security gates are green.</p>
      </div>`},{question:"How will you integrate Azure Key Vault in pipeline via script?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Key Vault Integration in Pipelines</strong></h3>
        <p>Options (choose based on CI platform):</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Native variable group linking (Azure DevOps):</strong> Link Key Vault to variable group  pipeline consumes secrets as variables automatically.</li>
          <li><strong>az cli fetch (generic):</strong> Use service principal to fetch secrets at runtime.</li>
          <li><strong>Terraform data source:</strong> Use <code>data.azurerm_key_vault_secret</code> in TF modules (secrets not stored in code).</li>
        </ul>
        <h4>Example  Bash (using az cli)</h4>
        <pre><code># Login using service principal (in CI, use secure vars)
az login --service-principal -u &dollar;SP_APPID -p &dollar;SP_SECRET --tenant &dollar;TENANT

# Fetch secret
DB_PASS=&dollar;(az keyvault secret show --vault-name my-kv --name db-password --query value -o tsv)
export TF_VAR_db_password="&dollar;DB_PASS"

# Then run terraform plan/apply
terraform plan -var "db_password=&dollar;DB_PASS"</code></pre>
        <h4>Example  Azure DevOps variable group</h4>
        <pre><code>variables:
- group: KeyVault-Linked-Variables  # secrets are injected automatically</code></pre>
        <p><strong>In Practice:</strong> I prefer linking Key Vault to pipeline variable groups for secure, auditable, and least-privilege secret retrieval. For ephemeral runs, I fetch via az cli and store in memory-only env vars, never write to disk.</p>
      </div>`},{question:"How do you use scripting to detect secret leaks?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Detecting Secret Leaks via Scripting</strong></h3>
        <p>Secret leak detection scripts scan repositories, commits, or config files for sensitive patterns (API keys, tokens, passwords, certificates).</p>
        <h4> Methods:</h4>
        <ul style="margin-left:1.2rem;">
          <li><strong>Regex scan</strong> for known secret formats (AWS, Azure, JWT, etc.).</li>
          <li><strong>Entropy check</strong>  detect random high-entropy strings (likely secrets).</li>
          <li><strong>Integration with tools</strong> like <code>TruffleHog</code>, <code>gitleaks</code>, or <code>detect-secrets</code>.</li>
        </ul>
        <h4> Example  Bash script using grep + regex</h4>
        <pre><code>#!/bin/bash
# secret-scan.sh
echo " Scanning repo for potential secrets..."
grep -rEn --color "AKIA[0-9A-Z]{16}|passwords*=s*|secret|keys*=s*" . --exclude-dir=.git
if [ &dollar;? -eq 0 ]; then
  echo " Potential secrets found! Please review above lines."
else
  echo " No obvious secrets detected."
fi</code></pre>
        <p><strong>In Practice:</strong> I run secret scans in pre-commit hooks & CI pipelines using TruffleHog or Gitleaks. If secrets are found  block PR + alert DevSecOps.</p>
      </div>`},{question:"What is AWK command in Linux?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AWK Command  Linux Text Processing Utility</strong></h3>
        <p><strong>AWK</strong> is a powerful text processing tool used to extract, manipulate, and format data from structured text files like CSV, logs, or system outputs.</p>
        <h4> Syntax:</h4>
        <pre><code>awk 'pattern { action }' filename</code></pre>
        <h4> Examples:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Print second column: <code>awk '{print &dollar;2}' file.txt</code></li>
          <li>Filter CPU usage &gt; 80%: <code>ps aux | awk '&dollar;3&gt;80 {print &dollar;1, &dollar;3, &dollar;11}'</code></li>
          <li>Sum column: <code>awk '{sum+=&dollar;3} END {print sum}' usage.csv</code></li>
        </ul>
        <p><strong>In Practice:</strong> I use <code>awk</code> in monitoring scripts (CPU, memory usage) and in parsing Terraform outputs or log files.</p>
      </div>`},{question:"What is &dollar;_ and &dollar;? in shell scripting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Special Shell Variables  &dollar;_ and &dollar;?</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>&dollar;_</code>  holds the <strong>last argument</strong> of the previous command.</li>
          <li><code>&dollar;?</code>  holds the <strong>exit status</strong> of the last command (0 = success, non-zero = failure).</li>
        </ul>
        <h4> Example:</h4>
        <pre><code>echo "Hello"
echo &dollar;_       # Output: Hello
ls /tmp
echo &dollar;?       # Output: 0 (if successful)</code></pre>
        <p><strong>In Practice:</strong> I use <code>&dollar;?</code> in monitoring & retry logic  e.g., rerun a failed API call automatically if exit code != 0.</p>
      </div>`},{question:"Write a bash script for sending alert mail if VM disk is 80% full.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Bash Script  Disk Utilization Alert</strong></h3>
        <pre><code>#!/bin/bash
THRESHOLD=80
EMAIL="admin@example.com"
HOST=&dollar;(hostname)

df -H | grep -vE '^Filesystem|tmpfs|cdrom' | awk '{print &dollar;5 " " &dollar;1}' | while read output; do
  usage=&dollar;(echo &dollar;output | awk '{print &dollar;1}' | sed 's/%//')
  partition=&dollar;(echo &dollar;output | awk '{print &dollar;2}')
  if [ &dollar;usage -ge &dollar;THRESHOLD ]; then
    echo " Disk usage on &dollar;HOST (&dollar;partition) is at &dollar;{usage}%!" |     mail -s "Disk Alert: &dollar;HOST &dollar;partition &dollar;{usage}%" &dollar;EMAIL
  fi
done</code></pre>
        <p><strong>In Practice:</strong> I run such scripts via cron (every 10 mins) or include them in health-monitoring pods in AKS to send alerts via SMTP or webhook.</p>
      </div>`},{question:"Write Hello bash script using loop function.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Simple Loop Example in Bash</strong></h3>
        <pre><code>#!/bin/bash
for i in {1..5}
do
  echo "Hello Bash!  iteration &dollar;i"
done</code></pre>
        <p><strong>Output:</strong><br>
        Hello Bash!  iteration 1<br>
        Hello Bash!  iteration 2 ...</p>
        <p><strong>In Practice:</strong> Loops are often used to automate repetitive tasks  e.g., looping over multiple Terraform workspaces or VM IDs.</p>
      </div>`},{question:"What are loops in bash scripting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Loops in Bash  for, while, until</strong></h3>
        <p>Loops allow execution of a block of commands multiple times.</p>
        <h4> Types:</h4>
        <ul style="margin-left:1.2rem;">
          <li><strong>for loop</strong>: Iterate over list or range.</li>
          <li><strong>while loop</strong>: Execute while condition is true.</li>
          <li><strong>until loop</strong>: Execute until condition becomes true.</li>
        </ul>
        <h4>Examples:</h4>
        <pre><code># for loop
for file in *.log; do echo &dollar;file; done

# while loop
count=1
while [ &dollar;count -le 5 ]; do echo &dollar;count; ((count++)); done

# until loop
until [ -f /tmp/ready.flag ]; do sleep 5; done</code></pre>
        <p><strong>In Practice:</strong> I use loops for log rotation, multi-resource deletion, or retry logic during provisioning.</p>
      </div>`},{question:"What is PowerShell and how is it used in automation?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>PowerShell  Task Automation Framework</strong></h3>
        <p>PowerShell is a command-line shell and scripting language built on .NET for automating administrative tasks in Windows and Azure.</p>
        <h4> Common Automation Use-Cases:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Azure resource management using Az modules</li>
          <li>VM lifecycle management (create, reboot, patch)</li>
          <li>File system & registry automation</li>
          <li>CI/CD orchestration (invoke REST APIs, send notifications)</li>
        </ul>
        <h4>Example:</h4>
        <pre><code># Create Resource Group in Azure
Connect-AzAccount
New-AzResourceGroup -Name "RG-Demo" -Location "EastUS"</code></pre>
        <p><strong>In Practice:</strong> I use PowerShell in hybrid setups (Windows nodes in AKS or Azure Arc VMs) for provisioning & patch automation.</p>
      </div>`},{question:"How do you automate patching using scripting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automated VM Patching via Script</strong></h3>
        <p>Patching is automated using Bash (Linux) or PowerShell (Windows) scripts scheduled via cron or Task Scheduler.</p>
        <h4>Example  Linux (Bash)</h4>
        <pre><code>#!/bin/bash
LOG=/var/log/patching.log
echo " Starting patching: &dollar;(date)" >> &dollar;LOG
sudo apt update -y && sudo apt upgrade -y >> &dollar;LOG
if [ &dollar;? -eq 0 ]; then
  echo " Patch successful: &dollar;(date)" >> &dollar;LOG
else
  echo " Patch failed!" >> &dollar;LOG
fi</code></pre>
        <h4>Example  PowerShell (Windows)</h4>
        <pre><code>Install-Module PSWindowsUpdate -Force
Get-WindowsUpdate -AcceptAll -Install -AutoReboot</code></pre>
        <p><strong>In Practice:</strong> I schedule these scripts via Azure Automation Account or runbooks, integrated with Log Analytics for compliance tracking.</p>
      </div>`},{question:"How do you run Terraform scripts in CI/CD pipeline automatically?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Terraform in CI/CD</strong></h3>
        <p>Terraform automation is typically implemented in pipelines (Azure DevOps, GitHub Actions, Jenkins) triggered by code commits or PR merges.</p>
        <h4>Pipeline Steps:</h4>
        <ol style="margin-left:1.2rem;">
          <li><strong>Checkout code</strong></li>
          <li><strong>Setup Terraform</strong> and init backend</li>
          <li><strong>Validate and plan</strong></li>
          <li><strong>Manual approval</strong> for prod</li>
          <li><strong>Apply infrastructure changes</strong></li>
        </ol>
        <h4>Example  Azure DevOps YAML</h4>
        <pre><code>trigger:
- main

stages:
- stage: Plan
  jobs:
  - job: Terraform_Plan
    steps:
    - script: |
        terraform init -backend-config=backend.tfvars
        terraform validate
        terraform plan -out=tfplan
      displayName: "Terraform Init & Plan"

- stage: Apply
  dependsOn: Plan
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/main'))
  jobs:
  - job: Terraform_Apply
    steps:
    - script: terraform apply -auto-approve tfplan
      displayName: "Terraform Apply"</code></pre>
        <p><strong>In Practice:</strong> I include Checkov/TFLint steps pre-plan and use Key Vault variable groups for secure secrets injection. The entire workflow is automated via pipeline triggers.</p>
      </div>`},{question:"Have you worked with Bash or PowerShell scripting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Experience with Bash & PowerShell</strong></h3>
        <p>
          Yes  I use <strong>Bash</strong> daily for Linux automation, CI tasks, and lightweight glue scripts; and <strong>PowerShell</strong> for Windows automation, Azure CLI/ARM tasks, and cross-platform scripts where richer object handling is required.
        </p>
        <p><strong>Practical:</strong> Bash for container hosts, cron jobs, and pipeline steps; PowerShell for Windows VM provisioning, Azure runbooks, and scripting complex Azure AD interactions.</p>
      </div>`},{question:"What is a shell script and how to create one?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Shell Script  Definition & Creation</strong></h3>
        <p>
          A shell script is a plain-text file containing shell commands (Bash, sh, PowerShell) executed by the shell interpreter to automate tasks.
        </p>
        <ol style="margin-left:1.2rem;">
          <li>Create a file <code>script.sh</code>.</li>
          <li>Add a shebang: <code>#!/usr/bin/env bash</code> at the top for Bash.</li>
          <li>Make it executable: <code>chmod +x script.sh</code>.</li>
          <li>Run: <code>./script.sh</code> or <code>bash script.sh</code>.</li>
        </ol>
        <pre><code>#!/usr/bin/env bash
echo "Hello from shell script"</code></pre>
        <p><strong>Tip:</strong> Add error handling (<code>set -euo pipefail</code>) and logging for production scripts.</p>
      </div>`},{question:"What is the difference between Bash and PowerShell?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Bash vs PowerShell  Key Differences</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Platform:</strong> Bash native to Unix/Linux (also available on Windows via WSL). PowerShell is cross-platform (PowerShell Core) but traditionally Windows-first.</li>
          <li><strong>Data model:</strong> Bash deals with text streams; PowerShell uses objects (PSObjects) which is powerful for passing structured data.</li>
          <li><strong>Syntax:</strong> Different syntax and built-in cmdlets. PowerShell has a rich module ecosystem for Windows APIs and Azure modules; Bash relies on CLI tools and text utilities (sed, awk, jq).</li>
          <li><strong>Use cases:</strong> Bash for lightweight shell automation, container hosts; PowerShell for Windows admin tasks, Azure automation with Az module, and complex orchestration needing objects.</li>
        </ul>
        <p><strong>Practical:</strong> Choose Bash for portability in Linux containers and PowerShell when manipulating structured data or using Windows-specific APIs.</p>
      </div>`},{question:"How do you automate Terraform commands using scripting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Terraform via Scripts</strong></h3>
        <p>
          Wrap Terraform steps in a script or pipeline job to make runs repeatable and idempotent.
        </p>
        <pre><code>#!/usr/bin/env bash
set -euo pipefail

export ARM_SUBSCRIPTION_ID="..."; export ARM_CLIENT_ID="..."; export ARM_CLIENT_SECRET="..."; export ARM_TENANT_ID="..."

cd infra/terraform
terraform init -input=false -backend-config="key=env/prod.tfstate"
terraform validate
terraform plan -out=tfplan -input=false
# optional: publish tfplan artifact or require approval
terraform apply -input=false -auto-approve tfplan</code></pre>
        <p><strong>Best practices:</strong> - Use remote state (Azure blob) with locking - Use <code>plan -out</code> and apply saved plan - Avoid embedding secrets: pass via env or Key Vault - Add retries/timeouts in CI.</p>
      </div>`},{question:"What is a null resource and how can scripting trigger it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform null_resource & Provisioners</strong></h3>
        <p>
          <code>null_resource</code> is a Terraform resource with no provider-managed infrastructure; used to run provisioners or trigger actions based on changes (using <code>triggers</code>).
        </p>
        <pre><code>resource "null_resource" "run_script" {
  triggers = {
    timestamp = timestamp()   # change to force run
    config_hash = sha1(file("config.yml"))
  }

  provisioner "local-exec" {
    command = "bash ./scripts/post_deploy.sh &dollar;{var.some_value}"
  }
}</code></pre>
        <p><strong>How scripting triggers it:</strong> The <code>local-exec</code> runs the script when Terraform creates/changes the null_resource. Use deterministic triggers (file hash, var change) to run only when needed.</p>
        <p><strong>Warning:</strong> Prefer external orchestration (CI/CD) over Terraform provisioners for complex tasks to keep Terraform declarative.</p>
      </div>`},{question:"What are environment variables and how to use them in scripts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Environment Variables  Use & Examples</strong></h3>
        <p>
          Environment variables are key/value pairs available to processes  used to pass configuration and secrets to scripts without hardcoding.
        </p>
        <pre><code># set
export DB_USER='appuser'
export DB_PASS='s3cret'

# use in Bash
echo "DB user is &dollar;DB_USER"

# use in PowerShell
&dollar;env:DB_USER</code></pre>
        <p><strong>Best practices:</strong> - Mark secrets as pipeline secrets so agents mask them - Avoid echoing secrets in logs - Use managed identities where possible instead of secrets.</p>
      </div>`},{question:"How to pass dynamic values from script to pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Passing Values from Script to Pipeline</strong></h3>
        <p>
          CI systems provide mechanisms to set pipeline variables from scripts. Example for Azure DevOps (Bash):
        </p>
        <pre><code># compute value in script
IMAGE_TAG=&dollar;(git rev-parse --short HEAD)
echo "##vso[task.setvariable variable=IMAGE_TAG]&dollar;IMAGE_TAG"

# later in pipeline use &dollar;(IMAGE_TAG)</code></pre>
        <p><strong>Other CI:</strong> GitHub Actions uses <code>::set-output</code> or environment files; GitLab uses <code>echo "VAR=value" &gt;&gt; &dollar;GITHUB_ENV</code>-style mechanisms. Always mark secrets as secret variables.</p>
      </div>`},{question:"What is the AWK command in Linux?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>AWK  Text Processing Utility</strong></h3>
        <p>
          <strong>awk</strong> is a powerful text-processing language used to parse and transform structured text (like columns in logs / CSV).
        </p>
        <pre><code># print 1st and 3rd column from whitespace-separated file
awk '{print &dollar;1, &dollar;3}' /var/log/syslog

# filter and sum a numeric column
awk '/ERROR/ {sum += &dollar;5} END {print sum}' logfile</code></pre>
        <p><strong>In Practice:</strong> I use awk with tail/grep/jq for quick one-liners in debugging and pipelines where installing heavy tools isn't desired.</p>
      </div>`},{question:"What is &dollar;_ and &dollar;? in shell scripting?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Special Shell Variables: &dollar;_ and &dollar;?</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><code>&dollar;?</code>  Exit status of the last executed command (0 = success, non-zero = failure). Useful for conditional checks.</li>
          <li><code>&dollar;_</code>  In many shells, holds the last argument of the previous command. In interactive shells also used by some utilities; behavior can vary between shells.</li>
        </ul>
        <pre><code>some_command
if [ &dollar;? -ne 0 ]; then
  echo 'command failed'
fi

echo "Last arg of previous cmd: &dollar;_"</code></pre>
        <p><strong>Practical:</strong> Prefer explicit variables for clarity in scripts; rely on <code>&dollar;?</code> for quick error handling.</p>
      </div>`},{question:"Write a simple Bash script to print Hello Bash.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Hello Bash Script</strong></h3>
        <pre><code>#!/usr/bin/env bash
# hello.sh
set -euo pipefail

echo "Hello Bash"</code></pre>
        <p>Make executable: <code>chmod +x hello.sh</code>. Run: <code>./hello.sh</code>.</p>
      </div>`},{question:"Write a script with loop function example.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Loop Example (Bash)</strong></h3>
        <pre><code>#!/usr/bin/env bash
set -euo pipefail

for i in {1..5}; do
  echo "Iteration: &dollar;i"
  sleep 1
done

# function with while loop
countdown() {
  local n=&dollar;1
  while [ &dollar;n -gt 0 ]; do
    echo "Countdown: &dollar;n"
    n=&dollar;((n-1))
  done
}

countdown 3</code></pre>
      </div>`},{question:"Write a Bash script to send email alert when disk usage exceeds 80%.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Disk Usage Alert Script (Bash)</strong></h3>
        <p>
          This example uses <code>mailx</code> (or <code>sendmail</code>/SMTP relay) available on many Linux systems. In production use centralized alerting (Prometheus Alertmanager, Azure Monitor).
        </p>
        <pre><code>#!/usr/bin/env bash
set -euo pipefail

THRESHOLD=80
EMAIL="ops@company.com"

usage=&dollar;(df / | tail -1 | awk '{print &dollar;5}' | sed 's/%//')
if [ "&dollar;usage" -ge "&dollar;THRESHOLD" ]; then
  subject="ALERT: Disk usage &dollar;{usage}% on &dollar;(hostname)"
  body="Disk usage is &dollar;{usage}% on &dollar;(hostname) at &dollar;(date)"
  echo "&dollar;body" | mailx -s "&dollar;subject" "&dollar;EMAIL"
  echo "Alert sent to &dollar;EMAIL"
else
  echo "Disk usage &dollar;{usage}%  OK"
fi</code></pre>
        <p><strong>Notes:</strong> Configure mail relay or use API-based notifications (SendGrid, Teams webhook) in cloud environments to avoid managing SMTP.</p>
      </div>`},{question:"What are loops in Bash (for, while, until)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Bash Loops  for, while, until</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>for</strong>  iterate a known list or range:
            <pre><code>for item in a b c; do
  echo &dollar;item
done</code></pre>
          </li>
          <li><strong>while</strong>  run while condition is true:
            <pre><code>n=3
while [ &dollar;n -gt 0 ]; do
  echo &dollar;n
  n=&dollar;((n-1))
done</code></pre>
          </li>
          <li><strong>until</strong>  run until condition becomes true (opposite of while):
            <pre><code>x=0
until [ &dollar;x -ge 5 ]; do
  echo &dollar;x
  x=&dollar;((x+1))
done</code></pre>
          </li>
        </ul>
        <p><strong>Practical tip:</strong> Use <code>set -euo pipefail</code> plus proper quoting to avoid subtle loop bugs when iterating filenames or command outputs.</p>
      </div>`},{question:"What is cron job and how to schedule it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cron Job  Schedule recurring tasks</strong></h3>
        <p>
          A <strong>cron job</strong> is a scheduled task on Unix-like systems run by the <code>crond</code> daemon. You schedule it by adding entries to a crontab file with the format:
        </p>
        <pre><code> minute (0 - 59)
  hour   (0 - 23)
   day    (1 - 31)
    month  (1 - 12)
     weekday (0 - 6) (Sunday=0)
* * * * *  command-to-run</code></pre>
        <p><strong>Examples:</strong></p>
        <pre><code># run backup at 02:00 daily
0 2 * * * /usr/local/bin/backup.sh

# run every 15 minutes
*/15 * * * * /usr/local/bin/healthcheck.sh</code></pre>
        <p><strong>Commands:</strong> <code>crontab -e</code> to edit current user's crontab, <code>crontab -l</code> to list.</p>
        <p><strong>Best practice:</strong> Redirect output to log files, set PATH in crontab, and use a wrapper that handles locking (flock) to prevent overlapping runs.</p>
      </div>`},{question:"How to remove sensitive passwords from scripts or repos?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Remove secrets from repo  Steps & Tools</strong></h3>
        <p>
          If secrets were committed, remove them and rotate credentials. Recommended tools:
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>BFG Repo-Cleaner</strong>  fast for removing files/strings from Git history.</li>
          <li><strong>git filter-repo</strong>  recommended modern tool for rewriting history precisely.</li>
        </ul>
        <pre><code># Example (git filter-repo)
pip install git-filter-repo
git clone --mirror git@repo:project.git
cd project.git
git filter-repo --path passwords.txt --invert-paths
git push --force --all
git push --force --tags</code></pre>
        <p><strong>Critical next steps:</strong> rotate the compromised secrets (API keys, passwords), invalidate old tokens, inform stakeholders, and add pre-commit hooks / scanners (TruffleHog) to prevent re-commit.</p>
      </div>`},{question:"How to securely store API keys used in automation scripts?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure storage for API keys</strong></h3>
        <p>Prefer a secrets manager over embedding keys in code:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Azure Key Vault:</strong> Use Key Vault + Managed Identity or Service Principal to fetch secrets at runtime.</li>
          <li><strong>Environment variables:</strong> Inject at runtime by pipeline (marked secret) or container runtime (Kubernetes Secrets or CSI Key Vault).</li>
          <li><strong>Vaults:</strong> HashiCorp Vault for multi-cloud or advanced rotation policies.</li>
        </ul>
        <pre><code># Example: fetch secret using Managed Identity (Azure CLI)
secret=&dollar;(az keyvault secret show --vault-name myVault --name APIKey --query value -o tsv)
export API_KEY=&dollar;secret</code></pre>
        <p><strong>Best practices:</strong> use short-lived tokens, rotate regularly, restrict Key Vault access using RBAC & firewall, and audit access via logs.</p>
      </div>`},{question:"What are linters and how are they used (TFLint, Checkov, TruffleHog)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Linters & Security Scanners</strong></h3>
        <p>
          Linters/statics scanners analyse IaC/code for correctness, best-practices, or secrets:
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>TFLint:</strong> Linter for Terraform  finds provider-specific misconfigurations and style issues.</li>
          <li><strong>Checkov:</strong> Policy-as-code scanner for Terraform/CloudFormation  flags insecure resources against CIS/OWASP-like rules.</li>
          <li><strong>TruffleHog:</strong> Scans Git history and commits for high-entropy strings (possible secrets).</li>
        </ul>
        <pre><code># Typical pipeline snippet
tflint --config .tflint.hcl
checkov -d .
trufflehog filesystem --directory .</code></pre>
        <p><strong>Integration:</strong> run them in PR pipelines; fail PR if critical findings; publish reports as artifacts or comments on PR for developer visibility.</p>
      </div>`},{question:"How do you integrate SonarQube using script?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>SonarQube integration via script</strong></h3>
        <p>
          Use the SonarScanner CLI to analyze code and submit results to SonarQube server. Provide authentication token from secret store.
        </p>
        <pre><code># Example Bash script (basic)
#!/usr/bin/env bash
set -euo pipefail

export SONAR_HOST_URL=https://sonarqube.example.com
export SONAR_TOKEN=&dollar;(az keyvault secret show --vault-name myVault --name SonarToken --query value -o tsv)

sonar-scanner   -Dsonar.projectKey=myproject   -Dsonar.sources=.   -Dsonar.host.url=&dollar;{SONAR_HOST_URL}   -Dsonar.login=&dollar;{SONAR_TOKEN}</code></pre>
        <p><strong>In CI:</strong> run scanner in build stage, and use SonarQube quality gates (fail pipeline if gate fails) via REST or plugin integrations.</p>
      </div>`},{question:"How to integrate Azure Key Vault in pipeline via script?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Key Vault from script in pipelines</strong></h3>
        <p>
          Two common approaches:
        </p>
        <ol style="margin-left:1.2rem;">
          <li><strong>AzureCLI task (Script):</strong> Use service connection; fetch secrets via az cli and set pipeline variables.</li>
          <li><strong>Key Vault-linked variable group:</strong> Configure variable group in ADO that pulls secrets from Key Vault  no script required.</li>
        </ol>
        <pre><code>- task: AzureCLI@2
  inputs:
    azureSubscription: 'sc-azure-prod'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: |
      secret=&dollar;(az keyvault secret show --vault-name myVault --name AppSecret --query value -o tsv)
      echo "##vso[task.setvariable variable=APP_SECRET;issecret=true]&dollar;secret"</code></pre>
        <p><strong>Security:</strong> Mark variable as secret (issecret=true), avoid echoing secret, and prefer Managed Identity over SPN for runtime access.</p>
      </div>`},{question:"How to fetch Key Vault secrets automatically when updated?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automatic secret refresh patterns</strong></h3>
        <p>
          Options depend on use-case (apps vs pipelines):
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Applications (runtime):</strong> Use Key Vault SDK / CSI Secrets Store in Kubernetes  it can pull secrets on demand or via refresh controllers.</li>
          <li><strong>Event-driven:</strong> Subscribe to Key Vault events via <strong>Event Grid</strong> (SecretNearExpiry / SecretNewVersion)  trigger Azure Function / Logic App to notify or update config.</li>
          <li><strong>Pipelines:</strong> Re-run pipelines or have a small job that checks secret version via <code>az keyvault secret show --id</code> and refreshes stored variables.</li>
        </ul>
        <pre><code># Example: Event Grid -> Azure Function idea
Key Vault (SecretNewVersion) -> Event Grid -> Function -> call deployment pipeline or push new config to AppConfig</code></pre>
        <p><strong>Practical note:</strong> For Kubernetes, CSI + rotation sidecars or HashiCorp Vault Agent auto-updating is a robust pattern for in-cluster secret refresh.</p>
      </div>`},{question:"How to use scripting for CI/CD automation and notifications?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Scripting for automation & notifications</strong></h3>
        <p>
          Scripts glue steps, read outputs, and call APIs for orchestration and notifications.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Orchestration: scripts run terraform/helm/az cli commands and set pipeline variables using built-in commands (<code>##vso[task.setvariable]</code> for ADO).</li>
          <li> Notifications: use curl to send messages to Slack/Teams/Email or call Azure DevOps REST API to update PR status.</li>
        </ul>
        <pre><code># Example: send Teams notification via webhook
payload='{"text":"Deployment succeeded: &dollar;APP_NAME - &dollar;BUILD_ID"}'
curl -H 'Content-Type: application/json' -d "&dollar;payload" &dollar;TEAMS_WEBHOOK_URL</code></pre>
        <p><strong>Best practice:</strong> mask secrets, implement retry/backoff for webhook calls, and centralize notification logic in small reusable scripts or functions.</p>
      </div>`},{question:"How to automate Terraform validation, plan, and apply steps?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Automating Terraform lifecycle in scripts/pipeline</strong></h3>
        <p>
          Use a controlled multi-stage flow: <strong>validate  plan  (approve)  apply</strong>. Example script that runs safely in CI/CD:
        </p>
        <pre><code>#!/usr/bin/env bash
set -euo pipefail

# env vars for Azure auth must be set (ARM_* or use az login)
terraform init -input=false -backend-config="key=env/prod.tfstate"
terraform validate

# plan and save artifact
terraform plan -out=tfplan -input=false
terraform show -json tfplan > tfplan.json
# publish tfplan.json as pipeline artifact (pipeline step)

# In Apply stage: download artifact and apply
terraform apply -input=false -auto-approve tfplan</code></pre>
        <p><strong>Production safeguards:</strong> require manual approval before apply, use saved plan file to guarantee apply matches plan, and implement retries with exponential backoff on transient failures. Always have an emergency unlock/runbook for state lock issues.</p>
      </div>`}]},{title:"38. Infra & Architecture Scenarios",questions:[{question:"If you're an architect, how would you propose infra to a customer?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>How I propose infrastructure  practical architecture approach</strong></h3>
        <p>As an architect I follow a repeatable pattern: requirements  constraints  design options  cost/ops trade-offs  implementation plan.</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Gather requirements:</strong> SLA, RPO/RTO, traffic patterns, compliance, peak loads, multi-region needs, budget, team skills.</li>
          <li><strong>Define constraints:</strong> e.g., legacy apps, license limitations, required regions, required on-prem connectivity (ExpressRoute/VPN).</li>
          <li><strong>Choose topology:</strong> Hub-and-spoke for multi-team orgs (hub: security, shared services; spokes: apps/tenant isolation).</li>
          <li><strong>Security & governance:</strong> Azure AD for auth, network segmentation with NSGs/UDRs, Azure FW/NVA, Private Endpoints, Key Vault for secrets, RBAC least privilege, policy enforcement via Azure Policy.</li>
          <li><strong>Resiliency & scale:</strong> Use Availability Zones/sets, autoscaling (VMSS/AKS HPA + Cluster Autoscaler), multi-region DR (ASR/replication), caching/CDN for performance.</li>
          <li><strong>Automation & IaC:</strong> Provision with Terraform + CI/CD, store state remotely (Azure Storage + blob locking), enforce code reviews and policy-as-code.</li>
          <li><strong>Observability:</strong> Logging/metrics (Azure Monitor / Log Analytics, Prometheus + Grafana), alerting, runbooks for ops runbooks + playbooks.</li>
          <li><strong>Deliverables:</strong> high-level diagram, cost estimate, security controls list, pilot implementation plan, rollback/drill plan.</li>
        </ul>
        <p><strong>Why this works:</strong> it's practical, repeatable and balances cost, security and operability  and maps directly to deployment automation and runbooks.</p>
      </div>`},{question:"How to create 10 storage accounts in 10 subscriptions using a single pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Create storage accounts across subscriptions  CI/CD pattern</strong></h3>
        <p>Two practical options: (A) Terraform with aliased providers or (B) pipeline loop/matrix that switches subscription context and runs the same IaC per subscription.</p>
        <h4>Option A  Terraform (preferred for declarative repeatability)</h4>
        <pre><code># providers.tf
provider "azurerm" {
  alias = "sub1"
  subscription_id = "00000000-0000-0000-0000-000000000001"
  features = {}
}
provider "azurerm" {
  alias = "sub2"
  subscription_id = "00000000-0000-0000-0000-000000000002"
  features = {}
}
# resources with for_each over providers
locals { subs = { sub1 = azurerm.sub1, sub2 = azurerm.sub2 /*...*/ } }

resource "azurerm_storage_account" "sa" {
  for_each = local.sub_map
  provider = azurerm.&dollar;{each.key}
  name = "st&dollar;{each.key}001"
  resource_group_name = each.value.rg
  location = each.value.location
  account_tier = "Standard"
  account_replication_type = "LRS"
}
</code></pre>
        <p>Use a service principal that has Contributor role in all target subscriptions or grant it scoped RBAC.</p>
        <h4>Option B  Pipeline Matrix (Azure DevOps / GitHub Actions)</h4>
        <pre><code># Azure DevOps YAML (concept)
strategy:
  matrix:
    sub1: { subscriptionId: 'id1', rg: 'rg1' }
    sub2: { subscriptionId: 'id2', rg: 'rg2' }
steps:
- script: az login --service-principal -u &dollar;(spId) -p &dollar;(spSecret) --tenant &dollar;(tenant)
- script: az account set --subscription &dollar;(subscriptionId)
- script: az storage account create --name mystorage&dollar;(subscriptionIndex) --resource-group &dollar;(rg) --location ...
</code></pre>
        <p><strong>Notes:</strong> Use managed identity or SP with least privilege, parallelize via matrix, and centralize naming and tagging rules.</p>
      </div>`},{question:"If VM created with a custom image is updated, what happens on terraform apply?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform behaviour when custom image changes</strong></h3>
        <p>It depends on how the VM references the image:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Managed Image / image_id changed in config:</strong> Terraform will usually plan to replace the VM (create new  destroy old) because OS disk/image is an immutable property for many providers.</li>
          <li><strong>Using a Shared Image Gallery version:</strong> If you update to a new image version in the VM resource, it may trigger an in-place update only if the provider supports it; often a replacement occurs.</li>
          <li><strong>If image content changed but image resource ID unchanged:</strong> Terraform won't detect change unless image resource attributes used by VM change  you must explicitly update VM resource to point to new version or force replacement (<code>terraform taint</code> or change a lifecycle).</li>
        </ul>
        <p><strong>Practical steps I follow:</strong> publish immutable images with versions (Shared Image Gallery), update VM image reference in Terraform to a new version, then run <code>terraform plan</code> to see replacement. Use rolling replacement strategies for scale sets or orchestrate redeploys to avoid downtime.</p>
      </div>`},{question:"What if a user hardcoded values in Terraform  will it show in plan?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Hardcoded values and Terraform plan visibility</strong></h3>
        <p>Yes  hardcoded values in .tf files are part of the configuration and therefore appear in <code>terraform plan</code>. A few important points:</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Plan shows diffs:</strong> If the code has changed (hardcoded value changed), plan shows difference between planned and current state.</li>
          <li> <strong>Secrets risk:</strong> If secrets are hardcoded (passwords, keys), they may appear in plan output and logs  avoid this.</li>
          <li> <strong>Best practice:</strong> Use variables (with <code>sensitive = true</code>), pass via environment/Terrafrom Cloud variables or Key Vault, and never commit secrets to repo.</li>
          <li> <strong>Detecting hardcoded values:</strong> Use linters (tflint, checkov, tfsec) and pre-commit hooks to catch anti-patterns before pipeline runs.</li>
        </ul>
      </div>`},{question:"How to get VM output in JSON format using Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Get VM details in JSON</strong></h3>
        <p>Multiple ways depending if you want Terraform outputs or Azure CLI:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Terraform outputs (if you declare outputs):</strong>
            <pre><code>terraform output -json vm_public_ip</code></pre>
            <p>Outputs are defined in <code>outputs.tf</code> and <code>-json</code> gives machine-readable format.</p>
          </li>
          <li><strong>Azure CLI (direct from Azure):</strong>
            <pre><code>az vm show --name vmname --resource-group rgname -o json</code></pre>
          </li>
          <li><strong>State file query (advanced):</strong>
            <pre><code>terraform state show azurerm_linux_virtual_machine.vm -no-color -json</code></pre>
          </li>
        </ul>
        <p><strong>Practical tip:</strong> Export <code>terraform output -json</code> into CI artifacts for downstream jobs to consume reliably.</p>
      </div>`},{question:"How to securely access resources in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure access patterns in Azure</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Managed Identities:</strong> Use system/user-assigned managed identities for VMs/AKS to access Key Vault, Storage, etc., eliminating static credentials.</li>
          <li> <strong>Azure AD + RBAC:</strong> Enforce least-privilege RBAC roles. Use Privileged Identity Management (PIM) for elevated access.</li>
          <li> <strong>Key Vault:</strong> Store secrets/certificates and use Key Vault access policies or Azure RBAC + Key Vault references.</li>
          <li> <strong>Private Endpoints:</strong> Use Private Endpoint / Service Endpoint for PaaS to avoid public endpoints.</li>
          <li> <strong>Network controls:</strong> NSGs, Azure Firewall, Application Gateway WAF, UDRs, and IDS/IPS where needed.</li>
          <li> <strong>Jump access:</strong> Bastion for admin access, break-glass accounts controlled via PIM, and Just-In-Time access for VMs.</li>
          <li> <strong>Audit & policy:</strong> Azure Policy to enforce configurations, Azure Monitor / Log Analytics for audit logs and alerts.</li>
        </ul>
        <p><strong>Operationally:</strong> Prefer Managed Identities & Key Vault for app-to-service auth, use short-lived tokens and avoid embedding secrets in code or pipelines.</p>
      </div>`},{question:"What is the network topology you worked with?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Network topologies I commonly design & operate</strong></h3>
        <p>I primarily use Hub-and-Spoke and Hybrid topologies depending on scale and isolation needs:</p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Hub-and-Spoke</strong>
            <ul style="margin-left:1.2rem;">
              <li>Hub: central shared services (Azure Firewall, Bastion, DNS, jumpboxes, central logging).</li>
              <li>Spokes: application VNets, each spoke isolated per team/env, peered with hub for shared services.</li>
              <li>Benefits: separation of duties, centralized security, easier routing and egress control.</li>
            </ul>
          </li>
          <li><strong>Flat VNet (small infra)</strong>  single VNet with multiple subnets and NSGs  simple but less isolation.</li>
          <li><strong>Hybrid (on-prem + cloud)</strong>  ExpressRoute or Site-to-Site VPN connecting on-prem networks to hub VNet; UDRs route to network virtual appliances for inspection.</li>
          <li><strong>Micro-segmentation:</strong> Use NSGs + Network Policies (AKS) to isolate workloads within VNet/subnets.</li>
        </ul>
        <p><strong>Example stack I deployed:</strong> Hub VNet with Azure Firewall and Transit Gateway, three spokes (prod, staging, dev), AKS in its own spoke with Azure CNI, Private AKS API, Private Link to Key Vault and Storage, ASR configured to paired region for DR.</p>
      </div>`},{question:"How to connect on-prem apps to cloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Connecting On-Prem to Cloud  Practical Patterns</strong></h3>
        <p>
          Choose connection type based on SLA, bandwidth, security and cost:
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>ExpressRoute:</strong> Private, high-throughput, low-latency link (recommended for production, sensitive data, hybrid DB replication).</li>
          <li><strong>Site-to-Site VPN:</strong> Encrypted tunnel over internet (good for dev/test or lower bandwidth needs).</li>
          <li><strong>Point-to-Site VPN / Azure AD Conditional Access:</strong> For admin/dev remote access to resources in a secure manner.</li>
          <li><strong>Azure Firewall / NVA + UDRs:</strong> Centralized egress/inspection in hub VNet.</li>
          <li><strong>Private Endpoint & Private Link:</strong> Expose PaaS services privately into on-prem networks without public endpoints.</li>
        </ul>
        <p><strong>Typical design:</strong> Hub-and-spoke VNet where hub holds ExpressRoute gateway or VPN gateway + Azure Firewall. Spokes host workloads (AKS, VMSS, App Services) peered to hub. Use UDRs to force egress through firewall/NVA and Private Endpoints for PaaS.</p>
        <p><strong>Operational notes:</strong> configure BGP with ExpressRoute for route propagation, run throughput tests, monitor via Network Watcher, enforce NSGs and Firewall policies, and secure management traffic via Bastion or jumpbox accessible only over private connectivity.</p>
      </div>`},{question:"How do you deploy a 3-tier architecture app?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>3-Tier App  Infrastructure & Deployment Steps</strong></h3>
        <p>
          Typical tiers: Presentation (web), Application (API), Data (DB). Deploy with isolation, security, and CI/CD:
        </p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Network:</strong> Create VNet with separate subnets (web/subnet, app/subnet, db/subnet). Apply NSGs and network segmentation rules.</li>
          <li><strong>Presentation Tier:</strong> App Service / VMSS / AKS Ingress behind Application Gateway (WAF) + Public IP or Front Door for global traffic.</li>
          <li><strong>Application Tier:</strong> AKS or VMSS running API services, Cluster/VM scale, Cluster Autoscaler/HPA for AKS, internal LoadBalancer / ClusterIP services.</li>
          <li><strong>Data Tier:</strong> Managed DB (Azure SQL / PostgreSQL / MySQL) or StatefulSet DB in AKS with Private Endpoint. Deploy with zone-redundant storage or read replicas for HA.</li>
          <li><strong>Security & Secrets:</strong> Key Vault for secrets/certs, Private Endpoints to DB and Storage, RBAC & Managed Identity for service-to-service auth.</li>
          <li><strong>CI/CD & Release Strategy:</strong> Use Azure DevOps / GitHub Actions for pipeline: build  image registry (ACR)  deploy (helm for AKS / slot swap for App Service). Implement blue-green or canary for low-risk releases.</li>
          <li><strong>Observability:</strong> App Insights + Prometheus/Grafana + centralized logging (Fluent Bit  Log Analytics / Loki).</li>
          <li><strong>DR & Backup:</strong> Automated DB backups, snapshots for disks, ASR for VMs if needed, and runbooks for failover.</li>
        </ol>
        <p><strong>Example command snippets:</strong></p>
        <pre><code># Create App Gateway (concept)
az network application-gateway create --name appgw --resource-group rg-prod --sku WAF_v2 --vnet-name prod-vnet --subnet appgw-subnet
# Deploy app to AKS via Helm
helm upgrade --install webapi ./charts/webapi -f values.prod.yaml</code></pre>
      </div>`},{question:"Which type of modular approach do you follow in Terraform?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Modular Pattern  Practical Approach</strong></h3>
        <p>
          I follow a layered, reusable module strategy focused on separation of concerns, reuse, and clear boundaries:
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Root modules per environment:</strong> A thin root that composes child modules (network, compute, storage, aks, security).</li>
          <li><strong>Reusable child modules:</strong> Small focused modules (vnet, subnet, nsg, vmss, aks, keyvault) kept in a central registry or Git submodule.</li>
          <li><strong>Module inputs/outputs:</strong> Well-defined variables & outputs to compose infra without tight coupling.</li>
          <li><strong>State management:</strong> Remote backend per environment / tenant (Azure Blob with locking). One state per logical boundary (e.g., per subscription or per environment) to limit blast radius.</li>
          <li><strong>Workspaces / Environments:</strong> Use separate workspaces or separate state backends for dev/staging/prod (avoid overloading workspaces for major infra).</li>
          <li><strong>Policy & linting:</strong> Use pre-commit hooks, tflint, checkov, and Terraform Cloud/Enterprise policy enforcement for governance.</li>
          <li><strong>Optional:</strong> Terragrunt for DRY/templating and orchestration across many accounts/subscriptions if you need advanced orchestration and dependencies.</li>
        </ul>
        <p><strong>Why this works:</strong> Modules reduce duplication, make testing easier, and enable a clear upgrade path. Keep modules versioned and break changes with major versions.</p>
      </div>`},{question:"How do you manage large scale subscriptions in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Managing Large-Scale Subscriptions  Governance & Automation</strong></h3>
        <p>
          Scale requires governance, naming, automation, and observability:
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Management Groups & Landing Zones:</strong> Use Management Groups aligned to org structure (prod, non-prod, security). Implement Azure CAF / landing zone patterns for consistent guardrails.</li>
          <li><strong>Azure Policy & Blueprints:</strong> Enforce tagging, SKUs, allowed regions, diagnostic settings, and resource locks at scale.</li>
          <li><strong>RBAC Design:</strong> Principle of least privilege through built-in & custom roles; use PIM for elevation.</li>
          <li><strong>Subscription Strategy:</strong> Subscription per business unit / environment / compliance domain to limit blast radius and billing segregation.</li>
          <li><strong>Automation:</strong> CI/CD for infra (Terraform), automation for onboarding subscriptions (ARM templates/Terraform + scripts), and automation for resource tagging and quotas.</li>
          <li><strong>Monitoring & Cost Management:</strong> Centralized Log Analytics workspaces or per-spoke with cross-workspace queries; budgets, alerts and chargeback via Cost Management and tagging.</li>
          <li><strong>Operations:</strong> Use Azure Lighthouse for multi-tenant management and delegated access when managing customer subscriptions at scale.</li>
        </ul>
        <p><strong>Operational checklist:</strong> automated onboarding pipeline, standard RBAC & policies, central logging + security posture (Secure Score), and periodic cost/security reviews.</p>
      </div>`},{question:"How to set up DR (Disaster Recovery) in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DR Setup  Multi-layer Strategy</strong></h3>
        <p>
          DR must be tiered: data, app, and infra. Approach:
        </p>
        <ol style="margin-left:1.2rem;">
          <li><strong>Define RPO/RTO:</strong> Business-driven SLAs (e.g., RPO  15 min, RTO  1 hr).</li>
          <li><strong>Data Replication:</strong> For storage use GRS/RA-GRS, for DBs use geo-replication or read-replicas (Azure SQL geo-replication, Cosmos DB multi-region).</li>
          <li><strong>Workload Replication:</strong> Use Azure Site Recovery (ASR) for VMs and specialized replication for other services.</li>
          <li><strong>Infrastructure as Code:</strong> Keep ARM/Terraform templates for full environment reprovisioning in target region.</li>
          <li><strong>Network & Failover:</strong> Configure DNS failover (Traffic Manager / Front Door) or runbooks to update DNS to the recovery region.</li>
          <li><strong>Testing & Runbooks:</strong> Schedule test failovers, maintain runbooks for failover/failback, and run regular DR drills.</li>
          <li><strong>Automation & Orchestration:</strong> Orchestrate failover steps (db failover, app bring-up, cutover) with Azure Automation / Logic Apps / pipelines.</li>
          <li><strong>Cost vs RTO tradeoff:</strong> Warm standby (reduced capacity) vs Hot (fully provisioned) vs Cold (on-demand provisioning) depends on budget and SLA.</li>
        </ol>
        <p><strong>Example:</strong> For critical VMs: ASR replicates to paired region, use private endpoints in DR VNet, and automated DNS switch via Traffic Manager on failover.</p>
      </div>`},{question:"What is your approach to cost optimization as an architect?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cost Optimization  Architecture & Process</strong></h3>
        <p>
          Cost optimisation is continuous: design-time choices + run-time controls:
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>Right-size compute:</strong> Right-size VMs/VMSS/AKS node pools based on observed metrics; use autoscaling).</li>
          <li><strong>Reserved Instances & Savings Plans:</strong> Use 1/3-year Reserved VM Instances or Savings Plans for steady-state workloads.</li>
          <li><strong>Spot Instances:</strong> Use Spot/Low-priority VMs for cheaper, fault-tolerant batch/worker jobs.</li>
          <li><strong>Storage tiering:</strong> Move cold data to Cool/Archive tiers; use lifecycle policies for Blob Storage.</li>
          <li><strong>Use PaaS:</strong> Prefer managed services (App Service, Azure SQL) where TCO is lower than self-managed VMs.</li>
          <li><strong>Scheduling non-prod:</strong> Auto-shutdown dev/test resources outside business hours using automation to save costs.</li>
          <li><strong>Optimize networking:</strong> Avoid unnecessary egress, choose regional services wisely, and consolidate gateways where possible.</li>
          <li><strong>Visibility & governance:</strong> Enforce tagging for chargeback, use Azure Cost Management + Budgets + Alerts, and run monthly cost reviews driven by Cloud FinOps principles.</li>
          <li><strong>Continuous improvement:</strong> Use Azure Advisor, Cost recommendations, and custom scripts to detect unused resources (orphaned disks, unattached IPs).</li>
        </ul>
        <p><strong>Outcome:</strong> I combine architectural choices (reserved capacity, spot, serverless) with operational controls (auto-shutdown, rightsizing, tagging & budgets) to reduce cloud spend while meeting business SLAs.</p>
      </div>`}]},{title:"39. General / Uncategorized",questions:[{question:"Introduce yourself.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Introduction</strong></h3>
        <p>
          Hi, Im <strong>Ritesh Sharma</strong>, a DevOps and Cloud Engineer with over <strong>10 years of experience</strong> in infrastructure automation, cloud deployments, and CI/CD implementations. 
        </p>
        <p>
          Currently, Im working at <strong>Litmus Information System LLP</strong> where I handle end-to-end DevOps activities  from VM provisioning, patching, and monitoring to containerization and Azure DevOps pipelines.
        </p>
        <p>
          I specialize in <strong>Azure, Terraform, Docker, Kubernetes (AKS), GitHub Actions, Jenkins, and monitoring stacks</strong> like Prometheus and Grafana.
        </p>
        <p>
          My focus is always on <strong>automation, scalability, and reliability</strong>  ensuring our environments are secure, cost-optimized, and ready for continuous delivery.
        </p>
      </div>`},{question:"Tell me about your recent project.",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Recent Project Overview</strong></h3>
        <p>
          Recently, I worked on a <strong>centralized patient monitoring system</strong> for a European healthcare provider. The goal was to collect, process, and visualize real-time health data from IoT-connected devices across hospitals.
        </p>
        <h4> Key Responsibilities:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Designed and deployed the environment on <strong>Azure</strong> using <strong>Terraform</strong> with remote state storage in Azure Storage Account.</li>
          <li>Containerized microservices using <strong>Docker</strong> and orchestrated with <strong>AKS (Azure Kubernetes Service)</strong>.</li>
          <li>Implemented <strong>CI/CD pipelines</strong> in Azure DevOps for automated builds, security scans (SonarQube), and rolling deployments.</li>
          <li>Set up monitoring using <strong>Prometheus + Grafana</strong> and integrated alerting into Slack for real-time notifications.</li>
          <li>Used <strong>Azure Key Vault</strong> for managing sensitive secrets and certificates securely.</li>
        </ul>
        <p><strong>Outcome:</strong> Reduced release time by 60%, improved uptime with autoscaling, and ensured full compliance with healthcare data regulations.</p>
      </div>`},{question:"What all projects do you work on?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Projects I Work On</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Healthcare Monitoring System:</strong> Built centralized data aggregation for patient telemetry using AKS, Azure Monitor, and IoT Hub.</li>
          <li> <strong>Logistics Visibility Platform:</strong> Developed real-time shipment tracking similar to FourKites  deployed microservices using Kubernetes and Azure DevOps.</li>
          <li> <strong>Cloud Migration:</strong> Migrated legacy apps from on-prem to Azure, leveraging Terraform and VMSS for scalable deployment.</li>
          <li> <strong>DevOps Pipelines:</strong> Automated build and deploy using YAML pipelines, SonarQube quality gates, and ACR for image management.</li>
          <li> <strong>Monitoring & Cost Optimization:</strong> Integrated Prometheus-Grafana and Azure Cost Management for proactive performance and spend control.</li>
        </ul>
        <p><strong>Focus Areas:</strong> IaC automation, CI/CD pipelines, container orchestration (AKS), and implementing secure, resilient architectures.</p>
      </div>`},{question:"What are the next rounds of interviews?","answerHtml:":`
      <div class="answer-rich">
        <h3> <strong>Typical Interview Rounds</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li>1 <strong>Technical Round:</strong> Covers cloud fundamentals, Terraform, CI/CD, Docker, Kubernetes, and troubleshooting.</li>
          <li>2 <strong>Scenario / Hands-on Round:</strong> Given a real-world DevOps problem (like building a pipeline or fixing YAML deployment).</li>
          <li>3 <strong>Architectural / Design Round:</strong> Focuses on designing end-to-end infrastructure or solution architecture in Azure.</li>
          <li>4 <strong>Managerial / HR Round:</strong> Behavioral, communication, project delivery experience, and team collaboration discussions.</li>
        </ul>
        <p><strong>Tip:</strong> Be ready with detailed real-world examples of how you solved deployment, scaling, or monitoring challenges.</p>
      </div>`},{question:"Do we use scripting here?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Use of Scripting in DevOps</strong></h3>
        <p>
          Yes  scripting is a crucial part of DevOps automation. I use it across multiple areas:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Bash / Shell:</strong> For VM provisioning, service restarts, log cleanup, patching automation, and container entrypoint scripts.</li>
          <li> <strong>Python:</strong> For API integrations, report generation, and automation with Azure SDK or REST APIs.</li>
          <li> <strong>PowerShell:</strong> For Azure resource operations and Windows VM automation.</li>
          <li> <strong>YAML / Groovy:</strong> For CI/CD pipelines (Azure DevOps, GitHub Actions, Jenkins).</li>
        </ul>
        <p><strong>In Practice:</strong> I maintain reusable shell scripts in pipelines and integrate them with Terraform for deployment orchestration.</p>
      </div>`},{question:"What is your role and responsibilities?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Role & Responsibilities</strong></h3>
        <p>
          As a <strong>DevOps Engineer at Litmus Information System LLP</strong>, my role focuses on designing, automating, and maintaining cloud infrastructure and CI/CD processes.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Provision and manage infrastructure using Terraform (Azure as provider).</li>
          <li> Develop and maintain CI/CD pipelines for AKS and App Service deployments.</li>
          <li> Containerize applications with Docker and manage AKS clusters (scaling, monitoring, and security).</li>
          <li> Implement monitoring and alerting via Grafana, Prometheus, and Azure Monitor.</li>
          <li> Manage patching, OS upgrades, NFS setup, and VM lifecycle management.</li>
          <li> Enforce DevSecOps practices using SonarQube and Checkov for code quality and compliance.</li>
        </ul>
        <p><strong>Goal:</strong> Ensure every deployment is automated, repeatable, and aligned with infrastructure best practices.</p>
      </div>`},{question:"What is VNet in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VNet  Azure Virtual Network</strong></h3>
        <p>
          A <strong>VNet (Virtual Network)</strong> is a private, isolated section of Azure where you can securely run your resources such as VMs, AKS clusters, and PaaS services.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Acts like an on-prem data center network in the cloud.</li>
          <li> You define subnets, IP ranges, and routing rules.</li>
          <li> Supports hybrid connectivity using VPN Gateway or ExpressRoute.</li>
          <li> Enables network segmentation with NSGs and route control via UDRs.</li>
          <li> Connect VNets using peering for inter-environment communication (e.g., hub-spoke model).</li>
        </ul>
        <p><strong>In Practice:</strong> I design VNets per environment (prod, stage, dev), implement NSGs for subnet-level isolation, and integrate them with Private Endpoints for PaaS services.</p>
      </div>`},{question:"What is NAT and Application Gateway?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>NAT vs Application Gateway  Network Services</strong></h3>
        <p>
          Both manage traffic flow, but serve different purposes:
        </p>
        <h4> NAT (Network Address Translation):</h4>
        <ul style="margin-left:1.2rem;">
          <li>Converts private IPs to public IPs for outbound traffic (egress).</li>
          <li>In Azure, implemented using <strong>Azure NAT Gateway</strong> for consistent outbound IP and better scalability than SNAT from load balancers.</li>
          <li>Attached to subnets  all outbound traffic routes through NAT Gateway.</li>
        </ul>
        <h4> Application Gateway:</h4>
        <ul style="margin-left:1.2rem;">
          <li>Layer 7 (HTTP/HTTPS) load balancer with advanced routing (path-based, host-based).</li>
          <li>Supports SSL termination, Web Application Firewall (WAF), and redirection.</li>
          <li>Used for inbound traffic distribution to backend pools (VMs, AKS Ingress, App Services).</li>
        </ul>
        <p><strong>In Practice:</strong> I use NAT Gateway for consistent outbound traffic from private subnets, and Application Gateway (WAF_v2) for inbound web app traffic in 3-tier or AKS architectures.</p>
      </div>`},{question:"What is Load Balancer?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Load Balancer  Overview</strong></h3>
        <p>
          A <strong>Load Balancer</strong> distributes incoming network traffic across multiple backend resources (like VMs or VMSS) to ensure high availability, fault tolerance, and optimal resource utilization.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Operates at <strong>Layer 4 (TCP/UDP)</strong> of the OSI model.</li>
          <li> Supports inbound and outbound scenarios.</li>
          <li> Provides health probes to detect unhealthy instances and reroute traffic automatically.</li>
          <li> Integrates with Azure Virtual Machines, Scale Sets, and AKS nodes for distribution.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Load Balancer for distributing API traffic across backend VMSS nodes with health probe monitoring and NAT rules for admin access.</p>
      </div>`},{question:"How many types of load balancers exist?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Types of Load Balancers in Azure</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Azure Load Balancer (Layer 4):</strong> Works at the transport layer; supports both <strong>Public</strong> (internet-facing) and <strong>Internal</strong> (private) load balancing.</li>
          <li> <strong>Application Gateway (Layer 7):</strong> Web traffic (HTTP/HTTPS) load balancer with routing, SSL termination, and WAF (Web Application Firewall).</li>
          <li> <strong>Traffic Manager (DNS-based):</strong> Global load balancing via DNS routing based on endpoint health, priority, or geographic location.</li>
          <li> <strong>Front Door (Layer 7, Global):</strong> Global application acceleration with caching, routing, and security for web apps (CDN + WAF integration).</li>
        </ul>
        <p><strong>In Practice:</strong> For AKS and web apps, I prefer Application Gateway (WAF_v2) with path-based routing; for multi-region DR, I use Azure Front Door or Traffic Manager.</p>
      </div>`},{question:"What is Public and Private IP?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Public vs Private IP in Azure</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Public IP:</strong> Accessible from the internet; assigned to Azure resources like Load Balancers, Application Gateways, or VMs with public exposure.</li>
          <li><strong>Private IP:</strong> Internal IP address within a VNet/subnet; used for secure communication between Azure resources or hybrid setups.</li>
        </ul>
        <p><strong>Public IP Example:</strong> Used by web servers, gateways, or VPN endpoints.</p>
        <p><strong>Private IP Example:</strong> Used for backend databases, app-tier VMs, and private endpoints.</p>
        <p><strong>In Practice:</strong> I ensure production workloads use <strong>Private IP + NAT Gateway</strong> for outbound traffic, keeping resources off the public internet.</p>
      </div>`},{question:"What is a Service Endpoint and Private Link?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Service Endpoint vs Private Link</strong></h3>
        <p>
          Both secure Azure PaaS resources, but differ in approach:
        </p>
        <h4> <strong>Service Endpoint:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Extends your VNets private IP space to Azure PaaS services (like Storage, SQL, Cosmos DB).</li>
          <li>Traffic flows over Microsofts backbone network  not over the public internet.</li>
          <li>Still uses the PaaS services public IP  no private endpoint created.</li>
        </ul>
        <h4> <strong>Private Link (Private Endpoint):</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Provides a <strong>private IP</strong> inside your VNet for a PaaS resource.</li>
          <li>Fully isolates access  PaaS service becomes part of your private network.</li>
          <li>Preferred for highly secure and regulated environments.</li>
        </ul>
        <p><strong>In Practice:</strong> I use <strong>Private Link</strong> for all sensitive workloads (e.g., SQL DBs, Key Vault, Storage) and <strong>Service Endpoints</strong> for lower-risk internal services.</p>
      </div>`},{question:"How do you securely access your Azure resources?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Secure Access to Azure Resources</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Private Endpoints:</strong> Access PaaS services privately within VNets.</li>
          <li> <strong>Network Controls:</strong> Use NSGs, ASGs, and Azure Firewall to control inbound/outbound access.</li>
          <li> <strong>Azure AD & RBAC:</strong> Role-based access and conditional MFA for user and service access.</li>
          <li> <strong>Managed Identities:</strong> Eliminate credentials; use MI for VM, AKS, and Function authentication to Key Vault and Storage.</li>
          <li> <strong>Bastion:</strong> Secure RDP/SSH over TLS directly from Azure Portal without public IPs.</li>
          <li> <strong>Azure Policy:</strong> Enforce allowed SKUs, region restrictions, and mandatory tagging.</li>
          <li> <strong>Logging & Alerts:</strong> Azure Monitor and Defender for Cloud for continuous auditing and alerting.</li>
        </ul>
        <p><strong>In Practice:</strong> I enforce private endpoints for all data services, access control via RBAC + PIM, and automate identity rotation using Managed Identity.</p>
      </div>`},{question:"What is Bastion and why is it used?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Bastion  Secure VM Access</strong></h3>
        <p>
          Azure Bastion allows <strong>secure, browser-based RDP/SSH access</strong> to Azure VMs directly through the Azure Portal, without exposing any public IPs.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Removes the need for public IP or inbound NSG rules for management ports (22/3389).</li>
          <li> Sessions run entirely over SSL within the Azure Portal.</li>
          <li> Deployed inside a subnet named <code>AzureBastionSubnet</code>.</li>
          <li> Integrated with NSGs and private subnets for minimal attack surface.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Bastion for admin SSH/RDP access to private VMs and jump servers  fully replacing jumpboxes in secure architectures.</p>
      </div>`},{question:"What are NSG rules and priorities?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>NSG Rules & Priorities</strong></h3>
        <p>
          <strong>Network Security Groups (NSG)</strong> control inbound/outbound traffic to Azure resources using rules defined by priority, direction, protocol, and source/destination.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Priority:</strong> Range 1004096 (lower number = higher priority).</li>
          <li> Rules evaluated in order until a match is found (allow or deny).</li>
          <li> Default rules exist for VNet, AzureLoadBalancer, and Internet traffic.</li>
          <li> You can apply NSGs at subnet or NIC level.</li>
        </ul>
        <pre><code># Example
Priority: 100
Direction: Inbound
Source: Internet
Port: 443
Action: Allow</code></pre>
        <p><strong>In Practice:</strong> I use subnet-level NSGs for broad rules and NIC-level NSGs for fine-grained restrictions, keeping least-privilege principle intact.</p>
      </div>`},{question:"How to restrict any service on a resource?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Restricting Services in Azure</strong></h3>
        <p>To restrict specific services or access patterns, combine network and policy-based controls:</p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>NSG Rules:</strong> Block inbound/outbound ports or IP ranges.</li>
          <li> <strong>Azure Firewall / NVA:</strong> Apply Layer 7 filtering to restrict URLs/domains.</li>
          <li> <strong>Private Endpoints:</strong> Disable public network access for PaaS services (like SQL, Storage).</li>
          <li> <strong>Azure Policy:</strong> Deny creation of public IP-enabled resources or restrict SKUs/regions.</li>
          <li> <strong>RBAC:</strong> Restrict user/service principal permissions (e.g., no delete or write access).</li>
          <li> <strong>Conditional Access:</strong> Apply identity-level controls via Azure AD.</li>
        </ul>
        <p><strong>Example:</strong> To restrict SQL DB public access  enable Private Endpoint and set <code>publicNetworkAccess = Disabled</code> in Terraform/portal.</p>
      </div>`},{question:"How to identify who is using port 80 in the system?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Identify Port Usage on a System</strong></h3>
        <p>
          You can identify which process or service is using port 80 using OS-level commands:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Linux:</strong>
            <pre><code>sudo netstat -tulnp | grep :80
sudo lsof -i :80
sudo ss -tuln | grep :80</code></pre>
          </li>
          <li> <strong>Windows:</strong>
            <pre><code>netstat -ano | findstr :80
tasklist /fi "PID eq <PID>"</code></pre>
          </li>
          <li> <strong>PowerShell (Windows):</strong>
            <pre><code>Get-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess</code></pre>
          </li>
        </ul>
        <p><strong>In Practice:</strong> I use these commands to trace conflicts when multiple services (like nginx, IIS, or Apache) try to bind port 80 during deployment validation.</p>
      </div>`},{question:"How to close any running process?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Closing Running Processes in Linux</strong></h3>
        <p>
          To terminate a running process, you can use the <code>kill</code> or <code>killall</code> commands. First, identify the process using <code>ps</code> or <code>top</code>.
        </p>
        <pre><code># Step 1: Find process ID (PID)
ps -ef | grep nginx

# Step 2: Kill process by PID
kill &lt;PID&gt;

# Step 3: Force kill if it doesn't stop
kill -9 &lt;PID&gt;

# Alternative: kill by process name
killall nginx
        </code></pre>
        <p><strong>In Practice:</strong> I generally check active processes via <code>ps -aux | grep &lt;app&gt;</code> before killing them, to avoid terminating critical system services.</p>
      </div>`},{question:"What is the purpose of -9 command?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Purpose of the <code>-9</code> Option</strong></h3>
        <p>
          The <code>-9</code> flag in Linux is used with the <code>kill</code> command to send the <strong>SIGKILL</strong> signal  it immediately terminates the process without giving it a chance to clean up.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <code>kill -9 &lt;PID&gt;</code>  Forces the OS to stop the process instantly.</li>
          <li> The process cannot ignore or trap this signal.</li>
          <li> Should be used as a last resort  it may leave temporary files or locks behind.</li>
        </ul>
        <p><strong>In Practice:</strong> I use <code>-9</code> only when a process hangs and does not respond to a normal <code>kill</code> or <code>SIGTERM</code>.</p>
      </div>`},{question:"What is the top command in Linux?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>top  Real-Time Process Monitoring</strong></h3>
        <p>
          The <code>top</code> command displays a dynamic, real-time view of running processes  showing CPU, memory, and load usage.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Displays system summary and per-process statistics.</li>
          <li> Sorts by CPU usage, memory usage, or process ID.</li>
          <li> Updates every few seconds by default.</li>
          <li> Use <code>k</code> inside <code>top</code> to kill a process, or <code>q</code> to quit.</li>
        </ul>
        <pre><code># Example
top
# Show threads
top -H
# Interactive filter by process name
top | grep nginx</code></pre>
        <p><strong>In Practice:</strong> I use <code>top</code> for quick debugging of CPU/memory spikes before moving to <code>htop</code> or Grafana dashboards for detailed analysis.</p>
      </div>`},{question:"What is a cron job?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Cron Job  Linux Task Scheduler</strong></h3>
        <p>
          A <strong>cron job</strong> is a scheduled task in Linux that runs automatically at predefined times or intervals using the <code>crond</code> daemon.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Configured in the <code>/etc/crontab</code> or user crontab (<code>crontab -e</code>).</li>
          <li> Format: <code>minute hour day month weekday command</code>.</li>
        </ul>
        <pre><code># Run backup script every day at 2 AM
0 2 * * * /usr/local/bin/db_backup.sh

# List user cron jobs
crontab -l
        </code></pre>
        <p><strong>In Practice:</strong> I use cron jobs for routine tasks like log cleanup, health checks, and backup scripts across Linux VMs and container hosts.</p>
      </div>`},{question:"What is the sudo command and when do we use it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>sudo  Superuser Do Command</strong></h3>
        <p>
          The <code>sudo</code> command lets a permitted user run a command as the superuser (root) or another user, as defined in the <code>/etc/sudoers</code> file.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Adds administrative privileges without switching to root.</li>
          <li> Logs command usage for audit and security.</li>
          <li> Safer than logging in as root directly.</li>
        </ul>
        <pre><code># Example
sudo apt update
sudo systemctl restart nginx
        </code></pre>
        <p><strong>In Practice:</strong> I use <code>sudo</code> for package installation, service restarts, and modifying system files during patching and configuration management.</p>
      </div>`},{question:"How to change directory permissions in Linux?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Changing Directory Permissions</strong></h3>
        <p>
          Use the <code>chmod</code> and <code>chown</code> commands to modify permissions and ownership.
        </p>
        <ul style="margin-left:1.2rem;">
          <li><strong>chmod:</strong> Changes read (r), write (w), and execute (x) permissions.</li>
          <li><strong>chown:</strong> Changes file/directory ownership.</li>
        </ul>
        <pre><code># Give full permissions to owner
chmod 700 /data/app

# Allow read/write to owner and group
chmod 770 /var/logs

# Change ownership to user 'azureuser' and group 'devops'
chown azureuser:devops /data/app
        </code></pre>
        <p><strong>In Practice:</strong> I often fix permissions for web directories (nginx logs, docker volumes) to ensure least privilege and prevent permission-denied errors.</p>
      </div>`},{question:"What is middleware and how do you install it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Middleware  The Integration Layer</strong></h3>
        <p>
          <strong>Middleware</strong> is software that connects different systems or applications  enabling communication, messaging, or data management between them.
        </p>
        <ul style="margin-left:1.2rem;">
          <li>Examples: Web servers (Apache, Nginx), application servers (Tomcat, JBoss), message brokers (RabbitMQ, Kafka).</li>
          <li>Installed using package managers or container images.</li>
        </ul>
        <pre><code># Example: Install Tomcat
sudo apt install tomcat9 -y

# Example: Install Nginx (middleware for HTTP proxy)
sudo yum install nginx -y

# Container-based installation
docker run -d -p 8080:8080 tomcat:9-jdk11
        </code></pre>
        <p><strong>In Practice:</strong> I deploy middleware as containers (Tomcat, Nginx, Redis) and manage configuration through Helm charts in AKS for easier scaling and rollback.</p>
      </div>`},{question:"What is NAT?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>NAT  Network Address Translation</strong></h3>
        <p>
          <strong>NAT</strong> translates private IP addresses to public IP addresses for outbound traffic, and vice versa for inbound traffic.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Enables private resources to access the internet securely.</li>
          <li> Hides internal IPs from external networks.</li>
          <li> In Azure, handled by <strong>Azure NAT Gateway</strong> or Load Balancer SNAT.</li>
        </ul>
        <p><strong>Example:</strong> NAT Gateway provides a consistent outbound IP for a subnet  useful for whitelisting and secure API calls.</p>
        <p><strong>In Practice:</strong> I assign NAT Gateway to AKS subnets for predictable outbound traffic and compliance with firewall whitelisting policies.</p>
      </div>`},{question:"What is DNS?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>DNS  Domain Name System</strong></h3>
        <p>
          <strong>DNS</strong> translates human-readable domain names (like <code>www.example.com</code>) into machine-readable IP addresses (like <code>192.168.10.1</code>).
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Acts as the phonebook of the internet.</li>
          <li> Types of records: A (IPv4), AAAA (IPv6), CNAME (alias), MX (mail), TXT (metadata), NS (nameserver).</li>
          <li> Azure provides <strong>Azure DNS</strong> for managing custom domain zones.</li>
        </ul>
        <pre><code># Example: Query DNS record
nslookup www.microsoft.com

# Linux alternative
dig azure.com</code></pre>
        <p><strong>In Practice:</strong> I use Azure DNS for internal and external domains and integrate with Application Gateway or Front Door for routing and SSL management.</p>
      </div>`},{question:"What is Azure Front Door?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Front Door  Global Application Delivery</strong></h3>
        <p>
          <strong>Azure Front Door</strong> is a globally distributed Layer 7 (HTTP/HTTPS) service that provides <strong>application acceleration, load balancing, and security</strong> using Microsoft's global edge network.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Global load balancing using anycast and split TCP.</li>
          <li> Caches static content at edge locations for low latency.</li>
          <li> Offers built-in Web Application Firewall (WAF) and DDoS protection.</li>
          <li> Supports URL-based routing, session affinity, and health probes.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Front Door for multi-region app deployments to route users to the nearest region and enable automatic failover during regional outages.</p>
      </div>`},{question:"What is bare metal?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Bare Metal  Physical Server Infrastructure</strong></h3>
        <p>
          <strong>Bare metal</strong> refers to a <strong>physical machine without any virtualization layer</strong>. Applications run directly on the hardware's OS without a hypervisor in between.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Provides full hardware control and highest performance.</li>
          <li> Common in high-performance computing (HPC) or workloads with licensing restrictions.</li>
          <li> In cloud: offered via Azure BareMetal Instances (SAP HANA, Oracle DB, etc.).</li>
        </ul>
        <p><strong>In Practice:</strong> Ive worked with bare metal for database clusters (Oracle RAC, HANA) where performance isolation and low latency were mandatory.</p>
      </div>`},{question:"What is blue-green deployment rollback?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Blue-Green Deployment & Rollback</strong></h3>
        <p>
          <strong>Blue-Green Deployment</strong> is a release strategy where two environments (Blue - current, Green - new) run simultaneously to enable instant rollback and zero downtime deployments.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Deploy new version to Green environment while Blue serves users.</li>
          <li> Test & validate Green environment.</li>
          <li> Switch traffic (DNS or load balancer) from Blue to Green once validated.</li>
          <li> Rollback: Simply switch back to Blue environment if issues are found.</li>
        </ul>
        <p><strong>In Practice:</strong> I implement blue-green in Azure DevOps pipelines using two AKS namespaces or App Service deployment slots for quick rollback within minutes.</p>
      </div>`},{question:"What is a Helm chart?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Helm Chart  Kubernetes Packaging Unit</strong></h3>
        <p>
          A <strong>Helm chart</strong> is a collection of YAML templates that define a Kubernetes application. It simplifies deployment, upgrades, and rollback through parameterized templates.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <code>Chart.yaml</code>: Metadata (name, version).</li>
          <li> <code>values.yaml</code>: Default configuration values.</li>
          <li> <code>templates/</code>: Kubernetes manifest templates.</li>
          <li> <code>helm upgrade/rollback</code> for lifecycle management.</li>
        </ul>
        <p><strong>In Practice:</strong> I create Helm charts for microservices in AKS and manage values via environment-specific YAMLs integrated with CI/CD pipelines.</p>
      </div>`},{question:"What is Replica?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Replica  Ensuring Availability in Kubernetes</strong></h3>
        <p>
          A <strong>Replica</strong> is a copy of a Pod in Kubernetes, maintained by a <strong>ReplicaSet</strong> to ensure the desired number of instances are always running.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Provides fault tolerance and load distribution.</li>
          <li> Automatically replaces crashed or terminated Pods.</li>
          <li> Works with Horizontal Pod Autoscaler (HPA) for scaling.</li>
        </ul>
        <pre><code>kubectl scale deployment webapp --replicas=3</code></pre>
        <p><strong>In Practice:</strong> I use 23 replicas per microservice in production AKS clusters for HA and scale them dynamically via HPA metrics.</p>
      </div>`},{question:"What is VPC?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>VPC  Virtual Private Cloud</strong></h3>
        <p>
          <strong>VPC (Virtual Private Cloud)</strong> is a private, isolated virtual network in cloud platforms (AWS/GCP)  equivalent to a <strong>VNet (Virtual Network)</strong> in Azure.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Provides private IP address space for resources.</li>
          <li> Enables subnet segmentation, routing, and firewalls.</li>
          <li> Connects to on-prem using VPN or Direct Connect (ExpressRoute in Azure).</li>
        </ul>
        <p><strong>In Practice:</strong> I design VPCs/VNets per environment and connect them using VPC peering or hub-spoke topology for secure, controlled traffic flow.</p>
      </div>`},{question:"What is API and how is it used in pipeline?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>API  Application Programming Interface</strong></h3>
        <p>
          An <strong>API</strong> allows systems to communicate programmatically using HTTP/REST endpoints. In DevOps, APIs are heavily used in pipelines for automation.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Used for triggering builds, deployments, and monitoring.</li>
          <li> Integrate with third-party tools (GitHub, SonarQube, Azure DevOps, ACR) via REST APIs.</li>
          <li> Authenticated using tokens (PAT, OAuth, or service principals).</li>
        </ul>
        <pre><code># Example: Trigger ADO pipeline using REST API
curl -u :&dollar;(ADO_PAT) -X POST \\
https://dev.azure.com/org/project/_apis/pipelines/12/runs?api-version=6.0
        </code></pre>
        <p><strong>In Practice:</strong> I use APIs to trigger downstream pipelines, post deployment statuses to Teams, and automate ACR image tagging post successful build.</p>
      </div>`},{question:"What is difference between App Service and VM Service?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>App Service vs VM  Comparison</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>App Service</th><th>Virtual Machine</th></tr>
          <tr><td>Type</td><td>PaaS (Platform as a Service)</td><td>IaaS (Infrastructure as a Service)</td></tr>
          <tr><td>Management</td><td>Fully managed by Azure (no OS patching)</td><td>User manages OS, updates, and software</td></tr>
          <tr><td>Scaling</td><td>Auto-scale built-in</td><td>Manual or script-based scaling</td></tr>
          <tr><td>Use Case</td><td>Web apps, APIs, microservices</td><td>Custom apps needing OS-level access</td></tr>
          <tr><td>Cost</td><td>Pay for plan tiers</td><td>Pay per compute instance</td></tr>
        </table>
        <p><strong>In Practice:</strong> I prefer App Service for stateless apps and APIs, and VMs/VMSS for stateful or legacy workloads requiring OS customization.</p>
      </div>`},{question:"What is Azure Function?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Function  Serverless Compute</strong></h3>
        <p>
          <strong>Azure Functions</strong> is a serverless compute service that runs code in response to events or triggers without provisioning or managing infrastructure.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Supports triggers  HTTP, Timer, Blob, Queue, Event Hub, etc.</li>
          <li> Scales automatically based on event load.</li>
          <li> Integrates with Azure services like Key Vault, Storage, and Cosmos DB.</li>
          <li> Pay-per-execution model  cost-efficient for intermittent workloads.</li>
        </ul>
        <p><strong>In Practice:</strong> I use Azure Functions for automation tasks like VM auto-shutdown, ACR image cleanup, and CI/CD event-based notifications.</p>
      </div>`},{question:"What is Azure Dashboard?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Dashboard  Unified Monitoring View</strong></h3>
        <p>
          <strong>Azure Dashboard</strong> provides a customizable, unified view of metrics, logs, and resources across Azure. It helps monitor infrastructure health and application performance in real time.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Visualize metrics from Monitor, Application Insights, and Log Analytics.</li>
          <li> Supports custom tiles (VM status, alerts, cost charts).</li>
          <li> Share dashboards among teams for centralized observability.</li>
        </ul>
        <p><strong>In Practice:</strong> I create dashboards combining VM metrics, AKS node health, and cost trends for daily operational visibility.</p>
      </div>`},{question:"What is Availability Set and Zone?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Availability Set vs Availability Zone</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Availability Set</th><th>Availability Zone</th></tr>
          <tr><td>Definition</td><td>Logical grouping within a data center for fault/update domain isolation.</td><td>Physically separate data centers within a region.</td></tr>
          <tr><td>Fault Tolerance</td><td>Protects against rack/power failures.</td><td>Protects against entire datacenter failure.</td></tr>
          <tr><td>Use Case</td><td>High availability within a single datacenter.</td><td>Regional redundancy and DR.</td></tr>
          <tr><td>SLA</td><td>99.95%</td><td>99.99%</td></tr>
        </table>
        <p><strong>In Practice:</strong> I deploy critical VMs in Availability Zones for regional redundancy, while Availability Sets are used for legacy workloads within a single zone.</p>
      </div>`},{question:"What are replication types in Azure?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Replication Types in Azure Storage</strong></h3>
        <p>
          Azure provides multiple replication options to ensure durability and high availability of data, both within and across regions.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>LRS (Locally Redundant Storage):</strong> Keeps 3 copies within a single data center. Cheapest, but limited fault tolerance.</li>
          <li> <strong>GRS (Geo-Redundant Storage):</strong> Replicates data to a paired region (6 total copies). Provides regional disaster recovery.</li>
          <li> <strong>RA-GRS (Read-Access Geo-Redundant Storage):</strong> Same as GRS, but allows read access from secondary region.</li>
          <li> <strong>ZRS (Zone-Redundant Storage):</strong> Spreads data across availability zones within a region for higher resiliency.</li>
        </ul>
        <p><strong>In Practice:</strong> I use ZRS for high-availability workloads (AKS logs, Terraform state) and GRS for compliance backups or DR scenarios.</p>
      </div>`},{question:"What is Key Vault?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Key Vault  Secrets, Keys & Certificates Management</strong></h3>
        <p>
          <strong>Azure Key Vault</strong> securely stores and manages sensitive data like API keys, passwords, certificates, and connection strings.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Supports secrets, encryption keys (RSA/HSM), and SSL certificates.</li>
          <li> Integrated with Azure AD for access control (RBAC, Managed Identity).</li>
          <li> Accessible via REST APIs, CLI, Terraform, or pipelines.</li>
          <li> Automatically rotates secrets and integrates with Azure services (AKS, App Service).</li>
        </ul>
        <pre><code># Example: Retrieve secret
az keyvault secret show --vault-name myVault --name dbPassword
        </code></pre>
        <p><strong>In Practice:</strong> I store all pipeline secrets in Key Vault and reference them securely in Azure DevOps and Terraform without hardcoding credentials.</p>
      </div>`},{question:"What is Service Principal and Managed Identity?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Service Principal vs Managed Identity</strong></h3>
        <p>
          Both are used to authenticate applications or services in Azure without using user credentials.
        </p>
        <h4> <strong>Service Principal:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Represents an app/service in Azure AD.</li>
          <li>Requires manual creation and management of credentials (client ID, secret).</li>
          <li>Used by Terraform, pipelines, or APIs for automation.</li>
        </ul>
        <h4> <strong>Managed Identity (MI):</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li>Automatically managed identity for Azure resources.</li>
          <li>No secret management required  Azure handles authentication.</li>
          <li>Ideal for secure resource-to-resource access (e.g., VM  Key Vault).</li>
        </ul>
        <p><strong>In Practice:</strong> I use Service Principals for Terraform automation and Managed Identity for AKS, VMs, or App Services that need secure token-based access.</p>
      </div>`},{question:"What is Least Privilege Access?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Least Privilege Access Principle</strong></h3>
        <p>
          <strong>Least Privilege</strong> means giving users or services only the permissions necessary to perform their tasks  nothing more.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Prevents accidental or malicious misuse of resources.</li>
          <li> Enforced through RBAC roles and custom role definitions.</li>
          <li> Often combined with <strong>Privileged Identity Management (PIM)</strong> for just-in-time access.</li>
        </ul>
        <p><strong>In Practice:</strong> I assign least-privilege custom roles to CI/CD pipelines (e.g., read-only Key Vault, contributor for resource group) and monitor access through Azure AD logs.</p>
      </div>`},{question:"What is ACR (Azure Container Registry)?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Azure Container Registry  Private Docker Image Storage</strong></h3>
        <p>
          <strong>ACR</strong> is a private, fully-managed Docker registry in Azure used to store and manage container images and Helm charts.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Supports image versioning and Geo-replication.</li>
          <li> Integrates with AKS, App Service, and pipelines (ADO/GitHub).</li>
          <li> Authentication via Service Principal or Managed Identity.</li>
        </ul>
        <pre><code># Example: Push Docker image
az acr login --name myRegistry
docker tag myapp:v1 myregistry.azurecr.io/myapp:v1
docker push myregistry.azurecr.io/myapp:v1</code></pre>
        <p><strong>In Practice:</strong> I use ACR for storing application images built in pipelines, integrate it with AKS for auto-deployment via Helm or manifests.</p>
      </div>`},{question:"What is Stateful vs Stateless?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Stateful vs Stateless Applications</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li><strong>Stateful:</strong> Applications retain client data or session info (e.g., databases, Kafka, Redis). Require persistent storage.</li>
          <li><strong>Stateless:</strong> No session dependency  every request is independent (e.g., APIs, web servers). Easier to scale horizontally.</li>
        </ul>
        <p><strong>In Practice:</strong> I deploy stateless apps in AKS for horizontal scaling, and use StatefulSets for persistent apps like PostgreSQL or RabbitMQ.</p>
      </div>`},{question:"What is difference between Terraform Plan and Apply?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Terraform Plan vs Apply</strong></h3>
        <p>
          Terraform follows a two-step process for infrastructure deployment:
        </p>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Plan:</strong> Shows what changes will be made  a dry run that outputs the execution plan (create, modify, destroy).</li>
          <li> <strong>Apply:</strong> Executes the planned actions to actually modify the infrastructure.</li>
        </ul>
        <pre><code>terraform plan -out=tfplan
terraform apply tfplan</code></pre>
        <p><strong>In Practice:</strong> I use <code>terraform plan</code> in CI pipelines for approval gates and <code>apply</code> only post manual validation.</p>
      </div>`},{question:"What is the difference between public cloud and private cloud?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Public Cloud vs Private Cloud</strong></h3>
        <table style="width:100%;border-collapse:collapse;">
          <tr><th>Aspect</th><th>Public Cloud</th><th>Private Cloud</th></tr>
          <tr><td>Ownership</td><td>Managed by third-party providers (Azure, AWS, GCP)</td><td>Owned and managed by an organization</td></tr>
          <tr><td>Access</td><td>Accessible over the Internet</td><td>Accessible only within organization</td></tr>
          <tr><td>Scalability</td><td>Virtually unlimited</td><td>Limited by internal hardware</td></tr>
          <tr><td>Cost</td><td>Pay-as-you-go</td><td>High upfront infrastructure cost</td></tr>
          <tr><td>Use Case</td><td>Startups, global workloads</td><td>Regulated industries, sensitive data</td></tr>
        </table>
        <p><strong>In Practice:</strong> Ive designed hybrid setups  combining public Azure resources with on-prem private infrastructure via VPN/ExpressRoute.</p>
      </div>`},{question:"What are key metrics to monitor production health?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Key Production Health Metrics</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Compute:</strong> CPU, memory, disk IOPS, and utilization.</li>
          <li> <strong>Network:</strong> Latency, throughput, dropped packets.</li>
          <li> <strong>Application:</strong> Response time, request rate, error rate (App Insights).</li>
          <li> <strong>Container/Pod:</strong> Restart count, pending pods, node pressure.</li>
          <li> <strong>Cost Metrics:</strong> Budget consumption, spend per resource group.</li>
          <li> <strong>SLI/SLO metrics:</strong> Uptime %, response time SLA, and availability zone distribution.</li>
        </ul>
        <p><strong>In Practice:</strong> I integrate Prometheus, Grafana, and Azure Monitor dashboards with alerts via Action Groups for proactive health visibility.</p>
      </div>`},{question:"What is load testing vs stress testing?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Load Testing vs Stress Testing</strong></h3>
        <ul style="margin-left:1.2rem;">
          <li> <strong>Load Testing:</strong> Evaluates system performance under expected real-world load.</li>
          <li> <strong>Stress Testing:</strong> Pushes system beyond limits to identify breaking points and recovery capability.</li>
        </ul>
        <p><strong>Example:</strong> Load testing ensures an API handles 1000 requests/sec smoothly; stress testing finds when it fails at 5000 requests/sec.</p>
        <p><strong>In Practice:</strong> I use Azure Load Testing or JMeter in CI/CD to simulate traffic before production release and ensure SLA compliance.</p>
      </div>`},{question:"What is rollback and why is it important?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Rollback  Deployment Safety Mechanism</strong></h3>
        <p>
          <strong>Rollback</strong> is reverting an application or infrastructure to a previous stable version in case of failure or instability.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Ensures quick recovery from failed releases.</li>
          <li> Reduces downtime during production deployment issues.</li>
          <li> Implemented via blue-green, canary, or pipeline versioning.</li>
        </ul>
        <p><strong>In Practice:</strong> I configure automated rollback in pipelines using Helm <code>rollback</code> or ADO deployment slots to restore previous app versions instantly.</p>
      </div>`},{question:"What is Infrastructure Drift and how do you manage it?",answerHtml:`
      <div class="answer-rich">
        <h3> <strong>Infrastructure Drift  Configuration Deviation</strong></h3>
        <p>
          <strong>Infrastructure Drift</strong> occurs when the actual cloud infrastructure diverges from the declared Infrastructure-as-Code (IaC) configuration.
        </p>
        <ul style="margin-left:1.2rem;">
          <li> Happens due to manual changes in portal or CLI outside Terraform/ARM pipelines.</li>
          <li> Causes inconsistency, failed deployments, or misconfigurations.</li>
        </ul>
        <h4> <strong>How to Manage Drift:</strong></h4>
        <ul style="margin-left:1.2rem;">
          <li> Use <code>terraform plan</code> regularly to detect drifts.</li>
          <li> Restrict manual changes using Azure Policy or RBAC.</li>
          <li> Enforce deployment via pipelines only (GitOps model).</li>
          <li> Automate drift detection via Terraform Cloud or Azure DevOps scheduled runs.</li>
        </ul>
        <p><strong>In Practice:</strong> I run nightly Terraform plan checks to detect drift and alert via Slack when infra deviates from code.</p>
      </div>`}]}];function rb(){const[e,t]=m.useState(new Set),{viewedCount:r,bookmarkedCount:o,markAsViewed:n,toggleBookmark:s,isBookmarked:i}=qt("terraform"),a=wm.reduce((c,p)=>c+p.questions.length,0),d=c=>{t(p=>{const u=new Set(p);return u.has(c)?u.delete(c):(u.add(c),n(c)),u})};return l.jsxs("div",{className:"container max-w-4xl px-4 py-12",children:[l.jsxs("div",{className:"mb-12 flex items-center gap-4",children:[l.jsx("div",{className:"flex h-16 w-16 items-center justify-center rounded-2xl bg-gradient-to-br from-purple-500 to-pink-500 shadow-glow",children:l.jsx(Us,{className:"h-8 w-8 text-white"})}),l.jsxs("div",{children:[l.jsx("h1",{className:"text-4xl font-bold",children:"Terraform"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Infrastructure as Code Interview Questions"})]})]}),l.jsxs("div",{className:"prose prose-slate dark:prose-invert max-w-none",children:[l.jsx(Lt,{totalQuestions:a,viewedCount:r,bookmarkedCount:o}),l.jsx("p",{className:"text-lg mb-8",children:"Master Terraform with this comprehensive collection of interview questions covering basics, state management, modules, workflows, and advanced concepts."}),l.jsx(Nt,{type:"multiple",className:"space-y-4",children:wm.map((c,p)=>l.jsxs(mt,{value:`section-${p}`,className:"border rounded-lg px-6 shadow-card hover-lift",children:[l.jsx(ht,{className:"text-lg font-semibold hover:text-primary",children:c.title}),l.jsx(ft,{children:l.jsx("div",{className:"space-y-4 mt-4",children:c.questions.map((u,f)=>{const g=`${p}-${f}`,k=e.has(g),y=typeof u=="string"?u:(u==null?void 0:u.question)||"",b=typeof u=="object"&&u&&"answer"in u&&!!u.answer,v=typeof u=="object"&&u&&"answerHtml"in u&&!!u.answerHtml,h=b?u.answer:null,w=b||v;return l.jsx("div",{className:"border-l-2 border-primary/30 pl-4 py-2",children:l.jsxs("div",{className:"flex items-start gap-3",children:[l.jsx("span",{className:"text-primary font-bold mt-1",children:""}),l.jsxs("div",{className:"flex-1",children:[l.jsxs("div",{className:"flex items-start justify-between gap-2 mb-2",children:[l.jsx("p",{className:"text-foreground font-medium flex-1",children:y}),l.jsx(Pe,{variant:"ghost",size:"icon",className:"h-8 w-8 shrink-0",onClick:()=>s(g),children:l.jsx(pt,{className:`h-4 w-4 ${i(g)?"fill-secondary text-secondary":""}`})})]}),w&&l.jsxs(l.Fragment,{children:[l.jsxs("button",{onClick:()=>d(g),className:"text-sm text-primary hover:text-primary/80 font-medium flex items-center gap-1 mb-2 transition-smooth",children:[k?"Hide":"Show"," Answer",l.jsx(gt,{className:`h-4 w-4 transition-transform ${k?"rotate-180":""}`})]}),k&&l.jsx("div",{className:"mt-2 p-4 bg-muted/50 rounded-lg border border-border animate-fade-in",children:v?l.jsx("div",{className:"text-sm text-muted-foreground leading-relaxed",dangerouslySetInnerHTML:{__html:u.answerHtml}}):l.jsx("p",{className:"text-sm text-muted-foreground leading-relaxed",children:h})})]})]})]})},f)})})})]},p))})]})]})}const aD=Object.freeze(Object.defineProperty({__proto__:null,default:rb},Symbol.toStringTag,{value:"Module"})),bm=[{title:"Terraform",description:"Infrastructure as Code - Master Terraform basics, state management, modules, and best practices",icon:Us,color:"from-purple-500 to-pink-500",link:"/terraform"},{title:"Azure Core",description:"Cloud Computing - Azure fundamentals, services, networking, and security concepts",icon:ja,color:"from-blue-500 to-cyan-500",link:"/azure"},{title:"Azure DevOps",description:"CI/CD Pipelines - Learn about pipelines, repos, boards, and automation",icon:Ga,color:"from-indigo-500 to-blue-500",link:"/azure-devops"},{title:"Docker",description:"Containerization - Docker containers, images, networking, and orchestration",icon:Ua,color:"from-cyan-500 to-teal-500",link:"/docker"},{title:"Kubernetes",description:"Container Orchestration - K8s architecture, pods, services, and deployments",icon:xs,color:"from-blue-600 to-indigo-600",link:"/kubernetes"}],km={terraform:{icon:bg,color:"from-purple-500 to-pink-500"},azure:{icon:ja,color:"from-blue-500 to-cyan-500"},"azure-devops":{icon:Ga,color:"from-indigo-500 to-blue-500"},docker:{icon:Ua,color:"from-cyan-500 to-teal-500"},kubernetes:{icon:jS,color:"from-blue-600 to-indigo-600"},git:{icon:wu,color:"from-slate-600 to-slate-500"},"azure-monitor":{icon:wg,color:"from-emerald-500 to-green-400"},linux:{icon:da,color:"from-orange-500 to-yellow-400"},behavioral:{icon:od,color:"from-violet-500 to-purple-400"},miscellaneous:{icon:US,color:"from-rose-500 to-pink-400"},finops:{icon:yu,color:"from-amber-500 to-yellow-400"},jenkins:{icon:ZS,color:"from-red-500 to-orange-500"},sonarqube:{icon:Ag,color:"from-sky-500 to-cyan-500"},nexus:{icon:yg,color:"from-teal-500 to-green-400"},helm:{icon:Kl,color:"from-indigo-500 to-purple-500"},ansible:{icon:Kl,color:"from-red-600 to-rose-500"},prometheus:{icon:kg,color:"from-orange-500 to-red-500"},grafana:{icon:wg,color:"from-yellow-500 to-orange-500"},scripting:{icon:Us,color:"from-gray-500 to-slate-400"},python:{icon:BS,color:"from-yellow-400 to-orange-400"},shell:{icon:da,color:"from-gray-500 to-gray-700"},networking:{icon:QS,color:"from-blue-500 to-sky-400"},storage:{icon:GS,color:"from-amber-500 to-orange-400"},databases:{icon:yg,color:"from-green-600 to-emerald-500"},security:{icon:Ag,color:"from-rose-500 to-red-400"},cicd:{icon:vu,color:"from-cyan-500 to-indigo-500"},devops:{icon:bg,color:"from-indigo-500 to-blue-600"},automation:{icon:Kl,color:"from-teal-500 to-cyan-500"},troubleshooting:{icon:_S,color:"from-orange-400 to-red-500"},secrets:{icon:KS,color:"from-yellow-500 to-amber-400"},observability:{icon:kg,color:"from-green-500 to-emerald-400"}};function lD(){const[e,t]=m.useState(()=>bm);return m.useEffect(()=>{try{const r=Object.assign({"./Azure.tsx":$3,"./AzureDevOps.tsx":Q3,"./AzureMonitor.tsx":Y3,"./Behavioral.tsx":J3,"./Docker.tsx":Z3,"./FinOps.tsx":X3,"./Git.tsx":eD,"./Index.tsx":rD,"./Kubernetes.tsx":oD,"./Linux.tsx":nD,"./Miscellaneous.tsx":sD,"./NotFound.tsx":iD,"./Terraform.tsx":aD}),o=c=>"/"+c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").replace(/([A-Z])([A-Z][a-z])/g,"$1-$2").toLowerCase(),n=c=>c.replace(/([a-z0-9])([A-Z])/g,"$1 $2").replace(/([A-Z])([A-Z][a-z])/g,"$1 $2"),s=new Set(["index","home","notfound","not-found","404"]),i=Object.keys(r).map(c=>{const p=c.split("/").pop().replace(".tsx",""),u=p.toLowerCase(),f=r[c],g=(f==null?void 0:f.meta)??{};if(s.has(u)||g.hideFromExplore)return null;const k=o(p),y=n(p),b=g.description??`Explore questions and notes for ${y}`,v=k.replace(/^\//,""),h=km[v]??km[p.toLowerCase()]??null;return{title:y,description:b,icon:(h==null?void 0:h.icon)??xs,color:(h==null?void 0:h.color)??"from-gray-500 to-gray-600",link:k}}).filter(Boolean),a=new Set(bm.map(c=>c.link.toLowerCase())),d=i.filter(c=>!a.has(c.link.toLowerCase()));d.length>0&&t(c=>[...c,...d])}catch(r){console.error("Explore Topics discovery error:",r)}},[]),l.jsxs("div",{className:"min-h-screen",children:[l.jsxs("section",{className:"relative overflow-hidden",children:[l.jsx("div",{className:"absolute inset-0 gradient-hero opacity-10"}),l.jsx("div",{className:"container relative px-4 py-20 md:py-32",children:l.jsxs("div",{className:"mx-auto max-w-4xl text-center animate-fade-in",children:[l.jsxs("div",{className:"mb-8 inline-flex items-center gap-2 rounded-full border bg-card px-4 py-2 text-sm",children:[l.jsx(od,{className:"h-4 w-4 text-primary"}),l.jsx("span",{children:"Your Complete DevOps Interview Preparation Guide"})]}),l.jsxs("h1",{className:"mb-6 text-5xl md:text-7xl font-bold tracking-tight",children:["Master"," ",l.jsx("span",{className:"bg-gradient-to-r from-primary via-accent to-secondary bg-clip-text text-transparent",children:"DevOps"})," ","Interviews"]}),l.jsx("p",{className:"mb-8 text-xl text-muted-foreground md:text-2xl",children:"Comprehensive collection of interview questions for Terraform, Azure, Azure DevOps, Docker, and Kubernetes. Everything you need to ace your next DevOps interview."}),l.jsxs("div",{className:"flex flex-col sm:flex-row gap-4 justify-center",children:[l.jsx(ba,{to:"/terraform",children:l.jsxs(Pe,{size:"lg",className:"gap-2 shadow-glow hover-lift w-full",children:["Start Learning",l.jsx(Fl,{className:"h-5 w-5"})]})}),l.jsx("a",{href:"https://github.com/Riteshatri/",target:"_blank",rel:"noopener noreferrer",children:l.jsx(Pe,{size:"lg",variant:"outline",className:"gap-2 hover-lift w-full",children:"Ritesh Sharma's GitHub"})})]})]})})]}),l.jsxs("section",{className:"container px-4 py-20",children:[l.jsxs("div",{className:"mb-12 text-center",children:[l.jsx("h2",{className:"mb-4 text-3xl md:text-4xl font-bold",children:"Explore Topics"}),l.jsx("p",{className:"text-lg text-muted-foreground",children:"Choose a topic to start your interview preparation journey"})]}),l.jsx("div",{className:"grid gap-6 md:grid-cols-2 lg:grid-cols-3",children:e.map((r,o)=>{const n=r.icon??xs;return l.jsx(ba,{to:r.link,children:l.jsxs(Ku,{className:"group h-full transition-smooth hover-lift border-2 hover:border-primary cursor-pointer",style:{animationDelay:`${o*100}ms`},children:[l.jsxs(Ww,{children:[l.jsx("div",{className:`mb-4 inline-flex h-12 w-12 items-center justify-center rounded-lg bg-gradient-to-br ${r.color} shadow-lg`,children:l.jsx(n,{className:"h-6 w-6 text-white"})}),l.jsx(jw,{className:"group-hover:text-primary transition-smooth",children:r.title}),l.jsx(Uw,{children:r.description})]}),l.jsx($u,{children:l.jsxs("div",{className:"flex items-center gap-2 text-sm font-medium text-primary",children:["Explore questions",l.jsx(Fl,{className:"h-4 w-4 transition-transform group-hover:translate-x-1"})]})})]})},r.link??r.title+o)})})]}),l.jsx("section",{className:"border-t bg-muted/30 py-20",children:l.jsx("div",{className:"container px-4",children:l.jsxs("div",{className:"mx-auto max-w-4xl text-center",children:[l.jsx("h2",{className:"mb-4 text-3xl md:text-4xl font-bold",children:"Why This Guide?"}),l.jsxs("div",{className:"mt-12 grid gap-8 md:grid-cols-3",children:[l.jsxs("div",{className:"space-y-2",children:[l.jsx("div",{className:"mx-auto flex h-12 w-12 items-center justify-center rounded-full bg-primary/10",children:l.jsx(od,{className:"h-6 w-6 text-primary"})}),l.jsx("h3",{className:"font-semibold",children:"Comprehensive"}),l.jsx("p",{className:"text-sm text-muted-foreground",children:"Covers all major DevOps tools and technologies in depth"})]}),l.jsxs("div",{className:"space-y-2",children:[l.jsx("div",{className:"mx-auto flex h-12 w-12 items-center justify-center rounded-full bg-accent/10",children:l.jsx(Us,{className:"h-6 w-6 text-accent"})}),l.jsx("h3",{className:"font-semibold",children:"Well-Organized"}),l.jsx("p",{className:"text-sm text-muted-foreground",children:"Questions categorized by topics for easy navigation"})]}),l.jsxs("div",{className:"space-y-2",children:[l.jsx("div",{className:"mx-auto flex h-12 w-12 items-center justify-center rounded-full bg-secondary/10",children:l.jsx(Fl,{className:"h-6 w-6 text-secondary"})}),l.jsx("h3",{className:"font-semibold",children:"Interview Ready"}),l.jsx("p",{className:"text-sm text-muted-foreground",children:"Real-world questions asked in actual DevOps interviews"})]})]})]})})})]})}const cD=new Tx,dD=()=>l.jsx(Rx,{client:cD,children:l.jsx(N1,{defaultTheme:"dark",storageKey:"devops-guide-theme",children:l.jsxs(cy,{children:[l.jsx(qI,{}),l.jsx(mP,{}),l.jsx(x1,{children:l.jsx(C3,{children:l.jsxs(w1,{children:[l.jsx(Ee,{path:"/",element:l.jsx(lD,{})}),l.jsx(Ee,{path:"/terraform",element:l.jsx(rb,{})}),l.jsx(Ee,{path:"/azure",element:l.jsx(Gw,{})}),l.jsx(Ee,{path:"/azure-dev-ops",element:l.jsx($g,{to:"/azure-devops",replace:!0})}),l.jsx(Ee,{path:"/azure-devops",element:l.jsx(Fw,{})}),l.jsx(Ee,{path:"/docker",element:l.jsx(Qw,{})}),l.jsx(Ee,{path:"/kubernetes",element:l.jsx(Zw,{})}),l.jsx(Ee,{path:"/git",element:l.jsx(Jw,{})}),l.jsx(Ee,{path:"/azure-monitor",element:l.jsx(Kw,{})}),l.jsx(Ee,{path:"/linux",element:l.jsx(Xw,{})}),l.jsx(Ee,{path:"/behavioral",element:l.jsx($w,{})}),l.jsx(Ee,{path:"/miscellaneous",element:l.jsx(eb,{})}),l.jsx(Ee,{path:"/fin-ops",element:l.jsx($g,{to:"/finops",replace:!0})}),l.jsx(Ee,{path:"/finops",element:l.jsx(Yw,{})}),l.jsx(Ee,{path:"*",element:l.jsx(tb,{})})]})})})]})})});Gf(document.getElementById("root")).render(l.jsx(dD,{}));
